{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.2.2', '1.10.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "(keras.__version__, tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-31 05:02:00</td>\n",
       "      <td>948.000</td>\n",
       "      <td>948.000</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.083403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-31 05:03:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-31 05:04:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-31 05:05:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-31 05:06:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     open     high      low    close    volume\n",
       "0  2016-12-31 05:02:00  948.000  948.000  942.899  942.899  0.083403\n",
       "1  2016-12-31 05:03:00  942.899  942.899  942.899  942.899  0.000000\n",
       "2  2016-12-31 05:04:00  942.899  942.899  942.899  942.899  0.000000\n",
       "3  2016-12-31 05:05:00  942.899  942.899  942.899  942.899  0.000000\n",
       "4  2016-12-31 05:06:00  942.899  942.899  942.899  942.899  0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exch = 'BTRX'\n",
    "pair = 'BTC/USDT'\n",
    "\n",
    "df = pd.read_csv(f\"{exch}_{pair.replace('/', '-')}_ohlcv.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp     object\n",
       "open         float64\n",
       "high         float64\n",
       "low          float64\n",
       "close        float64\n",
       "volume       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the stock prices for the last day\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_finance import candlestick_ohlc\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plot_last_n_minutes = 60 * 12  # 1/2 day\n",
    "cs_frame = df.iloc[-1 * plot_last_n_minutes:].copy()  # Create the candlestick frame\n",
    "\n",
    "#if necessary convert to datetime\n",
    "cs_frame.timestamp = pd.to_datetime(cs_frame.timestamp)\n",
    "\n",
    "cs_frame = cs_frame[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "cs_frame[\"timestamp\"] = cs_frame[\"timestamp\"].apply(mdates.date2num)\n",
    "\n",
    "f1 = plt.subplot2grid((6, 1), (0, 0), rowspan=6, colspan=1, facecolor='#07000d')\n",
    "candlestick_ohlc(f1, cs_frame.values, width=.0001, colorup='#53c156', colordown='#ff1717', alpha=.75)\n",
    "f1.xaxis_date()\n",
    "f1.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d %H:%M'))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Date Hours:Minutes')\n",
    "plt.show()\n",
    "\n",
    "# Cleanup memory\n",
    "%reset_selective -f \"^cs_frame$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add a minute moving average over period\n",
    "def add_moving_avg(df, period=30):\n",
    "    #df[f\"{period}_ma\"] = pd.rolling_mean(df['close'], period)\n",
    "    df[f\"{period}_ma\"] = df.close.rolling(period).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>30_ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>532493</th>\n",
       "      <td>2018-01-04 23:55:00</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15200.00000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>10.446506</td>\n",
       "      <td>15063.411186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532494</th>\n",
       "      <td>2018-01-04 23:56:00</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15220.00000</td>\n",
       "      <td>15195.582639</td>\n",
       "      <td>15220.000000</td>\n",
       "      <td>5.457758</td>\n",
       "      <td>15070.777853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532495</th>\n",
       "      <td>2018-01-04 23:57:00</td>\n",
       "      <td>15220.000000</td>\n",
       "      <td>15238.00000</td>\n",
       "      <td>15200.000000</td>\n",
       "      <td>15201.000000</td>\n",
       "      <td>7.473745</td>\n",
       "      <td>15077.644520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532496</th>\n",
       "      <td>2018-01-04 23:58:00</td>\n",
       "      <td>15200.000000</td>\n",
       "      <td>15202.21529</td>\n",
       "      <td>15085.001000</td>\n",
       "      <td>15101.591266</td>\n",
       "      <td>7.258691</td>\n",
       "      <td>15081.030929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532497</th>\n",
       "      <td>2018-01-04 23:59:00</td>\n",
       "      <td>15101.591266</td>\n",
       "      <td>15199.00000</td>\n",
       "      <td>15085.001000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>1.777752</td>\n",
       "      <td>15087.697595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp          open         high           low  \\\n",
       "532493  2018-01-04 23:55:00  15199.000000  15200.00000  15199.000000   \n",
       "532494  2018-01-04 23:56:00  15199.000000  15220.00000  15195.582639   \n",
       "532495  2018-01-04 23:57:00  15220.000000  15238.00000  15200.000000   \n",
       "532496  2018-01-04 23:58:00  15200.000000  15202.21529  15085.001000   \n",
       "532497  2018-01-04 23:59:00  15101.591266  15199.00000  15085.001000   \n",
       "\n",
       "               close     volume         30_ma  \n",
       "532493  15199.000000  10.446506  15063.411186  \n",
       "532494  15220.000000   5.457758  15070.777853  \n",
       "532495  15201.000000   7.473745  15077.644520  \n",
       "532496  15101.591266   7.258691  15081.030929  \n",
       "532497  15199.000000   1.777752  15087.697595  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_moving_avg(df)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stock_gym.envs import stocks\n",
    "import gym\n",
    "\n",
    "env = gym.make('ContSinMarketEnv-v0')\n",
    "#env.add_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, GRU, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, (1, 64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "(env.n_actions, env.observation_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 16)                3888      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 3,939\n",
      "Trainable params: 3,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(16,\n",
    "                #input_shape=env.observation_space.shape,\n",
    "                input_shape=env.observation_shape(),\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.5,\n",
    "                #return_sequences=True,\n",
    "               #stateful=False,\n",
    "             ))\n",
    "# model.add(GRU(128,\n",
    "#                 dropout=0.1,\n",
    "#                 recurrent_dropout=0.5,\n",
    "#                 return_sequences=True,\n",
    "#              ))\n",
    "# model.add(GRU(128,\n",
    "#                 dropout=0.1,\n",
    "#                 recurrent_dropout=0.5,\n",
    "#              ))\n",
    "#model.add(Dense(64))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(env.n_actions, kernel_initializer='lecun_uniform', activation='linear'))\n",
    "#model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "#model.add(Dense(env.n_actions, activation='linear'))\n",
    "\n",
    "# model.add(Flatten(input_shape=env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(env.n_actions, activation='linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=env.total_space_size, window_length=env.n_features)\n",
    "policy = BoltzmannQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=env.n_actions, memory=memory, nb_steps_warmup=100,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.compile(\n",
    "    #loss='mse',\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    metrics=['mae'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "    2/5000: episode: 1, duration: 0.221s, episode steps: 2, steps per second: 9, episode reward: -1000.003, mean reward: -500.001 [-1000.002, -0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.728 [0.151, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3/5000: episode: 2, duration: 0.004s, episode steps: 1, steps per second: 251, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.296, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4/5000: episode: 3, duration: 0.002s, episode steps: 1, steps per second: 432, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.392 [0.000, 0.966], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5/5000: episode: 4, duration: 0.003s, episode steps: 1, steps per second: 362, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.523], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6/5000: episode: 5, duration: 0.003s, episode steps: 1, steps per second: 344, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    7/5000: episode: 6, duration: 0.002s, episode steps: 1, steps per second: 440, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.077, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    8/5000: episode: 7, duration: 0.003s, episode steps: 1, steps per second: 396, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.648], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    9/5000: episode: 8, duration: 0.002s, episode steps: 1, steps per second: 492, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.806 [0.376, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   10/5000: episode: 9, duration: 0.002s, episode steps: 1, steps per second: 497, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.658 [0.072, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   11/5000: episode: 10, duration: 0.003s, episode steps: 1, steps per second: 373, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.293, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   12/5000: episode: 11, duration: 0.002s, episode steps: 1, steps per second: 471, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.818 [0.504, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   13/5000: episode: 12, duration: 0.002s, episode steps: 1, steps per second: 516, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.373, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   14/5000: episode: 13, duration: 0.002s, episode steps: 1, steps per second: 489, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.786 [0.291, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   15/5000: episode: 14, duration: 0.002s, episode steps: 1, steps per second: 453, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.548], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   16/5000: episode: 15, duration: 0.002s, episode steps: 1, steps per second: 511, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.764 [0.231, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   17/5000: episode: 16, duration: 0.002s, episode steps: 1, steps per second: 497, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.762 [0.227, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   18/5000: episode: 17, duration: 0.002s, episode steps: 1, steps per second: 480, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.215 [0.000, 0.711], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   19/5000: episode: 18, duration: 0.002s, episode steps: 1, steps per second: 512, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.748 [0.196, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   20/5000: episode: 19, duration: 0.002s, episode steps: 1, steps per second: 493, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.760 [0.221, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   21/5000: episode: 20, duration: 0.002s, episode steps: 1, steps per second: 508, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.496 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   22/5000: episode: 21, duration: 0.002s, episode steps: 1, steps per second: 508, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.744 [0.188, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   23/5000: episode: 22, duration: 0.002s, episode steps: 1, steps per second: 503, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   24/5000: episode: 23, duration: 0.002s, episode steps: 1, steps per second: 550, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.818 [0.504, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   25/5000: episode: 24, duration: 0.002s, episode steps: 1, steps per second: 499, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.628 [0.047, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   26/5000: episode: 25, duration: 0.002s, episode steps: 1, steps per second: 412, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.434 [0.000, 0.986], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   27/5000: episode: 26, duration: 0.002s, episode steps: 1, steps per second: 466, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.468 [0.000, 0.996], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   28/5000: episode: 27, duration: 0.002s, episode steps: 1, steps per second: 453, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.198, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   29/5000: episode: 28, duration: 0.002s, episode steps: 1, steps per second: 429, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.166, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   30/5000: episode: 29, duration: 0.002s, episode steps: 1, steps per second: 442, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.139, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   31/5000: episode: 30, duration: 0.002s, episode steps: 1, steps per second: 431, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.017, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   32/5000: episode: 31, duration: 0.002s, episode steps: 1, steps per second: 441, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.359 [0.000, 0.942], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   33/5000: episode: 32, duration: 0.002s, episode steps: 1, steps per second: 402, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.194 [0.000, 0.625], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   34/5000: episode: 33, duration: 0.002s, episode steps: 1, steps per second: 438, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.225 [0.000, 0.742], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   35/5000: episode: 34, duration: 0.002s, episode steps: 1, steps per second: 406, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.296 [0.000, 0.876], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   36/5000: episode: 35, duration: 0.002s, episode steps: 1, steps per second: 414, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.201, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   37/5000: episode: 36, duration: 0.002s, episode steps: 1, steps per second: 445, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.431 [0.000, 0.985], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   38/5000: episode: 37, duration: 0.003s, episode steps: 1, steps per second: 376, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.735 [0.172, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   39/5000: episode: 38, duration: 0.003s, episode steps: 1, steps per second: 371, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.278 [0.000, 0.850], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   40/5000: episode: 39, duration: 0.003s, episode steps: 1, steps per second: 330, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.715 [0.140, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   41/5000: episode: 40, duration: 0.002s, episode steps: 1, steps per second: 401, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.523], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   42/5000: episode: 41, duration: 0.002s, episode steps: 1, steps per second: 433, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.248 [0.000, 0.795], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   43/5000: episode: 42, duration: 0.002s, episode steps: 1, steps per second: 523, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.372 [0.000, 0.953], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   44/5000: episode: 43, duration: 0.002s, episode steps: 1, steps per second: 513, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.510], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   45/5000: episode: 44, duration: 0.002s, episode steps: 1, steps per second: 480, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.810 [0.400, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   46/5000: episode: 45, duration: 0.002s, episode steps: 1, steps per second: 439, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.705], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   47/5000: episode: 46, duration: 0.002s, episode steps: 1, steps per second: 474, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.214, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   48/5000: episode: 47, duration: 0.002s, episode steps: 1, steps per second: 440, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.551 [0.008, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   49/5000: episode: 48, duration: 0.002s, episode steps: 1, steps per second: 452, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.452 [0.000, 0.992], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   50/5000: episode: 49, duration: 0.002s, episode steps: 1, steps per second: 402, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.196 [0.000, 0.639], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   51/5000: episode: 50, duration: 0.002s, episode steps: 1, steps per second: 482, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.567 [0.014, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   52/5000: episode: 51, duration: 0.002s, episode steps: 1, steps per second: 493, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   53/5000: episode: 52, duration: 0.002s, episode steps: 1, steps per second: 469, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.182 [0.000, 0.508], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   54/5000: episode: 53, duration: 0.002s, episode steps: 1, steps per second: 477, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.807 [0.382, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   55/5000: episode: 54, duration: 0.002s, episode steps: 1, steps per second: 503, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.474 [0.000, 0.997], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   56/5000: episode: 55, duration: 0.002s, episode steps: 1, steps per second: 458, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.191 [0.000, 0.608], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   57/5000: episode: 56, duration: 0.002s, episode steps: 1, steps per second: 504, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.463 [0.000, 0.995], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   58/5000: episode: 57, duration: 0.002s, episode steps: 1, steps per second: 466, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.258 [0.000, 0.815], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   59/5000: episode: 58, duration: 0.002s, episode steps: 1, steps per second: 463, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.320, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   60/5000: episode: 59, duration: 0.002s, episode steps: 1, steps per second: 469, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.182 [0.000, 0.517], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   61/5000: episode: 60, duration: 0.002s, episode steps: 1, steps per second: 506, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.227 [0.000, 0.746], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   62/5000: episode: 61, duration: 0.002s, episode steps: 1, steps per second: 486, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.000, 0.963], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   63/5000: episode: 62, duration: 0.002s, episode steps: 1, steps per second: 451, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.372 [0.000, 0.953], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   64/5000: episode: 63, duration: 0.002s, episode steps: 1, steps per second: 475, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.360 [0.000, 0.944], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   65/5000: episode: 64, duration: 0.002s, episode steps: 1, steps per second: 459, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.465 [0.000, 0.996], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   66/5000: episode: 65, duration: 0.002s, episode steps: 1, steps per second: 503, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.818 [0.491, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   67/5000: episode: 66, duration: 0.002s, episode steps: 1, steps per second: 461, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.238 [0.000, 0.774], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   68/5000: episode: 67, duration: 0.002s, episode steps: 1, steps per second: 454, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.813 [0.423, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   69/5000: episode: 68, duration: 0.002s, episode steps: 1, steps per second: 484, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.182 [0.000, 0.520], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   70/5000: episode: 69, duration: 0.002s, episode steps: 1, steps per second: 479, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.747 [0.193, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   71/5000: episode: 70, duration: 0.002s, episode steps: 1, steps per second: 482, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.689 [0.105, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   72/5000: episode: 71, duration: 0.002s, episode steps: 1, steps per second: 475, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.801 [0.349, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   73/5000: episode: 72, duration: 0.002s, episode steps: 1, steps per second: 492, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.815 [0.435, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   74/5000: episode: 73, duration: 0.002s, episode steps: 1, steps per second: 458, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.745 [0.190, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   75/5000: episode: 74, duration: 0.002s, episode steps: 1, steps per second: 418, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.605 [0.032, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   76/5000: episode: 75, duration: 0.002s, episode steps: 1, steps per second: 499, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.802 [0.356, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   77/5000: episode: 76, duration: 0.002s, episode steps: 1, steps per second: 539, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.260 [0.000, 0.819], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   78/5000: episode: 77, duration: 0.002s, episode steps: 1, steps per second: 505, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.799 [0.342, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   79/5000: episode: 78, duration: 0.002s, episode steps: 1, steps per second: 499, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.684 [0.099, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   80/5000: episode: 79, duration: 0.002s, episode steps: 1, steps per second: 446, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.182 [0.000, 0.491], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   81/5000: episode: 80, duration: 0.002s, episode steps: 1, steps per second: 490, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.671 [0.085, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   82/5000: episode: 81, duration: 0.003s, episode steps: 1, steps per second: 366, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.290 [0.000, 0.867], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   83/5000: episode: 82, duration: 0.003s, episode steps: 1, steps per second: 358, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.808 [0.388, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   84/5000: episode: 83, duration: 0.002s, episode steps: 1, steps per second: 421, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.529 [0.003, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   85/5000: episode: 84, duration: 0.003s, episode steps: 1, steps per second: 388, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.292 [0.000, 0.870], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   86/5000: episode: 85, duration: 0.002s, episode steps: 1, steps per second: 484, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.301 [0.000, 0.882], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   87/5000: episode: 86, duration: 0.002s, episode steps: 1, steps per second: 480, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.688 [0.104, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   88/5000: episode: 87, duration: 0.002s, episode steps: 1, steps per second: 404, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   89/5000: episode: 88, duration: 0.002s, episode steps: 1, steps per second: 406, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.728 [0.159, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   90/5000: episode: 89, duration: 0.003s, episode steps: 1, steps per second: 379, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.196 [0.000, 0.636], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   91/5000: episode: 90, duration: 0.003s, episode steps: 1, steps per second: 346, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.746 [0.193, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   92/5000: episode: 91, duration: 0.004s, episode steps: 1, steps per second: 282, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.388 [0.000, 0.964], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   93/5000: episode: 92, duration: 0.003s, episode steps: 1, steps per second: 296, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.613 [0.037, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   94/5000: episode: 93, duration: 0.003s, episode steps: 1, steps per second: 362, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.817 [0.472, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   95/5000: episode: 94, duration: 0.003s, episode steps: 1, steps per second: 387, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.182 [0.000, 0.508], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   96/5000: episode: 95, duration: 0.003s, episode steps: 1, steps per second: 372, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.773 [0.254, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   97/5000: episode: 96, duration: 0.003s, episode steps: 1, steps per second: 362, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.182 [0.000, 0.516], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   98/5000: episode: 97, duration: 0.003s, episode steps: 1, steps per second: 305, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.759 [0.219, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   99/5000: episode: 98, duration: 0.003s, episode steps: 1, steps per second: 395, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.721], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  100/5000: episode: 99, duration: 0.002s, episode steps: 1, steps per second: 469, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.602 [0.030, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  101/5000: episode: 100, duration: 1.291s, episode steps: 1, steps per second: 1, episode reward: -1000.002, mean reward: -1000.002 [-1000.002, -1000.002], mean action: 2.000 [2.000, 2.000], mean observation: 0.303 [0.000, 0.885], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  102/5000: episode: 101, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.187, 1.000], loss: 375153.093750, mean_absolute_error: 250.293884, mean_q: 0.236857\n",
      "  103/5000: episode: 102, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.815 [0.439, 1.000], loss: 359340.031250, mean_absolute_error: 239.851685, mean_q: 0.142923\n",
      "  104/5000: episode: 103, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.601 [0.030, 1.000], loss: 390565.750000, mean_absolute_error: 260.703735, mean_q: 0.012901\n",
      "  105/5000: episode: 104, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.790 [0.305, 1.000], loss: 312361.406250, mean_absolute_error: 208.692398, mean_q: -0.123250\n",
      "  106/5000: episode: 105, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.554 [0.010, 1.000], loss: 343411.937500, mean_absolute_error: 229.478516, mean_q: -0.351678\n",
      "  107/5000: episode: 106, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.185, 1.000], loss: 405725.343750, mean_absolute_error: 271.158691, mean_q: -0.490268\n",
      "  108/5000: episode: 107, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.406 [0.000, 0.974], loss: 389999.718750, mean_absolute_error: 260.751892, mean_q: -0.591619\n",
      "  109/5000: episode: 108, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.818 [0.498, 1.000], loss: 389777.750000, mean_absolute_error: 260.797913, mean_q: -0.760432\n",
      "  110/5000: episode: 109, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 374194.031250, mean_absolute_error: 250.433228, mean_q: -0.733604\n",
      "  111/5000: episode: 110, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.000, 0.973], loss: 420828.875000, mean_absolute_error: 281.699799, mean_q: -0.858500\n",
      "  112/5000: episode: 111, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.000, 0.958], loss: 342911.625000, mean_absolute_error: 229.723358, mean_q: -0.786907\n",
      "  113/5000: episode: 112, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 389472.125000, mean_absolute_error: 260.857635, mean_q: -0.818749\n",
      "  114/5000: episode: 113, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.568 [0.014, 1.000], loss: 327017.843750, mean_absolute_error: 219.462982, mean_q: -0.956398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  115/5000: episode: 114, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.622 [0.043, 1.000], loss: 280360.968750, mean_absolute_error: 188.293259, mean_q: -0.859543\n",
      "  116/5000: episode: 115, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.047, 1.000], loss: 358000.375000, mean_absolute_error: 240.242218, mean_q: -0.984811\n",
      "  117/5000: episode: 116, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.280 [0.000, 0.852], loss: 357930.843750, mean_absolute_error: 240.283051, mean_q: -1.010729\n",
      "  118/5000: episode: 117, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.000, 0.993], loss: 342408.062500, mean_absolute_error: 229.876709, mean_q: -0.943733\n",
      "  119/5000: episode: 118, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.585], loss: 373146.593750, mean_absolute_error: 250.687973, mean_q: -1.113736\n",
      "  120/5000: episode: 119, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.766], loss: 357615.062500, mean_absolute_error: 240.439346, mean_q: -1.147563\n",
      "  121/5000: episode: 120, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.538], loss: 310885.125000, mean_absolute_error: 209.255829, mean_q: -1.102639\n",
      "  122/5000: episode: 121, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.005, 1.000], loss: 388527.718750, mean_absolute_error: 261.276855, mean_q: -1.241791\n",
      "  123/5000: episode: 122, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.000, 0.850], loss: 403727.375000, mean_absolute_error: 271.622894, mean_q: -1.339381\n",
      "  124/5000: episode: 123, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.721], loss: 357082.500000, mean_absolute_error: 240.592102, mean_q: -1.329494\n",
      "  125/5000: episode: 124, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.220, 1.000], loss: 294953.437500, mean_absolute_error: 199.143707, mean_q: -1.285529\n",
      "  126/5000: episode: 125, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.466, 1.000], loss: 325662.687500, mean_absolute_error: 219.967606, mean_q: -1.455425\n",
      "  127/5000: episode: 126, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.774], loss: 325739.687500, mean_absolute_error: 219.983856, mean_q: -1.414369\n",
      "  128/5000: episode: 127, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 279004.656250, mean_absolute_error: 188.991058, mean_q: -1.480437\n",
      "  129/5000: episode: 128, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 372229.906250, mean_absolute_error: 251.112732, mean_q: -1.408299\n",
      "  130/5000: episode: 129, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.665], loss: 325328.812500, mean_absolute_error: 220.196777, mean_q: -1.587353\n",
      "  131/5000: episode: 130, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.497], loss: 201553.421875, mean_absolute_error: 137.622482, mean_q: -1.478525\n",
      "  132/5000: episode: 131, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -1000.000, mean reward: -1000.000 [-1000.000, -1000.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.336 [0.000, 0.922], loss: 325355.500000, mean_absolute_error: 220.313889, mean_q: -1.546621\n",
      "  133/5000: episode: 132, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 309618.812500, mean_absolute_error: 209.803787, mean_q: -1.519275\n",
      "  134/5000: episode: 133, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.227 [0.000, 0.747], loss: 216856.437500, mean_absolute_error: 147.822906, mean_q: -1.410277\n",
      "  135/5000: episode: 134, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 262965.843750, mean_absolute_error: 178.852676, mean_q: -1.490577\n",
      "  136/5000: episode: 135, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 1.000 [1.000, 1.000], mean observation: 0.185 [0.000, 0.555], loss: 325052.187500, mean_absolute_error: 220.273285, mean_q: -1.494605\n",
      "  137/5000: episode: 136, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.365, 1.000], loss: 340426.281250, mean_absolute_error: 230.471222, mean_q: -1.430078\n",
      "  138/5000: episode: 137, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.440, 1.000], loss: 278295.656250, mean_absolute_error: 189.516113, mean_q: -1.611172\n",
      "  139/5000: episode: 138, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.270, 1.000], loss: 340029.437500, mean_absolute_error: 230.801926, mean_q: -1.634738\n",
      "  140/5000: episode: 139, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.608 [0.034, 1.000], loss: 324632.375000, mean_absolute_error: 220.577545, mean_q: -1.597429\n",
      "  141/5000: episode: 140, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.289, 1.000], loss: 278190.843750, mean_absolute_error: 189.468872, mean_q: -1.492367\n",
      "  142/5000: episode: 141, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.427 [0.000, 0.984], loss: 293901.625000, mean_absolute_error: 199.985764, mean_q: -1.491832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  143/5000: episode: 142, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.259, 1.000], loss: 324030.781250, mean_absolute_error: 220.573090, mean_q: -1.670188\n",
      "  144/5000: episode: 143, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.196, 1.000], loss: 339729.562500, mean_absolute_error: 230.808472, mean_q: -1.528084\n",
      "  145/5000: episode: 144, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -1000.001, mean reward: -1000.001 [-1000.001, -1000.001], mean action: 2.000 [2.000, 2.000], mean observation: 0.184 [0.000, 0.552], loss: 262498.625000, mean_absolute_error: 179.601578, mean_q: -1.596015\n",
      "  146/5000: episode: 145, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.787], loss: 324001.500000, mean_absolute_error: 220.680267, mean_q: -1.594498\n",
      "  147/5000: episode: 146, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.631], loss: 416978.750000, mean_absolute_error: 282.507843, mean_q: -1.502860\n",
      "  148/5000: episode: 147, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.564], loss: 247047.093750, mean_absolute_error: 169.275772, mean_q: -1.474182\n",
      "  149/5000: episode: 148, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.630 [0.049, 1.000], loss: 370213.406250, mean_absolute_error: 251.602051, mean_q: -1.534114\n",
      "  150/5000: episode: 149, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.019, 1.000], loss: 185127.812500, mean_absolute_error: 128.366745, mean_q: -1.548309\n",
      "  151/5000: episode: 150, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.600], loss: 231233.531250, mean_absolute_error: 158.821625, mean_q: -1.425185\n",
      "  152/5000: episode: 151, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.049, 1.000], loss: 308225.437500, mean_absolute_error: 210.492584, mean_q: -1.511954\n",
      "  153/5000: episode: 152, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.672], loss: 293050.218750, mean_absolute_error: 200.395935, mean_q: -1.471247\n",
      "  154/5000: episode: 153, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 246591.390625, mean_absolute_error: 169.564880, mean_q: -1.472057\n",
      "  155/5000: episode: 154, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.658], loss: 277282.750000, mean_absolute_error: 190.054810, mean_q: -1.472131\n",
      "  156/5000: episode: 155, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.802], loss: 277237.750000, mean_absolute_error: 190.103119, mean_q: -1.461352\n",
      "  157/5000: episode: 156, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.066, 1.000], loss: 369409.031250, mean_absolute_error: 251.705719, mean_q: -1.476547\n",
      "  158/5000: episode: 157, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.594], loss: 307696.937500, mean_absolute_error: 210.472900, mean_q: -1.431438\n",
      "  159/5000: episode: 158, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.037, 1.000], loss: 323311.187500, mean_absolute_error: 220.879257, mean_q: -1.393403\n",
      "  160/5000: episode: 159, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.466 [0.000, 0.996], loss: 246306.734375, mean_absolute_error: 169.606537, mean_q: -1.408813\n",
      "  161/5000: episode: 160, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.069, 1.000], loss: 307776.218750, mean_absolute_error: 210.722137, mean_q: -1.385077\n",
      "  162/5000: episode: 161, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.007, 1.000], loss: 153954.671875, mean_absolute_error: 108.196014, mean_q: -1.368306\n",
      "  163/5000: episode: 162, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 200010.140625, mean_absolute_error: 138.908585, mean_q: -1.355615\n",
      "  164/5000: episode: 163, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.000, 0.959], loss: 245952.218750, mean_absolute_error: 169.699371, mean_q: -1.366815\n",
      "  165/5000: episode: 164, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.221, 1.000], loss: 276441.500000, mean_absolute_error: 190.330688, mean_q: -1.408151\n",
      "  166/5000: episode: 165, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.000, 0.997], loss: 215311.453125, mean_absolute_error: 149.175034, mean_q: -1.299020\n",
      "  167/5000: episode: 166, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.761], loss: 246314.656250, mean_absolute_error: 169.762817, mean_q: -1.270971\n",
      "  168/5000: episode: 167, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.189, 1.000], loss: 169056.031250, mean_absolute_error: 118.315659, mean_q: -1.255400\n",
      "  169/5000: episode: 168, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.539], loss: 291691.187500, mean_absolute_error: 200.653473, mean_q: -1.359922\n",
      "  170/5000: episode: 169, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.686], loss: 184477.546875, mean_absolute_error: 128.683151, mean_q: -1.233889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  171/5000: episode: 170, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.056, 1.000], loss: 291887.062500, mean_absolute_error: 200.623703, mean_q: -1.289813\n",
      "  172/5000: episode: 171, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.535 [0.005, 1.000], loss: 230656.171875, mean_absolute_error: 159.553955, mean_q: -1.211856\n",
      "  173/5000: episode: 172, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.034, 1.000], loss: 307063.375000, mean_absolute_error: 210.810852, mean_q: -1.263484\n",
      "  174/5000: episode: 173, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.391, 1.000], loss: 276242.250000, mean_absolute_error: 190.510483, mean_q: -1.305567\n",
      "  175/5000: episode: 174, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.000, 0.823], loss: 276250.343750, mean_absolute_error: 190.477448, mean_q: -1.278720\n",
      "  176/5000: episode: 175, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.492], loss: 199743.156250, mean_absolute_error: 139.212631, mean_q: -1.212623\n",
      "  177/5000: episode: 176, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.446, 1.000], loss: 260772.218750, mean_absolute_error: 180.269440, mean_q: -1.260923\n",
      "  178/5000: episode: 177, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.784], loss: 307009.312500, mean_absolute_error: 210.729950, mean_q: -1.189458\n",
      "  179/5000: episode: 178, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.000, 0.816], loss: 276144.187500, mean_absolute_error: 190.661682, mean_q: -1.267288\n",
      "  180/5000: episode: 179, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.679], loss: 337783.375000, mean_absolute_error: 231.559631, mean_q: -1.213211\n",
      "  181/5000: episode: 180, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.880], loss: 214703.859375, mean_absolute_error: 149.708832, mean_q: -1.227575\n",
      "  182/5000: episode: 181, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.197, 1.000], loss: 153281.843750, mean_absolute_error: 108.815659, mean_q: -1.228643\n",
      "  183/5000: episode: 182, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.787], loss: 306800.625000, mean_absolute_error: 210.973801, mean_q: -1.181353\n",
      "  184/5000: episode: 183, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.177, 1.000], loss: 260594.125000, mean_absolute_error: 180.411713, mean_q: -1.214482\n",
      "  185/5000: episode: 184, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.269, 1.000], loss: 337061.187500, mean_absolute_error: 231.466675, mean_q: -1.212708\n",
      "  186/5000: episode: 185, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.468 [0.000, 0.996], loss: 214601.375000, mean_absolute_error: 149.866150, mean_q: -1.214307\n",
      "  187/5000: episode: 186, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.000, 0.900], loss: 245232.031250, mean_absolute_error: 170.330353, mean_q: -1.194986\n",
      "  188/5000: episode: 187, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.618], loss: 260466.656250, mean_absolute_error: 180.452271, mean_q: -1.185586\n",
      "  189/5000: episode: 188, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.000, 0.695], loss: 290834.281250, mean_absolute_error: 201.048065, mean_q: -1.241414\n",
      "  190/5000: episode: 189, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 214457.609375, mean_absolute_error: 149.865860, mean_q: -1.168978\n",
      "  191/5000: episode: 190, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.574], loss: 306111.406250, mean_absolute_error: 211.202850, mean_q: -1.196571\n",
      "  192/5000: episode: 191, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.943], loss: 168340.046875, mean_absolute_error: 119.158081, mean_q: -1.144538\n",
      "  193/5000: episode: 192, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.579], loss: 214377.281250, mean_absolute_error: 150.028717, mean_q: -1.164499\n",
      "  194/5000: episode: 193, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.015, 1.000], loss: 153306.687500, mean_absolute_error: 108.948921, mean_q: -1.086037\n",
      "  195/5000: episode: 194, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.000, 0.764], loss: 122504.250000, mean_absolute_error: 88.662613, mean_q: -1.125332\n",
      "  196/5000: episode: 195, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.000, 0.964], loss: 214313.437500, mean_absolute_error: 149.715454, mean_q: -1.081127\n",
      "  197/5000: episode: 196, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 214281.718750, mean_absolute_error: 149.976318, mean_q: -1.109491\n",
      "  198/5000: episode: 197, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.622 [0.043, 1.000], loss: 244724.156250, mean_absolute_error: 170.363312, mean_q: -1.124347\n",
      "  199/5000: episode: 198, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.712], loss: 229633.500000, mean_absolute_error: 160.099869, mean_q: -1.071547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  200/5000: episode: 199, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 198765.000000, mean_absolute_error: 140.000488, mean_q: -1.137427\n",
      "  201/5000: episode: 200, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.395, 1.000], loss: 214082.015625, mean_absolute_error: 150.027756, mean_q: -1.092204\n",
      "  202/5000: episode: 201, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.240, 1.000], loss: 259973.593750, mean_absolute_error: 180.744873, mean_q: -1.104923\n",
      "  203/5000: episode: 202, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.409, 1.000], loss: 198723.609375, mean_absolute_error: 139.726135, mean_q: -1.066268\n",
      "  204/5000: episode: 203, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.000, 0.876], loss: 168165.109375, mean_absolute_error: 119.744720, mean_q: -1.112566\n",
      "  205/5000: episode: 204, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 168137.875000, mean_absolute_error: 119.273422, mean_q: -1.024344\n",
      "  206/5000: episode: 205, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.328, 1.000], loss: 198590.671875, mean_absolute_error: 140.055222, mean_q: -1.089218\n",
      "  207/5000: episode: 206, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.000, 0.967], loss: 213995.281250, mean_absolute_error: 150.300507, mean_q: -1.074713\n",
      "  208/5000: episode: 207, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.239, 1.000], loss: 213984.968750, mean_absolute_error: 150.071594, mean_q: -1.029907\n",
      "  209/5000: episode: 208, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 183659.093750, mean_absolute_error: 129.785614, mean_q: -1.009057\n",
      "  210/5000: episode: 209, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.702], loss: 229164.921875, mean_absolute_error: 160.428757, mean_q: -1.043048\n",
      "  211/5000: episode: 210, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.411, 1.000], loss: 213978.343750, mean_absolute_error: 150.382065, mean_q: -1.042651\n",
      "  212/5000: episode: 211, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.758], loss: 183179.031250, mean_absolute_error: 129.847977, mean_q: -1.022779\n",
      "  213/5000: episode: 212, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.000, 0.996], loss: 259571.734375, mean_absolute_error: 181.042465, mean_q: -1.059707\n",
      "  214/5000: episode: 213, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.516], loss: 290299.937500, mean_absolute_error: 201.374435, mean_q: -1.026012\n",
      "  215/5000: episode: 214, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 137436.968750, mean_absolute_error: 99.698021, mean_q: -1.045092\n",
      "  216/5000: episode: 215, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.417 [0.000, 0.979], loss: 183098.718750, mean_absolute_error: 130.131653, mean_q: -1.038096\n",
      "  217/5000: episode: 216, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.042, 1.000], loss: 274661.750000, mean_absolute_error: 191.104248, mean_q: -1.012555\n",
      "  218/5000: episode: 217, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.000, 0.785], loss: 198318.437500, mean_absolute_error: 140.207581, mean_q: -1.004458\n",
      "  219/5000: episode: 218, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 213539.125000, mean_absolute_error: 150.554520, mean_q: -1.023088\n",
      "  220/5000: episode: 219, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.016, 1.000], loss: 198292.921875, mean_absolute_error: 140.107498, mean_q: -0.964149\n",
      "  221/5000: episode: 220, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.000, 0.883], loss: 259396.593750, mean_absolute_error: 181.186005, mean_q: -1.007572\n",
      "  222/5000: episode: 221, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.317, 1.000], loss: 213397.093750, mean_absolute_error: 150.583801, mean_q: -1.005651\n",
      "  223/5000: episode: 222, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 228826.640625, mean_absolute_error: 160.688354, mean_q: -0.969261\n",
      "  224/5000: episode: 223, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.000, 0.847], loss: 167805.921875, mean_absolute_error: 120.216064, mean_q: -0.988783\n",
      "  225/5000: episode: 224, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.026, 1.000], loss: 213459.453125, mean_absolute_error: 150.596756, mean_q: -0.967824\n",
      "  226/5000: episode: 225, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.556 [0.010, 1.000], loss: 167693.953125, mean_absolute_error: 120.167717, mean_q: -0.968113\n",
      "  227/5000: episode: 226, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.130, 1.000], loss: 152472.828125, mean_absolute_error: 110.047653, mean_q: -0.958902\n",
      "  228/5000: episode: 227, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.000, 0.961], loss: 182869.968750, mean_absolute_error: 130.279724, mean_q: -0.941451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  229/5000: episode: 228, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.612 [0.036, 1.000], loss: 183170.562500, mean_absolute_error: 130.335114, mean_q: -0.907710\n",
      "  230/5000: episode: 229, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.519 [0.002, 1.000], loss: 228569.671875, mean_absolute_error: 160.913727, mean_q: -0.945588\n",
      "  231/5000: episode: 230, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.285, 1.000], loss: 228573.593750, mean_absolute_error: 161.015137, mean_q: -0.949276\n",
      "  232/5000: episode: 231, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.544 [0.007, 1.000], loss: 152330.468750, mean_absolute_error: 110.146042, mean_q: -0.926855\n",
      "  233/5000: episode: 232, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.374 [0.000, 0.954], loss: 213342.375000, mean_absolute_error: 150.848633, mean_q: -0.921215\n",
      "  234/5000: episode: 233, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.320 [0.000, 0.906], loss: 182855.953125, mean_absolute_error: 130.365097, mean_q: -0.888419\n",
      "  235/5000: episode: 234, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.000, 0.935], loss: 167800.406250, mean_absolute_error: 120.330139, mean_q: -0.875267\n",
      "  236/5000: episode: 235, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.363, 1.000], loss: 152333.843750, mean_absolute_error: 110.136253, mean_q: -0.884893\n",
      "  237/5000: episode: 236, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.491, 1.000], loss: 60898.859375, mean_absolute_error: 49.347626, mean_q: -0.895796\n",
      "  238/5000: episode: 237, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.354 [0.000, 0.938], loss: 213127.390625, mean_absolute_error: 150.924179, mean_q: -0.903303\n",
      "  239/5000: episode: 238, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 258830.968750, mean_absolute_error: 181.373291, mean_q: -0.889143\n",
      "  240/5000: episode: 239, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.529], loss: 152287.187500, mean_absolute_error: 110.258522, mean_q: -0.869152\n",
      "  241/5000: episode: 240, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.014, 1.000], loss: 182580.890625, mean_absolute_error: 130.643616, mean_q: -0.885540\n",
      "  242/5000: episode: 241, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.462 [0.000, 0.995], loss: 212985.562500, mean_absolute_error: 151.043213, mean_q: -0.895097\n",
      "  243/5000: episode: 242, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.337, 1.000], loss: 136870.312500, mean_absolute_error: 100.335358, mean_q: -0.887462\n",
      "  244/5000: episode: 243, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.008, 1.000], loss: 197723.781250, mean_absolute_error: 140.918884, mean_q: -0.882468\n",
      "  245/5000: episode: 244, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 167390.062500, mean_absolute_error: 120.586166, mean_q: -0.860035\n",
      "  246/5000: episode: 245, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.069, 1.000], loss: 152172.171875, mean_absolute_error: 110.435097, mean_q: -0.848459\n",
      "  247/5000: episode: 246, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.758], loss: 182433.703125, mean_absolute_error: 130.861359, mean_q: -0.875174\n",
      "  248/5000: episode: 247, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.088, 1.000], loss: 136944.156250, mean_absolute_error: 100.440155, mean_q: -0.853412\n",
      "  249/5000: episode: 248, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.622 [0.043, 1.000], loss: 197609.625000, mean_absolute_error: 140.911011, mean_q: -0.847479\n",
      "  250/5000: episode: 249, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.000, 0.945], loss: 212886.046875, mean_absolute_error: 151.157318, mean_q: -0.849053\n",
      "  251/5000: episode: 250, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.702], loss: 228000.906250, mean_absolute_error: 161.314087, mean_q: -0.848664\n",
      "  252/5000: episode: 251, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.000, 0.884], loss: 182529.296875, mean_absolute_error: 130.970856, mean_q: -0.831326\n",
      "  253/5000: episode: 252, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.801], loss: 151952.218750, mean_absolute_error: 110.606323, mean_q: -0.823871\n",
      "  254/5000: episode: 253, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.000, 1.000], loss: 121658.765625, mean_absolute_error: 90.391068, mean_q: -0.810518\n",
      "  255/5000: episode: 254, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.389, 1.000], loss: 258393.453125, mean_absolute_error: 181.581726, mean_q: -0.810280\n",
      "  256/5000: episode: 255, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 121681.406250, mean_absolute_error: 90.396530, mean_q: -0.792247\n",
      "  257/5000: episode: 256, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.067, 1.000], loss: 151960.390625, mean_absolute_error: 110.729401, mean_q: -0.802525\n",
      "  258/5000: episode: 257, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.354 [0.000, 0.939], loss: 227959.031250, mean_absolute_error: 161.400604, mean_q: -0.795175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  259/5000: episode: 258, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.468 [0.000, 0.996], loss: 106359.648438, mean_absolute_error: 80.373199, mean_q: -0.787744\n",
      "  260/5000: episode: 259, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.221, 1.000], loss: 167219.421875, mean_absolute_error: 120.867950, mean_q: -0.765761\n",
      "  261/5000: episode: 260, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.575], loss: 151870.781250, mean_absolute_error: 110.895462, mean_q: -0.790553\n",
      "  262/5000: episode: 261, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.000, 0.698], loss: 167019.937500, mean_absolute_error: 120.981003, mean_q: -0.780786\n",
      "  263/5000: episode: 262, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.430 [0.000, 0.985], loss: 166988.046875, mean_absolute_error: 120.960922, mean_q: -0.769718\n",
      "  264/5000: episode: 263, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.397 [0.000, 0.969], loss: 197410.468750, mean_absolute_error: 141.212006, mean_q: -0.758923\n",
      "  265/5000: episode: 264, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 166913.156250, mean_absolute_error: 121.000168, mean_q: -0.763863\n",
      "  266/5000: episode: 265, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.348 [0.000, 0.934], loss: 121385.281250, mean_absolute_error: 90.683838, mean_q: -0.757175\n",
      "  267/5000: episode: 266, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 258046.437500, mean_absolute_error: 181.684860, mean_q: -0.742434\n",
      "  268/5000: episode: 267, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.538], loss: 136615.937500, mean_absolute_error: 100.796310, mean_q: -0.737715\n",
      "  269/5000: episode: 268, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.754], loss: 182097.671875, mean_absolute_error: 131.184937, mean_q: -0.739481\n",
      "  270/5000: episode: 269, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.196, 1.000], loss: 197326.593750, mean_absolute_error: 141.375259, mean_q: -0.733151\n",
      "  271/5000: episode: 270, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.370, 1.000], loss: 197175.796875, mean_absolute_error: 141.321594, mean_q: -0.735057\n",
      "  272/5000: episode: 271, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.954], loss: 181977.937500, mean_absolute_error: 131.324158, mean_q: -0.742054\n",
      "  273/5000: episode: 272, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 166872.359375, mean_absolute_error: 121.294235, mean_q: -0.737334\n",
      "  274/5000: episode: 273, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.648], loss: 60635.328125, mean_absolute_error: 50.454689, mean_q: -0.726369\n",
      "  275/5000: episode: 274, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.584], loss: 136544.484375, mean_absolute_error: 100.950020, mean_q: -0.708838\n",
      "  276/5000: episode: 275, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.104, 1.000], loss: 257822.531250, mean_absolute_error: 181.853973, mean_q: -0.707893\n",
      "  277/5000: episode: 276, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.160, 1.000], loss: 91013.445312, mean_absolute_error: 70.778122, mean_q: -0.709397\n",
      "  278/5000: episode: 277, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.000, 0.959], loss: 106156.039062, mean_absolute_error: 80.814377, mean_q: -0.694684\n",
      "  279/5000: episode: 278, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.747], loss: 136472.625000, mean_absolute_error: 101.210472, mean_q: -0.709712\n",
      "  280/5000: episode: 279, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.381, 1.000], loss: 136542.406250, mean_absolute_error: 101.008011, mean_q: -0.679307\n",
      "  281/5000: episode: 280, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.280, 1.000], loss: 166780.140625, mean_absolute_error: 121.389374, mean_q: -0.692342\n",
      "  282/5000: episode: 281, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.125, 1.000], loss: 227356.031250, mean_absolute_error: 161.745285, mean_q: -0.681966\n",
      "  283/5000: episode: 282, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.679], loss: 75751.312500, mean_absolute_error: 60.692059, mean_q: -0.674858\n",
      "  284/5000: episode: 283, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.326, 1.000], loss: 151599.531250, mean_absolute_error: 111.301208, mean_q: -0.672068\n",
      "  285/5000: episode: 284, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.060, 1.000], loss: 121240.953125, mean_absolute_error: 91.189758, mean_q: -0.678484\n",
      "  286/5000: episode: 285, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.679], loss: 136349.656250, mean_absolute_error: 101.215919, mean_q: -0.668993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  287/5000: episode: 286, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 106041.804688, mean_absolute_error: 81.005829, mean_q: -0.659836\n",
      "  288/5000: episode: 287, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.914], loss: 106129.960938, mean_absolute_error: 81.121964, mean_q: -0.659138\n",
      "  289/5000: episode: 288, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 181816.187500, mean_absolute_error: 131.524414, mean_q: -0.648777\n",
      "  290/5000: episode: 289, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 166598.062500, mean_absolute_error: 121.506447, mean_q: -0.652669\n",
      "  291/5000: episode: 290, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.454, 1.000], loss: 90815.382812, mean_absolute_error: 70.910286, mean_q: -0.635673\n",
      "  292/5000: episode: 291, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.000, 0.800], loss: 136243.968750, mean_absolute_error: 101.357269, mean_q: -0.645730\n",
      "  293/5000: episode: 292, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.261 [0.000, 0.821], loss: 121083.500000, mean_absolute_error: 91.202553, mean_q: -0.635863\n",
      "  294/5000: episode: 293, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.625], loss: 121165.515625, mean_absolute_error: 91.276222, mean_q: -0.628895\n",
      "  295/5000: episode: 294, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.056, 1.000], loss: 151509.312500, mean_absolute_error: 111.452942, mean_q: -0.622678\n",
      "  296/5000: episode: 295, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 90747.687500, mean_absolute_error: 71.140083, mean_q: -0.631039\n",
      "  297/5000: episode: 296, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.644], loss: 136301.812500, mean_absolute_error: 101.443069, mean_q: -0.618996\n",
      "  298/5000: episode: 297, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 181709.843750, mean_absolute_error: 131.799011, mean_q: -0.621577\n",
      "  299/5000: episode: 298, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 151313.687500, mean_absolute_error: 111.556610, mean_q: -0.615727\n",
      "  300/5000: episode: 299, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.738], loss: 136205.421875, mean_absolute_error: 101.449432, mean_q: -0.602057\n",
      "  301/5000: episode: 300, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 181584.093750, mean_absolute_error: 131.729904, mean_q: -0.603500\n",
      "  302/5000: episode: 301, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.690 [0.106, 1.000], loss: 105960.984375, mean_absolute_error: 81.381119, mean_q: -0.597498\n",
      "  303/5000: episode: 302, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.537 [0.005, 1.000], loss: 121033.937500, mean_absolute_error: 91.415894, mean_q: -0.585903\n",
      "  304/5000: episode: 303, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.046, 1.000], loss: 196724.593750, mean_absolute_error: 141.925537, mean_q: -0.585258\n",
      "  305/5000: episode: 304, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.735 [0.171, 1.000], loss: 151261.953125, mean_absolute_error: 111.682755, mean_q: -0.584628\n",
      "  306/5000: episode: 305, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.529 [0.003, 1.000], loss: 136143.250000, mean_absolute_error: 101.540245, mean_q: -0.572545\n",
      "  307/5000: episode: 306, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.167, 1.000], loss: 196625.468750, mean_absolute_error: 141.897522, mean_q: -0.571957\n",
      "  308/5000: episode: 307, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.299, 1.000], loss: 105908.046875, mean_absolute_error: 81.498123, mean_q: -0.569433\n",
      "  309/5000: episode: 308, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.312 [0.000, 0.896], loss: 75661.218750, mean_absolute_error: 61.345798, mean_q: -0.563702\n",
      "  310/5000: episode: 309, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.000, 0.790], loss: 121008.828125, mean_absolute_error: 91.567207, mean_q: -0.557489\n",
      "  311/5000: episode: 310, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.506, 1.000], loss: 151337.718750, mean_absolute_error: 111.719475, mean_q: -0.547427\n",
      "  312/5000: episode: 311, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.531], loss: 166394.625000, mean_absolute_error: 121.812057, mean_q: -0.547632\n",
      "  313/5000: episode: 312, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.002, 1.000], loss: 136058.437500, mean_absolute_error: 101.651672, mean_q: -0.541544\n",
      "  314/5000: episode: 313, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.117, 1.000], loss: 120896.906250, mean_absolute_error: 91.679871, mean_q: -0.543194\n",
      "  315/5000: episode: 314, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.368 [0.000, 0.950], loss: 166457.046875, mean_absolute_error: 121.989723, mean_q: -0.535326\n",
      "  316/5000: episode: 315, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.290, 1.000], loss: 75559.687500, mean_absolute_error: 61.498016, mean_q: -0.532558\n",
      "  317/5000: episode: 316, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.001, 1.000], loss: 135979.750000, mean_absolute_error: 101.614212, mean_q: -0.512994\n",
      "  318/5000: episode: 317, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.482, 1.000], loss: 151129.343750, mean_absolute_error: 111.933350, mean_q: -0.521723\n",
      "  319/5000: episode: 318, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.347, 1.000], loss: 136110.187500, mean_absolute_error: 101.935829, mean_q: -0.517949\n",
      "  320/5000: episode: 319, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.438, 1.000], loss: 75609.960938, mean_absolute_error: 61.577393, mean_q: -0.507415\n",
      "  321/5000: episode: 320, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.643], loss: 151147.375000, mean_absolute_error: 111.931366, mean_q: -0.502474\n",
      "  322/5000: episode: 321, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.007, 1.000], loss: 120883.750000, mean_absolute_error: 91.851181, mean_q: -0.501904\n",
      "  323/5000: episode: 322, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.000, 0.976], loss: 136005.656250, mean_absolute_error: 101.912544, mean_q: -0.490038\n",
      "  324/5000: episode: 323, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.760], loss: 181185.609375, mean_absolute_error: 132.103745, mean_q: -0.491252\n",
      "  325/5000: episode: 324, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.051, 1.000], loss: 120833.281250, mean_absolute_error: 91.898834, mean_q: -0.487449\n",
      "  326/5000: episode: 325, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.055, 1.000], loss: 181217.750000, mean_absolute_error: 132.181793, mean_q: -0.484166\n",
      "  327/5000: episode: 326, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.543], loss: 105659.992188, mean_absolute_error: 81.795616, mean_q: -0.473316\n",
      "  328/5000: episode: 327, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.647 [0.063, 1.000], loss: 105673.718750, mean_absolute_error: 81.908829, mean_q: -0.476982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  329/5000: episode: 328, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.954], loss: 90565.648438, mean_absolute_error: 71.829865, mean_q: -0.469887\n",
      "  330/5000: episode: 329, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 135837.937500, mean_absolute_error: 102.002258, mean_q: -0.465347\n",
      "  331/5000: episode: 330, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.006, 1.000], loss: 120824.492188, mean_absolute_error: 92.000237, mean_q: -0.458790\n",
      "  332/5000: episode: 331, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.454, 1.000], loss: 165993.281250, mean_absolute_error: 122.175430, mean_q: -0.460169\n",
      "  333/5000: episode: 332, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.508], loss: 150882.875000, mean_absolute_error: 112.173668, mean_q: -0.456796\n",
      "  334/5000: episode: 333, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.198, 1.000], loss: 166055.781250, mean_absolute_error: 122.259506, mean_q: -0.450716\n",
      "  335/5000: episode: 334, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.006, 1.000], loss: 135779.593750, mean_absolute_error: 102.108772, mean_q: -0.443642\n",
      "  336/5000: episode: 335, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 30180.832031, mean_absolute_error: 31.707815, mean_q: -0.438508\n",
      "  337/5000: episode: 336, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.000, 0.969], loss: 75438.750000, mean_absolute_error: 61.960087, mean_q: -0.437056\n",
      "  338/5000: episode: 337, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.193, 1.000], loss: 135735.000000, mean_absolute_error: 102.201660, mean_q: -0.433388\n",
      "  339/5000: episode: 338, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.619 [0.041, 1.000], loss: 135820.750000, mean_absolute_error: 102.229004, mean_q: -0.428397\n",
      "  340/5000: episode: 339, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 181029.937500, mean_absolute_error: 132.406601, mean_q: -0.422613\n",
      "  341/5000: episode: 340, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 180987.468750, mean_absolute_error: 132.365112, mean_q: -0.413563\n",
      "  342/5000: episode: 341, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.016, 1.000], loss: 75336.750000, mean_absolute_error: 61.980225, mean_q: -0.411291\n",
      "  343/5000: episode: 342, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.342, 1.000], loss: 105546.093750, mean_absolute_error: 82.192963, mean_q: -0.411013\n",
      "  344/5000: episode: 343, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 135741.984375, mean_absolute_error: 102.341415, mean_q: -0.405265\n",
      "  345/5000: episode: 344, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.682], loss: 105538.414062, mean_absolute_error: 82.205147, mean_q: -0.398718\n",
      "  346/5000: episode: 345, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.768], loss: 105530.601562, mean_absolute_error: 82.198524, mean_q: -0.394007\n",
      "  347/5000: episode: 346, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.602], loss: 105573.304688, mean_absolute_error: 82.203163, mean_q: -0.385121\n",
      "  348/5000: episode: 347, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 90452.976562, mean_absolute_error: 72.229027, mean_q: -0.385579\n",
      "  349/5000: episode: 348, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.398, 1.000], loss: 120533.828125, mean_absolute_error: 92.289932, mean_q: -0.379474\n",
      "  350/5000: episode: 349, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.184, 1.000], loss: 210972.437500, mean_absolute_error: 152.628586, mean_q: -0.380608\n",
      "  351/5000: episode: 350, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.662], loss: 90438.828125, mean_absolute_error: 72.256203, mean_q: -0.372967\n",
      "  352/5000: episode: 351, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.189, 1.000], loss: 150662.281250, mean_absolute_error: 112.456741, mean_q: -0.370626\n",
      "  353/5000: episode: 352, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.544 [0.007, 1.000], loss: 150759.421875, mean_absolute_error: 112.549591, mean_q: -0.366448\n",
      "  354/5000: episode: 353, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.388 [0.000, 0.964], loss: 165686.625000, mean_absolute_error: 122.528793, mean_q: -0.362196\n",
      "  355/5000: episode: 354, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.025, 1.000], loss: 75319.562500, mean_absolute_error: 62.344395, mean_q: -0.358713\n",
      "  356/5000: episode: 355, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.258, 1.000], loss: 90315.875000, mean_absolute_error: 72.396576, mean_q: -0.356615\n",
      "  357/5000: episode: 356, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.599], loss: 75314.578125, mean_absolute_error: 62.366848, mean_q: -0.350646\n",
      "  358/5000: episode: 357, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.170, 1.000], loss: 195789.781250, mean_absolute_error: 142.741791, mean_q: -0.349448\n",
      "  359/5000: episode: 358, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.390, 1.000], loss: 90295.328125, mean_absolute_error: 72.421165, mean_q: -0.344238\n",
      "  360/5000: episode: 359, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.442, 1.000], loss: 105401.390625, mean_absolute_error: 82.473045, mean_q: -0.336118\n",
      "  361/5000: episode: 360, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 75285.296875, mean_absolute_error: 62.478806, mean_q: -0.336876\n",
      "  362/5000: episode: 361, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.539], loss: 120465.046875, mean_absolute_error: 92.538071, mean_q: -0.329867\n",
      "  363/5000: episode: 362, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.378, 1.000], loss: 165741.593750, mean_absolute_error: 122.771263, mean_q: -0.327809\n",
      "  364/5000: episode: 363, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.792], loss: 120455.523438, mean_absolute_error: 92.610306, mean_q: -0.323433\n",
      "  365/5000: episode: 364, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.030, 1.000], loss: 150571.375000, mean_absolute_error: 112.639633, mean_q: -0.314509\n",
      "  366/5000: episode: 365, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.192, 1.000], loss: 90333.609375, mean_absolute_error: 72.583466, mean_q: -0.314394\n",
      "  367/5000: episode: 366, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.118, 1.000], loss: 120418.546875, mean_absolute_error: 92.651505, mean_q: -0.310549\n",
      "  368/5000: episode: 367, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.752], loss: 150464.000000, mean_absolute_error: 112.735001, mean_q: -0.306846\n",
      "  369/5000: episode: 368, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.567], loss: 150602.125000, mean_absolute_error: 112.802284, mean_q: -0.301638\n",
      "  370/5000: episode: 369, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.406, 1.000], loss: 120401.578125, mean_absolute_error: 92.725494, mean_q: -0.296304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  371/5000: episode: 370, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.133, 1.000], loss: 150429.968750, mean_absolute_error: 112.778229, mean_q: -0.293007\n",
      "  372/5000: episode: 371, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 90274.843750, mean_absolute_error: 72.673698, mean_q: -0.286228\n",
      "  373/5000: episode: 372, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.128, 1.000], loss: 180440.906250, mean_absolute_error: 132.833725, mean_q: -0.285778\n",
      "  374/5000: episode: 373, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 105295.617188, mean_absolute_error: 82.786392, mean_q: -0.280707\n",
      "  375/5000: episode: 374, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.908], loss: 120389.460938, mean_absolute_error: 92.888649, mean_q: -0.277272\n",
      "  376/5000: episode: 375, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 135340.031250, mean_absolute_error: 102.894287, mean_q: -0.273556\n",
      "  377/5000: episode: 376, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.539], loss: 120315.492188, mean_absolute_error: 92.884979, mean_q: -0.268378\n",
      "  378/5000: episode: 377, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.518], loss: 105300.507812, mean_absolute_error: 82.903969, mean_q: -0.264208\n",
      "  379/5000: episode: 378, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 105276.578125, mean_absolute_error: 82.885666, mean_q: -0.257098\n",
      "  380/5000: episode: 379, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 75148.101562, mean_absolute_error: 62.844765, mean_q: -0.254647\n",
      "  381/5000: episode: 380, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.008, 1.000], loss: 120292.226562, mean_absolute_error: 92.938934, mean_q: -0.247139\n",
      "  382/5000: episode: 381, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.000, 0.935], loss: 90170.296875, mean_absolute_error: 72.894463, mean_q: -0.244546\n",
      "  383/5000: episode: 382, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.146, 1.000], loss: 90218.484375, mean_absolute_error: 72.915047, mean_q: -0.241173\n",
      "  384/5000: episode: 383, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.142, 1.000], loss: 90126.789062, mean_absolute_error: 72.902573, mean_q: -0.237039\n",
      "  385/5000: episode: 384, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.000, 0.969], loss: 90169.484375, mean_absolute_error: 72.985229, mean_q: -0.233864\n",
      "  386/5000: episode: 385, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 135311.343750, mean_absolute_error: 103.069801, mean_q: -0.230715\n",
      "  387/5000: episode: 386, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.434, 1.000], loss: 150332.000000, mean_absolute_error: 113.053596, mean_q: -0.224313\n",
      "  388/5000: episode: 387, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.104, 1.000], loss: 75142.531250, mean_absolute_error: 63.034813, mean_q: -0.222298\n",
      "  389/5000: episode: 388, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.446 [0.000, 0.991], loss: 105173.648438, mean_absolute_error: 83.034378, mean_q: -0.217365\n",
      "  390/5000: episode: 389, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.189, 1.000], loss: 90129.851562, mean_absolute_error: 73.067986, mean_q: -0.215846\n",
      "  391/5000: episode: 390, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.392 [0.000, 0.966], loss: 150215.437500, mean_absolute_error: 113.102722, mean_q: -0.210351\n",
      "  392/5000: episode: 391, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.198, 1.000], loss: 105093.125000, mean_absolute_error: 83.052650, mean_q: -0.206728\n",
      "  393/5000: episode: 392, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.000, 0.963], loss: 105166.296875, mean_absolute_error: 83.144463, mean_q: -0.203496\n",
      "  394/5000: episode: 393, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.907], loss: 75075.265625, mean_absolute_error: 63.121235, mean_q: -0.201096\n",
      "  395/5000: episode: 394, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.525], loss: 105177.890625, mean_absolute_error: 83.171295, mean_q: -0.196802\n",
      "  396/5000: episode: 395, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.458, 1.000], loss: 45032.320312, mean_absolute_error: 43.088280, mean_q: -0.193903\n",
      "  397/5000: episode: 396, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.051, 1.000], loss: 105081.898438, mean_absolute_error: 83.119049, mean_q: -0.192282\n",
      "  398/5000: episode: 397, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.732], loss: 120025.046875, mean_absolute_error: 93.127434, mean_q: -0.190529\n",
      "  399/5000: episode: 398, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.315, 1.000], loss: 165190.312500, mean_absolute_error: 123.259781, mean_q: -0.187631\n",
      "  400/5000: episode: 399, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.290 [0.000, 0.868], loss: 105093.343750, mean_absolute_error: 83.223251, mean_q: -0.184491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  401/5000: episode: 400, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.320 [0.000, 0.905], loss: 90068.390625, mean_absolute_error: 73.240311, mean_q: -0.182780\n",
      "  402/5000: episode: 401, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.678 [0.093, 1.000], loss: 75079.687500, mean_absolute_error: 63.286125, mean_q: -0.181309\n",
      "  403/5000: episode: 402, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.617], loss: 150062.453125, mean_absolute_error: 113.201958, mean_q: -0.173175\n",
      "  404/5000: episode: 403, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.226, 1.000], loss: 120081.070312, mean_absolute_error: 93.267174, mean_q: -0.172560\n",
      "  405/5000: episode: 404, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 75126.914062, mean_absolute_error: 63.382801, mean_q: -0.171685\n",
      "  406/5000: episode: 405, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.656 [0.070, 1.000], loss: 30011.121094, mean_absolute_error: 33.307575, mean_q: -0.166385\n",
      "  407/5000: episode: 406, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.792], loss: 120134.015625, mean_absolute_error: 93.405930, mean_q: -0.164653\n",
      "  408/5000: episode: 407, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 135096.781250, mean_absolute_error: 103.388313, mean_q: -0.158946\n",
      "  409/5000: episode: 408, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.059, 1.000], loss: 104988.445312, mean_absolute_error: 83.340309, mean_q: -0.156369\n",
      "  410/5000: episode: 409, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.283, 1.000], loss: 90037.757812, mean_absolute_error: 73.426773, mean_q: -0.152877\n",
      "  411/5000: episode: 410, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.685], loss: 104979.562500, mean_absolute_error: 83.384735, mean_q: -0.149714\n",
      "  412/5000: episode: 411, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.438, 1.000], loss: 135065.250000, mean_absolute_error: 103.454758, mean_q: -0.146555\n",
      "  413/5000: episode: 412, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.027, 1.000], loss: 119957.835938, mean_absolute_error: 93.398315, mean_q: -0.142096\n",
      "  414/5000: episode: 413, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.659], loss: 120061.304688, mean_absolute_error: 93.468529, mean_q: -0.137448\n",
      "  415/5000: episode: 414, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.162, 1.000], loss: 135025.578125, mean_absolute_error: 103.504044, mean_q: -0.135544\n",
      "  416/5000: episode: 415, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.386, 1.000], loss: 104996.765625, mean_absolute_error: 83.496765, mean_q: -0.130721\n",
      "  417/5000: episode: 416, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.432, 1.000], loss: 149994.984375, mean_absolute_error: 113.498123, mean_q: -0.126921\n",
      "  418/5000: episode: 417, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.077, 1.000], loss: 59910.320312, mean_absolute_error: 53.540382, mean_q: -0.123650\n",
      "  419/5000: episode: 418, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 44962.546875, mean_absolute_error: 43.486229, mean_q: -0.116725\n",
      "  420/5000: episode: 419, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.000, 0.989], loss: 119989.851562, mean_absolute_error: 93.598404, mean_q: -0.115867\n",
      "  421/5000: episode: 420, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.240, 1.000], loss: 134886.500000, mean_absolute_error: 103.535080, mean_q: -0.112313\n",
      "  422/5000: episode: 421, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.111, 1.000], loss: 29990.068359, mean_absolute_error: 33.651115, mean_q: -0.107764\n",
      "  423/5000: episode: 422, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.000, 0.937], loss: 104955.750000, mean_absolute_error: 83.656006, mean_q: -0.104312\n",
      "  424/5000: episode: 423, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.323, 1.000], loss: 29973.310547, mean_absolute_error: 33.699203, mean_q: -0.099731\n",
      "  425/5000: episode: 424, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.108, 1.000], loss: 134855.296875, mean_absolute_error: 103.576187, mean_q: -0.095401\n",
      "  426/5000: episode: 425, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.609], loss: 89880.156250, mean_absolute_error: 73.656754, mean_q: -0.094328\n",
      "  427/5000: episode: 426, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.059, 1.000], loss: 74979.750000, mean_absolute_error: 63.758705, mean_q: -0.091531\n",
      "  428/5000: episode: 427, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.578], loss: 104857.000000, mean_absolute_error: 83.653992, mean_q: -0.086728\n",
      "  429/5000: episode: 428, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.489], loss: 59896.406250, mean_absolute_error: 53.709064, mean_q: -0.084325\n",
      "  430/5000: episode: 429, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.031, 1.000], loss: 89947.937500, mean_absolute_error: 73.778732, mean_q: -0.081143\n",
      "  431/5000: episode: 430, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.004, 1.000], loss: 119846.890625, mean_absolute_error: 93.718857, mean_q: -0.077921\n",
      "  432/5000: episode: 431, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.456 [0.000, 0.993], loss: 119800.164062, mean_absolute_error: 93.713974, mean_q: -0.074214\n",
      "  433/5000: episode: 432, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.570 [0.015, 1.000], loss: 89916.804688, mean_absolute_error: 73.802689, mean_q: -0.070921\n",
      "  434/5000: episode: 433, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.347 [0.000, 0.933], loss: 149824.406250, mean_absolute_error: 113.717041, mean_q: -0.066999\n",
      "  435/5000: episode: 434, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.758], loss: 74910.296875, mean_absolute_error: 63.836452, mean_q: -0.064153\n",
      "  436/5000: episode: 435, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 60002.515625, mean_absolute_error: 53.899914, mean_q: -0.059806\n",
      "  437/5000: episode: 436, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.467, 1.000], loss: 149773.437500, mean_absolute_error: 113.785301, mean_q: -0.056573\n",
      "  438/5000: episode: 437, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.569], loss: 29956.945312, mean_absolute_error: 33.962265, mean_q: -0.052200\n",
      "  439/5000: episode: 438, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 104818.875000, mean_absolute_error: 83.892967, mean_q: -0.048738\n",
      "  440/5000: episode: 439, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.658 [0.072, 1.000], loss: 104811.875000, mean_absolute_error: 83.917877, mean_q: -0.045204\n",
      "  441/5000: episode: 440, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.228, 1.000], loss: 179662.187500, mean_absolute_error: 133.792847, mean_q: -0.041279\n",
      "  442/5000: episode: 441, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.109, 1.000], loss: 89761.406250, mean_absolute_error: 73.911766, mean_q: -0.037203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  443/5000: episode: 442, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.066, 1.000], loss: 74808.304688, mean_absolute_error: 63.964050, mean_q: -0.033665\n",
      "  444/5000: episode: 443, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.757], loss: 74810.703125, mean_absolute_error: 63.986835, mean_q: -0.028504\n",
      "  445/5000: episode: 444, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.000, 0.852], loss: 89764.109375, mean_absolute_error: 73.988113, mean_q: -0.027514\n",
      "  446/5000: episode: 445, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.062, 1.000], loss: 149677.156250, mean_absolute_error: 113.915146, mean_q: -0.025120\n",
      "  447/5000: episode: 446, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.759], loss: 59961.593750, mean_absolute_error: 54.114922, mean_q: -0.022051\n",
      "  448/5000: episode: 447, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.861], loss: 74838.406250, mean_absolute_error: 64.057724, mean_q: -0.018443\n",
      "  449/5000: episode: 448, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.376, 1.000], loss: 59880.265625, mean_absolute_error: 54.134060, mean_q: -0.016617\n",
      "  450/5000: episode: 449, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.000, 0.989], loss: 104721.320312, mean_absolute_error: 84.038292, mean_q: -0.014494\n",
      "  451/5000: episode: 450, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.090, 1.000], loss: 104833.484375, mean_absolute_error: 84.122452, mean_q: -0.011235\n",
      "  452/5000: episode: 451, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.759], loss: 179592.031250, mean_absolute_error: 133.979874, mean_q: -0.008793\n",
      "  453/5000: episode: 452, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.069, 1.000], loss: 74895.421875, mean_absolute_error: 64.206932, mean_q: -0.003053\n",
      "  454/5000: episode: 453, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.001, 1.000], loss: 119623.390625, mean_absolute_error: 94.059830, mean_q: -0.000932\n",
      "  455/5000: episode: 454, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.000, 0.970], loss: 44883.050781, mean_absolute_error: 44.279392, mean_q: 0.003052\n",
      "  456/5000: episode: 455, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.342 [0.000, 0.928], loss: 119649.601562, mean_absolute_error: 94.091644, mean_q: 0.007080\n",
      "  457/5000: episode: 456, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 134623.640625, mean_absolute_error: 104.124756, mean_q: 0.010317\n",
      "  458/5000: episode: 457, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.334 [0.000, 0.920], loss: 89739.562500, mean_absolute_error: 74.233521, mean_q: 0.012204\n",
      "  459/5000: episode: 458, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.958], loss: 44811.531250, mean_absolute_error: 44.358463, mean_q: 0.016822\n",
      "  460/5000: episode: 459, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.877], loss: 104579.429688, mean_absolute_error: 84.180115, mean_q: 0.021148\n",
      "  461/5000: episode: 460, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.000, 0.870], loss: 44840.988281, mean_absolute_error: 44.380745, mean_q: 0.024459\n",
      "  462/5000: episode: 461, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.853], loss: 0.000587, mean_absolute_error: 14.515259, mean_q: 0.027041\n",
      "  463/5000: episode: 462, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.000, 0.951], loss: 89668.070312, mean_absolute_error: 74.289085, mean_q: 0.028709\n",
      "  464/5000: episode: 463, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.000, 0.982], loss: 74739.625000, mean_absolute_error: 64.367523, mean_q: 0.031433\n",
      "  465/5000: episode: 464, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.508, 1.000], loss: 29904.744141, mean_absolute_error: 34.439140, mean_q: 0.034651\n",
      "  466/5000: episode: 465, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.479 [0.000, 0.998], loss: 104542.828125, mean_absolute_error: 84.278786, mean_q: 0.037016\n",
      "  467/5000: episode: 466, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.385 [0.000, 0.962], loss: 74729.851562, mean_absolute_error: 64.420662, mean_q: 0.039685\n",
      "  468/5000: episode: 467, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.523], loss: 74765.632812, mean_absolute_error: 64.452652, mean_q: 0.042635\n",
      "  469/5000: episode: 468, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 74764.406250, mean_absolute_error: 64.477325, mean_q: 0.046710\n",
      "  470/5000: episode: 469, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.608], loss: 104620.148438, mean_absolute_error: 84.338455, mean_q: 0.049409\n",
      "  471/5000: episode: 470, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.000, 0.867], loss: 44817.542969, mean_absolute_error: 44.564865, mean_q: 0.052876\n",
      "  472/5000: episode: 471, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.000, 0.900], loss: 74720.968750, mean_absolute_error: 64.490700, mean_q: 0.055459\n",
      "  473/5000: episode: 472, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.718], loss: 149343.031250, mean_absolute_error: 114.232452, mean_q: 0.058209\n",
      "  474/5000: episode: 473, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 104586.351562, mean_absolute_error: 84.453690, mean_q: 0.060779\n",
      "  475/5000: episode: 474, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.648], loss: 149398.265625, mean_absolute_error: 114.340057, mean_q: 0.063208\n",
      "  476/5000: episode: 475, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.906], loss: 89560.515625, mean_absolute_error: 74.508881, mean_q: 0.066594\n",
      "  477/5000: episode: 476, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.000, 0.957], loss: 104507.359375, mean_absolute_error: 84.476753, mean_q: 0.069890\n",
      "  478/5000: episode: 477, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.000, 1.000], loss: 164327.843750, mean_absolute_error: 124.351166, mean_q: 0.070989\n",
      "  479/5000: episode: 478, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.422, 1.000], loss: 74721.546875, mean_absolute_error: 64.656311, mean_q: 0.075480\n",
      "  480/5000: episode: 479, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.657 [0.071, 1.000], loss: 44814.179688, mean_absolute_error: 44.754295, mean_q: 0.079300\n",
      "  481/5000: episode: 480, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.302, 1.000], loss: 59783.449219, mean_absolute_error: 54.766716, mean_q: 0.081978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  482/5000: episode: 481, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.595], loss: 104473.843750, mean_absolute_error: 84.578262, mean_q: 0.083237\n",
      "  483/5000: episode: 482, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.496], loss: 149277.281250, mean_absolute_error: 114.447678, mean_q: 0.086879\n",
      "  484/5000: episode: 483, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.614 [0.038, 1.000], loss: 164133.203125, mean_absolute_error: 124.334572, mean_q: 0.089937\n",
      "  485/5000: episode: 484, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.000, 0.925], loss: 89547.101562, mean_absolute_error: 74.697861, mean_q: 0.091204\n",
      "  486/5000: episode: 485, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.000, 1.000], loss: 89570.906250, mean_absolute_error: 74.725594, mean_q: 0.093429\n",
      "  487/5000: episode: 486, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 74571.843750, mean_absolute_error: 64.739136, mean_q: 0.095228\n",
      "  488/5000: episode: 487, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.018, 1.000], loss: 59663.835938, mean_absolute_error: 54.834385, mean_q: 0.097359\n",
      "  489/5000: episode: 488, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.630 [0.049, 1.000], loss: 29848.246094, mean_absolute_error: 34.970528, mean_q: 0.100457\n",
      "  490/5000: episode: 489, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.385 [0.000, 0.962], loss: 134338.593750, mean_absolute_error: 104.619690, mean_q: 0.100593\n",
      "  491/5000: episode: 490, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.945], loss: 44796.125000, mean_absolute_error: 45.001385, mean_q: 0.103127\n",
      "  492/5000: episode: 491, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.683], loss: 119219.781250, mean_absolute_error: 94.625641, mean_q: 0.104909\n",
      "  493/5000: episode: 492, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.237, 1.000], loss: 104394.578125, mean_absolute_error: 84.700554, mean_q: 0.108364\n",
      "  494/5000: episode: 493, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.718], loss: 134182.687500, mean_absolute_error: 104.628601, mean_q: 0.109688\n",
      "  495/5000: episode: 494, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.008, 1.000], loss: 89427.656250, mean_absolute_error: 74.800697, mean_q: 0.111704\n",
      "  496/5000: episode: 495, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.490 [0.000, 0.999], loss: 104347.484375, mean_absolute_error: 84.766769, mean_q: 0.113283\n",
      "  497/5000: episode: 496, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.541], loss: 74577.859375, mean_absolute_error: 64.967049, mean_q: 0.114051\n",
      "  498/5000: episode: 497, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.301, 1.000], loss: 119222.906250, mean_absolute_error: 94.740921, mean_q: 0.116267\n",
      "  499/5000: episode: 498, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.541], loss: 59643.722656, mean_absolute_error: 55.026161, mean_q: 0.118278\n",
      "  500/5000: episode: 499, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.129, 1.000], loss: 104356.937500, mean_absolute_error: 84.871964, mean_q: 0.119275\n",
      "  501/5000: episode: 500, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 74550.640625, mean_absolute_error: 65.042198, mean_q: 0.119778\n",
      "  502/5000: episode: 501, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 104369.515625, mean_absolute_error: 84.891312, mean_q: 0.121964\n",
      "  503/5000: episode: 502, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.017, 1.000], loss: 74483.539062, mean_absolute_error: 65.045830, mean_q: 0.122106\n",
      "  504/5000: episode: 503, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.563], loss: 134072.953125, mean_absolute_error: 104.784645, mean_q: 0.124789\n",
      "  505/5000: episode: 504, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.326, 1.000], loss: 74630.281250, mean_absolute_error: 65.143837, mean_q: 0.125890\n",
      "  506/5000: episode: 505, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.669], loss: 44708.496094, mean_absolute_error: 45.265774, mean_q: 0.128210\n",
      "  507/5000: episode: 506, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.007, 1.000], loss: 59581.750000, mean_absolute_error: 55.207085, mean_q: 0.130330\n",
      "  508/5000: episode: 507, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.064, 1.000], loss: 74430.046875, mean_absolute_error: 65.105141, mean_q: 0.133024\n",
      "  509/5000: episode: 508, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.568 [0.014, 1.000], loss: 44807.062500, mean_absolute_error: 45.365929, mean_q: 0.134830\n",
      "  510/5000: episode: 509, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.531], loss: 59588.273438, mean_absolute_error: 55.236736, mean_q: 0.137419\n",
      "  511/5000: episode: 510, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.306, 1.000], loss: 163940.265625, mean_absolute_error: 124.772133, mean_q: 0.139981\n",
      "  512/5000: episode: 511, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 74500.468750, mean_absolute_error: 65.258659, mean_q: 0.141115\n",
      "  513/5000: episode: 512, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.469 [0.000, 0.996], loss: 89478.125000, mean_absolute_error: 75.194931, mean_q: 0.144632\n",
      "  514/5000: episode: 513, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.915], loss: 104330.078125, mean_absolute_error: 85.115128, mean_q: 0.146240\n",
      "  515/5000: episode: 514, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.000, 0.885], loss: 89345.921875, mean_absolute_error: 75.190559, mean_q: 0.148822\n",
      "  516/5000: episode: 515, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.792], loss: 59593.589844, mean_absolute_error: 55.354233, mean_q: 0.154162\n",
      "  517/5000: episode: 516, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.715], loss: 44642.625000, mean_absolute_error: 45.411655, mean_q: 0.156394\n",
      "  518/5000: episode: 517, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.498], loss: 104304.312500, mean_absolute_error: 85.219887, mean_q: 0.158432\n",
      "  519/5000: episode: 518, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.505 [0.000, 1.000], loss: 44715.738281, mean_absolute_error: 45.506084, mean_q: 0.161802\n",
      "  520/5000: episode: 519, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.335, 1.000], loss: 44607.527344, mean_absolute_error: 45.493305, mean_q: 0.163642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  521/5000: episode: 520, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.447 [0.000, 0.991], loss: 74395.390625, mean_absolute_error: 65.368790, mean_q: 0.165741\n",
      "  522/5000: episode: 521, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 89423.093750, mean_absolute_error: 75.346100, mean_q: 0.168828\n",
      "  523/5000: episode: 522, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.575], loss: 119194.453125, mean_absolute_error: 95.200539, mean_q: 0.170885\n",
      "  524/5000: episode: 523, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.512, 1.000], loss: 148882.375000, mean_absolute_error: 115.043205, mean_q: 0.171637\n",
      "  525/5000: episode: 524, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.102, 1.000], loss: 44654.144531, mean_absolute_error: 45.611027, mean_q: 0.174349\n",
      "  526/5000: episode: 525, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.000, 0.996], loss: 44696.007812, mean_absolute_error: 45.663116, mean_q: 0.177028\n",
      "  527/5000: episode: 526, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.368 [0.000, 0.950], loss: 44694.039062, mean_absolute_error: 45.657696, mean_q: 0.179280\n",
      "  528/5000: episode: 527, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.349 [0.000, 0.934], loss: 74375.882812, mean_absolute_error: 65.450989, mean_q: 0.181949\n",
      "  529/5000: episode: 528, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.770], loss: 89393.226562, mean_absolute_error: 75.458588, mean_q: 0.185021\n",
      "  530/5000: episode: 529, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.663], loss: 89341.968750, mean_absolute_error: 75.461166, mean_q: 0.187977\n",
      "  531/5000: episode: 530, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.481 [0.000, 0.998], loss: 89294.882812, mean_absolute_error: 75.439011, mean_q: 0.190184\n",
      "  532/5000: episode: 531, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.000, 0.930], loss: 59552.351562, mean_absolute_error: 55.670929, mean_q: 0.193887\n",
      "  533/5000: episode: 532, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.000, 0.984], loss: 74444.726562, mean_absolute_error: 65.639816, mean_q: 0.196953\n",
      "  534/5000: episode: 533, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.663], loss: 178580.281250, mean_absolute_error: 134.995575, mean_q: 0.200596\n",
      "  535/5000: episode: 534, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.593 [0.026, 1.000], loss: 118992.460938, mean_absolute_error: 95.351204, mean_q: 0.203484\n",
      "  536/5000: episode: 535, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.081, 1.000], loss: 74430.593750, mean_absolute_error: 65.692001, mean_q: 0.206476\n",
      "  537/5000: episode: 536, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.702], loss: 89235.015625, mean_absolute_error: 75.579437, mean_q: 0.209498\n",
      "  538/5000: episode: 537, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.783], loss: 44616.882812, mean_absolute_error: 45.837151, mean_q: 0.212702\n",
      "  539/5000: episode: 538, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.540], loss: 74375.140625, mean_absolute_error: 65.709671, mean_q: 0.215768\n",
      "  540/5000: episode: 539, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.678 [0.092, 1.000], loss: 89252.953125, mean_absolute_error: 75.623795, mean_q: 0.217747\n",
      "  541/5000: episode: 540, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.017, 1.000], loss: 89218.171875, mean_absolute_error: 75.586418, mean_q: 0.220754\n",
      "  542/5000: episode: 541, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.000, 0.850], loss: 44657.933594, mean_absolute_error: 45.959419, mean_q: 0.222061\n",
      "  543/5000: episode: 542, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 44650.000000, mean_absolute_error: 46.000221, mean_q: 0.223796\n",
      "  544/5000: episode: 543, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 59526.386719, mean_absolute_error: 55.868385, mean_q: 0.226534\n",
      "  545/5000: episode: 544, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.000, 0.958], loss: 59492.984375, mean_absolute_error: 55.895538, mean_q: 0.227500\n",
      "  546/5000: episode: 545, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.752], loss: 103981.796875, mean_absolute_error: 85.539337, mean_q: 0.229511\n",
      "  547/5000: episode: 546, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.045, 1.000], loss: 74389.070312, mean_absolute_error: 65.881691, mean_q: 0.231150\n",
      "  548/5000: episode: 547, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.000, 0.822], loss: 89222.343750, mean_absolute_error: 75.774979, mean_q: 0.232565\n",
      "  549/5000: episode: 548, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 74264.531250, mean_absolute_error: 65.837387, mean_q: 0.234954\n",
      "  550/5000: episode: 549, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.088, 1.000], loss: 29795.041016, mean_absolute_error: 36.227661, mean_q: 0.237359\n",
      "  551/5000: episode: 550, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.758], loss: 59601.195312, mean_absolute_error: 56.089996, mean_q: 0.239095\n",
      "  552/5000: episode: 551, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.284, 1.000], loss: 89281.500000, mean_absolute_error: 75.883102, mean_q: 0.242240\n",
      "  553/5000: episode: 552, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.519], loss: 118935.343750, mean_absolute_error: 95.660858, mean_q: 0.244642\n",
      "  554/5000: episode: 553, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.894], loss: 59463.136719, mean_absolute_error: 56.044327, mean_q: 0.247858\n",
      "  555/5000: episode: 554, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.028, 1.000], loss: 74352.625000, mean_absolute_error: 66.016785, mean_q: 0.250981\n",
      "  556/5000: episode: 555, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.209, 1.000], loss: 59525.984375, mean_absolute_error: 56.100178, mean_q: 0.254301\n",
      "  557/5000: episode: 556, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.855], loss: 74235.718750, mean_absolute_error: 65.968567, mean_q: 0.257366\n",
      "  558/5000: episode: 557, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.000, 0.940], loss: 133860.812500, mean_absolute_error: 105.670486, mean_q: 0.259767\n",
      "  559/5000: episode: 558, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 59455.695312, mean_absolute_error: 56.079643, mean_q: 0.262182\n",
      "  560/5000: episode: 559, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.912], loss: 74286.406250, mean_absolute_error: 66.019981, mean_q: 0.267474\n",
      "  561/5000: episode: 560, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.407 [0.000, 0.975], loss: 59397.781250, mean_absolute_error: 56.129421, mean_q: 0.269916\n",
      "  562/5000: episode: 561, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.108, 1.000], loss: 29730.580078, mean_absolute_error: 36.375000, mean_q: 0.271816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  563/5000: episode: 562, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.449, 1.000], loss: 118828.015625, mean_absolute_error: 95.779457, mean_q: 0.273814\n",
      "  564/5000: episode: 563, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.582 [0.020, 1.000], loss: 44598.515625, mean_absolute_error: 46.330605, mean_q: 0.276560\n",
      "  565/5000: episode: 564, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.176, 1.000], loss: 89108.625000, mean_absolute_error: 75.998367, mean_q: 0.279707\n",
      "  566/5000: episode: 565, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 74360.500000, mean_absolute_error: 66.215378, mean_q: 0.282217\n",
      "  567/5000: episode: 566, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 59429.917969, mean_absolute_error: 56.281536, mean_q: 0.283994\n",
      "  568/5000: episode: 567, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.000, 0.997], loss: 29728.566406, mean_absolute_error: 36.496384, mean_q: 0.288573\n",
      "  569/5000: episode: 568, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.000, 0.809], loss: 89135.281250, mean_absolute_error: 76.109741, mean_q: 0.290071\n",
      "  570/5000: episode: 569, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.108, 1.000], loss: 104079.039062, mean_absolute_error: 86.067146, mean_q: 0.293926\n",
      "  571/5000: episode: 570, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.430, 1.000], loss: 59358.484375, mean_absolute_error: 56.286995, mean_q: 0.296927\n",
      "  572/5000: episode: 571, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.066, 1.000], loss: 44577.820312, mean_absolute_error: 46.473389, mean_q: 0.298858\n",
      "  573/5000: episode: 572, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.589], loss: 74283.445312, mean_absolute_error: 66.284073, mean_q: 0.301223\n",
      "  574/5000: episode: 573, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.000, 0.989], loss: 59407.511719, mean_absolute_error: 56.417210, mean_q: 0.303862\n",
      "  575/5000: episode: 574, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.614 [0.037, 1.000], loss: 88980.625000, mean_absolute_error: 76.142761, mean_q: 0.307136\n",
      "  576/5000: episode: 575, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.038, 1.000], loss: 74381.500000, mean_absolute_error: 66.408188, mean_q: 0.308589\n",
      "  577/5000: episode: 576, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.856], loss: 29692.101562, mean_absolute_error: 36.654984, mean_q: 0.311532\n",
      "  578/5000: episode: 577, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.001, 1.000], loss: 89091.015625, mean_absolute_error: 76.274521, mean_q: 0.313821\n",
      "  579/5000: episode: 578, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.348, 1.000], loss: 74174.632812, mean_absolute_error: 66.310120, mean_q: 0.316670\n",
      "  580/5000: episode: 579, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.499], loss: 59441.441406, mean_absolute_error: 56.523933, mean_q: 0.318255\n",
      "  581/5000: episode: 580, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 44511.234375, mean_absolute_error: 46.586754, mean_q: 0.320567\n",
      "  582/5000: episode: 581, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.361, 1.000], loss: 59376.894531, mean_absolute_error: 56.484951, mean_q: 0.320567\n",
      "  583/5000: episode: 582, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.579], loss: 0.052632, mean_absolute_error: 17.005417, mean_q: 0.323289\n",
      "  584/5000: episode: 583, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.246, 1.000], loss: 74173.796875, mean_absolute_error: 66.428574, mean_q: 0.324226\n",
      "  585/5000: episode: 584, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.516], loss: 59360.761719, mean_absolute_error: 56.587017, mean_q: 0.326453\n",
      "  586/5000: episode: 585, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.036, 1.000], loss: 44489.726562, mean_absolute_error: 46.698372, mean_q: 0.327606\n",
      "  587/5000: episode: 586, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.680 [0.095, 1.000], loss: 103906.648438, mean_absolute_error: 86.305870, mean_q: 0.329472\n",
      "  588/5000: episode: 587, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.678], loss: 74238.960938, mean_absolute_error: 66.522362, mean_q: 0.331560\n",
      "  589/5000: episode: 588, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.505 [0.000, 1.000], loss: 133554.734375, mean_absolute_error: 106.086395, mean_q: 0.335214\n",
      "  590/5000: episode: 589, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.160, 1.000], loss: 74089.476562, mean_absolute_error: 66.510132, mean_q: 0.338413\n",
      "  591/5000: episode: 590, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.199, 1.000], loss: 29667.171875, mean_absolute_error: 36.926655, mean_q: 0.340257\n",
      "  592/5000: episode: 591, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 89016.289062, mean_absolute_error: 76.434387, mean_q: 0.342634\n",
      "  593/5000: episode: 592, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.238, 1.000], loss: 133366.656250, mean_absolute_error: 106.022751, mean_q: 0.345490\n",
      "  594/5000: episode: 593, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 29661.820312, mean_absolute_error: 36.964024, mean_q: 0.347362\n",
      "  595/5000: episode: 594, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.000, 0.963], loss: 89102.804688, mean_absolute_error: 76.602844, mean_q: 0.350088\n",
      "  596/5000: episode: 595, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.514], loss: 74128.937500, mean_absolute_error: 66.660301, mean_q: 0.352761\n",
      "  597/5000: episode: 596, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.032, 1.000], loss: 44572.968750, mean_absolute_error: 46.978073, mean_q: 0.355753\n",
      "  598/5000: episode: 597, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.000, 0.976], loss: 59309.015625, mean_absolute_error: 56.822372, mean_q: 0.358886\n",
      "  599/5000: episode: 598, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.754], loss: 74183.375000, mean_absolute_error: 66.735275, mean_q: 0.362892\n",
      "  600/5000: episode: 599, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.565], loss: 29718.378906, mean_absolute_error: 37.141636, mean_q: 0.365910\n",
      "  601/5000: episode: 600, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.000, 0.900], loss: 44445.781250, mean_absolute_error: 46.992561, mean_q: 0.369940\n",
      "  602/5000: episode: 601, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.772], loss: 44500.781250, mean_absolute_error: 47.060822, mean_q: 0.372426\n",
      "  603/5000: episode: 602, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.000, 0.697], loss: 88996.640625, mean_absolute_error: 76.708290, mean_q: 0.375421\n",
      "  604/5000: episode: 603, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.000, 0.963], loss: 103676.562500, mean_absolute_error: 86.501923, mean_q: 0.378131\n",
      "  605/5000: episode: 604, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.894], loss: 59324.101562, mean_absolute_error: 56.957134, mean_q: 0.381500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  606/5000: episode: 605, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.000, 0.815], loss: 14796.308594, mean_absolute_error: 27.350546, mean_q: 0.384431\n",
      "  607/5000: episode: 606, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.754], loss: 44488.695312, mean_absolute_error: 47.132118, mean_q: 0.387157\n",
      "  608/5000: episode: 607, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.915], loss: 103711.640625, mean_absolute_error: 86.563805, mean_q: 0.389468\n",
      "  609/5000: episode: 608, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.129, 1.000], loss: 59238.906250, mean_absolute_error: 56.966446, mean_q: 0.391840\n",
      "  610/5000: episode: 609, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 89039.867188, mean_absolute_error: 76.840843, mean_q: 0.394500\n",
      "  611/5000: episode: 610, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.120, 1.000], loss: 59213.593750, mean_absolute_error: 57.012253, mean_q: 0.397080\n",
      "  612/5000: episode: 611, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.000, 0.891], loss: 88931.453125, mean_absolute_error: 76.785454, mean_q: 0.399660\n",
      "  613/5000: episode: 612, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.314 [0.000, 0.899], loss: 59219.457031, mean_absolute_error: 57.046833, mean_q: 0.402665\n",
      "  614/5000: episode: 613, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.099, 1.000], loss: 118481.937500, mean_absolute_error: 96.530029, mean_q: 0.404979\n",
      "  615/5000: episode: 614, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.427 [0.000, 0.984], loss: 103741.726562, mean_absolute_error: 86.730148, mean_q: 0.407675\n",
      "  616/5000: episode: 615, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.688], loss: 73982.265625, mean_absolute_error: 66.944939, mean_q: 0.410812\n",
      "  617/5000: episode: 616, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.599 [0.029, 1.000], loss: 74092.460938, mean_absolute_error: 67.030121, mean_q: 0.413320\n",
      "  618/5000: episode: 617, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.617], loss: 88882.007812, mean_absolute_error: 76.869995, mean_q: 0.415406\n",
      "  619/5000: episode: 618, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.632 [0.050, 1.000], loss: 44406.132812, mean_absolute_error: 47.287628, mean_q: 0.418558\n",
      "  620/5000: episode: 619, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.234, 1.000], loss: 44455.863281, mean_absolute_error: 47.347061, mean_q: 0.420901\n",
      "  621/5000: episode: 620, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.197, 1.000], loss: 59231.976562, mean_absolute_error: 57.230839, mean_q: 0.423040\n",
      "  622/5000: episode: 621, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.813], loss: 74014.203125, mean_absolute_error: 67.089752, mean_q: 0.425619\n",
      "  623/5000: episode: 622, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 74002.625000, mean_absolute_error: 67.104874, mean_q: 0.428379\n",
      "  624/5000: episode: 623, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.497, 1.000], loss: 44443.000000, mean_absolute_error: 47.419212, mean_q: 0.430510\n",
      "  625/5000: episode: 624, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 133136.265625, mean_absolute_error: 106.517891, mean_q: 0.433348\n",
      "  626/5000: episode: 625, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.858], loss: 74012.679688, mean_absolute_error: 67.153893, mean_q: 0.435287\n",
      "  627/5000: episode: 626, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.593 [0.026, 1.000], loss: 73934.062500, mean_absolute_error: 67.131622, mean_q: 0.436713\n",
      "  628/5000: episode: 627, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.652], loss: 103556.406250, mean_absolute_error: 86.882401, mean_q: 0.438962\n",
      "  629/5000: episode: 628, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.050, 1.000], loss: 148083.765625, mean_absolute_error: 116.546661, mean_q: 0.439991\n",
      "  630/5000: episode: 629, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.177, 1.000], loss: 59206.726562, mean_absolute_error: 57.387054, mean_q: 0.443693\n",
      "  631/5000: episode: 630, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.000, 0.999], loss: 73934.453125, mean_absolute_error: 67.207298, mean_q: 0.446012\n",
      "  632/5000: episode: 631, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.000, 0.965], loss: 14826.533203, mean_absolute_error: 27.885714, mean_q: 0.448438\n",
      "  633/5000: episode: 632, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.340, 1.000], loss: 103733.750000, mean_absolute_error: 87.129318, mean_q: 0.450070\n",
      "  634/5000: episode: 633, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.076, 1.000], loss: 44365.300781, mean_absolute_error: 47.594109, mean_q: 0.453833\n",
      "  635/5000: episode: 634, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.488], loss: 44424.054688, mean_absolute_error: 47.635132, mean_q: 0.454683\n",
      "  636/5000: episode: 635, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.894], loss: 44411.199219, mean_absolute_error: 47.636711, mean_q: 0.456683\n",
      "  637/5000: episode: 636, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.283 [0.000, 0.857], loss: 29646.199219, mean_absolute_error: 37.838486, mean_q: 0.458475\n",
      "  638/5000: episode: 637, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.000, 0.940], loss: 73991.203125, mean_absolute_error: 67.428833, mean_q: 0.461010\n",
      "  639/5000: episode: 638, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.045, 1.000], loss: 88760.398438, mean_absolute_error: 77.264618, mean_q: 0.463821\n",
      "  640/5000: episode: 639, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.000, 0.886], loss: 73926.421875, mean_absolute_error: 67.387871, mean_q: 0.466615\n",
      "  641/5000: episode: 640, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.357, 1.000], loss: 29580.074219, mean_absolute_error: 37.896263, mean_q: 0.468690\n",
      "  642/5000: episode: 641, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.703], loss: 73915.640625, mean_absolute_error: 67.405815, mean_q: 0.471558\n",
      "  643/5000: episode: 642, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.181, 1.000], loss: 44402.332031, mean_absolute_error: 47.755898, mean_q: 0.472108\n",
      "  644/5000: episode: 643, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.090, 1.000], loss: 59041.339844, mean_absolute_error: 57.559692, mean_q: 0.476307\n",
      "  645/5000: episode: 644, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.011, 1.000], loss: 59159.878906, mean_absolute_error: 57.654259, mean_q: 0.478429\n",
      "  646/5000: episode: 645, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.013, 1.000], loss: 103492.328125, mean_absolute_error: 87.190109, mean_q: 0.480917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  647/5000: episode: 646, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.596], loss: 29628.384766, mean_absolute_error: 37.996773, mean_q: 0.482418\n",
      "  648/5000: episode: 647, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.076, 1.000], loss: 147670.437500, mean_absolute_error: 116.661186, mean_q: 0.484676\n",
      "  649/5000: episode: 648, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.088, 1.000], loss: 103516.539062, mean_absolute_error: 87.282211, mean_q: 0.486922\n",
      "  650/5000: episode: 649, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.414 [0.000, 0.978], loss: 118203.828125, mean_absolute_error: 97.080719, mean_q: 0.489989\n",
      "  651/5000: episode: 650, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.000, 0.987], loss: 73949.976562, mean_absolute_error: 67.609177, mean_q: 0.491494\n",
      "  652/5000: episode: 651, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.159, 1.000], loss: 59135.589844, mean_absolute_error: 57.774490, mean_q: 0.494020\n",
      "  653/5000: episode: 652, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.000, 0.876], loss: 73866.828125, mean_absolute_error: 67.642105, mean_q: 0.497585\n",
      "  654/5000: episode: 653, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.070, 1.000], loss: 59128.156250, mean_absolute_error: 57.810871, mean_q: 0.498498\n",
      "  655/5000: episode: 654, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.035, 1.000], loss: 44308.578125, mean_absolute_error: 47.949913, mean_q: 0.501356\n",
      "  656/5000: episode: 655, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.079, 1.000], loss: 103467.960938, mean_absolute_error: 87.397949, mean_q: 0.503505\n",
      "  657/5000: episode: 656, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.259, 1.000], loss: 44369.917969, mean_absolute_error: 48.041618, mean_q: 0.505709\n",
      "  658/5000: episode: 657, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.000, 0.892], loss: 73853.085938, mean_absolute_error: 67.695206, mean_q: 0.507648\n",
      "  659/5000: episode: 658, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.543], loss: 88713.343750, mean_absolute_error: 77.624237, mean_q: 0.509556\n",
      "  660/5000: episode: 659, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.008, 1.000], loss: 59177.593750, mean_absolute_error: 57.974823, mean_q: 0.511786\n",
      "  661/5000: episode: 660, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.494], loss: 44286.871094, mean_absolute_error: 48.084671, mean_q: 0.513835\n",
      "  662/5000: episode: 661, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.156, 1.000], loss: 14810.143555, mean_absolute_error: 28.452835, mean_q: 0.515785\n",
      "  663/5000: episode: 662, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 29606.878906, mean_absolute_error: 38.317646, mean_q: 0.517332\n",
      "  664/5000: episode: 663, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.686], loss: 88512.609375, mean_absolute_error: 77.596832, mean_q: 0.520318\n",
      "  665/5000: episode: 664, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.128, 1.000], loss: 59020.789062, mean_absolute_error: 57.978447, mean_q: 0.521810\n",
      "  666/5000: episode: 665, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.000, 0.942], loss: 14740.413086, mean_absolute_error: 28.500069, mean_q: 0.522872\n",
      "  667/5000: episode: 666, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.740], loss: 14798.775391, mean_absolute_error: 28.541618, mean_q: 0.524093\n",
      "  668/5000: episode: 667, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.033, 1.000], loss: 118084.796875, mean_absolute_error: 97.361969, mean_q: 0.525700\n",
      "  669/5000: episode: 668, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.397, 1.000], loss: 59066.289062, mean_absolute_error: 58.088943, mean_q: 0.527558\n",
      "  670/5000: episode: 669, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.090, 1.000], loss: 88541.812500, mean_absolute_error: 77.718170, mean_q: 0.529266\n",
      "  671/5000: episode: 670, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.000, 0.764], loss: 29528.710938, mean_absolute_error: 38.399803, mean_q: 0.529952\n",
      "  672/5000: episode: 671, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.203, 1.000], loss: 59076.593750, mean_absolute_error: 58.117474, mean_q: 0.531987\n",
      "  673/5000: episode: 672, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.715], loss: 103372.640625, mean_absolute_error: 87.669403, mean_q: 0.534635\n",
      "  674/5000: episode: 673, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.252, 1.000], loss: 73926.343750, mean_absolute_error: 68.021492, mean_q: 0.537363\n",
      "  675/5000: episode: 674, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.254, 1.000], loss: 73853.390625, mean_absolute_error: 68.033279, mean_q: 0.539384\n",
      "  676/5000: episode: 675, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.320 [0.000, 0.906], loss: 44186.792969, mean_absolute_error: 48.291821, mean_q: 0.542811\n",
      "  677/5000: episode: 676, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.503, 1.000], loss: 73773.929688, mean_absolute_error: 68.001862, mean_q: 0.545432\n",
      "  678/5000: episode: 677, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.328, 1.000], loss: 44253.695312, mean_absolute_error: 48.358109, mean_q: 0.547640\n",
      "  679/5000: episode: 678, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.051, 1.000], loss: 58971.636719, mean_absolute_error: 58.192783, mean_q: 0.549828\n",
      "  680/5000: episode: 679, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.567], loss: 88578.984375, mean_absolute_error: 77.920715, mean_q: 0.552222\n",
      "  681/5000: episode: 680, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.284, 1.000], loss: 73754.320312, mean_absolute_error: 68.090309, mean_q: 0.555001\n",
      "  682/5000: episode: 681, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.000, 1.000], loss: 14786.869141, mean_absolute_error: 28.831947, mean_q: 0.556912\n",
      "  683/5000: episode: 682, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.282, 1.000], loss: 44231.507812, mean_absolute_error: 48.448162, mean_q: 0.558209\n",
      "  684/5000: episode: 683, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: 44228.996094, mean_absolute_error: 48.450180, mean_q: 0.560093\n",
      "  685/5000: episode: 684, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.232, 1.000], loss: 44250.933594, mean_absolute_error: 48.484882, mean_q: 0.560630\n",
      "  686/5000: episode: 685, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.791], loss: 44223.996094, mean_absolute_error: 48.472404, mean_q: 0.561719\n",
      "  687/5000: episode: 686, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.000, 0.840], loss: 58881.023438, mean_absolute_error: 58.265877, mean_q: 0.563784\n",
      "  688/5000: episode: 687, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.408, 1.000], loss: 0.159750, mean_absolute_error: 19.064690, mean_q: 0.564148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  689/5000: episode: 688, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.245, 1.000], loss: 44302.304688, mean_absolute_error: 48.592712, mean_q: 0.565322\n",
      "  690/5000: episode: 689, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.000, 0.957], loss: 88432.968750, mean_absolute_error: 78.010101, mean_q: 0.567164\n",
      "  691/5000: episode: 690, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.139, 1.000], loss: 29568.460938, mean_absolute_error: 38.803867, mean_q: 0.568712\n",
      "  692/5000: episode: 691, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.381, 1.000], loss: 58939.937500, mean_absolute_error: 58.385036, mean_q: 0.570483\n",
      "  693/5000: episode: 692, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.496], loss: 147371.687500, mean_absolute_error: 117.313972, mean_q: 0.572965\n",
      "  694/5000: episode: 693, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.781], loss: 29426.980469, mean_absolute_error: 38.773632, mean_q: 0.575354\n",
      "  695/5000: episode: 694, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.133, 1.000], loss: 44203.437500, mean_absolute_error: 48.630180, mean_q: 0.576829\n",
      "  696/5000: episode: 695, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 73626.835938, mean_absolute_error: 68.237778, mean_q: 0.578364\n",
      "  697/5000: episode: 696, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.804], loss: 58990.292969, mean_absolute_error: 58.524086, mean_q: 0.579972\n",
      "  698/5000: episode: 697, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 29554.664062, mean_absolute_error: 38.900986, mean_q: 0.580964\n",
      "  699/5000: episode: 698, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.944], loss: 14708.938477, mean_absolute_error: 29.020651, mean_q: 0.583277\n",
      "  700/5000: episode: 699, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 44220.765625, mean_absolute_error: 48.696991, mean_q: 0.584125\n",
      "  701/5000: episode: 700, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.533 [0.004, 1.000], loss: 73687.257812, mean_absolute_error: 68.331390, mean_q: 0.585533\n",
      "  702/5000: episode: 701, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.544], loss: 44187.617188, mean_absolute_error: 48.745651, mean_q: 0.587857\n",
      "  703/5000: episode: 702, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.731], loss: 44120.972656, mean_absolute_error: 48.717968, mean_q: 0.589291\n",
      "  704/5000: episode: 703, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.429, 1.000], loss: 88368.046875, mean_absolute_error: 78.202606, mean_q: 0.590540\n",
      "  705/5000: episode: 704, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 44191.832031, mean_absolute_error: 48.806427, mean_q: 0.592716\n",
      "  706/5000: episode: 705, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.686], loss: 59022.785156, mean_absolute_error: 58.682030, mean_q: 0.594149\n",
      "  707/5000: episode: 706, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.504 [0.000, 1.000], loss: 58878.417969, mean_absolute_error: 58.582226, mean_q: 0.594942\n",
      "  708/5000: episode: 707, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 58898.824219, mean_absolute_error: 58.627617, mean_q: 0.597073\n",
      "  709/5000: episode: 708, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.329, 1.000], loss: 73718.742188, mean_absolute_error: 68.521133, mean_q: 0.599354\n",
      "  710/5000: episode: 709, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.786], loss: 14702.220703, mean_absolute_error: 29.241652, mean_q: 0.602157\n",
      "  711/5000: episode: 710, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.089, 1.000], loss: 44187.039062, mean_absolute_error: 48.863770, mean_q: 0.602538\n",
      "  712/5000: episode: 711, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.630 [0.049, 1.000], loss: 29396.154297, mean_absolute_error: 39.077530, mean_q: 0.606194\n",
      "  713/5000: episode: 712, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.656], loss: 44161.441406, mean_absolute_error: 48.929371, mean_q: 0.608106\n",
      "  714/5000: episode: 713, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.028, 1.000], loss: 44173.812500, mean_absolute_error: 48.946835, mean_q: 0.609549\n",
      "  715/5000: episode: 714, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.294, 1.000], loss: 58788.476562, mean_absolute_error: 58.691338, mean_q: 0.610912\n",
      "  716/5000: episode: 715, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.546], loss: 58938.835938, mean_absolute_error: 58.772896, mean_q: 0.611750\n",
      "  717/5000: episode: 716, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.356, 1.000], loss: 44161.921875, mean_absolute_error: 48.991142, mean_q: 0.614839\n",
      "  718/5000: episode: 717, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.416, 1.000], loss: 73605.187500, mean_absolute_error: 68.604004, mean_q: 0.615821\n",
      "  719/5000: episode: 718, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.344, 1.000], loss: 58834.765625, mean_absolute_error: 58.812195, mean_q: 0.617581\n",
      "  720/5000: episode: 719, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.570], loss: 29380.332031, mean_absolute_error: 39.208015, mean_q: 0.619331\n",
      "  721/5000: episode: 720, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.454, 1.000], loss: 73532.507812, mean_absolute_error: 68.610168, mean_q: 0.620110\n",
      "  722/5000: episode: 721, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.647], loss: 58837.945312, mean_absolute_error: 58.843567, mean_q: 0.621191\n",
      "  723/5000: episode: 722, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.000, 0.885], loss: 44063.457031, mean_absolute_error: 49.014950, mean_q: 0.621748\n",
      "  724/5000: episode: 723, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.497, 1.000], loss: 44138.609375, mean_absolute_error: 49.112442, mean_q: 0.623722\n",
      "  725/5000: episode: 724, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.009, 1.000], loss: 102944.578125, mean_absolute_error: 88.259735, mean_q: 0.623863\n",
      "  726/5000: episode: 725, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.630], loss: 88336.625000, mean_absolute_error: 78.575302, mean_q: 0.626214\n",
      "  727/5000: episode: 726, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.526 [0.003, 1.000], loss: 88410.796875, mean_absolute_error: 78.623695, mean_q: 0.628637\n",
      "  728/5000: episode: 727, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.190, 1.000], loss: 73560.812500, mean_absolute_error: 68.792953, mean_q: 0.630847\n",
      "  729/5000: episode: 728, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 88404.210938, mean_absolute_error: 78.682991, mean_q: 0.632676\n",
      "  730/5000: episode: 729, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.702], loss: 58871.421875, mean_absolute_error: 59.055817, mean_q: 0.635473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  731/5000: episode: 730, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.295, 1.000], loss: 44187.128906, mean_absolute_error: 49.272072, mean_q: 0.637574\n",
      "  732/5000: episode: 731, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.308, 1.000], loss: 29359.050781, mean_absolute_error: 39.407867, mean_q: 0.639919\n",
      "  733/5000: episode: 732, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.161, 1.000], loss: 44188.390625, mean_absolute_error: 49.306427, mean_q: 0.641660\n",
      "  734/5000: episode: 733, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.773], loss: 73608.843750, mean_absolute_error: 68.913841, mean_q: 0.643227\n",
      "  735/5000: episode: 734, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.005, 1.000], loss: 14750.773438, mean_absolute_error: 29.720293, mean_q: 0.644933\n",
      "  736/5000: episode: 735, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 58862.023438, mean_absolute_error: 59.137039, mean_q: 0.647766\n",
      "  737/5000: episode: 736, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.300, 1.000], loss: 58775.281250, mean_absolute_error: 59.098770, mean_q: 0.650074\n",
      "  738/5000: episode: 737, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.547], loss: 44101.851562, mean_absolute_error: 49.340057, mean_q: 0.652038\n",
      "  739/5000: episode: 738, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.320, 1.000], loss: 29421.107422, mean_absolute_error: 39.602432, mean_q: 0.654470\n",
      "  740/5000: episode: 739, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.267 [0.000, 0.832], loss: 44174.320312, mean_absolute_error: 49.421497, mean_q: 0.655354\n",
      "  741/5000: episode: 740, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.372, 1.000], loss: 44104.113281, mean_absolute_error: 49.382145, mean_q: 0.656856\n",
      "  742/5000: episode: 741, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.055, 1.000], loss: 58924.996094, mean_absolute_error: 59.282780, mean_q: 0.658462\n",
      "  743/5000: episode: 742, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.000, 0.965], loss: 14745.375000, mean_absolute_error: 29.851269, mean_q: 0.660477\n",
      "  744/5000: episode: 743, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.002, 1.000], loss: 73648.187500, mean_absolute_error: 69.116180, mean_q: 0.661558\n",
      "  745/5000: episode: 744, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.740], loss: 88252.312500, mean_absolute_error: 78.857277, mean_q: 0.664640\n",
      "  746/5000: episode: 745, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.732], loss: 58847.992188, mean_absolute_error: 59.267578, mean_q: 0.666971\n",
      "  747/5000: episode: 746, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.044, 1.000], loss: 58910.460938, mean_absolute_error: 59.344208, mean_q: 0.669718\n",
      "  748/5000: episode: 747, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.585], loss: 58761.710938, mean_absolute_error: 59.252060, mean_q: 0.671438\n",
      "  749/5000: episode: 748, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.008, 1.000], loss: 14741.060547, mean_absolute_error: 29.940432, mean_q: 0.673545\n",
      "  750/5000: episode: 749, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.456 [0.000, 0.993], loss: 73571.468750, mean_absolute_error: 69.170914, mean_q: 0.676127\n",
      "  751/5000: episode: 750, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.249, 1.000], loss: 14739.054688, mean_absolute_error: 30.012043, mean_q: 0.677561\n",
      "  752/5000: episode: 751, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.216, 1.000], loss: 88092.015625, mean_absolute_error: 78.871880, mean_q: 0.678117\n",
      "  753/5000: episode: 752, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.334, 1.000], loss: 73388.984375, mean_absolute_error: 69.092575, mean_q: 0.679985\n",
      "  754/5000: episode: 753, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.428, 1.000], loss: 44066.117188, mean_absolute_error: 49.602810, mean_q: 0.681219\n",
      "  755/5000: episode: 754, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.364, 1.000], loss: 58799.726562, mean_absolute_error: 59.417152, mean_q: 0.681896\n",
      "  756/5000: episode: 755, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.766], loss: 44062.128906, mean_absolute_error: 49.635044, mean_q: 0.683091\n",
      "  757/5000: episode: 756, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.474, 1.000], loss: 44054.792969, mean_absolute_error: 49.632694, mean_q: 0.683977\n",
      "  758/5000: episode: 757, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.263, 1.000], loss: 43970.250000, mean_absolute_error: 49.592422, mean_q: 0.685600\n",
      "  759/5000: episode: 758, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.532], loss: 29398.212891, mean_absolute_error: 39.902115, mean_q: 0.685553\n",
      "  760/5000: episode: 759, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 44064.550781, mean_absolute_error: 49.703758, mean_q: 0.686142\n",
      "  761/5000: episode: 760, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.000, 0.940], loss: 132044.359375, mean_absolute_error: 108.309418, mean_q: 0.686256\n",
      "  762/5000: episode: 761, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.414 [0.000, 0.978], loss: 14652.298828, mean_absolute_error: 30.157654, mean_q: 0.686941\n",
      "  763/5000: episode: 762, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 14650.972656, mean_absolute_error: 30.140509, mean_q: 0.686214\n",
      "  764/5000: episode: 763, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.462 [0.000, 0.995], loss: 102799.976562, mean_absolute_error: 88.871223, mean_q: 0.686591\n",
      "  765/5000: episode: 764, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.812], loss: 73410.296875, mean_absolute_error: 69.336540, mean_q: 0.687139\n",
      "  766/5000: episode: 765, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.430, 1.000], loss: 58607.343750, mean_absolute_error: 59.486042, mean_q: 0.687786\n",
      "  767/5000: episode: 766, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.803], loss: 29378.130859, mean_absolute_error: 40.058975, mean_q: 0.688368\n",
      "  768/5000: episode: 767, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.447 [0.000, 0.991], loss: 29291.585938, mean_absolute_error: 39.992702, mean_q: 0.688742\n",
      "  769/5000: episode: 768, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 43931.210938, mean_absolute_error: 49.749031, mean_q: 0.687997\n",
      "  770/5000: episode: 769, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.000, 0.764], loss: 44026.527344, mean_absolute_error: 49.831932, mean_q: 0.687745\n",
      "  771/5000: episode: 770, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.409, 1.000], loss: 44200.011719, mean_absolute_error: 49.952995, mean_q: 0.687328\n",
      "  772/5000: episode: 771, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.325 [0.000, 0.911], loss: 29465.751953, mean_absolute_error: 40.154163, mean_q: 0.688402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  773/5000: episode: 772, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.000, 0.967], loss: 44199.078125, mean_absolute_error: 49.986530, mean_q: 0.688902\n",
      "  774/5000: episode: 773, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.579], loss: 58686.914062, mean_absolute_error: 59.639343, mean_q: 0.690476\n",
      "  775/5000: episode: 774, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.465 [0.000, 0.995], loss: 29369.173828, mean_absolute_error: 40.175217, mean_q: 0.692010\n",
      "  776/5000: episode: 775, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.788], loss: 58651.906250, mean_absolute_error: 59.668152, mean_q: 0.692921\n",
      "  777/5000: episode: 776, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 58748.250000, mean_absolute_error: 59.735458, mean_q: 0.694645\n",
      "  778/5000: episode: 777, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 87926.085938, mean_absolute_error: 79.197235, mean_q: 0.696455\n",
      "  779/5000: episode: 778, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.703], loss: 14636.426758, mean_absolute_error: 30.382633, mean_q: 0.696608\n",
      "  780/5000: episode: 779, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.281 [0.000, 0.854], loss: 44175.714844, mean_absolute_error: 50.104809, mean_q: 0.698481\n",
      "  781/5000: episode: 780, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.730 [0.164, 1.000], loss: 29280.439453, mean_absolute_error: 40.197472, mean_q: 0.700515\n",
      "  782/5000: episode: 781, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.595], loss: 73444.835938, mean_absolute_error: 69.642471, mean_q: 0.702585\n",
      "  783/5000: episode: 782, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.340, 1.000], loss: 88266.187500, mean_absolute_error: 79.528442, mean_q: 0.704548\n",
      "  784/5000: episode: 783, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 73265.593750, mean_absolute_error: 69.533882, mean_q: 0.706933\n",
      "  785/5000: episode: 784, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.000, 0.998], loss: 58886.617188, mean_absolute_error: 59.994598, mean_q: 0.709239\n",
      "  786/5000: episode: 785, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.223, 1.000], loss: 43978.707031, mean_absolute_error: 50.057278, mean_q: 0.711883\n",
      "  787/5000: episode: 786, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 43890.542969, mean_absolute_error: 50.023674, mean_q: 0.714523\n",
      "  788/5000: episode: 787, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 58512.089844, mean_absolute_error: 59.814598, mean_q: 0.717416\n",
      "  789/5000: episode: 788, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.498], loss: 43971.476562, mean_absolute_error: 50.136719, mean_q: 0.718390\n",
      "  790/5000: episode: 789, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.008, 1.000], loss: 43969.398438, mean_absolute_error: 50.137856, mean_q: 0.719700\n",
      "  791/5000: episode: 790, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.000, 0.903], loss: 58688.433594, mean_absolute_error: 59.983364, mean_q: 0.720985\n",
      "  792/5000: episode: 791, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.412, 1.000], loss: 29347.554688, mean_absolute_error: 40.433281, mean_q: 0.721605\n",
      "  793/5000: episode: 792, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 29345.203125, mean_absolute_error: 40.435860, mean_q: 0.722404\n",
      "  794/5000: episode: 793, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 43964.710938, mean_absolute_error: 50.210892, mean_q: 0.724256\n",
      "  795/5000: episode: 794, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.000, 0.922], loss: 29250.935547, mean_absolute_error: 40.409241, mean_q: 0.724762\n",
      "  796/5000: episode: 795, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.312 [0.000, 0.897], loss: 29241.693359, mean_absolute_error: 40.422661, mean_q: 0.725886\n",
      "  797/5000: episode: 796, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 29333.646484, mean_absolute_error: 40.516682, mean_q: 0.726249\n",
      "  798/5000: episode: 797, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.347 [0.000, 0.933], loss: 29238.193359, mean_absolute_error: 40.469902, mean_q: 0.727391\n",
      "  799/5000: episode: 798, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 73395.335938, mean_absolute_error: 69.873093, mean_q: 0.727714\n",
      "  800/5000: episode: 799, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.401, 1.000], loss: 44148.386719, mean_absolute_error: 50.443398, mean_q: 0.728859\n",
      "  801/5000: episode: 800, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 29328.939453, mean_absolute_error: 40.525749, mean_q: 0.729125\n",
      "  802/5000: episode: 801, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 58655.203125, mean_absolute_error: 60.129101, mean_q: 0.731907\n",
      "  803/5000: episode: 802, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.497, 1.000], loss: 58569.628906, mean_absolute_error: 60.079266, mean_q: 0.732959\n",
      "  804/5000: episode: 803, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.000, 0.871], loss: 73459.453125, mean_absolute_error: 70.008011, mean_q: 0.734525\n",
      "  805/5000: episode: 804, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.000, 0.959], loss: 29239.882812, mean_absolute_error: 40.553513, mean_q: 0.737024\n",
      "  806/5000: episode: 805, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.002, 1.000], loss: 87778.718750, mean_absolute_error: 79.582809, mean_q: 0.740584\n",
      "  807/5000: episode: 806, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.215, 1.000], loss: 117485.890625, mean_absolute_error: 99.398880, mean_q: 0.742426\n",
      "  808/5000: episode: 807, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.169, 1.000], loss: 73339.859375, mean_absolute_error: 70.010521, mean_q: 0.745037\n",
      "  809/5000: episode: 808, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 131973.500000, mean_absolute_error: 109.077194, mean_q: 0.747773\n",
      "  810/5000: episode: 809, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.499], loss: 73334.835938, mean_absolute_error: 70.046265, mean_q: 0.750819\n",
      "  811/5000: episode: 810, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.000, 0.931], loss: 58566.386719, mean_absolute_error: 60.217487, mean_q: 0.754157\n",
      "  812/5000: episode: 811, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.247, 1.000], loss: 43831.593750, mean_absolute_error: 50.442352, mean_q: 0.756326\n",
      "  813/5000: episode: 812, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 43820.386719, mean_absolute_error: 50.454113, mean_q: 0.758076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  814/5000: episode: 813, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.818], loss: 88020.609375, mean_absolute_error: 79.929276, mean_q: 0.760022\n",
      "  815/5000: episode: 814, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.784], loss: 87913.437500, mean_absolute_error: 79.865746, mean_q: 0.761853\n",
      "  816/5000: episode: 815, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.670 [0.084, 1.000], loss: 58602.332031, mean_absolute_error: 60.367821, mean_q: 0.763336\n",
      "  817/5000: episode: 816, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.067, 1.000], loss: 43903.855469, mean_absolute_error: 50.593693, mean_q: 0.765518\n",
      "  818/5000: episode: 817, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.663], loss: 58594.828125, mean_absolute_error: 60.423706, mean_q: 0.768143\n",
      "  819/5000: episode: 818, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.000, 0.893], loss: 29302.628906, mean_absolute_error: 40.934174, mean_q: 0.770043\n",
      "  820/5000: episode: 819, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.053, 1.000], loss: 58681.781250, mean_absolute_error: 60.519295, mean_q: 0.772207\n",
      "  821/5000: episode: 820, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.408, 1.000], loss: 14608.110352, mean_absolute_error: 31.139982, mean_q: 0.773532\n",
      "  822/5000: episode: 821, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 29205.089844, mean_absolute_error: 40.923943, mean_q: 0.776742\n",
      "  823/5000: episode: 822, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.265 [0.000, 0.829], loss: 58491.464844, mean_absolute_error: 60.426022, mean_q: 0.777423\n",
      "  824/5000: episode: 823, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.340, 1.000], loss: 87776.015625, mean_absolute_error: 79.963593, mean_q: 0.779771\n",
      "  825/5000: episode: 824, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.145, 1.000], loss: 58490.988281, mean_absolute_error: 60.460091, mean_q: 0.779887\n",
      "  826/5000: episode: 825, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.383, 1.000], loss: 43797.398438, mean_absolute_error: 50.691914, mean_q: 0.781304\n",
      "  827/5000: episode: 826, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.416 [0.000, 0.979], loss: 58673.500000, mean_absolute_error: 60.615131, mean_q: 0.781952\n",
      "  828/5000: episode: 827, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.678], loss: 58670.140625, mean_absolute_error: 60.634010, mean_q: 0.782382\n",
      "  829/5000: episode: 828, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.000, 0.947], loss: 29184.294922, mean_absolute_error: 41.018097, mean_q: 0.785163\n",
      "  830/5000: episode: 829, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.000, 0.882], loss: 102332.562500, mean_absolute_error: 89.732140, mean_q: 0.786082\n",
      "  831/5000: episode: 830, duration: 0.019s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 29294.285156, mean_absolute_error: 41.104774, mean_q: 0.786888\n",
      "  832/5000: episode: 831, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.065, 1.000], loss: 14595.625977, mean_absolute_error: 31.338158, mean_q: 0.788881\n",
      "  833/5000: episode: 832, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.255, 1.000], loss: 43869.781250, mean_absolute_error: 50.881317, mean_q: 0.790853\n",
      "  834/5000: episode: 833, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.552], loss: 29369.201172, mean_absolute_error: 41.219151, mean_q: 0.791650\n",
      "  835/5000: episode: 834, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.056, 1.000], loss: 58645.402344, mean_absolute_error: 60.748924, mean_q: 0.793519\n",
      "  836/5000: episode: 835, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.187, 1.000], loss: 73134.257812, mean_absolute_error: 70.405602, mean_q: 0.795479\n",
      "  837/5000: episode: 836, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.551], loss: 29182.246094, mean_absolute_error: 41.131470, mean_q: 0.796866\n",
      "  838/5000: episode: 837, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.737], loss: 29274.402344, mean_absolute_error: 41.222424, mean_q: 0.799359\n",
      "  839/5000: episode: 838, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.174, 1.000], loss: 14585.945312, mean_absolute_error: 31.428923, mean_q: 0.800906\n",
      "  840/5000: episode: 839, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.447, 1.000], loss: 73117.156250, mean_absolute_error: 70.448463, mean_q: 0.802773\n",
      "  841/5000: episode: 840, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.529 [0.003, 1.000], loss: 73209.242188, mean_absolute_error: 70.541809, mean_q: 0.804602\n",
      "  842/5000: episode: 841, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.751], loss: 43844.234375, mean_absolute_error: 50.995667, mean_q: 0.805853\n",
      "  843/5000: episode: 842, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.326128, mean_absolute_error: 21.818130, mean_q: 0.806602\n",
      "  844/5000: episode: 843, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.565], loss: 29258.144531, mean_absolute_error: 41.327950, mean_q: 0.808276\n",
      "  845/5000: episode: 844, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.420, 1.000], loss: 43836.382812, mean_absolute_error: 51.035103, mean_q: 0.810041\n",
      "  846/5000: episode: 845, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.396, 1.000], loss: 72993.851562, mean_absolute_error: 70.476379, mean_q: 0.810463\n",
      "  847/5000: episode: 846, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.219, 1.000], loss: 43929.593750, mean_absolute_error: 51.142242, mean_q: 0.811557\n",
      "  848/5000: episode: 847, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.580], loss: 117027.570312, mean_absolute_error: 99.816345, mean_q: 0.812481\n",
      "  849/5000: episode: 848, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.784], loss: 0.332181, mean_absolute_error: 21.877174, mean_q: 0.814028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  850/5000: episode: 849, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.918], loss: 73075.078125, mean_absolute_error: 70.590820, mean_q: 0.816802\n",
      "  851/5000: episode: 850, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.197, 1.000], loss: 29345.203125, mean_absolute_error: 41.491890, mean_q: 0.818408\n",
      "  852/5000: episode: 851, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.056, 1.000], loss: 43827.402344, mean_absolute_error: 51.154873, mean_q: 0.820679\n",
      "  853/5000: episode: 852, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.614 [0.037, 1.000], loss: 14671.793945, mean_absolute_error: 31.764261, mean_q: 0.822971\n",
      "  854/5000: episode: 853, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.133, 1.000], loss: 43721.359375, mean_absolute_error: 51.106247, mean_q: 0.825123\n",
      "  855/5000: episode: 854, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.051, 1.000], loss: 58486.984375, mean_absolute_error: 60.959244, mean_q: 0.826339\n",
      "  856/5000: episode: 855, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.485, 1.000], loss: 58507.921875, mean_absolute_error: 60.977211, mean_q: 0.827798\n",
      "  857/5000: episode: 856, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 58479.980469, mean_absolute_error: 61.006584, mean_q: 0.829948\n",
      "  858/5000: episode: 857, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.269, 1.000], loss: 43715.742188, mean_absolute_error: 51.169746, mean_q: 0.831450\n",
      "  859/5000: episode: 858, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.447, 1.000], loss: 29237.328125, mean_absolute_error: 41.549305, mean_q: 0.832434\n",
      "  860/5000: episode: 859, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.103, 1.000], loss: 43804.718750, mean_absolute_error: 51.244560, mean_q: 0.833301\n",
      "  861/5000: episode: 860, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.948], loss: 29240.792969, mean_absolute_error: 41.574089, mean_q: 0.834187\n",
      "  862/5000: episode: 861, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.425, 1.000], loss: 43815.929688, mean_absolute_error: 51.263672, mean_q: 0.835267\n",
      "  863/5000: episode: 862, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.814], loss: 43701.035156, mean_absolute_error: 51.244297, mean_q: 0.837899\n",
      "  864/5000: episode: 863, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.702], loss: 29235.365234, mean_absolute_error: 41.634113, mean_q: 0.839644\n",
      "  865/5000: episode: 864, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.467, 1.000], loss: 72928.742188, mean_absolute_error: 70.742966, mean_q: 0.841374\n",
      "  866/5000: episode: 865, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.000, 0.786], loss: 29129.558594, mean_absolute_error: 41.569317, mean_q: 0.841815\n",
      "  867/5000: episode: 866, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.244, 1.000], loss: 29128.039062, mean_absolute_error: 41.604408, mean_q: 0.843060\n",
      "  868/5000: episode: 867, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.897], loss: 14661.077148, mean_absolute_error: 31.979507, mean_q: 0.843148\n",
      "  869/5000: episode: 868, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.332, 1.000], loss: 87769.296875, mean_absolute_error: 80.700180, mean_q: 0.844575\n",
      "  870/5000: episode: 869, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.038, 1.000], loss: 72905.539062, mean_absolute_error: 70.810211, mean_q: 0.845958\n",
      "  871/5000: episode: 870, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 29129.082031, mean_absolute_error: 41.678699, mean_q: 0.847625\n",
      "  872/5000: episode: 871, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.242, 1.000], loss: 29319.748047, mean_absolute_error: 41.794689, mean_q: 0.849501\n",
      "  873/5000: episode: 872, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.485, 1.000], loss: 43872.652344, mean_absolute_error: 51.528084, mean_q: 0.850515\n",
      "  874/5000: episode: 873, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.477, 1.000], loss: 43778.273438, mean_absolute_error: 51.490097, mean_q: 0.853225\n",
      "  875/5000: episode: 874, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.000, 0.957], loss: 72905.414062, mean_absolute_error: 70.892578, mean_q: 0.855013\n",
      "  876/5000: episode: 875, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.132, 1.000], loss: 14556.246094, mean_absolute_error: 32.057404, mean_q: 0.856101\n",
      "  877/5000: episode: 876, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.477, 1.000], loss: 0.368782, mean_absolute_error: 22.376743, mean_q: 0.857784\n",
      "  878/5000: episode: 877, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 0.882], loss: 58316.000000, mean_absolute_error: 61.243061, mean_q: 0.859616\n",
      "  879/5000: episode: 878, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.529 [0.003, 1.000], loss: 29211.382812, mean_absolute_error: 41.866432, mean_q: 0.860488\n",
      "  880/5000: episode: 879, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.000, 0.847], loss: 58412.085938, mean_absolute_error: 61.349037, mean_q: 0.862480\n",
      "  881/5000: episode: 880, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.937], loss: 0.373266, mean_absolute_error: 22.435247, mean_q: 0.862991\n",
      "  882/5000: episode: 881, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.583], loss: 29113.574219, mean_absolute_error: 41.850773, mean_q: 0.865287\n",
      "  883/5000: episode: 882, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.300, 1.000], loss: 87524.882812, mean_absolute_error: 80.748497, mean_q: 0.866428\n",
      "  884/5000: episode: 883, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.787], loss: 29199.449219, mean_absolute_error: 41.956619, mean_q: 0.869463\n",
      "  885/5000: episode: 884, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 43951.800781, mean_absolute_error: 51.791405, mean_q: 0.870402\n",
      "  886/5000: episode: 885, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.490 [0.000, 0.999], loss: 14647.855469, mean_absolute_error: 32.294289, mean_q: 0.872661\n",
      "  887/5000: episode: 886, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.442, 1.000], loss: 14552.859375, mean_absolute_error: 32.240814, mean_q: 0.874745\n",
      "  888/5000: episode: 887, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.000, 0.999], loss: 0.384356, mean_absolute_error: 22.537937, mean_q: 0.875729\n",
      "  889/5000: episode: 888, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 43649.429688, mean_absolute_error: 51.625938, mean_q: 0.877155\n",
      "  890/5000: episode: 889, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.621 [0.042, 1.000], loss: 87582.195312, mean_absolute_error: 80.924759, mean_q: 0.879322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  891/5000: episode: 890, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.000, 0.961], loss: 43639.925781, mean_absolute_error: 51.656807, mean_q: 0.880305\n",
      "  892/5000: episode: 891, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.258, 1.000], loss: 58377.859375, mean_absolute_error: 61.499336, mean_q: 0.881021\n",
      "  893/5000: episode: 892, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.838], loss: 87568.414062, mean_absolute_error: 80.964096, mean_q: 0.882074\n",
      "  894/5000: episode: 893, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.918], loss: 43728.546875, mean_absolute_error: 51.789558, mean_q: 0.883118\n",
      "  895/5000: episode: 894, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.891], loss: 43930.925781, mean_absolute_error: 51.929466, mean_q: 0.884105\n",
      "  896/5000: episode: 895, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.475 [0.000, 0.997], loss: 43822.683594, mean_absolute_error: 51.890602, mean_q: 0.886189\n",
      "  897/5000: episode: 896, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.000, 0.942], loss: 43721.328125, mean_absolute_error: 51.835678, mean_q: 0.887998\n",
      "  898/5000: episode: 897, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.030, 1.000], loss: 29178.992188, mean_absolute_error: 42.171120, mean_q: 0.889857\n",
      "  899/5000: episode: 898, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.610], loss: 29078.625000, mean_absolute_error: 42.099770, mean_q: 0.891919\n",
      "  900/5000: episode: 899, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.660 [0.074, 1.000], loss: 14540.101562, mean_absolute_error: 32.459240, mean_q: 0.893023\n",
      "  901/5000: episode: 900, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.000, 0.822], loss: 43811.460938, mean_absolute_error: 51.964340, mean_q: 0.893620\n",
      "  902/5000: episode: 901, duration: 0.017s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.000, 0.931], loss: 43715.343750, mean_absolute_error: 51.910156, mean_q: 0.894768\n",
      "  903/5000: episode: 902, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.454, 1.000], loss: 43814.835938, mean_absolute_error: 51.995159, mean_q: 0.895704\n",
      "  904/5000: episode: 903, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.544], loss: 43808.320312, mean_absolute_error: 52.007133, mean_q: 0.897612\n",
      "  905/5000: episode: 904, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.119, 1.000], loss: 29169.078125, mean_absolute_error: 42.273148, mean_q: 0.898589\n",
      "  906/5000: episode: 905, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.335, 1.000], loss: 58335.949219, mean_absolute_error: 61.720119, mean_q: 0.900429\n",
      "  907/5000: episode: 906, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.283, 1.000], loss: 72782.992188, mean_absolute_error: 71.308189, mean_q: 0.900509\n",
      "  908/5000: episode: 907, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.825], loss: 58238.800781, mean_absolute_error: 61.655998, mean_q: 0.902128\n",
      "  909/5000: episode: 908, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.371, 1.000], loss: 72761.500000, mean_absolute_error: 71.354965, mean_q: 0.903230\n",
      "  910/5000: episode: 909, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.440, 1.000], loss: 58325.031250, mean_absolute_error: 61.756561, mean_q: 0.903911\n",
      "  911/5000: episode: 910, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 73052.703125, mean_absolute_error: 71.598206, mean_q: 0.905042\n",
      "  912/5000: episode: 911, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.337, 1.000], loss: 73155.117188, mean_absolute_error: 71.686035, mean_q: 0.906642\n",
      "  913/5000: episode: 912, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.000, 0.989], loss: 43584.972656, mean_absolute_error: 52.008591, mean_q: 0.909035\n",
      "  914/5000: episode: 913, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.000, 0.991], loss: 29156.207031, mean_absolute_error: 42.429344, mean_q: 0.911028\n",
      "  915/5000: episode: 914, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.616], loss: 87574.523438, mean_absolute_error: 81.325562, mean_q: 0.912054\n",
      "  916/5000: episode: 915, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.802], loss: 58418.164062, mean_absolute_error: 61.922989, mean_q: 0.913866\n",
      "  917/5000: episode: 916, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.507], loss: 43676.382812, mean_absolute_error: 52.145649, mean_q: 0.916884\n",
      "  918/5000: episode: 917, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.000, 0.930], loss: 43684.351562, mean_absolute_error: 52.148911, mean_q: 0.918191\n",
      "  919/5000: episode: 918, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.877], loss: 29156.636719, mean_absolute_error: 42.514580, mean_q: 0.920698\n",
      "  920/5000: episode: 919, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.061, 1.000], loss: 14623.739258, mean_absolute_error: 32.852730, mean_q: 0.922397\n",
      "  921/5000: episode: 920, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.349, 1.000], loss: 14622.265625, mean_absolute_error: 32.863976, mean_q: 0.923775\n",
      "  922/5000: episode: 921, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.732], loss: 29143.851562, mean_absolute_error: 42.550865, mean_q: 0.925486\n",
      "  923/5000: episode: 922, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.427, 1.000], loss: 43662.972656, mean_absolute_error: 52.233032, mean_q: 0.926369\n",
      "  924/5000: episode: 923, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.696 [0.114, 1.000], loss: 29043.798828, mean_absolute_error: 42.512383, mean_q: 0.927918\n",
      "  925/5000: episode: 924, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.315, 1.000], loss: 43771.445312, mean_absolute_error: 52.325653, mean_q: 0.928948\n",
      "  926/5000: episode: 925, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.030, 1.000], loss: 87320.796875, mean_absolute_error: 81.333572, mean_q: 0.929725\n",
      "  927/5000: episode: 926, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 29136.992188, mean_absolute_error: 42.616028, mean_q: 0.930770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  928/5000: episode: 927, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.022, 1.000], loss: 14617.686523, mean_absolute_error: 32.955376, mean_q: 0.932100\n",
      "  929/5000: episode: 928, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.607 [0.033, 1.000], loss: 58168.453125, mean_absolute_error: 61.977768, mean_q: 0.934254\n",
      "  930/5000: episode: 929, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.610], loss: 29133.763672, mean_absolute_error: 42.654114, mean_q: 0.936451\n",
      "  931/5000: episode: 930, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.290 [0.000, 0.867], loss: 58063.218750, mean_absolute_error: 61.936089, mean_q: 0.937577\n",
      "  932/5000: episode: 931, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.226 [0.000, 0.744], loss: 43644.183594, mean_absolute_error: 52.348640, mean_q: 0.938264\n",
      "  933/5000: episode: 932, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.177, 1.000], loss: 43544.476562, mean_absolute_error: 52.297729, mean_q: 0.939401\n",
      "  934/5000: episode: 933, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.407, 1.000], loss: 58152.355469, mean_absolute_error: 62.062046, mean_q: 0.941047\n",
      "  935/5000: episode: 934, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.770], loss: 29124.669922, mean_absolute_error: 42.744331, mean_q: 0.942003\n",
      "  936/5000: episode: 935, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.507 [0.001, 1.000], loss: 14611.995117, mean_absolute_error: 33.101440, mean_q: 0.943870\n",
      "  937/5000: episode: 936, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.376, 1.000], loss: 29121.667969, mean_absolute_error: 42.788269, mean_q: 0.945638\n",
      "  938/5000: episode: 937, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.000, 0.765], loss: 29019.843750, mean_absolute_error: 42.712006, mean_q: 0.946777\n",
      "  939/5000: episode: 938, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.024, 1.000], loss: 43728.656250, mean_absolute_error: 52.546658, mean_q: 0.949308\n",
      "  940/5000: episode: 939, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.374, 1.000], loss: 29121.554688, mean_absolute_error: 42.838921, mean_q: 0.951121\n",
      "  941/5000: episode: 940, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.428, 1.000], loss: 72751.375000, mean_absolute_error: 71.902794, mean_q: 0.952581\n",
      "  942/5000: episode: 941, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.230, 1.000], loss: 43627.734375, mean_absolute_error: 52.536064, mean_q: 0.954056\n",
      "  943/5000: episode: 942, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.000, 1.000], loss: 72729.609375, mean_absolute_error: 71.935547, mean_q: 0.954970\n",
      "  944/5000: episode: 943, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.000, 0.999], loss: 58334.980469, mean_absolute_error: 62.348518, mean_q: 0.955346\n",
      "  945/5000: episode: 944, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.470, 1.000], loss: 43622.011719, mean_absolute_error: 52.566330, mean_q: 0.956604\n",
      "  946/5000: episode: 945, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.052, 1.000], loss: 58116.324219, mean_absolute_error: 62.256577, mean_q: 0.957766\n",
      "  947/5000: episode: 946, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 58112.625000, mean_absolute_error: 62.265079, mean_q: 0.958312\n",
      "  948/5000: episode: 947, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.675], loss: 43500.441406, mean_absolute_error: 52.558464, mean_q: 0.959178\n",
      "  949/5000: episode: 948, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.774], loss: 14611.210938, mean_absolute_error: 33.344948, mean_q: 0.959543\n",
      "  950/5000: episode: 949, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.589 [0.024, 1.000], loss: 43516.679688, mean_absolute_error: 52.596260, mean_q: 0.960067\n",
      "  951/5000: episode: 950, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.406, 1.000], loss: 14601.871094, mean_absolute_error: 33.375664, mean_q: 0.960841\n",
      "  952/5000: episode: 951, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.061, 1.000], loss: 14499.106445, mean_absolute_error: 33.312271, mean_q: 0.961705\n",
      "  953/5000: episode: 952, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.608 [0.034, 1.000], loss: 57988.675781, mean_absolute_error: 62.277145, mean_q: 0.962097\n",
      "  954/5000: episode: 953, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.000, 0.975], loss: 101782.054688, mean_absolute_error: 91.457169, mean_q: 0.963083\n",
      "  955/5000: episode: 954, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.409, 1.000], loss: 58085.742188, mean_absolute_error: 62.393303, mean_q: 0.963909\n",
      "  956/5000: episode: 955, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.329, 1.000], loss: 43484.000000, mean_absolute_error: 52.644985, mean_q: 0.963842\n",
      "  957/5000: episode: 956, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.761], loss: 58077.617188, mean_absolute_error: 62.398548, mean_q: 0.964927\n",
      "  958/5000: episode: 957, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.000, 0.830], loss: 58181.296875, mean_absolute_error: 62.509514, mean_q: 0.966167\n",
      "  959/5000: episode: 958, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 14597.475586, mean_absolute_error: 33.497391, mean_q: 0.966492\n",
      "  960/5000: episode: 959, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.000, 1.000], loss: 43588.933594, mean_absolute_error: 52.821297, mean_q: 0.968100\n",
      "  961/5000: episode: 960, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.606], loss: 58274.578125, mean_absolute_error: 62.618870, mean_q: 0.969490\n",
      "  962/5000: episode: 961, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 43461.359375, mean_absolute_error: 52.774544, mean_q: 0.971715\n",
      "  963/5000: episode: 962, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.270, 1.000], loss: 43570.042969, mean_absolute_error: 52.874741, mean_q: 0.973681\n",
      "  964/5000: episode: 963, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.089, 1.000], loss: 58164.519531, mean_absolute_error: 62.593479, mean_q: 0.974626\n",
      "  965/5000: episode: 964, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.575 [0.017, 1.000], loss: 14484.990234, mean_absolute_error: 33.558098, mean_q: 0.977020\n",
      "  966/5000: episode: 965, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.947], loss: 28969.357422, mean_absolute_error: 43.188152, mean_q: 0.977987\n",
      "  967/5000: episode: 966, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.000, 0.972], loss: 58149.839844, mean_absolute_error: 62.645187, mean_q: 0.979330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  968/5000: episode: 967, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.291, 1.000], loss: 14482.774414, mean_absolute_error: 33.573399, mean_q: 0.980382\n",
      "  969/5000: episode: 968, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.000, 0.889], loss: 43446.156250, mean_absolute_error: 52.885487, mean_q: 0.982018\n",
      "  970/5000: episode: 969, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.000, 1.000], loss: 43658.304688, mean_absolute_error: 53.062599, mean_q: 0.983143\n",
      "  971/5000: episode: 970, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.312 [0.000, 0.897], loss: 43439.820312, mean_absolute_error: 52.921154, mean_q: 0.983959\n",
      "  972/5000: episode: 971, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.667], loss: 58026.710938, mean_absolute_error: 62.653877, mean_q: 0.984722\n",
      "  973/5000: episode: 972, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.860], loss: 14478.416992, mean_absolute_error: 33.683418, mean_q: 0.985478\n",
      "  974/5000: episode: 973, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.517 [0.001, 1.000], loss: 58127.195312, mean_absolute_error: 62.752224, mean_q: 0.985672\n",
      "  975/5000: episode: 974, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.060, 1.000], loss: 43651.027344, mean_absolute_error: 53.134102, mean_q: 0.986311\n",
      "  976/5000: episode: 975, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.477, 1.000], loss: 58130.507812, mean_absolute_error: 62.775646, mean_q: 0.986894\n",
      "  977/5000: episode: 976, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.292, 1.000], loss: 29172.722656, mean_absolute_error: 43.521893, mean_q: 0.988093\n",
      "  978/5000: episode: 977, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.894], loss: 72596.078125, mean_absolute_error: 72.443253, mean_q: 0.988639\n",
      "  979/5000: episode: 978, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.241, 1.000], loss: 58113.609375, mean_absolute_error: 62.823296, mean_q: 0.989937\n",
      "  980/5000: episode: 979, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.730 [0.162, 1.000], loss: 29058.244141, mean_absolute_error: 43.490021, mean_q: 0.992089\n",
      "  981/5000: episode: 980, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.750], loss: 0.495099, mean_absolute_error: 24.174078, mean_q: 0.994056\n",
      "  982/5000: episode: 981, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.226, 1.000], loss: 43419.429688, mean_absolute_error: 53.090927, mean_q: 0.995583\n",
      "  983/5000: episode: 982, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.000, 0.950], loss: 29160.117188, mean_absolute_error: 43.631886, mean_q: 0.997745\n",
      "  984/5000: episode: 983, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.430, 1.000], loss: 14469.178711, mean_absolute_error: 33.866798, mean_q: 0.999569\n",
      "  985/5000: episode: 984, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.004, 1.000], loss: 29049.664062, mean_absolute_error: 43.577614, mean_q: 1.000646\n",
      "  986/5000: episode: 985, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.324, 1.000], loss: 72353.531250, mean_absolute_error: 72.435806, mean_q: 1.003019\n",
      "  987/5000: episode: 986, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.066, 1.000], loss: 43629.937500, mean_absolute_error: 53.314129, mean_q: 1.004278\n",
      "  988/5000: episode: 987, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.520], loss: 43523.453125, mean_absolute_error: 53.268532, mean_q: 1.006004\n",
      "  989/5000: episode: 988, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.603], loss: 29047.140625, mean_absolute_error: 43.659409, mean_q: 1.007843\n",
      "  990/5000: episode: 989, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.044, 1.000], loss: 43503.796875, mean_absolute_error: 53.293839, mean_q: 1.009063\n",
      "  991/5000: episode: 990, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.147, 1.000], loss: 14573.655273, mean_absolute_error: 34.042953, mean_q: 1.010358\n",
      "  992/5000: episode: 991, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.450, 1.000], loss: 14574.500000, mean_absolute_error: 34.045158, mean_q: 1.011053\n",
      "  993/5000: episode: 992, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.510, 1.000], loss: 14464.117188, mean_absolute_error: 33.989075, mean_q: 1.012748\n",
      "  994/5000: episode: 993, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.000, 1.000], loss: 43502.773438, mean_absolute_error: 53.330765, mean_q: 1.013912\n",
      "  995/5000: episode: 994, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.000, 0.998], loss: 58065.308594, mean_absolute_error: 63.044231, mean_q: 1.014780\n",
      "  996/5000: episode: 995, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.918], loss: 57955.125000, mean_absolute_error: 62.993843, mean_q: 1.016463\n",
      "  997/5000: episode: 996, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.494, 1.000], loss: 14460.763672, mean_absolute_error: 34.058609, mean_q: 1.017179\n",
      "  998/5000: episode: 997, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.443, 1.000], loss: 57954.808594, mean_absolute_error: 63.006977, mean_q: 1.017183\n",
      "  999/5000: episode: 998, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.144, 1.000], loss: 72517.984375, mean_absolute_error: 72.741623, mean_q: 1.019347\n",
      " 1000/5000: episode: 999, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 72405.898438, mean_absolute_error: 72.684875, mean_q: 1.019766\n",
      " 1001/5000: episode: 1000, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.812], loss: 72511.734375, mean_absolute_error: 72.772430, mean_q: 1.020876\n",
      " 1002/5000: episode: 1001, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.515], loss: 14458.210938, mean_absolute_error: 34.151558, mean_q: 1.021772\n",
      " 1003/5000: episode: 1002, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 14458.103516, mean_absolute_error: 34.182297, mean_q: 1.023217\n",
      " 1004/5000: episode: 1003, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.802], loss: 58052.214844, mean_absolute_error: 63.206398, mean_q: 1.023296\n",
      " 1005/5000: episode: 1004, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.317 [0.000, 0.902], loss: 28914.330078, mean_absolute_error: 43.826286, mean_q: 1.024697\n",
      " 1006/5000: episode: 1005, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.000, 0.922], loss: 43373.039062, mean_absolute_error: 53.457138, mean_q: 1.025001\n",
      " 1007/5000: episode: 1006, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.812], loss: 43581.765625, mean_absolute_error: 53.617447, mean_q: 1.025564\n",
      " 1008/5000: episode: 1007, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.040, 1.000], loss: 29015.507812, mean_absolute_error: 43.927391, mean_q: 1.025230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1009/5000: episode: 1008, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 0.527967, mean_absolute_error: 24.637964, mean_q: 1.026579\n",
      " 1010/5000: episode: 1009, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 58141.921875, mean_absolute_error: 63.355164, mean_q: 1.026629\n",
      " 1011/5000: episode: 1010, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.000, 0.889], loss: 29127.871094, mean_absolute_error: 44.043453, mean_q: 1.027065\n",
      " 1012/5000: episode: 1011, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.298, 1.000], loss: 57903.148438, mean_absolute_error: 63.235764, mean_q: 1.028362\n",
      " 1013/5000: episode: 1012, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.191, 1.000], loss: 29007.695312, mean_absolute_error: 43.985813, mean_q: 1.028037\n",
      " 1014/5000: episode: 1013, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.207, 1.000], loss: 58011.144531, mean_absolute_error: 63.337105, mean_q: 1.030111\n",
      " 1015/5000: episode: 1014, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.541], loss: 29118.220703, mean_absolute_error: 44.094276, mean_q: 1.030897\n",
      " 1016/5000: episode: 1015, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.060, 1.000], loss: 72449.531250, mean_absolute_error: 72.968086, mean_q: 1.032746\n",
      " 1017/5000: episode: 1016, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 43446.195312, mean_absolute_error: 53.693855, mean_q: 1.034748\n",
      " 1018/5000: episode: 1017, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.000, 0.887], loss: 57998.898438, mean_absolute_error: 63.399937, mean_q: 1.036243\n",
      " 1019/5000: episode: 1018, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.335, 1.000], loss: 28883.957031, mean_absolute_error: 44.022270, mean_q: 1.037228\n",
      " 1020/5000: episode: 1019, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.690 [0.107, 1.000], loss: 28995.941406, mean_absolute_error: 44.103466, mean_q: 1.038324\n",
      " 1021/5000: episode: 1020, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.000, 0.882], loss: 28995.314453, mean_absolute_error: 44.128922, mean_q: 1.039579\n",
      " 1022/5000: episode: 1021, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 28994.830078, mean_absolute_error: 44.135654, mean_q: 1.040499\n",
      " 1023/5000: episode: 1022, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.026, 1.000], loss: 43431.652344, mean_absolute_error: 53.784584, mean_q: 1.042615\n",
      " 1024/5000: episode: 1023, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 43316.085938, mean_absolute_error: 53.725372, mean_q: 1.043985\n",
      " 1025/5000: episode: 1024, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.132, 1.000], loss: 86852.242188, mean_absolute_error: 82.715012, mean_q: 1.044770\n",
      " 1026/5000: episode: 1025, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.000, 0.988], loss: 43312.757812, mean_absolute_error: 53.745392, mean_q: 1.046106\n",
      " 1027/5000: episode: 1026, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.617], loss: 43535.753906, mean_absolute_error: 53.920395, mean_q: 1.046283\n",
      " 1028/5000: episode: 1027, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.300, 1.000], loss: 57869.523438, mean_absolute_error: 63.452484, mean_q: 1.045620\n",
      " 1029/5000: episode: 1028, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.000, 0.999], loss: 28877.726562, mean_absolute_error: 44.183392, mean_q: 1.047495\n",
      " 1030/5000: episode: 1029, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.608 [0.034, 1.000], loss: 29097.121094, mean_absolute_error: 44.359596, mean_q: 1.048138\n",
      " 1031/5000: episode: 1030, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 43415.632812, mean_absolute_error: 53.900593, mean_q: 1.048803\n",
      " 1032/5000: episode: 1031, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.770], loss: 57959.023438, mean_absolute_error: 63.581024, mean_q: 1.049523\n",
      " 1033/5000: episode: 1032, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 14546.914062, mean_absolute_error: 34.702827, mean_q: 1.050391\n",
      " 1034/5000: episode: 1033, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 57951.570312, mean_absolute_error: 63.602142, mean_q: 1.051366\n",
      " 1035/5000: episode: 1034, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.343, 1.000], loss: 14429.866211, mean_absolute_error: 34.655792, mean_q: 1.054314\n",
      " 1036/5000: episode: 1035, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.614], loss: 28858.187500, mean_absolute_error: 44.297577, mean_q: 1.054935\n",
      " 1037/5000: episode: 1036, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.390, 1.000], loss: 57946.449219, mean_absolute_error: 63.653904, mean_q: 1.055100\n",
      " 1038/5000: episode: 1037, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.000, 0.976], loss: 58060.789062, mean_absolute_error: 63.752014, mean_q: 1.055878\n",
      " 1039/5000: episode: 1038, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.157, 1.000], loss: 28969.201172, mean_absolute_error: 44.387009, mean_q: 1.056383\n",
      " 1040/5000: episode: 1039, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 28967.914062, mean_absolute_error: 44.430252, mean_q: 1.058704\n",
      " 1041/5000: episode: 1040, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 14426.823242, mean_absolute_error: 34.762829, mean_q: 1.060073\n",
      " 1042/5000: episode: 1041, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.582], loss: 14424.790039, mean_absolute_error: 34.771652, mean_q: 1.061367\n",
      " 1043/5000: episode: 1042, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.508, 1.000], loss: 28964.191406, mean_absolute_error: 44.469563, mean_q: 1.062348\n",
      " 1044/5000: episode: 1043, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 28846.841797, mean_absolute_error: 44.397125, mean_q: 1.063385\n",
      " 1045/5000: episode: 1044, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.256, 1.000], loss: 0.567199, mean_absolute_error: 25.198730, mean_q: 1.064070\n",
      " 1046/5000: episode: 1045, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 28960.425781, mean_absolute_error: 44.506508, mean_q: 1.065243\n",
      " 1047/5000: episode: 1046, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.325, 1.000], loss: 14537.629883, mean_absolute_error: 34.921589, mean_q: 1.066111\n",
      " 1048/5000: episode: 1047, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 72108.953125, mean_absolute_error: 73.233963, mean_q: 1.066587\n",
      " 1049/5000: episode: 1048, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.000, 0.988], loss: 28958.351562, mean_absolute_error: 44.528130, mean_q: 1.066985\n",
      " 1050/5000: episode: 1049, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.000, 0.916], loss: 43500.082031, mean_absolute_error: 54.230625, mean_q: 1.067913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1051/5000: episode: 1050, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.730 [0.163, 1.000], loss: 43263.089844, mean_absolute_error: 54.072845, mean_q: 1.068407\n",
      " 1052/5000: episode: 1051, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.496, 1.000], loss: 14419.010742, mean_absolute_error: 34.898453, mean_q: 1.069310\n",
      " 1053/5000: episode: 1052, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.022, 1.000], loss: 43494.566406, mean_absolute_error: 54.244507, mean_q: 1.068666\n",
      " 1054/5000: episode: 1053, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.710], loss: 43484.535156, mean_absolute_error: 54.279682, mean_q: 1.070565\n",
      " 1055/5000: episode: 1054, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.751], loss: 43369.324219, mean_absolute_error: 54.195621, mean_q: 1.070227\n",
      " 1056/5000: episode: 1055, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 43485.687500, mean_absolute_error: 54.310127, mean_q: 1.071027\n",
      " 1057/5000: episode: 1056, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 57782.398438, mean_absolute_error: 63.835167, mean_q: 1.071841\n",
      " 1058/5000: episode: 1057, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 29073.478516, mean_absolute_error: 44.752625, mean_q: 1.072557\n",
      " 1059/5000: episode: 1058, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.557], loss: 43475.453125, mean_absolute_error: 54.352360, mean_q: 1.073379\n",
      " 1060/5000: episode: 1059, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.000, 0.794], loss: 29060.101562, mean_absolute_error: 44.781567, mean_q: 1.074376\n",
      " 1061/5000: episode: 1060, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 28940.695312, mean_absolute_error: 44.720432, mean_q: 1.075959\n",
      " 1062/5000: episode: 1061, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.146, 1.000], loss: 14529.306641, mean_absolute_error: 35.135857, mean_q: 1.076893\n",
      " 1063/5000: episode: 1062, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.000, 0.810], loss: 28939.630859, mean_absolute_error: 44.754059, mean_q: 1.078300\n",
      " 1064/5000: episode: 1063, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.000, 0.789], loss: 0.583106, mean_absolute_error: 25.499027, mean_q: 1.078905\n",
      " 1065/5000: episode: 1064, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.723 [0.152, 1.000], loss: 43460.796875, mean_absolute_error: 54.434792, mean_q: 1.079277\n",
      " 1066/5000: episode: 1065, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.328, 1.000], loss: 28934.515625, mean_absolute_error: 44.795681, mean_q: 1.080410\n",
      " 1067/5000: episode: 1066, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.000, 0.887], loss: 28811.906250, mean_absolute_error: 44.731373, mean_q: 1.081247\n",
      " 1068/5000: episode: 1067, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.000, 0.930], loss: 14405.271484, mean_absolute_error: 35.135326, mean_q: 1.081313\n",
      " 1069/5000: episode: 1068, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.469 [0.000, 0.996], loss: 28934.095703, mean_absolute_error: 44.843033, mean_q: 1.082427\n",
      " 1070/5000: episode: 1069, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.245, 1.000], loss: 14403.750977, mean_absolute_error: 35.177971, mean_q: 1.082706\n",
      " 1071/5000: episode: 1070, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.000, 0.807], loss: 43449.664062, mean_absolute_error: 54.534431, mean_q: 1.083747\n",
      " 1072/5000: episode: 1071, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.132, 1.000], loss: 28926.615234, mean_absolute_error: 44.888397, mean_q: 1.085225\n",
      " 1073/5000: episode: 1072, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.000, 0.892], loss: 28811.470703, mean_absolute_error: 44.818226, mean_q: 1.085940\n",
      " 1074/5000: episode: 1073, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.327, 1.000], loss: 57969.894531, mean_absolute_error: 64.244087, mean_q: 1.086248\n",
      " 1075/5000: episode: 1074, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.000, 0.851], loss: 58082.269531, mean_absolute_error: 64.342873, mean_q: 1.087414\n",
      " 1076/5000: episode: 1075, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.197, 1.000], loss: 28799.621094, mean_absolute_error: 44.839886, mean_q: 1.087922\n",
      " 1077/5000: episode: 1076, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.261 [0.000, 0.821], loss: 14400.341797, mean_absolute_error: 35.285370, mean_q: 1.089550\n",
      " 1078/5000: episode: 1077, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.000, 0.876], loss: 0.594783, mean_absolute_error: 25.705910, mean_q: 1.089665\n",
      " 1079/5000: episode: 1078, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 0.595052, mean_absolute_error: 25.713066, mean_q: 1.089912\n",
      " 1080/5000: episode: 1079, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.000, 0.975], loss: 29035.447266, mean_absolute_error: 45.065662, mean_q: 1.091397\n",
      " 1081/5000: episode: 1080, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.845], loss: 14398.168945, mean_absolute_error: 35.328812, mean_q: 1.092685\n",
      " 1082/5000: episode: 1081, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.422, 1.000], loss: 72350.070312, mean_absolute_error: 73.904045, mean_q: 1.093336\n",
      " 1083/5000: episode: 1082, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 86856.101562, mean_absolute_error: 83.592346, mean_q: 1.094935\n",
      " 1084/5000: episode: 1083, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.357, 1.000], loss: 14515.206055, mean_absolute_error: 35.442638, mean_q: 1.095566\n",
      " 1085/5000: episode: 1084, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 0.602792, mean_absolute_error: 25.790592, mean_q: 1.096967\n",
      " 1086/5000: episode: 1085, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.044, 1.000], loss: 28912.582031, mean_absolute_error: 45.044895, mean_q: 1.097556\n",
      " 1087/5000: episode: 1086, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 28906.703125, mean_absolute_error: 45.071365, mean_q: 1.098924\n",
      " 1088/5000: episode: 1087, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.483, 1.000], loss: 43307.425781, mean_absolute_error: 54.673370, mean_q: 1.100585\n",
      " 1089/5000: episode: 1088, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.076, 1.000], loss: 86594.789062, mean_absolute_error: 83.507996, mean_q: 1.102040\n",
      " 1090/5000: episode: 1089, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.148, 1.000], loss: 28902.464844, mean_absolute_error: 45.128902, mean_q: 1.102962\n",
      " 1091/5000: episode: 1090, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.000, 0.900], loss: 43292.695312, mean_absolute_error: 54.714989, mean_q: 1.103874\n",
      " 1092/5000: episode: 1091, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.582], loss: 28900.437500, mean_absolute_error: 45.144646, mean_q: 1.104559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1093/5000: episode: 1092, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.267 [0.000, 0.832], loss: 43410.367188, mean_absolute_error: 54.824921, mean_q: 1.105809\n",
      " 1094/5000: episode: 1093, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.046, 1.000], loss: 43407.722656, mean_absolute_error: 54.835537, mean_q: 1.107397\n",
      " 1095/5000: episode: 1094, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.501, 1.000], loss: 57790.457031, mean_absolute_error: 64.418358, mean_q: 1.108513\n",
      " 1096/5000: episode: 1095, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.259, 1.000], loss: 43399.812500, mean_absolute_error: 54.856407, mean_q: 1.109211\n",
      " 1097/5000: episode: 1096, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.000, 0.997], loss: 57800.781250, mean_absolute_error: 64.434738, mean_q: 1.110795\n",
      " 1098/5000: episode: 1097, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 29017.210938, mean_absolute_error: 45.310764, mean_q: 1.112538\n",
      " 1099/5000: episode: 1098, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.897], loss: 43397.496094, mean_absolute_error: 54.925663, mean_q: 1.115423\n",
      " 1100/5000: episode: 1099, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.385, 1.000], loss: 43276.433594, mean_absolute_error: 54.844997, mean_q: 1.117055\n",
      " 1101/5000: episode: 1100, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.068, 1.000], loss: 57779.101562, mean_absolute_error: 64.529114, mean_q: 1.119290\n",
      " 1102/5000: episode: 1101, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.000, 0.778], loss: 14500.604492, mean_absolute_error: 35.743118, mean_q: 1.120546\n",
      " 1103/5000: episode: 1102, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.030, 1.000], loss: 43265.417969, mean_absolute_error: 54.890362, mean_q: 1.122229\n",
      " 1104/5000: episode: 1103, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.632336, mean_absolute_error: 26.124466, mean_q: 1.123571\n",
      " 1105/5000: episode: 1104, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 28764.066406, mean_absolute_error: 45.280960, mean_q: 1.124629\n",
      " 1106/5000: episode: 1105, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 28879.330078, mean_absolute_error: 45.373539, mean_q: 1.125590\n",
      " 1107/5000: episode: 1106, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.739], loss: 57873.875000, mean_absolute_error: 64.706245, mean_q: 1.127491\n",
      " 1108/5000: episode: 1107, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.226, 1.000], loss: 43487.593750, mean_absolute_error: 55.155037, mean_q: 1.129407\n",
      " 1109/5000: episode: 1108, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.509, 1.000], loss: 14495.003906, mean_absolute_error: 35.881248, mean_q: 1.131448\n",
      " 1110/5000: episode: 1109, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.274 [0.000, 0.843], loss: 14493.747070, mean_absolute_error: 35.887638, mean_q: 1.132474\n",
      " 1111/5000: episode: 1110, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.610], loss: 14377.564453, mean_absolute_error: 35.818104, mean_q: 1.134001\n",
      " 1112/5000: episode: 1111, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.620], loss: 43376.750000, mean_absolute_error: 55.122219, mean_q: 1.134651\n",
      " 1113/5000: episode: 1112, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 28869.062500, mean_absolute_error: 45.493435, mean_q: 1.136204\n",
      " 1114/5000: episode: 1113, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 71995.750000, mean_absolute_error: 74.225006, mean_q: 1.137867\n",
      " 1115/5000: episode: 1114, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.938], loss: 28864.179688, mean_absolute_error: 45.526917, mean_q: 1.138620\n",
      " 1116/5000: episode: 1115, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.490 [0.000, 0.999], loss: 57730.382812, mean_absolute_error: 64.756958, mean_q: 1.139392\n",
      " 1117/5000: episode: 1116, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.189, 1.000], loss: 14373.483398, mean_absolute_error: 35.916725, mean_q: 1.141130\n",
      " 1118/5000: episode: 1117, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 14372.874023, mean_absolute_error: 35.922375, mean_q: 1.142236\n",
      " 1119/5000: episode: 1118, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.584 [0.021, 1.000], loss: 14372.279297, mean_absolute_error: 35.928200, mean_q: 1.143330\n",
      " 1120/5000: episode: 1119, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.008, 1.000], loss: 57950.542969, mean_absolute_error: 64.978851, mean_q: 1.145064\n",
      " 1121/5000: episode: 1120, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.457 [0.000, 0.994], loss: 71855.281250, mean_absolute_error: 74.244087, mean_q: 1.146870\n",
      " 1122/5000: episode: 1121, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 57710.687500, mean_absolute_error: 64.847916, mean_q: 1.148212\n",
      " 1123/5000: episode: 1122, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.000, 0.930], loss: 14483.414062, mean_absolute_error: 36.064812, mean_q: 1.148695\n",
      " 1124/5000: episode: 1123, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.263, 1.000], loss: 43335.753906, mean_absolute_error: 55.318237, mean_q: 1.151193\n",
      " 1125/5000: episode: 1124, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.314 [0.000, 0.899], loss: 28849.322266, mean_absolute_error: 45.689289, mean_q: 1.153031\n",
      " 1126/5000: episode: 1125, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.398, 1.000], loss: 43215.406250, mean_absolute_error: 55.254463, mean_q: 1.153935\n",
      " 1127/5000: episode: 1126, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.143, 1.000], loss: 28732.939453, mean_absolute_error: 45.630234, mean_q: 1.155398\n",
      " 1128/5000: episode: 1127, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.218, 1.000], loss: 14366.140625, mean_absolute_error: 36.095741, mean_q: 1.156281\n",
      " 1129/5000: episode: 1128, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 0.669510, mean_absolute_error: 26.539209, mean_q: 1.156145\n",
      " 1130/5000: episode: 1129, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.733], loss: 28841.625000, mean_absolute_error: 45.760433, mean_q: 1.157037\n",
      " 1131/5000: episode: 1130, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.012, 1.000], loss: 0.671179, mean_absolute_error: 26.592299, mean_q: 1.157598\n",
      " 1132/5000: episode: 1131, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.358, 1.000], loss: 28844.394531, mean_absolute_error: 45.793629, mean_q: 1.157573\n",
      " 1133/5000: episode: 1132, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.281, 1.000], loss: 0.670686, mean_absolute_error: 26.596212, mean_q: 1.157166\n",
      " 1134/5000: episode: 1133, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.249, 1.000], loss: 14361.450195, mean_absolute_error: 36.183754, mean_q: 1.157929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1135/5000: episode: 1134, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 43089.015625, mean_absolute_error: 55.314606, mean_q: 1.158298\n",
      " 1136/5000: episode: 1135, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.149, 1.000], loss: 28721.142578, mean_absolute_error: 45.761223, mean_q: 1.158414\n",
      " 1137/5000: episode: 1136, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.000, 0.986], loss: 14475.087891, mean_absolute_error: 36.295258, mean_q: 1.158309\n",
      " 1138/5000: episode: 1137, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.370 [0.000, 0.951], loss: 28832.951172, mean_absolute_error: 45.865871, mean_q: 1.158943\n",
      " 1139/5000: episode: 1138, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.418, 1.000], loss: 14358.058594, mean_absolute_error: 36.238804, mean_q: 1.159487\n",
      " 1140/5000: episode: 1139, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.018, 1.000], loss: 28829.894531, mean_absolute_error: 45.877056, mean_q: 1.160093\n",
      " 1141/5000: episode: 1140, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.201, 1.000], loss: 43416.859375, mean_absolute_error: 55.634560, mean_q: 1.161790\n",
      " 1142/5000: episode: 1141, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 28826.832031, mean_absolute_error: 45.917942, mean_q: 1.162392\n",
      " 1143/5000: episode: 1142, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.298, 1.000], loss: 57536.089844, mean_absolute_error: 65.034180, mean_q: 1.163430\n",
      " 1144/5000: episode: 1143, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.452, 1.000], loss: 28939.390625, mean_absolute_error: 46.043331, mean_q: 1.165162\n",
      " 1145/5000: episode: 1144, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.484, 1.000], loss: 14469.235352, mean_absolute_error: 36.438438, mean_q: 1.166340\n",
      " 1146/5000: episode: 1145, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.578], loss: 71993.640625, mean_absolute_error: 74.711578, mean_q: 1.166710\n",
      " 1147/5000: episode: 1146, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.299, 1.000], loss: 43285.914062, mean_absolute_error: 55.650150, mean_q: 1.168216\n",
      " 1148/5000: episode: 1147, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.254, 1.000], loss: 28816.513672, mean_absolute_error: 46.048187, mean_q: 1.169468\n",
      " 1149/5000: episode: 1148, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 43280.843750, mean_absolute_error: 55.674522, mean_q: 1.169264\n",
      " 1150/5000: episode: 1149, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.396, 1.000], loss: 43279.027344, mean_absolute_error: 55.704716, mean_q: 1.170908\n",
      " 1151/5000: episode: 1150, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.617], loss: 0.687367, mean_absolute_error: 26.906403, mean_q: 1.171483\n",
      " 1152/5000: episode: 1151, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.606], loss: 43157.128906, mean_absolute_error: 55.653648, mean_q: 1.172495\n",
      " 1153/5000: episode: 1152, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.000, 0.765], loss: 43161.835938, mean_absolute_error: 55.660637, mean_q: 1.172675\n",
      " 1154/5000: episode: 1153, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.533 [0.004, 1.000], loss: 57616.535156, mean_absolute_error: 65.298325, mean_q: 1.173584\n",
      " 1155/5000: episode: 1154, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.279, 1.000], loss: 28806.777344, mean_absolute_error: 46.146038, mean_q: 1.174142\n",
      " 1156/5000: episode: 1155, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.317, 1.000], loss: 0.691518, mean_absolute_error: 26.985760, mean_q: 1.175021\n",
      " 1157/5000: episode: 1156, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.187, 1.000], loss: 43147.949219, mean_absolute_error: 55.705570, mean_q: 1.175497\n",
      " 1158/5000: episode: 1157, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 14343.068359, mean_absolute_error: 36.548447, mean_q: 1.175827\n",
      " 1159/5000: episode: 1158, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.284, 1.000], loss: 14342.409180, mean_absolute_error: 36.550388, mean_q: 1.175934\n",
      " 1160/5000: episode: 1159, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.556 [0.010, 1.000], loss: 28803.433594, mean_absolute_error: 46.215164, mean_q: 1.177562\n",
      " 1161/5000: episode: 1160, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.796], loss: 57718.949219, mean_absolute_error: 65.475227, mean_q: 1.178480\n",
      " 1162/5000: episode: 1161, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.462, 1.000], loss: 14340.645508, mean_absolute_error: 36.584579, mean_q: 1.178750\n",
      " 1163/5000: episode: 1162, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 28914.589844, mean_absolute_error: 46.325218, mean_q: 1.179861\n",
      " 1164/5000: episode: 1163, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.068, 1.000], loss: 14456.990234, mean_absolute_error: 36.725723, mean_q: 1.181096\n",
      " 1165/5000: episode: 1164, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.548], loss: 14338.911133, mean_absolute_error: 36.616756, mean_q: 1.182357\n",
      " 1166/5000: episode: 1165, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.504 [0.000, 1.000], loss: 43371.679688, mean_absolute_error: 55.991631, mean_q: 1.182436\n",
      " 1167/5000: episode: 1166, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.000, 0.951], loss: 72053.695312, mean_absolute_error: 75.086044, mean_q: 1.183553\n",
      " 1168/5000: episode: 1167, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.007, 1.000], loss: 28673.644531, mean_absolute_error: 46.234619, mean_q: 1.185185\n",
      " 1169/5000: episode: 1168, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.064, 1.000], loss: 43256.187500, mean_absolute_error: 55.956924, mean_q: 1.186273\n",
      " 1170/5000: episode: 1169, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.489], loss: 57458.941406, mean_absolute_error: 65.423912, mean_q: 1.187568\n",
      " 1171/5000: episode: 1170, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.031, 1.000], loss: 28669.542969, mean_absolute_error: 46.281078, mean_q: 1.188717\n",
      " 1172/5000: episode: 1171, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.731], loss: 57578.324219, mean_absolute_error: 65.533112, mean_q: 1.189457\n",
      " 1173/5000: episode: 1172, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.575], loss: 14333.657227, mean_absolute_error: 36.769489, mean_q: 1.190352\n",
      " 1174/5000: episode: 1173, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.325, 1.000], loss: 28784.800781, mean_absolute_error: 46.409363, mean_q: 1.191373\n",
      " 1175/5000: episode: 1174, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.683 [0.098, 1.000], loss: 57563.171875, mean_absolute_error: 65.577644, mean_q: 1.192472\n",
      " 1176/5000: episode: 1175, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.737], loss: 14331.652344, mean_absolute_error: 36.811531, mean_q: 1.192559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1177/5000: episode: 1176, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.000, 0.764], loss: 57322.820312, mean_absolute_error: 65.427422, mean_q: 1.193339\n",
      " 1178/5000: episode: 1177, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.505, 1.000], loss: 28902.015625, mean_absolute_error: 46.540855, mean_q: 1.193409\n",
      " 1179/5000: episode: 1178, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.160, 1.000], loss: 0.714388, mean_absolute_error: 27.336483, mean_q: 1.194308\n",
      " 1180/5000: episode: 1179, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.948], loss: 43101.382812, mean_absolute_error: 56.016251, mean_q: 1.193899\n",
      " 1181/5000: episode: 1180, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.494], loss: 14329.215820, mean_absolute_error: 36.876312, mean_q: 1.193391\n",
      " 1182/5000: episode: 1181, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.530], loss: 86195.796875, mean_absolute_error: 84.717934, mean_q: 1.193674\n",
      " 1183/5000: episode: 1182, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.452, 1.000], loss: 43214.695312, mean_absolute_error: 56.152412, mean_q: 1.193814\n",
      " 1184/5000: episode: 1183, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.031, 1.000], loss: 28768.853516, mean_absolute_error: 46.525429, mean_q: 1.193507\n",
      " 1185/5000: episode: 1184, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.427 [0.000, 0.984], loss: 14444.066406, mean_absolute_error: 37.021473, mean_q: 1.194409\n",
      " 1186/5000: episode: 1185, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.051, 1.000], loss: 43217.484375, mean_absolute_error: 56.167339, mean_q: 1.194137\n",
      " 1187/5000: episode: 1186, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.060, 1.000], loss: 14442.609375, mean_absolute_error: 37.050442, mean_q: 1.195994\n",
      " 1188/5000: episode: 1187, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.000, 0.989], loss: 0.716857, mean_absolute_error: 27.443529, mean_q: 1.196370\n",
      " 1189/5000: episode: 1188, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.185, 1.000], loss: 42965.351562, mean_absolute_error: 56.046951, mean_q: 1.196951\n",
      " 1190/5000: episode: 1189, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.958], loss: 14446.425781, mean_absolute_error: 37.089264, mean_q: 1.197037\n",
      " 1191/5000: episode: 1190, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 28882.542969, mean_absolute_error: 46.707172, mean_q: 1.196909\n",
      " 1192/5000: episode: 1191, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.215, 1.000], loss: 43082.070312, mean_absolute_error: 56.166584, mean_q: 1.197461\n",
      " 1193/5000: episode: 1192, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.342 [0.000, 0.928], loss: 14320.051758, mean_absolute_error: 37.035095, mean_q: 1.197623\n",
      " 1194/5000: episode: 1193, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.522 [0.002, 1.000], loss: 14319.548828, mean_absolute_error: 37.037922, mean_q: 1.197468\n",
      " 1195/5000: episode: 1194, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.001, 1.000], loss: 42965.281250, mean_absolute_error: 56.118027, mean_q: 1.198088\n",
      " 1196/5000: episode: 1195, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.405 [0.000, 0.974], loss: 28756.093750, mean_absolute_error: 46.665806, mean_q: 1.197675\n",
      " 1197/5000: episode: 1196, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.130, 1.000], loss: 43197.929688, mean_absolute_error: 56.278206, mean_q: 1.196828\n",
      " 1198/5000: episode: 1197, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.445, 1.000], loss: 14444.054688, mean_absolute_error: 37.175571, mean_q: 1.197889\n",
      " 1199/5000: episode: 1198, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.000, 1.000], loss: 14436.982422, mean_absolute_error: 37.176956, mean_q: 1.198459\n",
      " 1200/5000: episode: 1199, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 14442.718750, mean_absolute_error: 37.211056, mean_q: 1.199681\n",
      " 1201/5000: episode: 1200, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.417 [0.000, 0.980], loss: 43069.453125, mean_absolute_error: 56.248283, mean_q: 1.200050\n",
      " 1202/5000: episode: 1201, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.443, 1.000], loss: 14317.650391, mean_absolute_error: 37.143108, mean_q: 1.201104\n",
      " 1203/5000: episode: 1202, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.007, 1.000], loss: 28747.925781, mean_absolute_error: 46.750847, mean_q: 1.201205\n",
      " 1204/5000: episode: 1203, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.634], loss: 57252.968750, mean_absolute_error: 65.728500, mean_q: 1.201804\n",
      " 1205/5000: episode: 1204, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.680], loss: 43184.800781, mean_absolute_error: 56.401947, mean_q: 1.202411\n",
      " 1206/5000: episode: 1205, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.421, 1.000], loss: 14312.472656, mean_absolute_error: 37.181190, mean_q: 1.202780\n",
      " 1207/5000: episode: 1206, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.358, 1.000], loss: 14432.449219, mean_absolute_error: 37.291443, mean_q: 1.204131\n",
      " 1208/5000: episode: 1207, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 14310.823242, mean_absolute_error: 37.234474, mean_q: 1.205322\n",
      " 1209/5000: episode: 1208, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.759], loss: 28619.962891, mean_absolute_error: 46.778915, mean_q: 1.207259\n",
      " 1210/5000: episode: 1209, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.000, 0.927], loss: 14309.649414, mean_absolute_error: 37.261597, mean_q: 1.208053\n",
      " 1211/5000: episode: 1210, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.346 [0.000, 0.932], loss: 43048.933594, mean_absolute_error: 56.407726, mean_q: 1.208861\n",
      " 1212/5000: episode: 1211, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.517 [0.001, 1.000], loss: 42931.093750, mean_absolute_error: 56.340866, mean_q: 1.209489\n",
      " 1213/5000: episode: 1212, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.004, 1.000], loss: 14427.499023, mean_absolute_error: 37.374466, mean_q: 1.209214\n",
      " 1214/5000: episode: 1213, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 14427.224609, mean_absolute_error: 37.407532, mean_q: 1.210088\n",
      " 1215/5000: episode: 1214, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.516], loss: 14311.615234, mean_absolute_error: 37.335480, mean_q: 1.211140\n",
      " 1216/5000: episode: 1215, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.000, 0.947], loss: 28610.910156, mean_absolute_error: 46.861168, mean_q: 1.210494\n",
      " 1217/5000: episode: 1216, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.634], loss: 28729.789062, mean_absolute_error: 46.952141, mean_q: 1.210451\n",
      " 1218/5000: episode: 1217, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.656], loss: 28728.808594, mean_absolute_error: 46.974243, mean_q: 1.211163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1219/5000: episode: 1218, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.526 [0.003, 1.000], loss: 14303.645508, mean_absolute_error: 37.381680, mean_q: 1.211005\n",
      " 1220/5000: episode: 1219, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.515], loss: 28847.957031, mean_absolute_error: 47.078766, mean_q: 1.211210\n",
      " 1221/5000: episode: 1220, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.453, 1.000], loss: 57572.484375, mean_absolute_error: 66.193680, mean_q: 1.211218\n",
      " 1222/5000: episode: 1221, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.115, 1.000], loss: 28724.132812, mean_absolute_error: 47.007095, mean_q: 1.212142\n",
      " 1223/5000: episode: 1222, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.531], loss: 14301.330078, mean_absolute_error: 37.429844, mean_q: 1.212927\n",
      " 1224/5000: episode: 1223, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.717], loss: 71742.523438, mean_absolute_error: 75.677925, mean_q: 1.213870\n",
      " 1225/5000: episode: 1224, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 0.869], loss: 0.738163, mean_absolute_error: 27.950186, mean_q: 1.214038\n",
      " 1226/5000: episode: 1225, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.138, 1.000], loss: 0.738769, mean_absolute_error: 27.958561, mean_q: 1.214535\n",
      " 1227/5000: episode: 1226, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.705], loss: 28598.339844, mean_absolute_error: 47.007248, mean_q: 1.215008\n",
      " 1228/5000: episode: 1227, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 43014.328125, mean_absolute_error: 56.604088, mean_q: 1.214587\n",
      " 1229/5000: episode: 1228, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 28714.839844, mean_absolute_error: 47.112984, mean_q: 1.215398\n",
      " 1230/5000: episode: 1229, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 14295.780273, mean_absolute_error: 37.538307, mean_q: 1.216089\n",
      " 1231/5000: episode: 1230, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.233, 1.000], loss: 43128.257812, mean_absolute_error: 56.733360, mean_q: 1.217666\n",
      " 1232/5000: episode: 1231, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.102, 1.000], loss: 28710.503906, mean_absolute_error: 47.164291, mean_q: 1.217758\n",
      " 1233/5000: episode: 1232, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 28585.724609, mean_absolute_error: 47.087658, mean_q: 1.218469\n",
      " 1234/5000: episode: 1233, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 14415.130859, mean_absolute_error: 37.693878, mean_q: 1.219885\n",
      " 1235/5000: episode: 1234, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.911], loss: 71834.921875, mean_absolute_error: 75.910637, mean_q: 1.220321\n",
      " 1236/5000: episode: 1235, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.002, 1.000], loss: 28580.503906, mean_absolute_error: 47.148518, mean_q: 1.221599\n",
      " 1237/5000: episode: 1236, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.368, 1.000], loss: 14289.683594, mean_absolute_error: 37.672867, mean_q: 1.222778\n",
      " 1238/5000: episode: 1237, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.734], loss: 57524.933594, mean_absolute_error: 66.457001, mean_q: 1.223093\n",
      " 1239/5000: episode: 1238, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.680 [0.095, 1.000], loss: 14288.039062, mean_absolute_error: 37.682571, mean_q: 1.224033\n",
      " 1240/5000: episode: 1239, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.537], loss: 42983.921875, mean_absolute_error: 56.807529, mean_q: 1.225388\n",
      " 1241/5000: episode: 1240, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.553 [0.009, 1.000], loss: 14292.824219, mean_absolute_error: 37.727448, mean_q: 1.226120\n",
      " 1242/5000: episode: 1241, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.664 [0.078, 1.000], loss: 28694.523438, mean_absolute_error: 47.331814, mean_q: 1.227059\n",
      " 1243/5000: episode: 1242, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.958], loss: 28696.738281, mean_absolute_error: 47.343243, mean_q: 1.226990\n",
      " 1244/5000: episode: 1243, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 14407.783203, mean_absolute_error: 37.843788, mean_q: 1.227200\n",
      " 1245/5000: episode: 1244, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.176, 1.000], loss: 14284.150391, mean_absolute_error: 37.789017, mean_q: 1.228315\n",
      " 1246/5000: episode: 1245, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.397 [0.000, 0.969], loss: 14406.683594, mean_absolute_error: 37.871368, mean_q: 1.228114\n",
      " 1247/5000: episode: 1246, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 100346.906250, mean_absolute_error: 95.080620, mean_q: 1.229082\n",
      " 1248/5000: episode: 1247, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.312 [0.000, 0.896], loss: 28688.328125, mean_absolute_error: 47.411381, mean_q: 1.229590\n",
      " 1249/5000: episode: 1248, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.334 [0.000, 0.920], loss: 57381.625000, mean_absolute_error: 66.507927, mean_q: 1.229752\n",
      " 1250/5000: episode: 1249, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.811], loss: 28566.750000, mean_absolute_error: 47.356384, mean_q: 1.230905\n",
      " 1251/5000: episode: 1250, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.000, 0.982], loss: 57368.582031, mean_absolute_error: 66.544312, mean_q: 1.231860\n",
      " 1252/5000: episode: 1251, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.893], loss: 28804.580078, mean_absolute_error: 47.549290, mean_q: 1.232519\n",
      " 1253/5000: episode: 1252, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.554 [0.010, 1.000], loss: 0.761950, mean_absolute_error: 28.400364, mean_q: 1.233459\n",
      " 1254/5000: episode: 1253, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.641 [0.057, 1.000], loss: 71512.703125, mean_absolute_error: 75.989128, mean_q: 1.233920\n",
      " 1255/5000: episode: 1254, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.356, 1.000], loss: 0.765557, mean_absolute_error: 28.435608, mean_q: 1.236376\n",
      " 1256/5000: episode: 1255, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.860], loss: 14399.041016, mean_absolute_error: 38.038383, mean_q: 1.237775\n",
      " 1257/5000: episode: 1256, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.466, 1.000], loss: 42948.656250, mean_absolute_error: 57.062153, mean_q: 1.239433\n",
      " 1258/5000: episode: 1257, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.705], loss: 28671.931641, mean_absolute_error: 47.565884, mean_q: 1.240101\n",
      " 1259/5000: episode: 1258, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 28670.800781, mean_absolute_error: 47.589138, mean_q: 1.241112\n",
      " 1260/5000: episode: 1259, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.561], loss: 42825.722656, mean_absolute_error: 56.993752, mean_q: 1.240834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1261/5000: episode: 1260, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.238, 1.000], loss: 0.772282, mean_absolute_error: 28.531494, mean_q: 1.241801\n",
      " 1262/5000: episode: 1261, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.726], loss: 57334.628906, mean_absolute_error: 66.677582, mean_q: 1.241457\n",
      " 1263/5000: episode: 1262, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.492, 1.000], loss: 14394.436523, mean_absolute_error: 38.122765, mean_q: 1.241963\n",
      " 1264/5000: episode: 1263, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 28665.921875, mean_absolute_error: 47.642159, mean_q: 1.242680\n",
      " 1265/5000: episode: 1264, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.002, 1.000], loss: 0.773724, mean_absolute_error: 28.576279, mean_q: 1.242959\n",
      " 1266/5000: episode: 1265, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.108, 1.000], loss: 57209.757812, mean_absolute_error: 66.657089, mean_q: 1.244089\n",
      " 1267/5000: episode: 1266, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 42932.132812, mean_absolute_error: 57.174007, mean_q: 1.244690\n",
      " 1268/5000: episode: 1267, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.643], loss: 43057.640625, mean_absolute_error: 57.257538, mean_q: 1.244975\n",
      " 1269/5000: episode: 1268, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.731], loss: 28783.708984, mean_absolute_error: 47.808193, mean_q: 1.246863\n",
      " 1270/5000: episode: 1269, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.539 [0.005, 1.000], loss: 28535.281250, mean_absolute_error: 47.640778, mean_q: 1.248014\n",
      " 1271/5000: episode: 1270, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.731 [0.165, 1.000], loss: 0.781551, mean_absolute_error: 28.686466, mean_q: 1.249237\n",
      " 1272/5000: episode: 1271, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.599 [0.029, 1.000], loss: 42930.195312, mean_absolute_error: 57.256588, mean_q: 1.249845\n",
      " 1273/5000: episode: 1272, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 57311.109375, mean_absolute_error: 66.849113, mean_q: 1.251007\n",
      " 1274/5000: episode: 1273, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.558], loss: 57310.378906, mean_absolute_error: 66.858360, mean_q: 1.252103\n",
      " 1275/5000: episode: 1274, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.076, 1.000], loss: 0.787026, mean_absolute_error: 28.751339, mean_q: 1.253606\n",
      " 1276/5000: episode: 1275, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.565 [0.013, 1.000], loss: 57173.691406, mean_absolute_error: 66.810242, mean_q: 1.254395\n",
      " 1277/5000: episode: 1276, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.567], loss: 0.788739, mean_absolute_error: 28.783394, mean_q: 1.254973\n",
      " 1278/5000: episode: 1277, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.472 [0.000, 0.997], loss: 57411.500000, mean_absolute_error: 67.006241, mean_q: 1.255203\n",
      " 1279/5000: episode: 1278, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.317, 1.000], loss: 42905.179688, mean_absolute_error: 57.365673, mean_q: 1.256210\n",
      " 1280/5000: episode: 1279, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.480, 1.000], loss: 14260.862305, mean_absolute_error: 38.328674, mean_q: 1.256927\n",
      " 1281/5000: episode: 1280, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.564], loss: 42899.640625, mean_absolute_error: 57.383678, mean_q: 1.256704\n",
      " 1282/5000: episode: 1281, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.573], loss: 14380.537109, mean_absolute_error: 38.416412, mean_q: 1.256170\n",
      " 1283/5000: episode: 1282, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.175, 1.000], loss: 14258.436523, mean_absolute_error: 38.367462, mean_q: 1.257554\n",
      " 1284/5000: episode: 1283, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.199, 1.000], loss: 0.791456, mean_absolute_error: 28.881088, mean_q: 1.257129\n",
      " 1285/5000: episode: 1284, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.742], loss: 14383.706055, mean_absolute_error: 38.464684, mean_q: 1.257434\n",
      " 1286/5000: episode: 1285, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.410 [0.000, 0.976], loss: 0.792075, mean_absolute_error: 28.922905, mean_q: 1.257628\n",
      " 1287/5000: episode: 1286, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.176, 1.000], loss: 14255.721680, mean_absolute_error: 38.420311, mean_q: 1.257931\n",
      " 1288/5000: episode: 1287, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.770], loss: 71639.781250, mean_absolute_error: 76.597755, mean_q: 1.257577\n",
      " 1289/5000: episode: 1288, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.487, 1.000], loss: 14254.682617, mean_absolute_error: 38.428432, mean_q: 1.258247\n",
      " 1290/5000: episode: 1289, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.469, 1.000], loss: 14253.798828, mean_absolute_error: 38.458786, mean_q: 1.259000\n",
      " 1291/5000: episode: 1290, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.013, 1.000], loss: 0.793653, mean_absolute_error: 28.992847, mean_q: 1.258880\n",
      " 1292/5000: episode: 1291, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.000, 0.999], loss: 28634.041016, mean_absolute_error: 48.050049, mean_q: 1.258704\n",
      " 1293/5000: episode: 1292, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.158, 1.000], loss: 42876.917969, mean_absolute_error: 57.542404, mean_q: 1.258797\n",
      " 1294/5000: episode: 1293, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 14374.752930, mean_absolute_error: 38.584236, mean_q: 1.257999\n",
      " 1295/5000: episode: 1294, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.772 [0.250, 1.000], loss: 14251.083984, mean_absolute_error: 38.517365, mean_q: 1.258598\n",
      " 1296/5000: episode: 1295, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.065, 1.000], loss: 42748.750000, mean_absolute_error: 57.469067, mean_q: 1.258152\n",
      " 1297/5000: episode: 1296, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.722], loss: 71618.781250, mean_absolute_error: 76.708633, mean_q: 1.258608\n",
      " 1298/5000: episode: 1297, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.000, 0.982], loss: 28620.943359, mean_absolute_error: 48.117371, mean_q: 1.258777\n",
      " 1299/5000: episode: 1298, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.000, 0.991], loss: 57237.746094, mean_absolute_error: 67.169319, mean_q: 1.259060\n",
      " 1300/5000: episode: 1299, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.000, 0.992], loss: 28742.884766, mean_absolute_error: 48.219673, mean_q: 1.259119\n",
      " 1301/5000: episode: 1300, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.880], loss: 28616.457031, mean_absolute_error: 48.153481, mean_q: 1.259642\n",
      " 1302/5000: episode: 1301, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.778 [0.267, 1.000], loss: 57358.472656, mean_absolute_error: 67.286201, mean_q: 1.259931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1303/5000: episode: 1302, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.552], loss: 28493.769531, mean_absolute_error: 48.107933, mean_q: 1.261135\n",
      " 1304/5000: episode: 1303, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.802], loss: 28487.796875, mean_absolute_error: 48.118900, mean_q: 1.261614\n",
      " 1305/5000: episode: 1304, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.578 [0.018, 1.000], loss: 71475.226562, mean_absolute_error: 76.707390, mean_q: 1.260728\n",
      " 1306/5000: episode: 1305, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.001, 1.000], loss: 28610.455078, mean_absolute_error: 48.243706, mean_q: 1.262078\n",
      " 1307/5000: episode: 1306, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.404, 1.000], loss: 14242.900391, mean_absolute_error: 38.686886, mean_q: 1.262144\n",
      " 1308/5000: episode: 1307, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 28483.869141, mean_absolute_error: 48.184780, mean_q: 1.262843\n",
      " 1309/5000: episode: 1308, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.176, 1.000], loss: 57339.613281, mean_absolute_error: 67.400833, mean_q: 1.262609\n",
      " 1310/5000: episode: 1309, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.077, 1.000], loss: 42968.355469, mean_absolute_error: 57.869457, mean_q: 1.263566\n",
      " 1311/5000: episode: 1310, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.803], loss: 14239.782227, mean_absolute_error: 38.753841, mean_q: 1.263947\n",
      " 1312/5000: episode: 1311, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 0.800877, mean_absolute_error: 29.314461, mean_q: 1.264603\n",
      " 1313/5000: episode: 1312, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.081, 1.000], loss: 42833.957031, mean_absolute_error: 57.820900, mean_q: 1.264776\n",
      " 1314/5000: episode: 1313, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.003, 1.000], loss: 14234.801758, mean_absolute_error: 38.801552, mean_q: 1.264661\n",
      " 1315/5000: episode: 1314, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.509, 1.000], loss: 43093.671875, mean_absolute_error: 58.029312, mean_q: 1.264721\n",
      " 1316/5000: episode: 1315, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.000, 0.988], loss: 42828.226562, mean_absolute_error: 57.860649, mean_q: 1.265274\n",
      " 1317/5000: episode: 1316, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.334, 1.000], loss: 0.802271, mean_absolute_error: 29.389744, mean_q: 1.265702\n",
      " 1318/5000: episode: 1317, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.000, 0.976], loss: 42951.480469, mean_absolute_error: 57.981266, mean_q: 1.265882\n",
      " 1319/5000: episode: 1318, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.177, 1.000], loss: 85516.078125, mean_absolute_error: 86.307594, mean_q: 1.266005\n",
      " 1320/5000: episode: 1319, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.000, 0.956], loss: 28458.236328, mean_absolute_error: 48.362057, mean_q: 1.265779\n",
      " 1321/5000: episode: 1320, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.142, 1.000], loss: 57440.406250, mean_absolute_error: 67.661453, mean_q: 1.265349\n",
      " 1322/5000: episode: 1321, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.560], loss: 42947.972656, mean_absolute_error: 58.040894, mean_q: 1.265923\n",
      " 1323/5000: episode: 1322, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.150, 1.000], loss: 28585.910156, mean_absolute_error: 48.501247, mean_q: 1.266479\n",
      " 1324/5000: episode: 1323, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.049, 1.000], loss: 42807.316406, mean_absolute_error: 57.970371, mean_q: 1.266757\n",
      " 1325/5000: episode: 1324, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 42682.082031, mean_absolute_error: 57.903206, mean_q: 1.267084\n",
      " 1326/5000: episode: 1325, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 28454.894531, mean_absolute_error: 48.439560, mean_q: 1.266927\n",
      " 1327/5000: episode: 1326, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.104, 1.000], loss: 14355.438477, mean_absolute_error: 39.094437, mean_q: 1.267583\n",
      " 1328/5000: episode: 1327, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.959], loss: 14357.098633, mean_absolute_error: 39.119690, mean_q: 1.268461\n",
      " 1329/5000: episode: 1328, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.791], loss: 42932.335938, mean_absolute_error: 58.128021, mean_q: 1.268539\n",
      " 1330/5000: episode: 1329, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.681], loss: 14359.837891, mean_absolute_error: 39.156448, mean_q: 1.269508\n",
      " 1331/5000: episode: 1330, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 71364.937500, mean_absolute_error: 77.089325, mean_q: 1.269898\n",
      " 1332/5000: episode: 1331, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.433, 1.000], loss: 28703.625000, mean_absolute_error: 48.715580, mean_q: 1.269987\n",
      " 1333/5000: episode: 1332, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.032, 1.000], loss: 28569.796875, mean_absolute_error: 48.651344, mean_q: 1.270975\n",
      " 1334/5000: episode: 1333, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.421, 1.000], loss: 14350.826172, mean_absolute_error: 39.212540, mean_q: 1.271708\n",
      " 1335/5000: episode: 1334, duration: 0.019s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.811007, mean_absolute_error: 29.670399, mean_q: 1.272576\n",
      " 1336/5000: episode: 1335, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.914], loss: 0.810448, mean_absolute_error: 29.660400, mean_q: 1.272130\n",
      " 1337/5000: episode: 1336, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.675], loss: 42916.125000, mean_absolute_error: 58.252296, mean_q: 1.273502\n",
      " 1338/5000: episode: 1337, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.447 [0.000, 0.991], loss: 28432.542969, mean_absolute_error: 48.613800, mean_q: 1.273728\n",
      " 1339/5000: episode: 1338, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.634], loss: 14351.367188, mean_absolute_error: 39.268089, mean_q: 1.274144\n",
      " 1340/5000: episode: 1339, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 42777.742188, mean_absolute_error: 58.192818, mean_q: 1.274968\n",
      " 1341/5000: episode: 1340, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.767], loss: 0.814312, mean_absolute_error: 29.745934, mean_q: 1.275172\n",
      " 1342/5000: episode: 1341, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.366, 1.000], loss: 14214.468750, mean_absolute_error: 39.217201, mean_q: 1.276196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1343/5000: episode: 1342, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.000, 0.829], loss: 42902.546875, mean_absolute_error: 58.309273, mean_q: 1.275588\n",
      " 1344/5000: episode: 1343, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.333, 1.000], loss: 28557.140625, mean_absolute_error: 48.785679, mean_q: 1.275908\n",
      " 1345/5000: episode: 1344, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 28555.800781, mean_absolute_error: 48.805214, mean_q: 1.276344\n",
      " 1346/5000: episode: 1345, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.440 [0.000, 0.989], loss: 14211.833008, mean_absolute_error: 39.261890, mean_q: 1.276401\n",
      " 1347/5000: episode: 1346, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 56977.703125, mean_absolute_error: 67.725876, mean_q: 1.276623\n",
      " 1348/5000: episode: 1347, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.410, 1.000], loss: 42893.796875, mean_absolute_error: 58.388062, mean_q: 1.276463\n",
      " 1349/5000: episode: 1348, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.330, 1.000], loss: 57106.394531, mean_absolute_error: 67.847160, mean_q: 1.276316\n",
      " 1350/5000: episode: 1349, duration: 0.019s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.105, 1.000], loss: 14341.415039, mean_absolute_error: 39.416336, mean_q: 1.276513\n",
      " 1351/5000: episode: 1350, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.286, 1.000], loss: 14341.045898, mean_absolute_error: 39.419449, mean_q: 1.276623\n",
      " 1352/5000: episode: 1351, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.448, 1.000], loss: 42753.031250, mean_absolute_error: 58.337349, mean_q: 1.276957\n",
      " 1353/5000: episode: 1352, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.455, 1.000], loss: 28420.656250, mean_absolute_error: 48.807392, mean_q: 1.277261\n",
      " 1354/5000: episode: 1353, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.568 [0.014, 1.000], loss: 14339.035156, mean_absolute_error: 39.460152, mean_q: 1.277237\n",
      " 1355/5000: episode: 1354, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.486, 1.000], loss: 42881.718750, mean_absolute_error: 58.468105, mean_q: 1.277959\n",
      " 1356/5000: episode: 1355, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.591], loss: 14209.529297, mean_absolute_error: 39.396729, mean_q: 1.277658\n",
      " 1357/5000: episode: 1356, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.000, 0.922], loss: 28673.488281, mean_absolute_error: 49.061672, mean_q: 1.279763\n",
      " 1358/5000: episode: 1357, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 28406.386719, mean_absolute_error: 48.883694, mean_q: 1.280308\n",
      " 1359/5000: episode: 1358, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.090, 1.000], loss: 14335.526367, mean_absolute_error: 39.542690, mean_q: 1.280578\n",
      " 1360/5000: episode: 1359, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.592], loss: 14201.934570, mean_absolute_error: 39.476601, mean_q: 1.281319\n",
      " 1361/5000: episode: 1360, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.444 [0.000, 0.990], loss: 0.822017, mean_absolute_error: 30.034397, mean_q: 1.281195\n",
      " 1362/5000: episode: 1361, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.000, 0.937], loss: 42734.195312, mean_absolute_error: 58.481506, mean_q: 1.281633\n",
      " 1363/5000: episode: 1362, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.253, 1.000], loss: 14333.254883, mean_absolute_error: 39.601086, mean_q: 1.281273\n",
      " 1364/5000: episode: 1363, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.018, 1.000], loss: 14332.623047, mean_absolute_error: 39.631577, mean_q: 1.282057\n",
      " 1365/5000: episode: 1364, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.174, 1.000], loss: 14331.892578, mean_absolute_error: 39.637482, mean_q: 1.282436\n",
      " 1366/5000: episode: 1365, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.000, 0.940], loss: 28533.085938, mean_absolute_error: 49.090473, mean_q: 1.283100\n",
      " 1367/5000: episode: 1366, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.898], loss: 0.825152, mean_absolute_error: 30.129086, mean_q: 1.283637\n",
      " 1368/5000: episode: 1367, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.481 [0.000, 0.998], loss: 14329.841797, mean_absolute_error: 39.678871, mean_q: 1.284081\n",
      " 1369/5000: episode: 1368, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.000, 0.913], loss: 14329.193359, mean_absolute_error: 39.688931, mean_q: 1.284647\n",
      " 1370/5000: episode: 1369, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.282, 1.000], loss: 14196.474609, mean_absolute_error: 39.598671, mean_q: 1.284868\n",
      " 1371/5000: episode: 1370, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.000, 1.000], loss: 28525.074219, mean_absolute_error: 49.150391, mean_q: 1.285754\n",
      " 1372/5000: episode: 1371, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.000, 0.695], loss: 28525.449219, mean_absolute_error: 49.156445, mean_q: 1.286173\n",
      " 1373/5000: episode: 1372, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 57052.093750, mean_absolute_error: 68.148705, mean_q: 1.286851\n",
      " 1374/5000: episode: 1373, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.410 [0.000, 0.976], loss: 0.829637, mean_absolute_error: 30.204216, mean_q: 1.287123\n",
      " 1375/5000: episode: 1374, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.767], loss: 42580.179688, mean_absolute_error: 58.524643, mean_q: 1.286680\n",
      " 1376/5000: episode: 1375, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.778 [0.268, 1.000], loss: 85559.250000, mean_absolute_error: 87.148758, mean_q: 1.287147\n",
      " 1377/5000: episode: 1376, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.009, 1.000], loss: 42839.875000, mean_absolute_error: 58.745247, mean_q: 1.287180\n",
      " 1378/5000: episode: 1377, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.000, 0.970], loss: 42846.339844, mean_absolute_error: 58.768291, mean_q: 1.287180\n",
      " 1379/5000: episode: 1378, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.772], loss: 28514.308594, mean_absolute_error: 49.256439, mean_q: 1.287642\n",
      " 1380/5000: episode: 1379, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.496, 1.000], loss: 14322.916016, mean_absolute_error: 39.826851, mean_q: 1.287743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1381/5000: episode: 1380, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.472 [0.000, 0.997], loss: 71217.804688, mean_absolute_error: 77.670296, mean_q: 1.287718\n",
      " 1382/5000: episode: 1381, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.938], loss: 57168.117188, mean_absolute_error: 68.335838, mean_q: 1.287849\n",
      " 1383/5000: episode: 1382, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.115, 1.000], loss: 28508.929688, mean_absolute_error: 49.299019, mean_q: 1.289286\n",
      " 1384/5000: episode: 1383, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.861], loss: 71335.773438, mean_absolute_error: 77.811966, mean_q: 1.290291\n",
      " 1385/5000: episode: 1384, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.880], loss: 14187.914062, mean_absolute_error: 39.807957, mean_q: 1.291519\n",
      " 1386/5000: episode: 1385, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.277, 1.000], loss: 14186.431641, mean_absolute_error: 39.828140, mean_q: 1.292482\n",
      " 1387/5000: episode: 1386, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 28633.371094, mean_absolute_error: 49.476433, mean_q: 1.293618\n",
      " 1388/5000: episode: 1387, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 14184.571289, mean_absolute_error: 39.866882, mean_q: 1.294107\n",
      " 1389/5000: episode: 1388, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.000, 0.922], loss: 57139.976562, mean_absolute_error: 68.447662, mean_q: 1.293741\n",
      " 1390/5000: episode: 1389, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.444 [0.000, 0.990], loss: 14315.152344, mean_absolute_error: 40.002861, mean_q: 1.295281\n",
      " 1391/5000: episode: 1390, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.123, 1.000], loss: 28495.552734, mean_absolute_error: 49.420769, mean_q: 1.294918\n",
      " 1392/5000: episode: 1391, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 0.841441, mean_absolute_error: 30.493938, mean_q: 1.296255\n",
      " 1393/5000: episode: 1392, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.104, 1.000], loss: 14184.760742, mean_absolute_error: 39.938416, mean_q: 1.296899\n",
      " 1394/5000: episode: 1393, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 28360.546875, mean_absolute_error: 49.386951, mean_q: 1.297700\n",
      " 1395/5000: episode: 1394, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.919], loss: 42936.367188, mean_absolute_error: 59.112686, mean_q: 1.298357\n",
      " 1396/5000: episode: 1395, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.137, 1.000], loss: 42799.191406, mean_absolute_error: 59.019489, mean_q: 1.298511\n",
      " 1397/5000: episode: 1396, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 14178.798828, mean_absolute_error: 39.985519, mean_q: 1.299626\n",
      " 1398/5000: episode: 1397, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.536], loss: 28616.505859, mean_absolute_error: 49.629742, mean_q: 1.300881\n",
      " 1399/5000: episode: 1398, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.000, 0.807], loss: 0.849077, mean_absolute_error: 30.610443, mean_q: 1.302130\n",
      " 1400/5000: episode: 1399, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.215, 1.000], loss: 42665.582031, mean_absolute_error: 58.993992, mean_q: 1.302463\n",
      " 1401/5000: episode: 1400, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.000, 0.850], loss: 85580.023438, mean_absolute_error: 87.569656, mean_q: 1.303388\n",
      " 1402/5000: episode: 1401, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.508], loss: 28609.457031, mean_absolute_error: 49.684654, mean_q: 1.304302\n",
      " 1403/5000: episode: 1402, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.624], loss: 42524.425781, mean_absolute_error: 58.962212, mean_q: 1.306354\n",
      " 1404/5000: episode: 1403, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.170, 1.000], loss: 42783.539062, mean_absolute_error: 59.156376, mean_q: 1.307382\n",
      " 1405/5000: episode: 1404, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 0.857389, mean_absolute_error: 30.722223, mean_q: 1.308495\n",
      " 1406/5000: episode: 1405, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.772], loss: 42657.390625, mean_absolute_error: 59.094635, mean_q: 1.308879\n",
      " 1407/5000: episode: 1406, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.500, 1.000], loss: 28472.812500, mean_absolute_error: 49.687080, mean_q: 1.309814\n",
      " 1408/5000: episode: 1407, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 42788.535156, mean_absolute_error: 59.209938, mean_q: 1.310044\n",
      " 1409/5000: episode: 1408, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.265 [0.000, 0.828], loss: 14171.812500, mean_absolute_error: 40.184464, mean_q: 1.310672\n",
      " 1410/5000: episode: 1409, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.718], loss: 56815.093750, mean_absolute_error: 68.574013, mean_q: 1.311952\n",
      " 1411/5000: episode: 1410, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.655], loss: 28469.361328, mean_absolute_error: 49.738766, mean_q: 1.312472\n",
      " 1412/5000: episode: 1411, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.356, 1.000], loss: 14171.979492, mean_absolute_error: 40.235275, mean_q: 1.312854\n",
      " 1413/5000: episode: 1412, duration: 0.019s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.007, 1.000], loss: 57057.410156, mean_absolute_error: 68.777550, mean_q: 1.312835\n",
      " 1414/5000: episode: 1413, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.295 [0.000, 0.874], loss: 14298.575195, mean_absolute_error: 40.358650, mean_q: 1.313718\n",
      " 1415/5000: episode: 1414, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.704], loss: 14167.766602, mean_absolute_error: 40.291527, mean_q: 1.314314\n",
      " 1416/5000: episode: 1415, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.467, 1.000], loss: 99423.796875, mean_absolute_error: 97.005280, mean_q: 1.314604\n",
      " 1417/5000: episode: 1416, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.259, 1.000], loss: 28458.787109, mean_absolute_error: 49.826538, mean_q: 1.314851\n",
      " 1418/5000: episode: 1417, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.479 [0.000, 0.998], loss: 0.866704, mean_absolute_error: 30.926849, mean_q: 1.315587\n",
      " 1419/5000: episode: 1418, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.297, 1.000], loss: 28464.449219, mean_absolute_error: 49.861206, mean_q: 1.315796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1420/5000: episode: 1419, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.461, 1.000], loss: 0.867582, mean_absolute_error: 30.954128, mean_q: 1.316253\n",
      " 1421/5000: episode: 1420, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.000, 1.000], loss: 14162.774414, mean_absolute_error: 40.386490, mean_q: 1.316472\n",
      " 1422/5000: episode: 1421, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.042, 1.000], loss: 28327.035156, mean_absolute_error: 49.829285, mean_q: 1.316858\n",
      " 1423/5000: episode: 1422, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.000, 0.887], loss: 113801.039062, mean_absolute_error: 106.698204, mean_q: 1.316754\n",
      " 1424/5000: episode: 1423, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.575 [0.017, 1.000], loss: 14288.729492, mean_absolute_error: 40.527412, mean_q: 1.316968\n",
      " 1425/5000: episode: 1424, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.295, 1.000], loss: 14288.003906, mean_absolute_error: 40.536015, mean_q: 1.317196\n",
      " 1426/5000: episode: 1425, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.019, 1.000], loss: 14158.381836, mean_absolute_error: 40.484249, mean_q: 1.317959\n",
      " 1427/5000: episode: 1426, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.548], loss: 14157.468750, mean_absolute_error: 40.487679, mean_q: 1.317452\n",
      " 1428/5000: episode: 1427, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 42731.359375, mean_absolute_error: 59.509056, mean_q: 1.316980\n",
      " 1429/5000: episode: 1428, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 56893.964844, mean_absolute_error: 68.935928, mean_q: 1.316706\n",
      " 1430/5000: episode: 1429, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.171, 1.000], loss: 14155.022461, mean_absolute_error: 40.532082, mean_q: 1.316999\n",
      " 1431/5000: episode: 1430, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.856], loss: 28438.351562, mean_absolute_error: 50.047535, mean_q: 1.317137\n",
      " 1432/5000: episode: 1431, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.647], loss: 71156.945312, mean_absolute_error: 78.464783, mean_q: 1.316871\n",
      " 1433/5000: episode: 1432, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.662], loss: 42726.671875, mean_absolute_error: 59.580044, mean_q: 1.318283\n",
      " 1434/5000: episode: 1433, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 0.872023, mean_absolute_error: 31.185566, mean_q: 1.319621\n",
      " 1435/5000: episode: 1434, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.273, 1.000], loss: 0.872721, mean_absolute_error: 31.192204, mean_q: 1.320146\n",
      " 1436/5000: episode: 1435, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.686], loss: 0.873314, mean_absolute_error: 31.223192, mean_q: 1.320600\n",
      " 1437/5000: episode: 1436, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.107, 1.000], loss: 42716.281250, mean_absolute_error: 59.637390, mean_q: 1.320132\n",
      " 1438/5000: episode: 1437, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.802], loss: 14280.578125, mean_absolute_error: 40.735111, mean_q: 1.320269\n",
      " 1439/5000: episode: 1438, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.582], loss: 28445.265625, mean_absolute_error: 50.146172, mean_q: 1.319898\n",
      " 1440/5000: episode: 1439, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.505], loss: 28427.423828, mean_absolute_error: 50.171722, mean_q: 1.320553\n",
      " 1441/5000: episode: 1440, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.914], loss: 42441.835938, mean_absolute_error: 59.499565, mean_q: 1.321114\n",
      " 1442/5000: episode: 1441, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.281 [0.000, 0.855], loss: 42573.785156, mean_absolute_error: 59.618317, mean_q: 1.321379\n",
      " 1443/5000: episode: 1442, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.371 [0.000, 0.952], loss: 42699.867188, mean_absolute_error: 59.718056, mean_q: 1.321053\n",
      " 1444/5000: episode: 1443, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 28420.214844, mean_absolute_error: 50.238243, mean_q: 1.321415\n",
      " 1445/5000: episode: 1444, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.746], loss: 14275.640625, mean_absolute_error: 40.852875, mean_q: 1.321852\n",
      " 1446/5000: episode: 1445, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.121, 1.000], loss: 0.875592, mean_absolute_error: 31.364845, mean_q: 1.322318\n",
      " 1447/5000: episode: 1446, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.480, 1.000], loss: 14275.444336, mean_absolute_error: 40.890903, mean_q: 1.323070\n",
      " 1448/5000: episode: 1447, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.325, 1.000], loss: 0.877102, mean_absolute_error: 31.404461, mean_q: 1.323457\n",
      " 1449/5000: episode: 1448, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.068, 1.000], loss: 14272.633789, mean_absolute_error: 40.901260, mean_q: 1.322944\n",
      " 1450/5000: episode: 1449, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.054, 1.000], loss: 42684.886719, mean_absolute_error: 59.820774, mean_q: 1.323746\n",
      " 1451/5000: episode: 1450, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.045, 1.000], loss: 42551.476562, mean_absolute_error: 59.742569, mean_q: 1.324275\n",
      " 1452/5000: episode: 1451, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 42553.414062, mean_absolute_error: 59.748211, mean_q: 1.324295\n",
      " 1453/5000: episode: 1452, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.684], loss: 14269.932617, mean_absolute_error: 40.958618, mean_q: 1.324615\n",
      " 1454/5000: episode: 1453, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.395 [0.000, 0.968], loss: 28411.546875, mean_absolute_error: 50.380585, mean_q: 1.325216\n",
      " 1455/5000: episode: 1454, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 14137.224609, mean_absolute_error: 40.896828, mean_q: 1.325873\n",
      " 1456/5000: episode: 1455, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.156, 1.000], loss: 28275.531250, mean_absolute_error: 50.311020, mean_q: 1.326570\n",
      " 1457/5000: episode: 1456, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.791], loss: 28532.992188, mean_absolute_error: 50.520950, mean_q: 1.327512\n",
      " 1458/5000: episode: 1457, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.544], loss: 0.882702, mean_absolute_error: 31.551641, mean_q: 1.327681\n",
      " 1459/5000: episode: 1458, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.671], loss: 0.883522, mean_absolute_error: 31.556335, mean_q: 1.328291\n",
      " 1460/5000: episode: 1459, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.447, 1.000], loss: 28398.115234, mean_absolute_error: 50.470985, mean_q: 1.328729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1461/5000: episode: 1460, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 28399.316406, mean_absolute_error: 50.485462, mean_q: 1.329193\n",
      " 1462/5000: episode: 1461, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.067, 1.000], loss: 14133.074219, mean_absolute_error: 41.010635, mean_q: 1.329410\n",
      " 1463/5000: episode: 1462, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.610], loss: 28394.585938, mean_absolute_error: 50.513832, mean_q: 1.329480\n",
      " 1464/5000: episode: 1463, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.000, 0.977], loss: 14132.086914, mean_absolute_error: 41.029457, mean_q: 1.329545\n",
      " 1465/5000: episode: 1464, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.095, 1.000], loss: 42525.996094, mean_absolute_error: 59.928947, mean_q: 1.329233\n",
      " 1466/5000: episode: 1465, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.260, 1.000], loss: 14261.367188, mean_absolute_error: 41.142162, mean_q: 1.328832\n",
      " 1467/5000: episode: 1466, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.156, 1.000], loss: 14130.037109, mean_absolute_error: 41.062981, mean_q: 1.328744\n",
      " 1468/5000: episode: 1467, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.658 [0.072, 1.000], loss: 28389.220703, mean_absolute_error: 50.574478, mean_q: 1.328897\n",
      " 1469/5000: episode: 1468, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.507], loss: 14131.145508, mean_absolute_error: 41.086693, mean_q: 1.328185\n",
      " 1470/5000: episode: 1469, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.276, 1.000], loss: 14128.248047, mean_absolute_error: 41.107040, mean_q: 1.328306\n",
      " 1471/5000: episode: 1470, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.944], loss: 14259.016602, mean_absolute_error: 41.211506, mean_q: 1.328346\n",
      " 1472/5000: episode: 1471, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 28384.683594, mean_absolute_error: 50.605881, mean_q: 1.327992\n",
      " 1473/5000: episode: 1472, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.424, 1.000], loss: 28515.218750, mean_absolute_error: 50.714226, mean_q: 1.327870\n",
      " 1474/5000: episode: 1473, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.200, 1.000], loss: 14257.430664, mean_absolute_error: 41.245964, mean_q: 1.328281\n",
      " 1475/5000: episode: 1474, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 42509.183594, mean_absolute_error: 60.037651, mean_q: 1.328381\n",
      " 1476/5000: episode: 1475, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.533], loss: 42508.039062, mean_absolute_error: 60.048149, mean_q: 1.328089\n",
      " 1477/5000: episode: 1476, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 28510.246094, mean_absolute_error: 50.771473, mean_q: 1.329011\n",
      " 1478/5000: episode: 1477, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.024, 1.000], loss: 28254.367188, mean_absolute_error: 50.588615, mean_q: 1.328797\n",
      " 1479/5000: episode: 1478, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.028, 1.000], loss: 28508.011719, mean_absolute_error: 50.786720, mean_q: 1.329305\n",
      " 1480/5000: episode: 1479, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.540], loss: 14122.596680, mean_absolute_error: 41.231274, mean_q: 1.329791\n",
      " 1481/5000: episode: 1480, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.748], loss: 42627.371094, mean_absolute_error: 60.195084, mean_q: 1.329578\n",
      " 1482/5000: episode: 1481, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.648 [0.063, 1.000], loss: 28373.541016, mean_absolute_error: 50.738010, mean_q: 1.330177\n",
      " 1483/5000: episode: 1482, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.000, 0.999], loss: 42495.539062, mean_absolute_error: 60.141434, mean_q: 1.330312\n",
      " 1484/5000: episode: 1483, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.278, 1.000], loss: 42754.531250, mean_absolute_error: 60.336113, mean_q: 1.330379\n",
      " 1485/5000: episode: 1484, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.007, 1.000], loss: 42493.390625, mean_absolute_error: 60.166946, mean_q: 1.331325\n",
      " 1486/5000: episode: 1485, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.624], loss: 28367.560547, mean_absolute_error: 50.812538, mean_q: 1.332455\n",
      " 1487/5000: episode: 1486, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 42480.796875, mean_absolute_error: 60.200100, mean_q: 1.332724\n",
      " 1488/5000: episode: 1487, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.000, 0.997], loss: 14116.068359, mean_absolute_error: 41.359264, mean_q: 1.333518\n",
      " 1489/5000: episode: 1488, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.542], loss: 14115.084961, mean_absolute_error: 41.394154, mean_q: 1.334418\n",
      " 1490/5000: episode: 1489, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.000, 1.000], loss: 0.891397, mean_absolute_error: 32.020279, mean_q: 1.334209\n",
      " 1491/5000: episode: 1490, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.580], loss: 0.891659, mean_absolute_error: 32.042374, mean_q: 1.334408\n",
      " 1492/5000: episode: 1491, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.586], loss: 28488.685547, mean_absolute_error: 51.001289, mean_q: 1.334066\n",
      " 1493/5000: episode: 1492, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.000, 0.965], loss: 14249.016602, mean_absolute_error: 41.533947, mean_q: 1.333997\n",
      " 1494/5000: episode: 1493, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.683 [0.098, 1.000], loss: 14243.658203, mean_absolute_error: 41.551666, mean_q: 1.333814\n",
      " 1495/5000: episode: 1494, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.595], loss: 28491.242188, mean_absolute_error: 51.052589, mean_q: 1.334145\n",
      " 1496/5000: episode: 1495, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.638], loss: 14109.597656, mean_absolute_error: 41.488327, mean_q: 1.334181\n",
      " 1497/5000: episode: 1496, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.000, 0.975], loss: 28351.792969, mean_absolute_error: 50.980736, mean_q: 1.334283\n",
      " 1498/5000: episode: 1497, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.871], loss: 14241.751953, mean_absolute_error: 41.610760, mean_q: 1.334243\n",
      " 1499/5000: episode: 1498, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.031, 1.000], loss: 42453.917969, mean_absolute_error: 60.393360, mean_q: 1.334931\n",
      " 1500/5000: episode: 1499, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.247, 1.000], loss: 14106.838867, mean_absolute_error: 41.551956, mean_q: 1.335011\n",
      " 1501/5000: episode: 1500, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.016, 1.000], loss: 14244.191406, mean_absolute_error: 41.652313, mean_q: 1.334836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1502/5000: episode: 1501, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.959], loss: 42448.597656, mean_absolute_error: 60.437996, mean_q: 1.335739\n",
      " 1503/5000: episode: 1502, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.298, 1.000], loss: 14104.280273, mean_absolute_error: 41.599041, mean_q: 1.335932\n",
      " 1504/5000: episode: 1503, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.755], loss: 28472.875000, mean_absolute_error: 51.171402, mean_q: 1.335884\n",
      " 1505/5000: episode: 1504, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.274 [0.000, 0.843], loss: 14102.957031, mean_absolute_error: 41.639156, mean_q: 1.336989\n",
      " 1506/5000: episode: 1505, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.766], loss: 14235.576172, mean_absolute_error: 41.745419, mean_q: 1.336976\n",
      " 1507/5000: episode: 1506, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.323, 1.000], loss: 14234.934570, mean_absolute_error: 41.762138, mean_q: 1.337440\n",
      " 1508/5000: episode: 1507, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.006, 1.000], loss: 14234.261719, mean_absolute_error: 41.759895, mean_q: 1.337031\n",
      " 1509/5000: episode: 1508, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 14233.424805, mean_absolute_error: 41.775940, mean_q: 1.337232\n",
      " 1510/5000: episode: 1509, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 42696.640625, mean_absolute_error: 60.717285, mean_q: 1.337855\n",
      " 1511/5000: episode: 1510, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.290 [0.000, 0.867], loss: 0.898144, mean_absolute_error: 32.349213, mean_q: 1.339251\n",
      " 1512/5000: episode: 1511, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.643], loss: 0.897493, mean_absolute_error: 32.353081, mean_q: 1.338768\n",
      " 1513/5000: episode: 1512, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.265 [0.000, 0.828], loss: 14230.513672, mean_absolute_error: 41.844070, mean_q: 1.339345\n",
      " 1514/5000: episode: 1513, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.894], loss: 28326.115234, mean_absolute_error: 51.239532, mean_q: 1.339967\n",
      " 1515/5000: episode: 1514, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 28195.804688, mean_absolute_error: 51.149990, mean_q: 1.339809\n",
      " 1516/5000: episode: 1515, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.143, 1.000], loss: 71009.437500, mean_absolute_error: 79.652901, mean_q: 1.339853\n",
      " 1517/5000: episode: 1516, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 0.898565, mean_absolute_error: 32.429710, mean_q: 1.339568\n",
      " 1518/5000: episode: 1517, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.696 [0.113, 1.000], loss: 42415.027344, mean_absolute_error: 60.651093, mean_q: 1.340014\n",
      " 1519/5000: episode: 1518, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 42280.582031, mean_absolute_error: 60.565781, mean_q: 1.339908\n",
      " 1520/5000: episode: 1519, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.372, 1.000], loss: 14226.375977, mean_absolute_error: 41.942932, mean_q: 1.339909\n",
      " 1521/5000: episode: 1520, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.657 [0.071, 1.000], loss: 28450.802734, mean_absolute_error: 51.415443, mean_q: 1.339402\n",
      " 1522/5000: episode: 1521, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.446, 1.000], loss: 14225.155273, mean_absolute_error: 41.974918, mean_q: 1.340011\n",
      " 1523/5000: episode: 1522, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.026, 1.000], loss: 0.899626, mean_absolute_error: 32.526543, mean_q: 1.340360\n",
      " 1524/5000: episode: 1523, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.018, 1.000], loss: 56762.781250, mean_absolute_error: 70.273643, mean_q: 1.340124\n",
      " 1525/5000: episode: 1524, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 42534.441406, mean_absolute_error: 60.833717, mean_q: 1.340915\n",
      " 1526/5000: episode: 1525, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.825], loss: 28192.234375, mean_absolute_error: 51.297142, mean_q: 1.341281\n",
      " 1527/5000: episode: 1526, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.912], loss: 28441.449219, mean_absolute_error: 51.499050, mean_q: 1.342075\n",
      " 1528/5000: episode: 1527, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.238, 1.000], loss: 14223.494141, mean_absolute_error: 42.056412, mean_q: 1.343160\n",
      " 1529/5000: episode: 1528, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.414 [0.000, 0.978], loss: 42263.562500, mean_absolute_error: 60.721664, mean_q: 1.344366\n",
      " 1530/5000: episode: 1529, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.687], loss: 0.905424, mean_absolute_error: 32.634117, mean_q: 1.344674\n",
      " 1531/5000: episode: 1530, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.682], loss: 0.906017, mean_absolute_error: 32.649300, mean_q: 1.345115\n",
      " 1532/5000: episode: 1531, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.000, 0.815], loss: 28308.480469, mean_absolute_error: 51.487076, mean_q: 1.345505\n",
      " 1533/5000: episode: 1532, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.020, 1.000], loss: 14085.617188, mean_absolute_error: 42.046314, mean_q: 1.345830\n",
      " 1534/5000: episode: 1533, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.265 [0.000, 0.829], loss: 14085.693359, mean_absolute_error: 42.054039, mean_q: 1.345292\n",
      " 1535/5000: episode: 1534, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.275, 1.000], loss: 0.906893, mean_absolute_error: 32.711662, mean_q: 1.345768\n",
      " 1536/5000: episode: 1535, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.258, 1.000], loss: 14083.471680, mean_absolute_error: 42.031654, mean_q: 1.342159\n",
      " 1537/5000: episode: 1536, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.495], loss: 0.905298, mean_absolute_error: 32.727005, mean_q: 1.344578\n",
      " 1538/5000: episode: 1537, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 0.904788, mean_absolute_error: 32.745853, mean_q: 1.344203\n",
      " 1539/5000: episode: 1538, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.000, 0.992], loss: 56462.351562, mean_absolute_error: 70.287201, mean_q: 1.343547\n",
      " 1540/5000: episode: 1539, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.070, 1.000], loss: 28428.220703, mean_absolute_error: 51.665314, mean_q: 1.342909\n",
      " 1541/5000: episode: 1540, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.521], loss: 14212.574219, mean_absolute_error: 42.224579, mean_q: 1.342719\n",
      " 1542/5000: episode: 1541, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.742], loss: 42371.023438, mean_absolute_error: 60.954384, mean_q: 1.342505\n",
      " 1543/5000: episode: 1542, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.788], loss: 14216.102539, mean_absolute_error: 42.248039, mean_q: 1.342356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1544/5000: episode: 1543, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.011, 1.000], loss: 28288.406250, mean_absolute_error: 51.627129, mean_q: 1.342648\n",
      " 1545/5000: episode: 1544, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 28287.843750, mean_absolute_error: 51.622391, mean_q: 1.342252\n",
      " 1546/5000: episode: 1545, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.483, 1.000], loss: 28154.292969, mean_absolute_error: 51.557457, mean_q: 1.342507\n",
      " 1547/5000: episode: 1546, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.158, 1.000], loss: 14076.716797, mean_absolute_error: 42.221199, mean_q: 1.342440\n",
      " 1548/5000: episode: 1547, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.000, 0.975], loss: 0.901963, mean_absolute_error: 32.865349, mean_q: 1.342100\n",
      " 1549/5000: episode: 1548, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.000, 0.734], loss: 14207.958008, mean_absolute_error: 42.336884, mean_q: 1.342466\n",
      " 1550/5000: episode: 1549, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.122, 1.000], loss: 42488.425781, mean_absolute_error: 61.155731, mean_q: 1.342826\n",
      " 1551/5000: episode: 1550, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.474 [0.000, 0.997], loss: 0.902970, mean_absolute_error: 32.899258, mean_q: 1.342842\n",
      " 1552/5000: episode: 1551, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.516], loss: 14206.286133, mean_absolute_error: 42.372688, mean_q: 1.343213\n",
      " 1553/5000: episode: 1552, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.170, 1.000], loss: 14205.283203, mean_absolute_error: 42.388817, mean_q: 1.343335\n",
      " 1554/5000: episode: 1553, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.825], loss: 14073.254883, mean_absolute_error: 42.294411, mean_q: 1.342859\n",
      " 1555/5000: episode: 1554, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.105, 1.000], loss: 28412.226562, mean_absolute_error: 51.843857, mean_q: 1.342705\n",
      " 1556/5000: episode: 1555, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.682], loss: 42609.406250, mean_absolute_error: 61.308414, mean_q: 1.342825\n",
      " 1557/5000: episode: 1556, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.366, 1.000], loss: 14202.939453, mean_absolute_error: 42.418617, mean_q: 1.342985\n",
      " 1558/5000: episode: 1557, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.958], loss: 0.903535, mean_absolute_error: 32.983440, mean_q: 1.343264\n",
      " 1559/5000: episode: 1558, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.673], loss: 14201.583984, mean_absolute_error: 42.443581, mean_q: 1.343437\n",
      " 1560/5000: episode: 1559, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.290, 1.000], loss: 0.904526, mean_absolute_error: 33.017452, mean_q: 1.344007\n",
      " 1561/5000: episode: 1560, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 56417.125000, mean_absolute_error: 70.525406, mean_q: 1.344306\n",
      " 1562/5000: episode: 1561, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.406, 1.000], loss: 14202.715820, mean_absolute_error: 42.485260, mean_q: 1.344145\n",
      " 1563/5000: episode: 1562, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.566], loss: 14198.883789, mean_absolute_error: 42.498367, mean_q: 1.344340\n",
      " 1564/5000: episode: 1563, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 28270.511719, mean_absolute_error: 51.852188, mean_q: 1.344056\n",
      " 1565/5000: episode: 1564, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.101, 1.000], loss: 56791.382812, mean_absolute_error: 70.857559, mean_q: 1.345227\n",
      " 1566/5000: episode: 1565, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.000, 0.927], loss: 0.907110, mean_absolute_error: 33.096840, mean_q: 1.345928\n",
      " 1567/5000: episode: 1566, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.108, 1.000], loss: 0.907125, mean_absolute_error: 33.105156, mean_q: 1.345937\n",
      " 1568/5000: episode: 1567, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 28261.533203, mean_absolute_error: 51.911263, mean_q: 1.347250\n",
      " 1569/5000: episode: 1568, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 14067.536133, mean_absolute_error: 42.482239, mean_q: 1.347815\n",
      " 1570/5000: episode: 1569, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 28263.503906, mean_absolute_error: 51.937904, mean_q: 1.348231\n",
      " 1571/5000: episode: 1570, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.621 [0.042, 1.000], loss: 14192.167969, mean_absolute_error: 42.595234, mean_q: 1.348801\n",
      " 1572/5000: episode: 1571, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 14068.160156, mean_absolute_error: 42.519428, mean_q: 1.349046\n",
      " 1573/5000: episode: 1572, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.000, 0.899], loss: 42326.761719, mean_absolute_error: 61.325684, mean_q: 1.349462\n",
      " 1574/5000: episode: 1573, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.857], loss: 14065.426758, mean_absolute_error: 42.549805, mean_q: 1.349583\n",
      " 1575/5000: episode: 1574, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.156, 1.000], loss: 42325.367188, mean_absolute_error: 61.351982, mean_q: 1.349585\n",
      " 1576/5000: episode: 1575, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.628], loss: 14064.021484, mean_absolute_error: 42.580349, mean_q: 1.349655\n",
      " 1577/5000: episode: 1576, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.121, 1.000], loss: 28250.349609, mean_absolute_error: 52.027664, mean_q: 1.349522\n",
      " 1578/5000: episode: 1577, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.288, 1.000], loss: 14191.868164, mean_absolute_error: 42.688751, mean_q: 1.349258\n",
      " 1579/5000: episode: 1578, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.000, 0.851], loss: 14186.934570, mean_absolute_error: 42.715225, mean_q: 1.349593\n",
      " 1580/5000: episode: 1579, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.596], loss: 14060.695312, mean_absolute_error: 42.635674, mean_q: 1.349812\n",
      " 1581/5000: episode: 1580, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.340 [0.000, 0.926], loss: 42323.238281, mean_absolute_error: 61.441475, mean_q: 1.349057\n",
      " 1582/5000: episode: 1581, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.142, 1.000], loss: 70675.421875, mean_absolute_error: 80.325027, mean_q: 1.350216\n",
      " 1583/5000: episode: 1582, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 14058.353516, mean_absolute_error: 42.699066, mean_q: 1.350885\n",
      " 1584/5000: episode: 1583, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 14187.511719, mean_absolute_error: 42.808296, mean_q: 1.351564\n",
      " 1585/5000: episode: 1584, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.184, 1.000], loss: 28114.462891, mean_absolute_error: 52.078850, mean_q: 1.351912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1586/5000: episode: 1585, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.000, 0.924], loss: 14187.521484, mean_absolute_error: 42.846790, mean_q: 1.352023\n",
      " 1587/5000: episode: 1586, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.557], loss: 0.914971, mean_absolute_error: 33.416580, mean_q: 1.351749\n",
      " 1588/5000: episode: 1587, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.311, 1.000], loss: 56591.062500, mean_absolute_error: 71.065636, mean_q: 1.351743\n",
      " 1589/5000: episode: 1588, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.006, 1.000], loss: 70391.000000, mean_absolute_error: 80.251968, mean_q: 1.352509\n",
      " 1590/5000: episode: 1589, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.000, 0.997], loss: 42407.226562, mean_absolute_error: 61.669716, mean_q: 1.352845\n",
      " 1591/5000: episode: 1590, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.501, 1.000], loss: 14052.324219, mean_absolute_error: 42.833168, mean_q: 1.353086\n",
      " 1592/5000: episode: 1591, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.363 [0.000, 0.946], loss: 14050.791992, mean_absolute_error: 42.857231, mean_q: 1.353511\n",
      " 1593/5000: episode: 1592, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.000, 0.977], loss: 28224.552734, mean_absolute_error: 52.304932, mean_q: 1.353808\n",
      " 1594/5000: episode: 1593, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.648 [0.063, 1.000], loss: 42144.523438, mean_absolute_error: 61.566231, mean_q: 1.353745\n",
      " 1595/5000: episode: 1594, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.601], loss: 0.917914, mean_absolute_error: 33.577217, mean_q: 1.353923\n",
      " 1596/5000: episode: 1595, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.731], loss: 14173.579102, mean_absolute_error: 43.019737, mean_q: 1.353865\n",
      " 1597/5000: episode: 1596, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.658], loss: 42137.058594, mean_absolute_error: 61.621513, mean_q: 1.354182\n",
      " 1598/5000: episode: 1597, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.758], loss: 14046.530273, mean_absolute_error: 42.960884, mean_q: 1.354264\n",
      " 1599/5000: episode: 1598, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.918], loss: 28088.341797, mean_absolute_error: 52.290375, mean_q: 1.353431\n",
      " 1600/5000: episode: 1599, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 14043.740234, mean_absolute_error: 42.983185, mean_q: 1.353358\n",
      " 1601/5000: episode: 1600, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.000, 0.991], loss: 28093.083984, mean_absolute_error: 52.344101, mean_q: 1.353133\n",
      " 1602/5000: episode: 1601, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.679 [0.093, 1.000], loss: 28211.281250, mean_absolute_error: 52.442558, mean_q: 1.352708\n",
      " 1603/5000: episode: 1602, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.000, 0.879], loss: 14041.597656, mean_absolute_error: 43.037621, mean_q: 1.352389\n",
      " 1604/5000: episode: 1603, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.703], loss: 28208.689453, mean_absolute_error: 52.461254, mean_q: 1.351853\n",
      " 1605/5000: episode: 1604, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.008, 1.000], loss: 0.915069, mean_absolute_error: 33.723160, mean_q: 1.351823\n",
      " 1606/5000: episode: 1605, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.797], loss: 28337.628906, mean_absolute_error: 52.587624, mean_q: 1.352038\n",
      " 1607/5000: episode: 1606, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.279, 1.000], loss: 56411.863281, mean_absolute_error: 71.265045, mean_q: 1.352275\n",
      " 1608/5000: episode: 1607, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.772 [0.250, 1.000], loss: 28091.390625, mean_absolute_error: 52.433037, mean_q: 1.351870\n",
      " 1609/5000: episode: 1608, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.389, 1.000], loss: 28330.171875, mean_absolute_error: 52.634411, mean_q: 1.352423\n",
      " 1610/5000: episode: 1609, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 28071.472656, mean_absolute_error: 52.460766, mean_q: 1.352361\n",
      " 1611/5000: episode: 1610, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 14164.188477, mean_absolute_error: 43.249741, mean_q: 1.352533\n",
      " 1612/5000: episode: 1611, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.000, 1.000], loss: 42231.453125, mean_absolute_error: 61.919144, mean_q: 1.352572\n",
      " 1613/5000: episode: 1612, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 14037.958008, mean_absolute_error: 43.182331, mean_q: 1.352471\n",
      " 1614/5000: episode: 1613, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.262, 1.000], loss: 28323.498047, mean_absolute_error: 52.701710, mean_q: 1.352124\n",
      " 1615/5000: episode: 1614, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.230, 1.000], loss: 28068.152344, mean_absolute_error: 52.539173, mean_q: 1.352273\n",
      " 1616/5000: episode: 1615, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.328, 1.000], loss: 0.915726, mean_absolute_error: 33.904953, mean_q: 1.352308\n",
      " 1617/5000: episode: 1616, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.123, 1.000], loss: 14031.189453, mean_absolute_error: 43.255238, mean_q: 1.352477\n",
      " 1618/5000: episode: 1617, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.217, 1.000], loss: 0.914910, mean_absolute_error: 33.928818, mean_q: 1.351705\n",
      " 1619/5000: episode: 1618, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 14159.209961, mean_absolute_error: 43.370117, mean_q: 1.351743\n",
      " 1620/5000: episode: 1619, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.000, 0.793], loss: 56372.476562, mean_absolute_error: 71.440186, mean_q: 1.351680\n",
      " 1621/5000: episode: 1620, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.858], loss: 28056.041016, mean_absolute_error: 52.628532, mean_q: 1.352110\n",
      " 1622/5000: episode: 1621, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.000, 0.925], loss: 0.915090, mean_absolute_error: 33.988426, mean_q: 1.351838\n",
      " 1623/5000: episode: 1622, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.547], loss: 42216.261719, mean_absolute_error: 62.071033, mean_q: 1.351772\n",
      " 1624/5000: episode: 1623, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.432, 1.000], loss: 0.914664, mean_absolute_error: 34.018311, mean_q: 1.351524\n",
      " 1625/5000: episode: 1624, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.168, 1.000], loss: 14030.487305, mean_absolute_error: 43.346786, mean_q: 1.350465\n",
      " 1626/5000: episode: 1625, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.013, 1.000], loss: 42203.019531, mean_absolute_error: 62.109535, mean_q: 1.350604\n",
      " 1627/5000: episode: 1626, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.458 [0.000, 0.994], loss: 0.912072, mean_absolute_error: 34.054512, mean_q: 1.349605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1628/5000: episode: 1627, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.788], loss: 0.911435, mean_absolute_error: 34.074772, mean_q: 1.349136\n",
      " 1629/5000: episode: 1628, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.059, 1.000], loss: 42328.859375, mean_absolute_error: 62.234314, mean_q: 1.348893\n",
      " 1630/5000: episode: 1629, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.159, 1.000], loss: 14153.301758, mean_absolute_error: 43.511127, mean_q: 1.348694\n",
      " 1631/5000: episode: 1630, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.000, 0.696], loss: 28304.634766, mean_absolute_error: 52.927223, mean_q: 1.348828\n",
      " 1632/5000: episode: 1631, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.330, 1.000], loss: 28175.109375, mean_absolute_error: 52.859394, mean_q: 1.349730\n",
      " 1633/5000: episode: 1632, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.559 [0.011, 1.000], loss: 28171.923828, mean_absolute_error: 52.870953, mean_q: 1.350266\n",
      " 1634/5000: episode: 1633, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.054, 1.000], loss: 28170.214844, mean_absolute_error: 52.887714, mean_q: 1.350714\n",
      " 1635/5000: episode: 1634, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.008, 1.000], loss: 14019.914062, mean_absolute_error: 43.492348, mean_q: 1.350988\n",
      " 1636/5000: episode: 1635, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.474 [0.000, 0.997], loss: 42058.144531, mean_absolute_error: 62.143532, mean_q: 1.351088\n",
      " 1637/5000: episode: 1636, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.490], loss: 28044.388672, mean_absolute_error: 52.837143, mean_q: 1.350932\n",
      " 1638/5000: episode: 1637, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.503, 1.000], loss: 14018.059570, mean_absolute_error: 43.535484, mean_q: 1.350961\n",
      " 1639/5000: episode: 1638, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.595 [0.027, 1.000], loss: 14017.810547, mean_absolute_error: 43.559052, mean_q: 1.350940\n",
      " 1640/5000: episode: 1639, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.680 [0.095, 1.000], loss: 28031.136719, mean_absolute_error: 52.887421, mean_q: 1.350846\n",
      " 1641/5000: episode: 1640, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.007, 1.000], loss: 14144.755859, mean_absolute_error: 43.686218, mean_q: 1.350921\n",
      " 1642/5000: episode: 1641, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 42042.441406, mean_absolute_error: 62.229584, mean_q: 1.350622\n",
      " 1643/5000: episode: 1642, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.444, 1.000], loss: 42039.179688, mean_absolute_error: 62.239845, mean_q: 1.350253\n",
      " 1644/5000: episode: 1643, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.000, 0.997], loss: 28285.347656, mean_absolute_error: 53.121803, mean_q: 1.349594\n",
      " 1645/5000: episode: 1644, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.474, 1.000], loss: 56175.484375, mean_absolute_error: 71.666573, mean_q: 1.349169\n",
      " 1646/5000: episode: 1645, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.000, 0.915], loss: 42030.605469, mean_absolute_error: 62.288109, mean_q: 1.349099\n",
      " 1647/5000: episode: 1646, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 14141.269531, mean_absolute_error: 43.774422, mean_q: 1.348731\n",
      " 1648/5000: episode: 1647, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.462 [0.000, 0.995], loss: 28150.400391, mean_absolute_error: 53.101105, mean_q: 1.348306\n",
      " 1649/5000: episode: 1648, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.898], loss: 14140.095703, mean_absolute_error: 43.814953, mean_q: 1.348223\n",
      " 1650/5000: episode: 1649, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.285, 1.000], loss: 42152.179688, mean_absolute_error: 62.440830, mean_q: 1.347811\n",
      " 1651/5000: episode: 1650, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.958], loss: 28010.882812, mean_absolute_error: 53.057564, mean_q: 1.347688\n",
      " 1652/5000: episode: 1651, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.000, 0.950], loss: 70426.101562, mean_absolute_error: 81.272614, mean_q: 1.346740\n",
      " 1653/5000: episode: 1652, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.187, 1.000], loss: 14004.710938, mean_absolute_error: 43.772430, mean_q: 1.346606\n",
      " 1654/5000: episode: 1653, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.046, 1.000], loss: 28008.367188, mean_absolute_error: 53.099834, mean_q: 1.346091\n",
      " 1655/5000: episode: 1654, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.089, 1.000], loss: 42273.902344, mean_absolute_error: 62.612843, mean_q: 1.345634\n",
      " 1656/5000: episode: 1655, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.061, 1.000], loss: 28136.328125, mean_absolute_error: 53.225594, mean_q: 1.344906\n",
      " 1657/5000: episode: 1656, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.044, 1.000], loss: 42270.054688, mean_absolute_error: 62.640060, mean_q: 1.344806\n",
      " 1658/5000: episode: 1657, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.431 [0.000, 0.985], loss: 70403.343750, mean_absolute_error: 81.364433, mean_q: 1.344882\n",
      " 1659/5000: episode: 1658, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.510], loss: 56264.617188, mean_absolute_error: 71.991699, mean_q: 1.345912\n",
      " 1660/5000: episode: 1659, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.636], loss: 13997.175781, mean_absolute_error: 43.908859, mean_q: 1.346723\n",
      " 1661/5000: episode: 1660, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 28127.019531, mean_absolute_error: 53.324623, mean_q: 1.347335\n",
      " 1662/5000: episode: 1661, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.457 [0.000, 0.994], loss: 0.908842, mean_absolute_error: 34.644573, mean_q: 1.347211\n",
      " 1663/5000: episode: 1662, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 13995.127930, mean_absolute_error: 43.971340, mean_q: 1.347573\n",
      " 1664/5000: episode: 1663, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.817], loss: 42251.230469, mean_absolute_error: 62.780724, mean_q: 1.347857\n",
      " 1665/5000: episode: 1664, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 0.869], loss: 42114.417969, mean_absolute_error: 62.699940, mean_q: 1.347953\n",
      " 1666/5000: episode: 1665, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.914], loss: 0.910421, mean_absolute_error: 34.737198, mean_q: 1.348385\n",
      " 1667/5000: episode: 1666, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.295 [0.000, 0.874], loss: 14128.512695, mean_absolute_error: 44.130711, mean_q: 1.347652\n",
      " 1668/5000: episode: 1667, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.611], loss: 27981.820312, mean_absolute_error: 53.348610, mean_q: 1.347427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1669/5000: episode: 1668, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.249, 1.000], loss: 13988.841797, mean_absolute_error: 44.091553, mean_q: 1.347443\n",
      " 1670/5000: episode: 1669, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.499, 1.000], loss: 0.907845, mean_absolute_error: 34.801262, mean_q: 1.346473\n",
      " 1671/5000: episode: 1670, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.000, 0.811], loss: 0.906492, mean_absolute_error: 34.805450, mean_q: 1.345467\n",
      " 1672/5000: episode: 1671, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.432, 1.000], loss: 14124.348633, mean_absolute_error: 44.216022, mean_q: 1.345034\n",
      " 1673/5000: episode: 1672, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.397, 1.000], loss: 13985.288086, mean_absolute_error: 44.148804, mean_q: 1.345281\n",
      " 1674/5000: episode: 1673, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.723], loss: 13984.317383, mean_absolute_error: 44.154999, mean_q: 1.344901\n",
      " 1675/5000: episode: 1674, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.513], loss: 27968.173828, mean_absolute_error: 53.455452, mean_q: 1.344134\n",
      " 1676/5000: episode: 1675, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.317 [0.000, 0.902], loss: 0.904102, mean_absolute_error: 34.892239, mean_q: 1.343693\n",
      " 1677/5000: episode: 1676, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.550], loss: 42225.261719, mean_absolute_error: 62.973930, mean_q: 1.342816\n",
      " 1678/5000: episode: 1677, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.263, 1.000], loss: 42365.535156, mean_absolute_error: 63.078461, mean_q: 1.342012\n",
      " 1679/5000: episode: 1678, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.410 [0.000, 0.976], loss: 70180.500000, mean_absolute_error: 81.565186, mean_q: 1.341605\n",
      " 1680/5000: episode: 1679, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.279, 1.000], loss: 14119.753906, mean_absolute_error: 44.340500, mean_q: 1.342164\n",
      " 1681/5000: episode: 1680, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: 0.902605, mean_absolute_error: 34.965645, mean_q: 1.342573\n",
      " 1682/5000: episode: 1681, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: 28099.730469, mean_absolute_error: 53.660835, mean_q: 1.342662\n",
      " 1683/5000: episode: 1682, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.662], loss: 28093.779297, mean_absolute_error: 53.671852, mean_q: 1.342659\n",
      " 1684/5000: episode: 1683, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.762], loss: 13976.910156, mean_absolute_error: 44.306564, mean_q: 1.342637\n",
      " 1685/5000: episode: 1684, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.535], loss: 42206.074219, mean_absolute_error: 63.074120, mean_q: 1.341709\n",
      " 1686/5000: episode: 1685, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.803], loss: 42204.109375, mean_absolute_error: 63.109802, mean_q: 1.341704\n",
      " 1687/5000: episode: 1686, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 42202.226562, mean_absolute_error: 63.127068, mean_q: 1.341482\n",
      " 1688/5000: episode: 1687, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.593], loss: 28087.683594, mean_absolute_error: 53.763847, mean_q: 1.341470\n",
      " 1689/5000: episode: 1688, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 42055.941406, mean_absolute_error: 63.050308, mean_q: 1.341146\n",
      " 1690/5000: episode: 1689, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.000, 0.999], loss: 13971.318359, mean_absolute_error: 44.409515, mean_q: 1.341144\n",
      " 1691/5000: episode: 1690, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.008, 1.000], loss: 56305.195312, mean_absolute_error: 72.571487, mean_q: 1.340807\n",
      " 1692/5000: episode: 1691, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.004, 1.000], loss: 14111.543945, mean_absolute_error: 44.546761, mean_q: 1.340959\n",
      " 1693/5000: episode: 1692, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.065, 1.000], loss: 28078.486328, mean_absolute_error: 53.837986, mean_q: 1.340999\n",
      " 1694/5000: episode: 1693, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.402, 1.000], loss: 84229.039062, mean_absolute_error: 91.181915, mean_q: 1.341442\n",
      " 1695/5000: episode: 1694, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 0.902594, mean_absolute_error: 35.230705, mean_q: 1.342572\n",
      " 1696/5000: episode: 1695, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 14109.694336, mean_absolute_error: 44.627640, mean_q: 1.343392\n",
      " 1697/5000: episode: 1696, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.000, 0.949], loss: 0.904679, mean_absolute_error: 35.262615, mean_q: 1.344120\n",
      " 1698/5000: episode: 1697, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.299, 1.000], loss: 14104.766602, mean_absolute_error: 44.636559, mean_q: 1.344377\n",
      " 1699/5000: episode: 1698, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.943], loss: 13964.571289, mean_absolute_error: 44.574753, mean_q: 1.345424\n",
      " 1700/5000: episode: 1699, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.773], loss: 13963.819336, mean_absolute_error: 44.594242, mean_q: 1.345621\n",
      " 1701/5000: episode: 1700, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.690 [0.107, 1.000], loss: 14105.695312, mean_absolute_error: 44.717743, mean_q: 1.345757\n",
      " 1702/5000: episode: 1701, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 55847.027344, mean_absolute_error: 72.455475, mean_q: 1.345866\n",
      " 1703/5000: episode: 1702, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.719], loss: 14100.813477, mean_absolute_error: 44.748783, mean_q: 1.345600\n",
      " 1704/5000: episode: 1703, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.000, 0.969], loss: 0.906055, mean_absolute_error: 35.377686, mean_q: 1.345142\n",
      " 1705/5000: episode: 1704, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.016, 1.000], loss: 70214.742188, mean_absolute_error: 82.063889, mean_q: 1.344800\n",
      " 1706/5000: episode: 1705, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.618], loss: 13959.100586, mean_absolute_error: 44.701637, mean_q: 1.345506\n",
      " 1707/5000: episode: 1706, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.000, 0.930], loss: 13958.238281, mean_absolute_error: 44.717926, mean_q: 1.345736\n",
      " 1708/5000: episode: 1707, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.269 [0.000, 0.835], loss: 13957.403320, mean_absolute_error: 44.737137, mean_q: 1.345886\n",
      " 1709/5000: episode: 1708, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.658], loss: 27915.548828, mean_absolute_error: 54.030945, mean_q: 1.345879\n",
      " 1710/5000: episode: 1709, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.275, 1.000], loss: 27911.101562, mean_absolute_error: 54.035545, mean_q: 1.344936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1711/5000: episode: 1710, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.517 [0.002, 1.000], loss: 0.905861, mean_absolute_error: 35.506199, mean_q: 1.344986\n",
      " 1712/5000: episode: 1711, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 56245.132812, mean_absolute_error: 72.904907, mean_q: 1.343743\n",
      " 1713/5000: episode: 1712, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.321, 1.000], loss: 28190.166016, mean_absolute_error: 54.270023, mean_q: 1.344033\n",
      " 1714/5000: episode: 1713, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 56234.867188, mean_absolute_error: 72.930611, mean_q: 1.344320\n",
      " 1715/5000: episode: 1714, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.516], loss: 28043.605469, mean_absolute_error: 54.217094, mean_q: 1.345296\n",
      " 1716/5000: episode: 1715, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.299, 1.000], loss: 28042.707031, mean_absolute_error: 54.239082, mean_q: 1.346114\n",
      " 1717/5000: episode: 1716, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.706 [0.127, 1.000], loss: 27901.156250, mean_absolute_error: 54.147987, mean_q: 1.346267\n",
      " 1718/5000: episode: 1717, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.314 [0.000, 0.898], loss: 42125.781250, mean_absolute_error: 63.640869, mean_q: 1.346743\n",
      " 1719/5000: episode: 1718, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.443, 1.000], loss: 98196.656250, mean_absolute_error: 100.918716, mean_q: 1.346893\n",
      " 1720/5000: episode: 1719, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.706 [0.126, 1.000], loss: 14086.569336, mean_absolute_error: 45.048874, mean_q: 1.347694\n",
      " 1721/5000: episode: 1720, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.031, 1.000], loss: 28034.537109, mean_absolute_error: 54.320576, mean_q: 1.347696\n",
      " 1722/5000: episode: 1721, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.211, 1.000], loss: 27892.414062, mean_absolute_error: 54.257900, mean_q: 1.348679\n",
      " 1723/5000: episode: 1722, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.265 [0.000, 0.828], loss: 14083.458984, mean_absolute_error: 45.105404, mean_q: 1.348903\n",
      " 1724/5000: episode: 1723, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.278, 1.000], loss: 28026.593750, mean_absolute_error: 54.392502, mean_q: 1.349292\n",
      " 1725/5000: episode: 1724, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 0.911532, mean_absolute_error: 35.779396, mean_q: 1.349206\n",
      " 1726/5000: episode: 1725, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.908], loss: 42103.742188, mean_absolute_error: 63.782883, mean_q: 1.349095\n",
      " 1727/5000: episode: 1726, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.284, 1.000], loss: 27884.697266, mean_absolute_error: 54.339828, mean_q: 1.348826\n",
      " 1728/5000: episode: 1727, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.019, 1.000], loss: 27882.001953, mean_absolute_error: 54.369682, mean_q: 1.348742\n",
      " 1729/5000: episode: 1728, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.243, 1.000], loss: 13940.427734, mean_absolute_error: 45.121857, mean_q: 1.348151\n",
      " 1730/5000: episode: 1729, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.000, 0.992], loss: 0.909209, mean_absolute_error: 35.874378, mean_q: 1.347486\n",
      " 1731/5000: episode: 1730, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.252, 1.000], loss: 13938.828125, mean_absolute_error: 45.139496, mean_q: 1.346815\n",
      " 1732/5000: episode: 1731, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.735 [0.172, 1.000], loss: 28018.300781, mean_absolute_error: 54.518478, mean_q: 1.346396\n",
      " 1733/5000: episode: 1732, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.614], loss: 41812.851562, mean_absolute_error: 63.685295, mean_q: 1.345911\n",
      " 1734/5000: episode: 1733, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.638 [0.055, 1.000], loss: 41808.351562, mean_absolute_error: 63.692863, mean_q: 1.345453\n",
      " 1735/5000: episode: 1734, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.314, 1.000], loss: 42088.421875, mean_absolute_error: 63.919380, mean_q: 1.345203\n",
      " 1736/5000: episode: 1735, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 13934.794922, mean_absolute_error: 45.221397, mean_q: 1.345045\n",
      " 1737/5000: episode: 1736, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.470, 1.000], loss: 28007.601562, mean_absolute_error: 54.585018, mean_q: 1.344572\n",
      " 1738/5000: episode: 1737, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.758], loss: 14075.666992, mean_absolute_error: 45.338776, mean_q: 1.343953\n",
      " 1739/5000: episode: 1738, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.562], loss: 13932.875977, mean_absolute_error: 45.268585, mean_q: 1.344065\n",
      " 1740/5000: episode: 1739, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 0.903939, mean_absolute_error: 36.027672, mean_q: 1.343572\n",
      " 1741/5000: episode: 1740, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.437, 1.000], loss: 13930.732422, mean_absolute_error: 45.301052, mean_q: 1.343117\n",
      " 1742/5000: episode: 1741, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.045, 1.000], loss: 13929.952148, mean_absolute_error: 45.307537, mean_q: 1.342175\n",
      " 1743/5000: episode: 1742, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.312, 1.000], loss: 0.901174, mean_absolute_error: 36.061516, mean_q: 1.341512\n",
      " 1744/5000: episode: 1743, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.109, 1.000], loss: 0.900305, mean_absolute_error: 36.072578, mean_q: 1.340865\n",
      " 1745/5000: episode: 1744, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.234, 1.000], loss: 14070.224609, mean_absolute_error: 45.447174, mean_q: 1.340466\n",
      " 1746/5000: episode: 1745, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.686], loss: 41925.171875, mean_absolute_error: 63.959740, mean_q: 1.339994\n",
      " 1747/5000: episode: 1746, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.672], loss: 27853.818359, mean_absolute_error: 54.622807, mean_q: 1.339819\n",
      " 1748/5000: episode: 1747, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.649], loss: 13926.681641, mean_absolute_error: 45.372406, mean_q: 1.339386\n",
      " 1749/5000: episode: 1748, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.000, 0.800], loss: 14072.570312, mean_absolute_error: 45.480434, mean_q: 1.337937\n",
      " 1750/5000: episode: 1749, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.618 [0.040, 1.000], loss: 27991.759766, mean_absolute_error: 54.757706, mean_q: 1.337799\n",
      " 1751/5000: episode: 1750, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 0.895366, mean_absolute_error: 36.170425, mean_q: 1.337181\n",
      " 1752/5000: episode: 1751, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.898], loss: 27989.939453, mean_absolute_error: 54.780865, mean_q: 1.336515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1753/5000: episode: 1752, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.153, 1.000], loss: 14066.291016, mean_absolute_error: 45.545486, mean_q: 1.335736\n",
      " 1754/5000: episode: 1753, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 14065.788086, mean_absolute_error: 45.557693, mean_q: 1.335322\n",
      " 1755/5000: episode: 1754, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 27986.099609, mean_absolute_error: 54.814476, mean_q: 1.334814\n",
      " 1756/5000: episode: 1755, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.404, 1.000], loss: 0.891410, mean_absolute_error: 36.226036, mean_q: 1.334220\n",
      " 1757/5000: episode: 1756, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.506 [0.000, 1.000], loss: 27984.914062, mean_absolute_error: 54.831783, mean_q: 1.333590\n",
      " 1758/5000: episode: 1757, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.433, 1.000], loss: 55820.394531, mean_absolute_error: 73.343369, mean_q: 1.333277\n",
      " 1759/5000: episode: 1758, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.000, 0.889], loss: 0.889952, mean_absolute_error: 36.272537, mean_q: 1.333128\n",
      " 1760/5000: episode: 1759, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.678], loss: 0.890309, mean_absolute_error: 36.296242, mean_q: 1.333397\n",
      " 1761/5000: episode: 1760, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 55957.421875, mean_absolute_error: 73.492584, mean_q: 1.333342\n",
      " 1762/5000: episode: 1761, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.598 [0.028, 1.000], loss: 0.890250, mean_absolute_error: 36.317799, mean_q: 1.333351\n",
      " 1763/5000: episode: 1762, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.175, 1.000], loss: 27975.234375, mean_absolute_error: 54.918304, mean_q: 1.333320\n",
      " 1764/5000: episode: 1763, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 14058.633789, mean_absolute_error: 45.700783, mean_q: 1.333587\n",
      " 1765/5000: episode: 1764, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.000, 0.778], loss: 13915.381836, mean_absolute_error: 45.617592, mean_q: 1.333744\n",
      " 1766/5000: episode: 1765, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 0.890355, mean_absolute_error: 36.386280, mean_q: 1.333430\n",
      " 1767/5000: episode: 1766, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.000, 0.965], loss: 14056.640625, mean_absolute_error: 45.735683, mean_q: 1.332752\n",
      " 1768/5000: episode: 1767, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.121, 1.000], loss: 14057.239258, mean_absolute_error: 45.758087, mean_q: 1.332610\n",
      " 1769/5000: episode: 1768, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.444, 1.000], loss: 0.889004, mean_absolute_error: 36.429276, mean_q: 1.332418\n",
      " 1770/5000: episode: 1769, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.907], loss: 27829.929688, mean_absolute_error: 54.932674, mean_q: 1.332019\n",
      " 1771/5000: episode: 1770, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.460, 1.000], loss: 14053.952148, mean_absolute_error: 45.802956, mean_q: 1.331697\n",
      " 1772/5000: episode: 1771, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.030, 1.000], loss: 14053.168945, mean_absolute_error: 45.821213, mean_q: 1.331650\n",
      " 1773/5000: episode: 1772, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.246, 1.000], loss: 55788.453125, mean_absolute_error: 73.543343, mean_q: 1.331036\n",
      " 1774/5000: episode: 1773, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.109, 1.000], loss: 0.887672, mean_absolute_error: 36.514999, mean_q: 1.331418\n",
      " 1775/5000: episode: 1774, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.002, 1.000], loss: 41866.015625, mean_absolute_error: 64.337334, mean_q: 1.330712\n",
      " 1776/5000: episode: 1775, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.069, 1.000], loss: 0.886388, mean_absolute_error: 36.536083, mean_q: 1.330455\n",
      " 1777/5000: episode: 1776, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.168, 1.000], loss: 69959.812500, mean_absolute_error: 83.047699, mean_q: 1.330250\n",
      " 1778/5000: episode: 1777, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 55764.468750, mean_absolute_error: 73.627487, mean_q: 1.330506\n",
      " 1779/5000: episode: 1778, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.494], loss: 27812.654297, mean_absolute_error: 55.069912, mean_q: 1.330291\n",
      " 1780/5000: episode: 1779, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.505], loss: 55760.910156, mean_absolute_error: 73.663284, mean_q: 1.330218\n",
      " 1781/5000: episode: 1780, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.466, 1.000], loss: 41998.437500, mean_absolute_error: 64.546272, mean_q: 1.331001\n",
      " 1782/5000: episode: 1781, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.533 [0.004, 1.000], loss: 41991.214844, mean_absolute_error: 64.567993, mean_q: 1.330370\n",
      " 1783/5000: episode: 1782, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 14044.762695, mean_absolute_error: 46.018330, mean_q: 1.330391\n",
      " 1784/5000: episode: 1783, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.117, 1.000], loss: 13900.146484, mean_absolute_error: 45.934395, mean_q: 1.331107\n",
      " 1785/5000: episode: 1784, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.217, 1.000], loss: 0.887573, mean_absolute_error: 36.724861, mean_q: 1.331345\n",
      " 1786/5000: episode: 1785, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.954], loss: 41694.046875, mean_absolute_error: 64.441528, mean_q: 1.331448\n",
      " 1787/5000: episode: 1786, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.489, 1.000], loss: 13897.571289, mean_absolute_error: 45.996681, mean_q: 1.330975\n",
      " 1788/5000: episode: 1787, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.000, 0.882], loss: 13901.853516, mean_absolute_error: 46.015671, mean_q: 1.330133\n",
      " 1789/5000: episode: 1788, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.415, 1.000], loss: 14040.083984, mean_absolute_error: 46.123901, mean_q: 1.329126\n",
      " 1790/5000: episode: 1789, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.353, 1.000], loss: 13894.742188, mean_absolute_error: 46.037720, mean_q: 1.328452\n",
      " 1791/5000: episode: 1790, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.007, 1.000], loss: 28080.183594, mean_absolute_error: 55.487179, mean_q: 1.327788\n",
      " 1792/5000: episode: 1791, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.589 [0.024, 1.000], loss: 13893.251953, mean_absolute_error: 46.053066, mean_q: 1.327156\n",
      " 1793/5000: episode: 1792, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.664 [0.078, 1.000], loss: 27929.212891, mean_absolute_error: 55.410576, mean_q: 1.327418\n",
      " 1794/5000: episode: 1793, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 41675.562500, mean_absolute_error: 64.548859, mean_q: 1.327213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1795/5000: episode: 1794, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.000, 0.806], loss: 13890.741211, mean_absolute_error: 46.109489, mean_q: 1.327192\n",
      " 1796/5000: episode: 1795, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.338, 1.000], loss: 14035.275391, mean_absolute_error: 46.226349, mean_q: 1.326665\n",
      " 1797/5000: episode: 1796, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.000, 0.696], loss: 13889.100586, mean_absolute_error: 46.149452, mean_q: 1.326911\n",
      " 1798/5000: episode: 1797, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.206, 1.000], loss: 14037.465820, mean_absolute_error: 46.272465, mean_q: 1.326564\n",
      " 1799/5000: episode: 1798, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.052, 1.000], loss: 14041.622070, mean_absolute_error: 46.289032, mean_q: 1.326193\n",
      " 1800/5000: episode: 1799, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.013, 1.000], loss: 13886.807617, mean_absolute_error: 46.204365, mean_q: 1.326169\n",
      " 1801/5000: episode: 1800, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.288 [0.000, 0.865], loss: 27771.085938, mean_absolute_error: 55.445732, mean_q: 1.325591\n",
      " 1802/5000: episode: 1801, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.318, 1.000], loss: 13885.225586, mean_absolute_error: 46.234360, mean_q: 1.324680\n",
      " 1803/5000: episode: 1802, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.504], loss: 13883.751953, mean_absolute_error: 46.256538, mean_q: 1.323902\n",
      " 1804/5000: episode: 1803, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.000, 0.996], loss: 27912.744141, mean_absolute_error: 55.575726, mean_q: 1.322125\n",
      " 1805/5000: episode: 1804, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.039, 1.000], loss: 27763.222656, mean_absolute_error: 55.503639, mean_q: 1.322149\n",
      " 1806/5000: episode: 1805, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.261 [0.000, 0.822], loss: 0.874275, mean_absolute_error: 37.075485, mean_q: 1.321327\n",
      " 1807/5000: episode: 1806, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.000, 0.999], loss: 27910.382812, mean_absolute_error: 55.621559, mean_q: 1.320265\n",
      " 1808/5000: episode: 1807, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 28064.566406, mean_absolute_error: 55.742661, mean_q: 1.319810\n",
      " 1809/5000: episode: 1808, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.067, 1.000], loss: 27761.128906, mean_absolute_error: 55.542645, mean_q: 1.319522\n",
      " 1810/5000: episode: 1809, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.008, 1.000], loss: 27908.128906, mean_absolute_error: 55.665405, mean_q: 1.319306\n",
      " 1811/5000: episode: 1810, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.002, 1.000], loss: 13879.606445, mean_absolute_error: 46.356590, mean_q: 1.318970\n",
      " 1812/5000: episode: 1811, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.677], loss: 13877.314453, mean_absolute_error: 46.361237, mean_q: 1.318196\n",
      " 1813/5000: episode: 1812, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.591], loss: 41778.578125, mean_absolute_error: 64.922356, mean_q: 1.317733\n",
      " 1814/5000: episode: 1813, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.710], loss: 0.868483, mean_absolute_error: 37.184792, mean_q: 1.316939\n",
      " 1815/5000: episode: 1814, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.894], loss: 27901.560547, mean_absolute_error: 55.729301, mean_q: 1.315751\n",
      " 1816/5000: episode: 1815, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 14024.465820, mean_absolute_error: 46.532059, mean_q: 1.314999\n",
      " 1817/5000: episode: 1816, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.531], loss: 41922.871094, mean_absolute_error: 65.083084, mean_q: 1.314183\n",
      " 1818/5000: episode: 1817, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.669], loss: 14023.442383, mean_absolute_error: 46.558754, mean_q: 1.313448\n",
      " 1819/5000: episode: 1818, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.000, 0.891], loss: 28053.707031, mean_absolute_error: 55.895813, mean_q: 1.312669\n",
      " 1820/5000: episode: 1819, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.660], loss: 0.863182, mean_absolute_error: 37.275928, mean_q: 1.312909\n",
      " 1821/5000: episode: 1820, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.474 [0.000, 0.997], loss: 27891.779297, mean_absolute_error: 55.819557, mean_q: 1.312360\n",
      " 1822/5000: episode: 1821, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.226, 1.000], loss: 41913.593750, mean_absolute_error: 65.149979, mean_q: 1.311905\n",
      " 1823/5000: episode: 1822, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.533 [0.004, 1.000], loss: 0.862223, mean_absolute_error: 37.328857, mean_q: 1.312181\n",
      " 1824/5000: episode: 1823, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.859], loss: 55775.714844, mean_absolute_error: 74.391472, mean_q: 1.311672\n",
      " 1825/5000: episode: 1824, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.559], loss: 27733.087891, mean_absolute_error: 55.770180, mean_q: 1.311996\n",
      " 1826/5000: episode: 1825, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.702], loss: 14017.248047, mean_absolute_error: 46.686699, mean_q: 1.312380\n",
      " 1827/5000: episode: 1826, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.391, 1.000], loss: 27881.703125, mean_absolute_error: 55.904526, mean_q: 1.312243\n",
      " 1828/5000: episode: 1827, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 14015.375977, mean_absolute_error: 46.737816, mean_q: 1.312751\n",
      " 1829/5000: episode: 1828, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.023, 1.000], loss: 27877.615234, mean_absolute_error: 55.946030, mean_q: 1.312516\n",
      " 1830/5000: episode: 1829, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.000, 0.972], loss: 55753.687500, mean_absolute_error: 74.494400, mean_q: 1.312985\n",
      " 1831/5000: episode: 1830, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.054, 1.000], loss: 0.863816, mean_absolute_error: 37.477798, mean_q: 1.313394\n",
      " 1832/5000: episode: 1831, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.160, 1.000], loss: 27728.271484, mean_absolute_error: 55.901127, mean_q: 1.313002\n",
      " 1833/5000: episode: 1832, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.319, 1.000], loss: 13860.592773, mean_absolute_error: 46.721123, mean_q: 1.312891\n",
      " 1834/5000: episode: 1833, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.792], loss: 28019.945312, mean_absolute_error: 56.144585, mean_q: 1.312204\n",
      " 1835/5000: episode: 1834, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.008, 1.000], loss: 13858.583984, mean_absolute_error: 46.755646, mean_q: 1.311931\n",
      " 1836/5000: episode: 1835, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.190, 1.000], loss: 28017.494141, mean_absolute_error: 56.185196, mean_q: 1.311340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1837/5000: episode: 1836, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.340, 1.000], loss: 41726.773438, mean_absolute_error: 65.302299, mean_q: 1.310953\n",
      " 1838/5000: episode: 1837, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.306, 1.000], loss: 27862.736328, mean_absolute_error: 56.101582, mean_q: 1.310243\n",
      " 1839/5000: episode: 1838, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.620], loss: 0.859345, mean_absolute_error: 37.617706, mean_q: 1.309986\n",
      " 1840/5000: episode: 1839, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.000, 0.997], loss: 27859.843750, mean_absolute_error: 56.148819, mean_q: 1.309468\n",
      " 1841/5000: episode: 1840, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.049, 1.000], loss: 0.857748, mean_absolute_error: 37.653389, mean_q: 1.308768\n",
      " 1842/5000: episode: 1841, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 41712.210938, mean_absolute_error: 65.374359, mean_q: 1.307916\n",
      " 1843/5000: episode: 1842, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.305, 1.000], loss: 27856.580078, mean_absolute_error: 56.191254, mean_q: 1.307174\n",
      " 1844/5000: episode: 1843, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.197, 1.000], loss: 0.855089, mean_absolute_error: 37.705475, mean_q: 1.306736\n",
      " 1845/5000: episode: 1844, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.441, 1.000], loss: 14003.611328, mean_absolute_error: 47.024139, mean_q: 1.306414\n",
      " 1846/5000: episode: 1845, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.533], loss: 27854.019531, mean_absolute_error: 56.244061, mean_q: 1.306374\n",
      " 1847/5000: episode: 1846, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.391 [0.000, 0.966], loss: 0.853846, mean_absolute_error: 37.745571, mean_q: 1.305784\n",
      " 1848/5000: episode: 1847, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.404, 1.000], loss: 41852.621094, mean_absolute_error: 65.566902, mean_q: 1.305225\n",
      " 1849/5000: episode: 1848, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.778 [0.267, 1.000], loss: 27694.859375, mean_absolute_error: 56.168552, mean_q: 1.305197\n",
      " 1850/5000: episode: 1849, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.396, 1.000], loss: 27999.916016, mean_absolute_error: 56.394932, mean_q: 1.304593\n",
      " 1851/5000: episode: 1850, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.517], loss: 13845.311523, mean_absolute_error: 47.003944, mean_q: 1.304184\n",
      " 1852/5000: episode: 1851, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.000, 0.942], loss: 27997.167969, mean_absolute_error: 56.427700, mean_q: 1.302966\n",
      " 1853/5000: episode: 1852, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.133, 1.000], loss: 0.851690, mean_absolute_error: 37.860374, mean_q: 1.304136\n",
      " 1854/5000: episode: 1853, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.195, 1.000], loss: 27839.339844, mean_absolute_error: 56.359650, mean_q: 1.303709\n",
      " 1855/5000: episode: 1854, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 14001.481445, mean_absolute_error: 47.192612, mean_q: 1.303485\n",
      " 1856/5000: episode: 1855, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.031, 1.000], loss: 13841.716797, mean_absolute_error: 47.096912, mean_q: 1.303578\n",
      " 1857/5000: episode: 1856, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.000, 0.923], loss: 55667.390625, mean_absolute_error: 74.896873, mean_q: 1.303426\n",
      " 1858/5000: episode: 1857, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.207, 1.000], loss: 13997.488281, mean_absolute_error: 47.242283, mean_q: 1.303405\n",
      " 1859/5000: episode: 1858, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.620], loss: 13997.997070, mean_absolute_error: 47.268417, mean_q: 1.303473\n",
      " 1860/5000: episode: 1859, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.395 [0.000, 0.968], loss: 27982.849609, mean_absolute_error: 56.580208, mean_q: 1.303520\n",
      " 1861/5000: episode: 1860, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.007, 1.000], loss: 27831.097656, mean_absolute_error: 56.477978, mean_q: 1.303392\n",
      " 1862/5000: episode: 1861, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.576], loss: 27826.001953, mean_absolute_error: 56.502975, mean_q: 1.303921\n",
      " 1863/5000: episode: 1862, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.139, 1.000], loss: 55497.199219, mean_absolute_error: 74.905380, mean_q: 1.304602\n",
      " 1864/5000: episode: 1863, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.000, 0.823], loss: 0.852610, mean_absolute_error: 38.061127, mean_q: 1.304838\n",
      " 1865/5000: episode: 1864, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.676], loss: 0.852421, mean_absolute_error: 38.078770, mean_q: 1.304694\n",
      " 1866/5000: episode: 1865, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 27818.544922, mean_absolute_error: 56.575317, mean_q: 1.304625\n",
      " 1867/5000: episode: 1866, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.237, 1.000], loss: 13984.909180, mean_absolute_error: 47.410282, mean_q: 1.304695\n",
      " 1868/5000: episode: 1867, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.270 [0.000, 0.838], loss: 27968.890625, mean_absolute_error: 56.699333, mean_q: 1.304043\n",
      " 1869/5000: episode: 1868, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.689], loss: 13832.502930, mean_absolute_error: 47.336342, mean_q: 1.304758\n",
      " 1870/5000: episode: 1869, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.499, 1.000], loss: 27666.425781, mean_absolute_error: 56.538811, mean_q: 1.304473\n",
      " 1871/5000: episode: 1870, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.430 [0.000, 0.985], loss: 0.851096, mean_absolute_error: 38.169994, mean_q: 1.303678\n",
      " 1872/5000: episode: 1871, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.957], loss: 13830.770508, mean_absolute_error: 47.365616, mean_q: 1.302421\n",
      " 1873/5000: episode: 1872, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.495, 1.000], loss: 0.850038, mean_absolute_error: 38.207901, mean_q: 1.302867\n",
      " 1874/5000: episode: 1873, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: 13980.035156, mean_absolute_error: 47.508293, mean_q: 1.302136\n",
      " 1875/5000: episode: 1874, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.074, 1.000], loss: 55763.960938, mean_absolute_error: 75.289291, mean_q: 1.301590\n",
      " 1876/5000: episode: 1875, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.137, 1.000], loss: 0.847743, mean_absolute_error: 38.255688, mean_q: 1.301108\n",
      " 1877/5000: episode: 1876, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.548 [0.008, 1.000], loss: 0.846761, mean_absolute_error: 38.265495, mean_q: 1.300353\n",
      " 1878/5000: episode: 1877, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.513], loss: 27650.171875, mean_absolute_error: 56.628746, mean_q: 1.299457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1879/5000: episode: 1878, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.554 [0.010, 1.000], loss: 13977.334961, mean_absolute_error: 47.581749, mean_q: 1.298935\n",
      " 1880/5000: episode: 1879, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.687], loss: 0.843886, mean_absolute_error: 38.306877, mean_q: 1.298141\n",
      " 1881/5000: episode: 1880, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.000, 0.992], loss: 55603.242188, mean_absolute_error: 75.245392, mean_q: 1.297001\n",
      " 1882/5000: episode: 1881, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.943], loss: 41631.296875, mean_absolute_error: 65.982872, mean_q: 1.296538\n",
      " 1883/5000: episode: 1882, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.647], loss: 27796.537109, mean_absolute_error: 56.818016, mean_q: 1.295806\n",
      " 1884/5000: episode: 1883, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 13974.731445, mean_absolute_error: 47.648560, mean_q: 1.294883\n",
      " 1885/5000: episode: 1884, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.559 [0.011, 1.000], loss: 55585.632812, mean_absolute_error: 75.279739, mean_q: 1.293298\n",
      " 1886/5000: episode: 1885, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.758], loss: 0.838351, mean_absolute_error: 38.404617, mean_q: 1.293873\n",
      " 1887/5000: episode: 1886, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.273, 1.000], loss: 27792.876953, mean_absolute_error: 56.882195, mean_q: 1.293385\n",
      " 1888/5000: episode: 1887, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.000, 0.764], loss: 0.836735, mean_absolute_error: 38.435074, mean_q: 1.292621\n",
      " 1889/5000: episode: 1888, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.145, 1.000], loss: 55577.894531, mean_absolute_error: 75.372101, mean_q: 1.292050\n",
      " 1890/5000: episode: 1889, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 13814.831055, mean_absolute_error: 47.644981, mean_q: 1.291484\n",
      " 1891/5000: episode: 1890, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 0.833985, mean_absolute_error: 38.478344, mean_q: 1.290492\n",
      " 1892/5000: episode: 1891, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.009, 1.000], loss: 41599.824219, mean_absolute_error: 66.131561, mean_q: 1.290035\n",
      " 1893/5000: episode: 1892, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.039, 1.000], loss: 13969.229492, mean_absolute_error: 47.807190, mean_q: 1.289744\n",
      " 1894/5000: episode: 1893, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.012, 1.000], loss: 27779.011719, mean_absolute_error: 56.986633, mean_q: 1.289299\n",
      " 1895/5000: episode: 1894, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.151, 1.000], loss: 13967.867188, mean_absolute_error: 47.824959, mean_q: 1.288695\n",
      " 1896/5000: episode: 1895, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.397, 1.000], loss: 27618.830078, mean_absolute_error: 56.913013, mean_q: 1.288354\n",
      " 1897/5000: episode: 1896, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.462 [0.000, 0.995], loss: 13809.102539, mean_absolute_error: 47.751339, mean_q: 1.287519\n",
      " 1898/5000: episode: 1897, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 41423.285156, mean_absolute_error: 66.103851, mean_q: 1.286794\n",
      " 1899/5000: episode: 1898, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.553 [0.009, 1.000], loss: 0.828094, mean_absolute_error: 38.622101, mean_q: 1.285929\n",
      " 1900/5000: episode: 1899, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.772], loss: 0.826863, mean_absolute_error: 38.636116, mean_q: 1.284972\n",
      " 1901/5000: episode: 1900, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.707], loss: 13805.543945, mean_absolute_error: 47.806911, mean_q: 1.283668\n",
      " 1902/5000: episode: 1901, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.121, 1.000], loss: 27610.277344, mean_absolute_error: 56.986099, mean_q: 1.282636\n",
      " 1903/5000: episode: 1902, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.282, 1.000], loss: 41570.210938, mean_absolute_error: 66.289146, mean_q: 1.281822\n",
      " 1904/5000: episode: 1903, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.416, 1.000], loss: 0.821751, mean_absolute_error: 38.693981, mean_q: 1.280991\n",
      " 1905/5000: episode: 1904, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.000, 0.785], loss: 27764.218750, mean_absolute_error: 57.141602, mean_q: 1.280378\n",
      " 1906/5000: episode: 1905, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.516], loss: 27601.289062, mean_absolute_error: 57.025749, mean_q: 1.278662\n",
      " 1907/5000: episode: 1906, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.226 [0.000, 0.743], loss: 0.819855, mean_absolute_error: 38.747276, mean_q: 1.279511\n",
      " 1908/5000: episode: 1907, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.519], loss: 13799.260742, mean_absolute_error: 47.923233, mean_q: 1.278692\n",
      " 1909/5000: episode: 1908, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.465 [0.000, 0.995], loss: 27925.759766, mean_absolute_error: 57.321037, mean_q: 1.277483\n",
      " 1910/5000: episode: 1909, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.099, 1.000], loss: 13797.748047, mean_absolute_error: 47.939857, mean_q: 1.276818\n",
      " 1911/5000: episode: 1910, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.446, 1.000], loss: 0.815989, mean_absolute_error: 38.794147, mean_q: 1.276487\n",
      " 1912/5000: episode: 1911, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.009, 1.000], loss: 13958.875977, mean_absolute_error: 48.093117, mean_q: 1.276243\n",
      " 1913/5000: episode: 1912, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.468, 1.000], loss: 0.814715, mean_absolute_error: 38.812977, mean_q: 1.275484\n",
      " 1914/5000: episode: 1913, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.721], loss: 27754.968750, mean_absolute_error: 57.264137, mean_q: 1.274995\n",
      " 1915/5000: episode: 1914, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.154, 1.000], loss: 13795.119141, mean_absolute_error: 48.003124, mean_q: 1.274288\n",
      " 1916/5000: episode: 1915, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 27755.289062, mean_absolute_error: 57.294579, mean_q: 1.273491\n",
      " 1917/5000: episode: 1916, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 41708.640625, mean_absolute_error: 66.580101, mean_q: 1.272535\n",
      " 1918/5000: episode: 1917, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.504, 1.000], loss: 55332.371094, mean_absolute_error: 75.627525, mean_q: 1.271966\n",
      " 1919/5000: episode: 1918, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.588], loss: 27753.783203, mean_absolute_error: 57.335121, mean_q: 1.271078\n",
      " 1920/5000: episode: 1919, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.143, 1.000], loss: 13791.205078, mean_absolute_error: 48.087189, mean_q: 1.271477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1921/5000: episode: 1920, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.782], loss: 13790.779297, mean_absolute_error: 48.106464, mean_q: 1.271322\n",
      " 1922/5000: episode: 1921, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.003, 1.000], loss: 27577.380859, mean_absolute_error: 57.275734, mean_q: 1.270952\n",
      " 1923/5000: episode: 1922, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 0.808340, mean_absolute_error: 38.988297, mean_q: 1.270485\n",
      " 1924/5000: episode: 1923, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.000, 0.992], loss: 0.807636, mean_absolute_error: 39.005806, mean_q: 1.269932\n",
      " 1925/5000: episode: 1924, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.852], loss: 27905.947266, mean_absolute_error: 57.541878, mean_q: 1.268852\n",
      " 1926/5000: episode: 1925, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.625], loss: 13786.246094, mean_absolute_error: 48.181679, mean_q: 1.268500\n",
      " 1927/5000: episode: 1926, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.037, 1.000], loss: 13787.826172, mean_absolute_error: 48.171875, mean_q: 1.267259\n",
      " 1928/5000: episode: 1927, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.032, 1.000], loss: 13949.007812, mean_absolute_error: 48.325359, mean_q: 1.266845\n",
      " 1929/5000: episode: 1928, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.786], loss: 27731.753906, mean_absolute_error: 57.484207, mean_q: 1.266234\n",
      " 1930/5000: episode: 1929, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.782], loss: 13947.616211, mean_absolute_error: 48.343151, mean_q: 1.265651\n",
      " 1931/5000: episode: 1930, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.611], loss: 13949.320312, mean_absolute_error: 48.356945, mean_q: 1.265178\n",
      " 1932/5000: episode: 1931, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.011, 1.000], loss: 13782.510742, mean_absolute_error: 48.253174, mean_q: 1.264775\n",
      " 1933/5000: episode: 1932, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.000, 0.735], loss: 13945.642578, mean_absolute_error: 48.394382, mean_q: 1.264603\n",
      " 1934/5000: episode: 1933, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.461, 1.000], loss: 0.800298, mean_absolute_error: 39.139938, mean_q: 1.264145\n",
      " 1935/5000: episode: 1934, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.209 [0.000, 0.692], loss: 27728.693359, mean_absolute_error: 57.571266, mean_q: 1.263730\n",
      " 1936/5000: episode: 1935, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.878], loss: 0.799679, mean_absolute_error: 39.176991, mean_q: 1.263657\n",
      " 1937/5000: episode: 1936, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.554], loss: 41341.585938, mean_absolute_error: 66.624405, mean_q: 1.263000\n",
      " 1938/5000: episode: 1937, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.488, 1.000], loss: 0.798061, mean_absolute_error: 39.201527, mean_q: 1.262376\n",
      " 1939/5000: episode: 1938, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.017, 1.000], loss: 13778.201172, mean_absolute_error: 48.350914, mean_q: 1.261233\n",
      " 1940/5000: episode: 1939, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.464, 1.000], loss: 13941.414062, mean_absolute_error: 48.483261, mean_q: 1.260324\n",
      " 1941/5000: episode: 1940, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.727], loss: 0.794182, mean_absolute_error: 39.222324, mean_q: 1.259299\n",
      " 1942/5000: episode: 1941, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 0.793154, mean_absolute_error: 39.241623, mean_q: 1.258486\n",
      " 1943/5000: episode: 1942, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 27879.669922, mean_absolute_error: 57.782646, mean_q: 1.257636\n",
      " 1944/5000: episode: 1943, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.811], loss: 13775.480469, mean_absolute_error: 48.407085, mean_q: 1.256762\n",
      " 1945/5000: episode: 1944, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.000, 0.866], loss: 69201.335938, mean_absolute_error: 85.223213, mean_q: 1.255666\n",
      " 1946/5000: episode: 1945, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 41649.589844, mean_absolute_error: 66.952332, mean_q: 1.255058\n",
      " 1947/5000: episode: 1946, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.130, 1.000], loss: 0.788544, mean_absolute_error: 39.309242, mean_q: 1.254820\n",
      " 1948/5000: episode: 1947, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.000, 0.794], loss: 55252.867188, mean_absolute_error: 76.012749, mean_q: 1.254582\n",
      " 1949/5000: episode: 1948, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.545], loss: 13937.515625, mean_absolute_error: 48.608376, mean_q: 1.254555\n",
      " 1950/5000: episode: 1949, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.090, 1.000], loss: 27540.154297, mean_absolute_error: 57.608826, mean_q: 1.252855\n",
      " 1951/5000: episode: 1950, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.493], loss: 13934.907227, mean_absolute_error: 48.640087, mean_q: 1.254170\n",
      " 1952/5000: episode: 1951, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 27706.623047, mean_absolute_error: 57.788307, mean_q: 1.253504\n",
      " 1953/5000: episode: 1952, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.249, 1.000], loss: 13933.609375, mean_absolute_error: 48.681358, mean_q: 1.253820\n",
      " 1954/5000: episode: 1953, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 27865.773438, mean_absolute_error: 57.942944, mean_q: 1.253561\n",
      " 1955/5000: episode: 1954, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 13930.719727, mean_absolute_error: 48.716579, mean_q: 1.253988\n",
      " 1956/5000: episode: 1955, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 27695.335938, mean_absolute_error: 57.864929, mean_q: 1.254111\n",
      " 1957/5000: episode: 1956, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.000, 0.999], loss: 0.788013, mean_absolute_error: 39.495621, mean_q: 1.254398\n",
      " 1958/5000: episode: 1957, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.333, 1.000], loss: 41619.554688, mean_absolute_error: 67.159424, mean_q: 1.254712\n",
      " 1959/5000: episode: 1958, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 13764.738281, mean_absolute_error: 48.662460, mean_q: 1.254863\n",
      " 1960/5000: episode: 1959, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.008, 1.000], loss: 55218.707031, mean_absolute_error: 76.211273, mean_q: 1.255059\n",
      " 1961/5000: episode: 1960, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.000, 0.935], loss: 0.788417, mean_absolute_error: 39.558624, mean_q: 1.254668\n",
      " 1962/5000: episode: 1961, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.464, 1.000], loss: 41607.886719, mean_absolute_error: 67.219193, mean_q: 1.254534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1963/5000: episode: 1962, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 13922.704102, mean_absolute_error: 48.850422, mean_q: 1.254377\n",
      " 1964/5000: episode: 1963, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.846], loss: 13921.833984, mean_absolute_error: 48.877678, mean_q: 1.254367\n",
      " 1965/5000: episode: 1964, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.528 [0.003, 1.000], loss: 0.787335, mean_absolute_error: 39.638351, mean_q: 1.253856\n",
      " 1966/5000: episode: 1965, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.752], loss: 41439.218750, mean_absolute_error: 67.180969, mean_q: 1.253574\n",
      " 1967/5000: episode: 1966, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.681], loss: 55355.019531, mean_absolute_error: 76.435066, mean_q: 1.252810\n",
      " 1968/5000: episode: 1967, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.000, 0.964], loss: 0.785271, mean_absolute_error: 39.693626, mean_q: 1.252211\n",
      " 1969/5000: episode: 1968, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.237, 1.000], loss: 0.784349, mean_absolute_error: 39.712948, mean_q: 1.251476\n",
      " 1970/5000: episode: 1969, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.156, 1.000], loss: 41593.523438, mean_absolute_error: 67.335281, mean_q: 1.249757\n",
      " 1971/5000: episode: 1970, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.522 [0.002, 1.000], loss: 13755.677734, mean_absolute_error: 48.859406, mean_q: 1.249959\n",
      " 1972/5000: episode: 1971, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.670 [0.084, 1.000], loss: 41425.234375, mean_absolute_error: 67.251709, mean_q: 1.249347\n",
      " 1973/5000: episode: 1972, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.376, 1.000], loss: 13754.161133, mean_absolute_error: 48.898045, mean_q: 1.248829\n",
      " 1974/5000: episode: 1973, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.000, 0.970], loss: 55343.609375, mean_absolute_error: 76.531227, mean_q: 1.248303\n",
      " 1975/5000: episode: 1974, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.448, 1.000], loss: 41418.335938, mean_absolute_error: 67.300903, mean_q: 1.247858\n",
      " 1976/5000: episode: 1975, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.215, 1.000], loss: 41578.742188, mean_absolute_error: 67.434929, mean_q: 1.247434\n",
      " 1977/5000: episode: 1976, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.000, 0.924], loss: 13913.299805, mean_absolute_error: 49.085281, mean_q: 1.247139\n",
      " 1978/5000: episode: 1977, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 41573.191406, mean_absolute_error: 67.471436, mean_q: 1.246925\n",
      " 1979/5000: episode: 1978, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 27661.718750, mean_absolute_error: 58.238911, mean_q: 1.246622\n",
      " 1980/5000: episode: 1979, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.413 [0.000, 0.978], loss: 0.778275, mean_absolute_error: 39.898407, mean_q: 1.246613\n",
      " 1981/5000: episode: 1980, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.195, 1.000], loss: 13747.561523, mean_absolute_error: 49.051311, mean_q: 1.246471\n",
      " 1982/5000: episode: 1981, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.056, 1.000], loss: 13746.545898, mean_absolute_error: 49.067623, mean_q: 1.246096\n",
      " 1983/5000: episode: 1982, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.009, 1.000], loss: 27652.095703, mean_absolute_error: 58.326427, mean_q: 1.245557\n",
      " 1984/5000: episode: 1983, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.518], loss: 55465.453125, mean_absolute_error: 76.817413, mean_q: 1.245180\n",
      " 1985/5000: episode: 1984, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.325 [0.000, 0.911], loss: 41390.648438, mean_absolute_error: 67.483765, mean_q: 1.245063\n",
      " 1986/5000: episode: 1985, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.117, 1.000], loss: 27646.359375, mean_absolute_error: 58.374527, mean_q: 1.244727\n",
      " 1987/5000: episode: 1986, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.007, 1.000], loss: 0.775781, mean_absolute_error: 40.051834, mean_q: 1.244616\n",
      " 1988/5000: episode: 1987, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.338, 1.000], loss: 27645.929688, mean_absolute_error: 58.426502, mean_q: 1.244150\n",
      " 1989/5000: episode: 1988, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.859], loss: 41382.472656, mean_absolute_error: 67.571777, mean_q: 1.244188\n",
      " 1990/5000: episode: 1989, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 13901.804688, mean_absolute_error: 49.358742, mean_q: 1.244069\n",
      " 1991/5000: episode: 1990, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.630 [0.049, 1.000], loss: 13903.597656, mean_absolute_error: 49.381401, mean_q: 1.243965\n",
      " 1992/5000: episode: 1991, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 0.774695, mean_absolute_error: 40.162758, mean_q: 1.243740\n",
      " 1993/5000: episode: 1992, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.656 [0.070, 1.000], loss: 27635.937500, mean_absolute_error: 58.539207, mean_q: 1.243571\n",
      " 1994/5000: episode: 1993, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.293, 1.000], loss: 27468.761719, mean_absolute_error: 58.433941, mean_q: 1.243345\n",
      " 1995/5000: episode: 1994, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.003, 1.000], loss: 13897.455078, mean_absolute_error: 49.454712, mean_q: 1.243047\n",
      " 1996/5000: episode: 1995, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.759], loss: 0.773716, mean_absolute_error: 40.238846, mean_q: 1.242956\n",
      " 1997/5000: episode: 1996, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.675], loss: 13731.897461, mean_absolute_error: 49.374840, mean_q: 1.243165\n",
      " 1998/5000: episode: 1997, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.420, 1.000], loss: 0.773537, mean_absolute_error: 40.276314, mean_q: 1.242813\n",
      " 1999/5000: episode: 1998, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.064, 1.000], loss: 27787.197266, mean_absolute_error: 58.746338, mean_q: 1.242216\n",
      " 2000/5000: episode: 1999, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.149, 1.000], loss: 13730.834961, mean_absolute_error: 49.410484, mean_q: 1.241840\n",
      " 2001/5000: episode: 2000, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.000, 0.786], loss: 13728.948242, mean_absolute_error: 49.435539, mean_q: 1.241647\n",
      " 2002/5000: episode: 2001, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.607 [0.033, 1.000], loss: 27788.453125, mean_absolute_error: 58.786407, mean_q: 1.240921\n",
      " 2003/5000: episode: 2002, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.668 [0.081, 1.000], loss: 41509.335938, mean_absolute_error: 67.910873, mean_q: 1.240952\n",
      " 2004/5000: episode: 2003, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.004, 1.000], loss: 41180.789062, mean_absolute_error: 67.705551, mean_q: 1.241395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2005/5000: episode: 2004, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.144, 1.000], loss: 55390.753906, mean_absolute_error: 77.171288, mean_q: 1.241160\n",
      " 2006/5000: episode: 2005, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.000, 0.849], loss: 27775.470703, mean_absolute_error: 58.853165, mean_q: 1.241459\n",
      " 2007/5000: episode: 2006, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.481 [0.000, 0.998], loss: 55062.234375, mean_absolute_error: 76.976624, mean_q: 1.241970\n",
      " 2008/5000: episode: 2007, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 13727.089844, mean_absolute_error: 49.563236, mean_q: 1.242312\n",
      " 2009/5000: episode: 2008, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.557], loss: 13885.786133, mean_absolute_error: 49.701096, mean_q: 1.242434\n",
      " 2010/5000: episode: 2009, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.000, 0.807], loss: 13883.073242, mean_absolute_error: 49.718781, mean_q: 1.242528\n",
      " 2011/5000: episode: 2010, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.948], loss: 0.773139, mean_absolute_error: 40.524467, mean_q: 1.242494\n",
      " 2012/5000: episode: 2011, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.457 [0.000, 0.994], loss: 27442.130859, mean_absolute_error: 58.745720, mean_q: 1.242233\n",
      " 2013/5000: episode: 2012, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 27763.056641, mean_absolute_error: 58.990898, mean_q: 1.241818\n",
      " 2014/5000: episode: 2013, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.270 [0.000, 0.837], loss: 0.771939, mean_absolute_error: 40.565849, mean_q: 1.241527\n",
      " 2015/5000: episode: 2014, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 41480.429688, mean_absolute_error: 68.123207, mean_q: 1.241004\n",
      " 2016/5000: episode: 2015, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.121, 1.000], loss: 0.770634, mean_absolute_error: 40.583977, mean_q: 1.240474\n",
      " 2017/5000: episode: 2016, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.631], loss: 27438.294922, mean_absolute_error: 58.814888, mean_q: 1.240059\n",
      " 2018/5000: episode: 2017, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.006, 1.000], loss: 41471.699219, mean_absolute_error: 68.164383, mean_q: 1.239483\n",
      " 2019/5000: episode: 2018, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.672 [0.086, 1.000], loss: 13875.907227, mean_absolute_error: 49.866436, mean_q: 1.239258\n",
      " 2020/5000: episode: 2019, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.628], loss: 13875.101562, mean_absolute_error: 49.880699, mean_q: 1.238756\n",
      " 2021/5000: episode: 2020, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 27430.246094, mean_absolute_error: 58.873478, mean_q: 1.238233\n",
      " 2022/5000: episode: 2021, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.000, 0.975], loss: 13875.602539, mean_absolute_error: 49.918091, mean_q: 1.238208\n",
      " 2023/5000: episode: 2022, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.516], loss: 13713.960938, mean_absolute_error: 49.812134, mean_q: 1.237819\n",
      " 2024/5000: episode: 2023, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.492], loss: 41457.812500, mean_absolute_error: 68.251160, mean_q: 1.237180\n",
      " 2025/5000: episode: 2024, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.329, 1.000], loss: 27424.367188, mean_absolute_error: 58.950562, mean_q: 1.237034\n",
      " 2026/5000: episode: 2025, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.664], loss: 0.765440, mean_absolute_error: 40.769287, mean_q: 1.236287\n",
      " 2027/5000: episode: 2026, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.629], loss: 41292.367188, mean_absolute_error: 68.181648, mean_q: 1.235257\n",
      " 2028/5000: episode: 2027, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.194, 1.000], loss: 27583.433594, mean_absolute_error: 59.101692, mean_q: 1.234686\n",
      " 2029/5000: episode: 2028, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.657 [0.071, 1.000], loss: 41446.503906, mean_absolute_error: 68.274727, mean_q: 1.231476\n",
      " 2030/5000: episode: 2029, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.454, 1.000], loss: 13708.551758, mean_absolute_error: 49.918465, mean_q: 1.234237\n",
      " 2031/5000: episode: 2030, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.307, 1.000], loss: 13868.155273, mean_absolute_error: 50.053146, mean_q: 1.233229\n",
      " 2032/5000: episode: 2031, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.191, 1.000], loss: 41279.367188, mean_absolute_error: 68.266739, mean_q: 1.232799\n",
      " 2033/5000: episode: 2032, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.195, 1.000], loss: 0.760110, mean_absolute_error: 40.873886, mean_q: 1.231971\n",
      " 2034/5000: episode: 2033, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.614 [0.038, 1.000], loss: 0.759472, mean_absolute_error: 40.888798, mean_q: 1.231452\n",
      " 2035/5000: episode: 2034, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.477, 1.000], loss: 0.758821, mean_absolute_error: 40.899189, mean_q: 1.230925\n",
      " 2036/5000: episode: 2035, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.000, 0.992], loss: 0.758144, mean_absolute_error: 40.915394, mean_q: 1.230376\n",
      " 2037/5000: episode: 2036, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.757194, mean_absolute_error: 40.928123, mean_q: 1.229604\n",
      " 2038/5000: episode: 2037, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.616], loss: 13862.799805, mean_absolute_error: 50.143852, mean_q: 1.228966\n",
      " 2039/5000: episode: 2038, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.739 [0.178, 1.000], loss: 13703.434570, mean_absolute_error: 50.047600, mean_q: 1.228623\n",
      " 2040/5000: episode: 2039, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.130, 1.000], loss: 41582.992188, mean_absolute_error: 68.577515, mean_q: 1.227888\n",
      " 2041/5000: episode: 2040, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.871], loss: 0.754898, mean_absolute_error: 40.981533, mean_q: 1.227735\n",
      " 2042/5000: episode: 2041, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.000, 0.794], loss: 0.754250, mean_absolute_error: 40.988369, mean_q: 1.227208\n",
      " 2043/5000: episode: 2042, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.641 [0.057, 1.000], loss: 27402.042969, mean_absolute_error: 59.176468, mean_q: 1.226444\n",
      " 2044/5000: episode: 2043, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.380, 1.000], loss: 0.752917, mean_absolute_error: 41.025471, mean_q: 1.226123\n",
      " 2045/5000: episode: 2044, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 13701.409180, mean_absolute_error: 50.128101, mean_q: 1.225181\n",
      " 2046/5000: episode: 2045, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.429 [0.000, 0.985], loss: 0.750319, mean_absolute_error: 41.046127, mean_q: 1.224005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2047/5000: episode: 2046, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.000, 0.998], loss: 27555.167969, mean_absolute_error: 59.359154, mean_q: 1.222806\n",
      " 2048/5000: episode: 2047, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.588 [0.023, 1.000], loss: 13859.231445, mean_absolute_error: 50.277809, mean_q: 1.221316\n",
      " 2049/5000: episode: 2048, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.593], loss: 41408.406250, mean_absolute_error: 68.585297, mean_q: 1.220227\n",
      " 2050/5000: episode: 2049, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 13856.291992, mean_absolute_error: 50.305580, mean_q: 1.219149\n",
      " 2051/5000: episode: 2050, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 27555.439453, mean_absolute_error: 59.407383, mean_q: 1.218255\n",
      " 2052/5000: episode: 2051, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.769], loss: 41401.925781, mean_absolute_error: 68.634911, mean_q: 1.218070\n",
      " 2053/5000: episode: 2052, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.858], loss: 13692.776367, mean_absolute_error: 50.247261, mean_q: 1.217721\n",
      " 2054/5000: episode: 2053, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.485, 1.000], loss: 27383.328125, mean_absolute_error: 59.350071, mean_q: 1.217236\n",
      " 2055/5000: episode: 2054, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.000, 0.961], loss: 13691.227539, mean_absolute_error: 50.273819, mean_q: 1.216539\n",
      " 2056/5000: episode: 2055, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.371 [0.000, 0.952], loss: 27545.453125, mean_absolute_error: 59.489193, mean_q: 1.215917\n",
      " 2057/5000: episode: 2056, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.434 [0.000, 0.987], loss: 27703.396484, mean_absolute_error: 59.585762, mean_q: 1.214719\n",
      " 2058/5000: episode: 2057, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.340 [0.000, 0.926], loss: 55077.847656, mean_absolute_error: 77.807884, mean_q: 1.214721\n",
      " 2059/5000: episode: 2058, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.000, 0.947], loss: 27374.185547, mean_absolute_error: 59.432076, mean_q: 1.214314\n",
      " 2060/5000: episode: 2059, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 13854.410156, mean_absolute_error: 50.484810, mean_q: 1.213718\n",
      " 2061/5000: episode: 2060, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.593 [0.025, 1.000], loss: 41055.937500, mean_absolute_error: 68.550911, mean_q: 1.213157\n",
      " 2062/5000: episode: 2061, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.491, 1.000], loss: 13847.519531, mean_absolute_error: 50.521477, mean_q: 1.212364\n",
      " 2063/5000: episode: 2062, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.207, 1.000], loss: 13846.749023, mean_absolute_error: 50.533669, mean_q: 1.211425\n",
      " 2064/5000: episode: 2063, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.556 [0.010, 1.000], loss: 0.734345, mean_absolute_error: 41.355560, mean_q: 1.210892\n",
      " 2065/5000: episode: 2064, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.398, 1.000], loss: 27690.396484, mean_absolute_error: 59.760231, mean_q: 1.210054\n",
      " 2066/5000: episode: 2065, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 0.732726, mean_absolute_error: 41.398060, mean_q: 1.209558\n",
      " 2067/5000: episode: 2066, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.634], loss: 13681.479492, mean_absolute_error: 50.483330, mean_q: 1.208670\n",
      " 2068/5000: episode: 2067, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.234, 1.000], loss: 0.730399, mean_absolute_error: 41.418129, mean_q: 1.207633\n",
      " 2069/5000: episode: 2068, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.354 [0.000, 0.939], loss: 13843.405273, mean_absolute_error: 50.625095, mean_q: 1.206794\n",
      " 2070/5000: episode: 2069, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.247, 1.000], loss: 13678.643555, mean_absolute_error: 50.524124, mean_q: 1.206107\n",
      " 2071/5000: episode: 2070, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 13678.116211, mean_absolute_error: 50.530785, mean_q: 1.205244\n",
      " 2072/5000: episode: 2071, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 68713.570312, mean_absolute_error: 87.070343, mean_q: 1.204181\n",
      " 2073/5000: episode: 2072, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.739], loss: 0.725588, mean_absolute_error: 41.489624, mean_q: 1.203647\n",
      " 2074/5000: episode: 2073, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.770], loss: 13677.122070, mean_absolute_error: 50.561546, mean_q: 1.202474\n",
      " 2075/5000: episode: 2074, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.553 [0.009, 1.000], loss: 0.723239, mean_absolute_error: 41.496979, mean_q: 1.201690\n",
      " 2076/5000: episode: 2075, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.456, 1.000], loss: 54699.617188, mean_absolute_error: 77.802872, mean_q: 1.200780\n",
      " 2077/5000: episode: 2076, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.496, 1.000], loss: 0.721021, mean_absolute_error: 41.531006, mean_q: 1.199850\n",
      " 2078/5000: episode: 2077, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.017, 1.000], loss: 0.719772, mean_absolute_error: 41.543076, mean_q: 1.198809\n",
      " 2079/5000: episode: 2078, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.592], loss: 27511.373047, mean_absolute_error: 59.819103, mean_q: 1.197798\n",
      " 2080/5000: episode: 2079, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.417 [0.000, 0.979], loss: 13672.381836, mean_absolute_error: 50.642174, mean_q: 1.196839\n",
      " 2081/5000: episode: 2080, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.716170, mean_absolute_error: 41.577644, mean_q: 1.195804\n",
      " 2082/5000: episode: 2081, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.570], loss: 0.714961, mean_absolute_error: 41.591393, mean_q: 1.194793\n",
      " 2083/5000: episode: 2082, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 27340.830078, mean_absolute_error: 59.731895, mean_q: 1.193616\n",
      " 2084/5000: episode: 2083, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.584 [0.021, 1.000], loss: 0.712274, mean_absolute_error: 41.609535, mean_q: 1.192543\n",
      " 2085/5000: episode: 2084, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.399, 1.000], loss: 13669.570312, mean_absolute_error: 50.683575, mean_q: 1.191296\n",
      " 2086/5000: episode: 2085, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.190, 1.000], loss: 0.709427, mean_absolute_error: 41.629894, mean_q: 1.190155\n",
      " 2087/5000: episode: 2086, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.422, 1.000], loss: 27507.875000, mean_absolute_error: 59.892292, mean_q: 1.188734\n",
      " 2088/5000: episode: 2087, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.221, 1.000], loss: 13671.118164, mean_absolute_error: 50.711037, mean_q: 1.187438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2089/5000: episode: 2088, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.432 [0.000, 0.986], loss: 27337.378906, mean_absolute_error: 59.796204, mean_q: 1.186274\n",
      " 2090/5000: episode: 2089, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.641 [0.057, 1.000], loss: 13666.770508, mean_absolute_error: 50.731640, mean_q: 1.184718\n",
      " 2091/5000: episode: 2090, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.586], loss: 13848.197266, mean_absolute_error: 50.851387, mean_q: 1.182554\n",
      " 2092/5000: episode: 2091, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.247, 1.000], loss: 13665.580078, mean_absolute_error: 50.745262, mean_q: 1.181872\n",
      " 2093/5000: episode: 2092, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.571], loss: 13834.358398, mean_absolute_error: 50.876537, mean_q: 1.180588\n",
      " 2094/5000: episode: 2093, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.020, 1.000], loss: 27668.148438, mean_absolute_error: 60.064049, mean_q: 1.179290\n",
      " 2095/5000: episode: 2094, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.076, 1.000], loss: 13833.525391, mean_absolute_error: 50.892426, mean_q: 1.178527\n",
      " 2096/5000: episode: 2095, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.000, 1.000], loss: 0.694589, mean_absolute_error: 41.712635, mean_q: 1.177631\n",
      " 2097/5000: episode: 2096, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.000, 0.913], loss: 13835.144531, mean_absolute_error: 50.910271, mean_q: 1.176702\n",
      " 2098/5000: episode: 2097, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.039, 1.000], loss: 0.692725, mean_absolute_error: 41.742455, mean_q: 1.176051\n",
      " 2099/5000: episode: 2098, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.271, 1.000], loss: 13662.591797, mean_absolute_error: 50.804569, mean_q: 1.175044\n",
      " 2100/5000: episode: 2099, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.718], loss: 27494.814453, mean_absolute_error: 60.001099, mean_q: 1.174375\n",
      " 2101/5000: episode: 2100, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.000, 0.999], loss: 41321.175781, mean_absolute_error: 69.186356, mean_q: 1.173680\n",
      " 2102/5000: episode: 2101, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.679], loss: 13661.421875, mean_absolute_error: 50.844467, mean_q: 1.173519\n",
      " 2103/5000: episode: 2102, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.615], loss: 13828.886719, mean_absolute_error: 50.963402, mean_q: 1.172726\n",
      " 2104/5000: episode: 2103, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.000, 0.999], loss: 27320.761719, mean_absolute_error: 59.933380, mean_q: 1.172363\n",
      " 2105/5000: episode: 2104, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.000, 1.000], loss: 13659.982422, mean_absolute_error: 50.880398, mean_q: 1.171601\n",
      " 2106/5000: episode: 2105, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.438, 1.000], loss: 0.686753, mean_absolute_error: 41.833271, mean_q: 1.170966\n",
      " 2107/5000: episode: 2106, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.914], loss: 13827.484375, mean_absolute_error: 51.019840, mean_q: 1.169923\n",
      " 2108/5000: episode: 2107, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.456 [0.000, 0.993], loss: 13825.900391, mean_absolute_error: 51.034515, mean_q: 1.169328\n",
      " 2109/5000: episode: 2108, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.000, 0.992], loss: 13658.416992, mean_absolute_error: 50.920563, mean_q: 1.168757\n",
      " 2110/5000: episode: 2109, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.774], loss: 0.683086, mean_absolute_error: 41.868675, mean_q: 1.167832\n",
      " 2111/5000: episode: 2110, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.000, 0.777], loss: 0.682777, mean_absolute_error: 41.858849, mean_q: 1.167501\n",
      " 2112/5000: episode: 2111, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.320, 1.000], loss: 13657.410156, mean_absolute_error: 50.944916, mean_q: 1.166073\n",
      " 2113/5000: episode: 2112, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 13823.393555, mean_absolute_error: 51.071747, mean_q: 1.165363\n",
      " 2114/5000: episode: 2113, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.401 [0.000, 0.971], loss: 27480.519531, mean_absolute_error: 60.136604, mean_q: 1.164725\n",
      " 2115/5000: episode: 2114, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.721], loss: 0.679183, mean_absolute_error: 41.921246, mean_q: 1.164489\n",
      " 2116/5000: episode: 2115, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.265, 1.000], loss: 0.678330, mean_absolute_error: 41.929970, mean_q: 1.163757\n",
      " 2117/5000: episode: 2116, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.198, 1.000], loss: 13820.687500, mean_absolute_error: 51.107635, mean_q: 1.162802\n",
      " 2118/5000: episode: 2117, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 0.677294, mean_absolute_error: 41.941830, mean_q: 1.162855\n",
      " 2119/5000: episode: 2118, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.737], loss: 0.676208, mean_absolute_error: 41.959087, mean_q: 1.161934\n",
      " 2120/5000: episode: 2119, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.394, 1.000], loss: 0.675282, mean_absolute_error: 41.955353, mean_q: 1.161135\n",
      " 2121/5000: episode: 2120, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.000, 0.693], loss: 13656.952148, mean_absolute_error: 51.032650, mean_q: 1.160684\n",
      " 2122/5000: episode: 2121, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.052, 1.000], loss: 0.673725, mean_absolute_error: 41.980431, mean_q: 1.159796\n",
      " 2123/5000: episode: 2122, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.579 [0.019, 1.000], loss: 0.672712, mean_absolute_error: 41.988449, mean_q: 1.158922\n",
      " 2124/5000: episode: 2123, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.480, 1.000], loss: 0.671435, mean_absolute_error: 41.985588, mean_q: 1.157820\n",
      " 2125/5000: episode: 2124, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.107, 1.000], loss: 13816.649414, mean_absolute_error: 51.171745, mean_q: 1.156894\n",
      " 2126/5000: episode: 2125, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.203, 1.000], loss: 0.669368, mean_absolute_error: 42.001511, mean_q: 1.156036\n",
      " 2127/5000: episode: 2126, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.000, 0.999], loss: 13654.318359, mean_absolute_error: 51.077759, mean_q: 1.155424\n",
      " 2128/5000: episode: 2127, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.658], loss: 0.667285, mean_absolute_error: 42.017342, mean_q: 1.154234\n",
      " 2129/5000: episode: 2128, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.687], loss: 0.665961, mean_absolute_error: 42.020947, mean_q: 1.153087\n",
      " 2130/5000: episode: 2129, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.727], loss: 0.664645, mean_absolute_error: 42.031540, mean_q: 1.151947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2131/5000: episode: 2130, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.392, 1.000], loss: 0.662996, mean_absolute_error: 42.026123, mean_q: 1.150515\n",
      " 2132/5000: episode: 2131, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.000, 0.846], loss: 0.661754, mean_absolute_error: 42.043701, mean_q: 1.149437\n",
      " 2133/5000: episode: 2132, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 27470.109375, mean_absolute_error: 60.269985, mean_q: 1.147889\n",
      " 2134/5000: episode: 2133, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.610], loss: 0.658633, mean_absolute_error: 42.049976, mean_q: 1.146721\n",
      " 2135/5000: episode: 2134, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.011, 1.000], loss: 0.656971, mean_absolute_error: 42.054314, mean_q: 1.145272\n",
      " 2136/5000: episode: 2135, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.706 [0.127, 1.000], loss: 0.655470, mean_absolute_error: 42.055832, mean_q: 1.143961\n",
      " 2137/5000: episode: 2136, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.653817, mean_absolute_error: 42.060459, mean_q: 1.142517\n",
      " 2138/5000: episode: 2137, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.000, 0.992], loss: 0.652272, mean_absolute_error: 42.070877, mean_q: 1.141166\n",
      " 2139/5000: episode: 2138, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.813], loss: 13813.292969, mean_absolute_error: 51.240280, mean_q: 1.139644\n",
      " 2140/5000: episode: 2139, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.034, 1.000], loss: 0.648926, mean_absolute_error: 42.065735, mean_q: 1.138231\n",
      " 2141/5000: episode: 2140, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.331, 1.000], loss: 0.647662, mean_absolute_error: 42.074070, mean_q: 1.137122\n",
      " 2142/5000: episode: 2141, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.037, 1.000], loss: 0.646286, mean_absolute_error: 42.078377, mean_q: 1.135912\n",
      " 2143/5000: episode: 2142, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.140, 1.000], loss: 0.644821, mean_absolute_error: 42.081688, mean_q: 1.134623\n",
      " 2144/5000: episode: 2143, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.741 [0.183, 1.000], loss: 0.643052, mean_absolute_error: 42.074013, mean_q: 1.133062\n",
      " 2145/5000: episode: 2144, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.502], loss: 0.641678, mean_absolute_error: 42.086994, mean_q: 1.131853\n",
      " 2146/5000: episode: 2145, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.060, 1.000], loss: 0.640169, mean_absolute_error: 42.092422, mean_q: 1.130520\n",
      " 2147/5000: episode: 2146, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.169, 1.000], loss: 0.638579, mean_absolute_error: 42.088005, mean_q: 1.129113\n",
      " 2148/5000: episode: 2147, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 0.636947, mean_absolute_error: 42.082520, mean_q: 1.127667\n",
      " 2149/5000: episode: 2148, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.000, 0.734], loss: 0.635584, mean_absolute_error: 42.092720, mean_q: 1.126459\n",
      " 2150/5000: episode: 2149, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.337, 1.000], loss: 0.634010, mean_absolute_error: 42.092453, mean_q: 1.125063\n",
      " 2151/5000: episode: 2150, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.615], loss: 0.632493, mean_absolute_error: 42.095016, mean_q: 1.123714\n",
      " 2152/5000: episode: 2151, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.271, 1.000], loss: 0.630911, mean_absolute_error: 42.088535, mean_q: 1.122306\n",
      " 2153/5000: episode: 2152, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.510], loss: 0.629349, mean_absolute_error: 42.093548, mean_q: 1.120916\n",
      " 2154/5000: episode: 2153, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.391, 1.000], loss: 0.628052, mean_absolute_error: 42.099060, mean_q: 1.119759\n",
      " 2155/5000: episode: 2154, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.066, 1.000], loss: 0.626549, mean_absolute_error: 42.100529, mean_q: 1.118418\n",
      " 2156/5000: episode: 2155, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 0.625003, mean_absolute_error: 42.103409, mean_q: 1.117036\n",
      " 2157/5000: episode: 2156, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.267 [0.000, 0.831], loss: 0.623182, mean_absolute_error: 42.094086, mean_q: 1.115403\n",
      " 2158/5000: episode: 2157, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 0.621821, mean_absolute_error: 42.100349, mean_q: 1.114186\n",
      " 2159/5000: episode: 2158, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 0.620312, mean_absolute_error: 42.104500, mean_q: 1.112832\n",
      " 2160/5000: episode: 2159, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.612], loss: 0.618849, mean_absolute_error: 42.098156, mean_q: 1.111515\n",
      " 2161/5000: episode: 2160, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.514], loss: 0.617195, mean_absolute_error: 42.095589, mean_q: 1.110029\n",
      " 2162/5000: episode: 2161, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.161, 1.000], loss: 0.615695, mean_absolute_error: 42.100952, mean_q: 1.108678\n",
      " 2163/5000: episode: 2162, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.804], loss: 0.614122, mean_absolute_error: 42.102123, mean_q: 1.107260\n",
      " 2164/5000: episode: 2163, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.006, 1.000], loss: 0.612591, mean_absolute_error: 42.099228, mean_q: 1.105878\n",
      " 2165/5000: episode: 2164, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.010, 1.000], loss: 0.610882, mean_absolute_error: 42.098133, mean_q: 1.104332\n",
      " 2166/5000: episode: 2165, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.130, 1.000], loss: 0.609490, mean_absolute_error: 42.108704, mean_q: 1.103073\n",
      " 2167/5000: episode: 2166, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.149, 1.000], loss: 0.607797, mean_absolute_error: 42.104103, mean_q: 1.101539\n",
      " 2168/5000: episode: 2167, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.755], loss: 0.606054, mean_absolute_error: 42.099388, mean_q: 1.099956\n",
      " 2169/5000: episode: 2168, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.571], loss: 0.604610, mean_absolute_error: 42.107861, mean_q: 1.098645\n",
      " 2170/5000: episode: 2169, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.281, 1.000], loss: 0.602956, mean_absolute_error: 42.103329, mean_q: 1.097139\n",
      " 2171/5000: episode: 2170, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.002, 1.000], loss: 0.601154, mean_absolute_error: 42.096989, mean_q: 1.095495\n",
      " 2172/5000: episode: 2171, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.528], loss: 0.599429, mean_absolute_error: 42.094765, mean_q: 1.093922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2173/5000: episode: 2172, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.443 [0.000, 0.990], loss: 0.598059, mean_absolute_error: 42.106731, mean_q: 1.092671\n",
      " 2174/5000: episode: 2173, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.017, 1.000], loss: 0.596211, mean_absolute_error: 42.098953, mean_q: 1.090979\n",
      " 2175/5000: episode: 2174, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 0.594666, mean_absolute_error: 42.098480, mean_q: 1.089563\n",
      " 2176/5000: episode: 2175, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 13812.002930, mean_absolute_error: 51.256966, mean_q: 1.087728\n",
      " 2177/5000: episode: 2176, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.643], loss: 0.591493, mean_absolute_error: 42.096718, mean_q: 1.086649\n",
      " 2178/5000: episode: 2177, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.479, 1.000], loss: 0.591200, mean_absolute_error: 42.082867, mean_q: 1.086260\n",
      " 2179/5000: episode: 2178, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 0.587811, mean_absolute_error: 42.087349, mean_q: 1.083256\n",
      " 2180/5000: episode: 2179, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.534], loss: 0.586982, mean_absolute_error: 42.100063, mean_q: 1.082494\n",
      " 2181/5000: episode: 2180, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.338, 1.000], loss: 0.585404, mean_absolute_error: 42.090164, mean_q: 1.081034\n",
      " 2182/5000: episode: 2181, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.584282, mean_absolute_error: 42.108055, mean_q: 1.080001\n",
      " 2183/5000: episode: 2182, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.211, 1.000], loss: 0.582729, mean_absolute_error: 42.101021, mean_q: 1.078562\n",
      " 2184/5000: episode: 2183, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.641 [0.057, 1.000], loss: 0.581320, mean_absolute_error: 42.103577, mean_q: 1.077255\n",
      " 2185/5000: episode: 2184, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.457 [0.000, 0.994], loss: 0.579895, mean_absolute_error: 42.109596, mean_q: 1.075935\n",
      " 2186/5000: episode: 2185, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.478, 1.000], loss: 0.578867, mean_absolute_error: 42.099987, mean_q: 1.074931\n",
      " 2187/5000: episode: 2186, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.294, 1.000], loss: 0.576791, mean_absolute_error: 42.106441, mean_q: 1.073048\n",
      " 2188/5000: episode: 2187, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.147, 1.000], loss: 0.575201, mean_absolute_error: 42.107986, mean_q: 1.071567\n",
      " 2189/5000: episode: 2188, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.046, 1.000], loss: 0.573640, mean_absolute_error: 42.107422, mean_q: 1.070111\n",
      " 2190/5000: episode: 2189, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.000, 0.866], loss: 0.572124, mean_absolute_error: 42.110107, mean_q: 1.068695\n",
      " 2191/5000: episode: 2190, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.404, 1.000], loss: 0.570628, mean_absolute_error: 42.104790, mean_q: 1.067293\n",
      " 2192/5000: episode: 2191, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.022, 1.000], loss: 0.568923, mean_absolute_error: 42.108383, mean_q: 1.065698\n",
      " 2193/5000: episode: 2192, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.906], loss: 0.567390, mean_absolute_error: 42.108265, mean_q: 1.064260\n",
      " 2194/5000: episode: 2193, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.623 [0.044, 1.000], loss: 0.565634, mean_absolute_error: 42.102245, mean_q: 1.062608\n",
      " 2195/5000: episode: 2194, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.812], loss: 0.563977, mean_absolute_error: 42.099758, mean_q: 1.061049\n",
      " 2196/5000: episode: 2195, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.455 [0.000, 0.993], loss: 0.562627, mean_absolute_error: 42.107613, mean_q: 1.059780\n",
      " 2197/5000: episode: 2196, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.813], loss: 0.561051, mean_absolute_error: 42.107811, mean_q: 1.058293\n",
      " 2198/5000: episode: 2197, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.462 [0.000, 0.995], loss: 0.559436, mean_absolute_error: 42.102135, mean_q: 1.056766\n",
      " 2199/5000: episode: 2198, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.557873, mean_absolute_error: 42.107426, mean_q: 1.055288\n",
      " 2200/5000: episode: 2199, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.462 [0.000, 0.995], loss: 0.556026, mean_absolute_error: 42.094402, mean_q: 1.053535\n",
      " 2201/5000: episode: 2200, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.453, 1.000], loss: 0.554733, mean_absolute_error: 42.105461, mean_q: 1.052311\n",
      " 2202/5000: episode: 2201, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.530], loss: 0.552977, mean_absolute_error: 42.093086, mean_q: 1.050640\n",
      " 2203/5000: episode: 2202, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.409, 1.000], loss: 0.551441, mean_absolute_error: 42.099964, mean_q: 1.049181\n",
      " 2204/5000: episode: 2203, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.050, 1.000], loss: 0.549975, mean_absolute_error: 42.100372, mean_q: 1.047783\n",
      " 2205/5000: episode: 2204, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.025, 1.000], loss: 0.548222, mean_absolute_error: 42.095360, mean_q: 1.046110\n",
      " 2206/5000: episode: 2205, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.780], loss: 0.546662, mean_absolute_error: 42.092728, mean_q: 1.044620\n",
      " 2207/5000: episode: 2206, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.912], loss: 0.545120, mean_absolute_error: 42.096825, mean_q: 1.043141\n",
      " 2208/5000: episode: 2207, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.000, 0.819], loss: 0.543623, mean_absolute_error: 42.094570, mean_q: 1.041708\n",
      " 2209/5000: episode: 2208, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.352, 1.000], loss: 0.542626, mean_absolute_error: 42.096291, mean_q: 1.040736\n",
      " 2210/5000: episode: 2209, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 0.881], loss: 0.540387, mean_absolute_error: 42.089622, mean_q: 1.038601\n",
      " 2211/5000: episode: 2210, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.320, 1.000], loss: 0.539030, mean_absolute_error: 42.100349, mean_q: 1.037296\n",
      " 2212/5000: episode: 2211, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.537279, mean_absolute_error: 42.091778, mean_q: 1.035606\n",
      " 2213/5000: episode: 2212, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.561 [0.012, 1.000], loss: 0.535723, mean_absolute_error: 42.089279, mean_q: 1.034105\n",
      " 2214/5000: episode: 2213, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.065, 1.000], loss: 0.534352, mean_absolute_error: 42.098816, mean_q: 1.032780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2215/5000: episode: 2214, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.325 [0.000, 0.911], loss: 0.532573, mean_absolute_error: 42.085884, mean_q: 1.031056\n",
      " 2216/5000: episode: 2215, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.108, 1.000], loss: 0.530919, mean_absolute_error: 42.076508, mean_q: 1.029450\n",
      " 2217/5000: episode: 2216, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.442, 1.000], loss: 0.529772, mean_absolute_error: 42.098324, mean_q: 1.028341\n",
      " 2218/5000: episode: 2217, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.100, 1.000], loss: 0.528113, mean_absolute_error: 42.087723, mean_q: 1.026725\n",
      " 2219/5000: episode: 2218, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.385 [0.000, 0.962], loss: 0.526678, mean_absolute_error: 42.092400, mean_q: 1.025330\n",
      " 2220/5000: episode: 2219, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.495, 1.000], loss: 0.525117, mean_absolute_error: 42.093750, mean_q: 1.023807\n",
      " 2221/5000: episode: 2220, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.659], loss: 0.523506, mean_absolute_error: 42.089497, mean_q: 1.022235\n",
      " 2222/5000: episode: 2221, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.918], loss: 0.522060, mean_absolute_error: 42.091042, mean_q: 1.020818\n",
      " 2223/5000: episode: 2222, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.000, 0.879], loss: 0.520558, mean_absolute_error: 42.094604, mean_q: 1.019350\n",
      " 2224/5000: episode: 2223, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.543], loss: 0.518887, mean_absolute_error: 42.089138, mean_q: 1.017710\n",
      " 2225/5000: episode: 2224, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.704], loss: 0.517616, mean_absolute_error: 42.099220, mean_q: 1.016463\n",
      " 2226/5000: episode: 2225, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.658], loss: 0.516003, mean_absolute_error: 42.092335, mean_q: 1.014875\n",
      " 2227/5000: episode: 2226, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.447 [0.000, 0.991], loss: 0.514385, mean_absolute_error: 42.084854, mean_q: 1.013281\n",
      " 2228/5000: episode: 2227, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.021, 1.000], loss: 0.513037, mean_absolute_error: 42.092751, mean_q: 1.011953\n",
      " 2229/5000: episode: 2228, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.629], loss: 0.511370, mean_absolute_error: 42.082939, mean_q: 1.010303\n",
      " 2230/5000: episode: 2229, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.265, 1.000], loss: 0.509791, mean_absolute_error: 42.078758, mean_q: 1.008740\n",
      " 2231/5000: episode: 2230, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 0.508491, mean_absolute_error: 42.086079, mean_q: 1.007453\n",
      " 2232/5000: episode: 2231, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 0.506962, mean_absolute_error: 42.085495, mean_q: 1.005937\n",
      " 2233/5000: episode: 2232, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.633], loss: 0.505274, mean_absolute_error: 42.069603, mean_q: 1.004255\n",
      " 2234/5000: episode: 2233, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.674], loss: 0.503920, mean_absolute_error: 42.080585, mean_q: 1.002910\n",
      " 2235/5000: episode: 2234, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.268 [0.000, 0.834], loss: 0.502377, mean_absolute_error: 42.078369, mean_q: 1.001370\n",
      " 2236/5000: episode: 2235, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 0.501041, mean_absolute_error: 42.086525, mean_q: 1.000039\n",
      " 2237/5000: episode: 2236, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.000, 0.996], loss: 0.499498, mean_absolute_error: 42.083542, mean_q: 0.998495\n",
      " 2238/5000: episode: 2237, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.722], loss: 0.498083, mean_absolute_error: 42.087242, mean_q: 0.997081\n",
      " 2239/5000: episode: 2238, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 0.496571, mean_absolute_error: 42.083908, mean_q: 0.995565\n",
      " 2240/5000: episode: 2239, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.427 [0.000, 0.984], loss: 0.494972, mean_absolute_error: 42.071457, mean_q: 0.993956\n",
      " 2241/5000: episode: 2240, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.422, 1.000], loss: 0.493635, mean_absolute_error: 42.080643, mean_q: 0.992613\n",
      " 2242/5000: episode: 2241, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 0.492227, mean_absolute_error: 42.087326, mean_q: 0.991196\n",
      " 2243/5000: episode: 2242, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.595], loss: 0.490755, mean_absolute_error: 42.089188, mean_q: 0.989712\n",
      " 2244/5000: episode: 2243, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.438 [0.000, 0.988], loss: 0.489278, mean_absolute_error: 42.083847, mean_q: 0.988219\n",
      " 2245/5000: episode: 2244, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 0.487800, mean_absolute_error: 42.080322, mean_q: 0.986723\n",
      " 2246/5000: episode: 2245, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.853], loss: 0.486058, mean_absolute_error: 42.068180, mean_q: 0.984956\n",
      " 2247/5000: episode: 2246, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.491, 1.000], loss: 0.485044, mean_absolute_error: 42.072968, mean_q: 0.983920\n",
      " 2248/5000: episode: 2247, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.089, 1.000], loss: 0.483358, mean_absolute_error: 42.075451, mean_q: 0.982215\n",
      " 2249/5000: episode: 2248, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.205, 1.000], loss: 0.481975, mean_absolute_error: 42.075459, mean_q: 0.980808\n",
      " 2250/5000: episode: 2249, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 0.480487, mean_absolute_error: 42.079304, mean_q: 0.979292\n",
      " 2251/5000: episode: 2250, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.160, 1.000], loss: 0.479042, mean_absolute_error: 42.073914, mean_q: 0.977815\n",
      " 2252/5000: episode: 2251, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.159, 1.000], loss: 0.477582, mean_absolute_error: 42.073990, mean_q: 0.976323\n",
      " 2253/5000: episode: 2252, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.000, 1.000], loss: 0.476128, mean_absolute_error: 42.069176, mean_q: 0.974833\n",
      " 2254/5000: episode: 2253, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.228, 1.000], loss: 0.474745, mean_absolute_error: 42.076752, mean_q: 0.973417\n",
      " 2255/5000: episode: 2254, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.751], loss: 0.473340, mean_absolute_error: 42.072769, mean_q: 0.971974\n",
      " 2256/5000: episode: 2255, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.350, 1.000], loss: 0.471802, mean_absolute_error: 42.068207, mean_q: 0.970391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2257/5000: episode: 2256, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.000, 0.956], loss: 0.470418, mean_absolute_error: 42.066521, mean_q: 0.968965\n",
      " 2258/5000: episode: 2257, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 0.468947, mean_absolute_error: 42.067207, mean_q: 0.967445\n",
      " 2259/5000: episode: 2258, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 0.467564, mean_absolute_error: 42.067238, mean_q: 0.966017\n",
      " 2260/5000: episode: 2259, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.524 [0.003, 1.000], loss: 0.466253, mean_absolute_error: 42.075001, mean_q: 0.964663\n",
      " 2261/5000: episode: 2260, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.194, 1.000], loss: 0.464611, mean_absolute_error: 42.060814, mean_q: 0.962957\n",
      " 2262/5000: episode: 2261, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.216, 1.000], loss: 0.463676, mean_absolute_error: 42.072632, mean_q: 0.961985\n",
      " 2263/5000: episode: 2262, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.712], loss: 0.461963, mean_absolute_error: 42.070530, mean_q: 0.960209\n",
      " 2264/5000: episode: 2263, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.709], loss: 0.460600, mean_absolute_error: 42.066826, mean_q: 0.958790\n",
      " 2265/5000: episode: 2264, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.617], loss: 0.459254, mean_absolute_error: 42.070686, mean_q: 0.957387\n",
      " 2266/5000: episode: 2265, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 0.457811, mean_absolute_error: 42.070709, mean_q: 0.955880\n",
      " 2267/5000: episode: 2266, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.000, 0.984], loss: 0.456463, mean_absolute_error: 42.073746, mean_q: 0.954471\n",
      " 2268/5000: episode: 2267, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.105, 1.000], loss: 0.454913, mean_absolute_error: 42.065506, mean_q: 0.952844\n",
      " 2269/5000: episode: 2268, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.000, 0.975], loss: 0.453548, mean_absolute_error: 42.064598, mean_q: 0.951414\n",
      " 2270/5000: episode: 2269, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.662], loss: 0.452213, mean_absolute_error: 42.067451, mean_q: 0.950011\n",
      " 2271/5000: episode: 2270, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.708], loss: 0.450818, mean_absolute_error: 42.069138, mean_q: 0.948544\n",
      " 2272/5000: episode: 2271, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.220, 1.000], loss: 0.449490, mean_absolute_error: 42.068977, mean_q: 0.947145\n",
      " 2273/5000: episode: 2272, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.326, 1.000], loss: 0.448117, mean_absolute_error: 42.066307, mean_q: 0.945696\n",
      " 2274/5000: episode: 2273, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.814], loss: 0.446621, mean_absolute_error: 42.058060, mean_q: 0.944110\n",
      " 2275/5000: episode: 2274, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.445322, mean_absolute_error: 42.063896, mean_q: 0.942739\n",
      " 2276/5000: episode: 2275, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.401, 1.000], loss: 0.443921, mean_absolute_error: 42.061562, mean_q: 0.941251\n",
      " 2277/5000: episode: 2276, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.495, 1.000], loss: 0.442423, mean_absolute_error: 42.056751, mean_q: 0.939660\n",
      " 2278/5000: episode: 2277, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 0.441126, mean_absolute_error: 42.055435, mean_q: 0.938279\n",
      " 2279/5000: episode: 2278, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.268 [0.000, 0.833], loss: 0.439778, mean_absolute_error: 42.060871, mean_q: 0.936844\n",
      " 2280/5000: episode: 2279, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.166, 1.000], loss: 0.438537, mean_absolute_error: 42.064236, mean_q: 0.935521\n",
      " 2281/5000: episode: 2280, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.740], loss: 0.437228, mean_absolute_error: 42.068436, mean_q: 0.934123\n",
      " 2282/5000: episode: 2281, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.632], loss: 0.435748, mean_absolute_error: 42.060555, mean_q: 0.932538\n",
      " 2283/5000: episode: 2282, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.527], loss: 0.434426, mean_absolute_error: 42.061726, mean_q: 0.931121\n",
      " 2284/5000: episode: 2283, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.000, 0.999], loss: 0.433037, mean_absolute_error: 42.058136, mean_q: 0.929629\n",
      " 2285/5000: episode: 2284, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 0.431759, mean_absolute_error: 42.061222, mean_q: 0.928256\n",
      " 2286/5000: episode: 2285, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.278, 1.000], loss: 0.430374, mean_absolute_error: 42.059074, mean_q: 0.926763\n",
      " 2287/5000: episode: 2286, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.772 [0.251, 1.000], loss: 0.429125, mean_absolute_error: 42.063763, mean_q: 0.925417\n",
      " 2288/5000: episode: 2287, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.544], loss: 0.427673, mean_absolute_error: 42.055439, mean_q: 0.923847\n",
      " 2289/5000: episode: 2288, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.671], loss: 0.425933, mean_absolute_error: 42.046421, mean_q: 0.921948\n",
      " 2290/5000: episode: 2289, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.000, 0.925], loss: 0.424994, mean_absolute_error: 42.054966, mean_q: 0.920946\n",
      " 2291/5000: episode: 2290, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.022, 1.000], loss: 0.423625, mean_absolute_error: 42.048889, mean_q: 0.919458\n",
      " 2292/5000: episode: 2291, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 0.422400, mean_absolute_error: 42.059464, mean_q: 0.918129\n",
      " 2293/5000: episode: 2292, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.572], loss: 0.421118, mean_absolute_error: 42.058422, mean_q: 0.916733\n",
      " 2294/5000: episode: 2293, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.458 [0.000, 0.994], loss: 0.419741, mean_absolute_error: 42.051533, mean_q: 0.915230\n",
      " 2295/5000: episode: 2294, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 0.418395, mean_absolute_error: 42.049568, mean_q: 0.913759\n",
      " 2296/5000: episode: 2295, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.181, 1.000], loss: 0.417113, mean_absolute_error: 42.051487, mean_q: 0.912358\n",
      " 2297/5000: episode: 2296, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.553 [0.009, 1.000], loss: 0.415836, mean_absolute_error: 42.055717, mean_q: 0.910959\n",
      " 2298/5000: episode: 2297, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 0.414183, mean_absolute_error: 42.043045, mean_q: 0.909142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2299/5000: episode: 2298, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.060, 1.000], loss: 0.413255, mean_absolute_error: 42.057503, mean_q: 0.908125\n",
      " 2300/5000: episode: 2299, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.174, 1.000], loss: 0.411884, mean_absolute_error: 42.051254, mean_q: 0.906614\n",
      " 2301/5000: episode: 2300, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.331 [0.000, 0.917], loss: 0.410599, mean_absolute_error: 42.050102, mean_q: 0.905198\n",
      " 2302/5000: episode: 2301, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.413 [0.000, 0.977], loss: 0.409062, mean_absolute_error: 42.049522, mean_q: 0.903502\n",
      " 2303/5000: episode: 2302, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.897], loss: 0.407976, mean_absolute_error: 42.046417, mean_q: 0.902298\n",
      " 2304/5000: episode: 2303, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.727], loss: 0.406663, mean_absolute_error: 42.047024, mean_q: 0.900843\n",
      " 2305/5000: episode: 2304, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.000, 0.987], loss: 0.405451, mean_absolute_error: 42.052277, mean_q: 0.899500\n",
      " 2306/5000: episode: 2305, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.000, 1.000], loss: 0.403969, mean_absolute_error: 42.042191, mean_q: 0.897848\n",
      " 2307/5000: episode: 2306, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.844], loss: 0.402871, mean_absolute_error: 42.050583, mean_q: 0.896629\n",
      " 2308/5000: episode: 2307, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.844], loss: 0.401522, mean_absolute_error: 42.046112, mean_q: 0.895126\n",
      " 2309/5000: episode: 2308, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 0.400229, mean_absolute_error: 42.045616, mean_q: 0.893681\n",
      " 2310/5000: episode: 2309, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.267 [0.000, 0.831], loss: 0.398992, mean_absolute_error: 42.044533, mean_q: 0.892298\n",
      " 2311/5000: episode: 2310, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.000, 0.840], loss: 0.397712, mean_absolute_error: 42.044342, mean_q: 0.890862\n",
      " 2312/5000: episode: 2311, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.016, 1.000], loss: 0.396482, mean_absolute_error: 42.045380, mean_q: 0.889484\n",
      " 2313/5000: episode: 2312, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.130, 1.000], loss: 0.395143, mean_absolute_error: 42.038300, mean_q: 0.887976\n",
      " 2314/5000: episode: 2313, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.488, 1.000], loss: 0.393962, mean_absolute_error: 42.046417, mean_q: 0.886649\n",
      " 2315/5000: episode: 2314, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.601], loss: 0.392724, mean_absolute_error: 42.048759, mean_q: 0.885254\n",
      " 2316/5000: episode: 2315, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.000, 0.826], loss: 0.391367, mean_absolute_error: 42.043213, mean_q: 0.883721\n",
      " 2317/5000: episode: 2316, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.373, 1.000], loss: 0.390174, mean_absolute_error: 42.044052, mean_q: 0.882371\n",
      " 2318/5000: episode: 2317, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.562], loss: 0.389059, mean_absolute_error: 42.043232, mean_q: 0.881107\n",
      " 2319/5000: episode: 2318, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.761], loss: 0.387711, mean_absolute_error: 42.046814, mean_q: 0.879580\n",
      " 2320/5000: episode: 2319, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.000, 0.819], loss: 0.386431, mean_absolute_error: 42.041645, mean_q: 0.878122\n",
      " 2321/5000: episode: 2320, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.000, 0.925], loss: 0.385215, mean_absolute_error: 42.044556, mean_q: 0.876740\n",
      " 2322/5000: episode: 2321, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.014, 1.000], loss: 0.383928, mean_absolute_error: 42.039505, mean_q: 0.875272\n",
      " 2323/5000: episode: 2322, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.031, 1.000], loss: 0.382651, mean_absolute_error: 42.039551, mean_q: 0.873814\n",
      " 2324/5000: episode: 2323, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.176, 1.000], loss: 0.381384, mean_absolute_error: 42.031406, mean_q: 0.872361\n",
      " 2325/5000: episode: 2324, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.542], loss: 0.380211, mean_absolute_error: 42.038635, mean_q: 0.871019\n",
      " 2326/5000: episode: 2325, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.274, 1.000], loss: 0.379007, mean_absolute_error: 42.041161, mean_q: 0.869639\n",
      " 2327/5000: episode: 2326, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.944], loss: 0.377680, mean_absolute_error: 42.032337, mean_q: 0.868111\n",
      " 2328/5000: episode: 2327, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.856], loss: 0.376519, mean_absolute_error: 42.034786, mean_q: 0.866776\n",
      " 2329/5000: episode: 2328, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.675], loss: 0.375272, mean_absolute_error: 42.035881, mean_q: 0.865337\n",
      " 2330/5000: episode: 2329, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 0.374051, mean_absolute_error: 42.032043, mean_q: 0.863926\n",
      " 2331/5000: episode: 2330, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.653], loss: 0.372866, mean_absolute_error: 42.037197, mean_q: 0.862556\n",
      " 2332/5000: episode: 2331, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 0.371633, mean_absolute_error: 42.034798, mean_q: 0.861127\n",
      " 2333/5000: episode: 2332, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.320 [0.000, 0.905], loss: 0.370444, mean_absolute_error: 42.036430, mean_q: 0.859745\n",
      " 2334/5000: episode: 2333, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.907], loss: 0.369210, mean_absolute_error: 42.033062, mean_q: 0.858311\n",
      " 2335/5000: episode: 2334, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.654], loss: 0.367989, mean_absolute_error: 42.032280, mean_q: 0.856889\n",
      " 2336/5000: episode: 2335, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.389, 1.000], loss: 0.366829, mean_absolute_error: 42.033325, mean_q: 0.855536\n",
      " 2337/5000: episode: 2336, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.000, 0.999], loss: 0.365617, mean_absolute_error: 42.034531, mean_q: 0.854121\n",
      " 2338/5000: episode: 2337, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.158, 1.000], loss: 0.364394, mean_absolute_error: 42.034622, mean_q: 0.852690\n",
      " 2339/5000: episode: 2338, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.197, 1.000], loss: 0.363192, mean_absolute_error: 42.035328, mean_q: 0.851281\n",
      " 2340/5000: episode: 2339, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.484, 1.000], loss: 0.362023, mean_absolute_error: 42.031128, mean_q: 0.849907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2341/5000: episode: 2340, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.000, 0.996], loss: 0.360766, mean_absolute_error: 42.026169, mean_q: 0.848427\n",
      " 2342/5000: episode: 2341, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.054, 1.000], loss: 0.359630, mean_absolute_error: 42.031410, mean_q: 0.847090\n",
      " 2343/5000: episode: 2342, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.261 [0.000, 0.821], loss: 0.358471, mean_absolute_error: 42.030029, mean_q: 0.845722\n",
      " 2344/5000: episode: 2343, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.445 [0.000, 0.990], loss: 0.357279, mean_absolute_error: 42.031433, mean_q: 0.844314\n",
      " 2345/5000: episode: 2344, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.052, 1.000], loss: 0.356086, mean_absolute_error: 42.027390, mean_q: 0.842900\n",
      " 2346/5000: episode: 2345, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.361, 1.000], loss: 0.354924, mean_absolute_error: 42.026741, mean_q: 0.841522\n",
      " 2347/5000: episode: 2346, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.353741, mean_absolute_error: 42.025719, mean_q: 0.840116\n",
      " 2348/5000: episode: 2347, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 0.352603, mean_absolute_error: 42.031971, mean_q: 0.838764\n",
      " 2349/5000: episode: 2348, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.122, 1.000], loss: 0.351530, mean_absolute_error: 42.027740, mean_q: 0.837484\n",
      " 2350/5000: episode: 2349, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.722], loss: 0.350222, mean_absolute_error: 42.027836, mean_q: 0.835924\n",
      " 2351/5000: episode: 2350, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.381, 1.000], loss: 0.349052, mean_absolute_error: 42.032791, mean_q: 0.834525\n",
      " 2352/5000: episode: 2351, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.708], loss: 0.347769, mean_absolute_error: 42.017784, mean_q: 0.832985\n",
      " 2353/5000: episode: 2352, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 0.346793, mean_absolute_error: 42.028778, mean_q: 0.831818\n",
      " 2354/5000: episode: 2353, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.014, 1.000], loss: 0.345635, mean_absolute_error: 42.026688, mean_q: 0.830425\n",
      " 2355/5000: episode: 2354, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.475 [0.000, 0.997], loss: 0.344474, mean_absolute_error: 42.027603, mean_q: 0.829027\n",
      " 2356/5000: episode: 2355, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.947], loss: 0.343311, mean_absolute_error: 42.024387, mean_q: 0.827626\n",
      " 2357/5000: episode: 2356, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.909], loss: 0.342168, mean_absolute_error: 42.023659, mean_q: 0.826244\n",
      " 2358/5000: episode: 2357, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.403, 1.000], loss: 0.341003, mean_absolute_error: 42.025326, mean_q: 0.824834\n",
      " 2359/5000: episode: 2358, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 0.339888, mean_absolute_error: 42.026913, mean_q: 0.823484\n",
      " 2360/5000: episode: 2359, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.500, 1.000], loss: 0.338754, mean_absolute_error: 42.019054, mean_q: 0.822106\n",
      " 2361/5000: episode: 2360, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.348 [0.000, 0.933], loss: 0.337567, mean_absolute_error: 42.022469, mean_q: 0.820664\n",
      " 2362/5000: episode: 2361, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.000, 0.965], loss: 0.336439, mean_absolute_error: 42.018291, mean_q: 0.819290\n",
      " 2363/5000: episode: 2362, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.382, 1.000], loss: 0.335353, mean_absolute_error: 42.020451, mean_q: 0.817961\n",
      " 2364/5000: episode: 2363, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.614], loss: 0.334206, mean_absolute_error: 42.023453, mean_q: 0.816563\n",
      " 2365/5000: episode: 2364, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 0.333026, mean_absolute_error: 42.019089, mean_q: 0.815117\n",
      " 2366/5000: episode: 2365, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 0.331959, mean_absolute_error: 42.017883, mean_q: 0.813809\n",
      " 2367/5000: episode: 2366, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.696 [0.113, 1.000], loss: 0.330844, mean_absolute_error: 42.022469, mean_q: 0.812440\n",
      " 2368/5000: episode: 2367, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.329636, mean_absolute_error: 42.010971, mean_q: 0.810948\n",
      " 2369/5000: episode: 2368, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.065, 1.000], loss: 0.328542, mean_absolute_error: 42.012573, mean_q: 0.809604\n",
      " 2370/5000: episode: 2369, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.327446, mean_absolute_error: 42.013016, mean_q: 0.808250\n",
      " 2371/5000: episode: 2370, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.616], loss: 0.326407, mean_absolute_error: 42.020340, mean_q: 0.806967\n",
      " 2372/5000: episode: 2371, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.438, 1.000], loss: 0.325298, mean_absolute_error: 42.015430, mean_q: 0.805591\n",
      " 2373/5000: episode: 2372, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.220, 1.000], loss: 0.324287, mean_absolute_error: 42.016800, mean_q: 0.804340\n",
      " 2374/5000: episode: 2373, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.664 [0.078, 1.000], loss: 0.323077, mean_absolute_error: 42.019791, mean_q: 0.802836\n",
      " 2375/5000: episode: 2374, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.511], loss: 0.321975, mean_absolute_error: 42.021133, mean_q: 0.801464\n",
      " 2376/5000: episode: 2375, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.272, 1.000], loss: 0.320813, mean_absolute_error: 42.006645, mean_q: 0.800013\n",
      " 2377/5000: episode: 2376, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.160, 1.000], loss: 0.319712, mean_absolute_error: 42.008698, mean_q: 0.798636\n",
      " 2378/5000: episode: 2377, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.548], loss: 0.318699, mean_absolute_error: 42.015800, mean_q: 0.797370\n",
      " 2379/5000: episode: 2378, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.000, 0.852], loss: 0.317553, mean_absolute_error: 42.007462, mean_q: 0.795933\n",
      " 2380/5000: episode: 2379, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.372, 1.000], loss: 0.316508, mean_absolute_error: 42.014198, mean_q: 0.794620\n",
      " 2381/5000: episode: 2380, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 0.315399, mean_absolute_error: 42.011620, mean_q: 0.793225\n",
      " 2382/5000: episode: 2381, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.000, 0.892], loss: 0.314244, mean_absolute_error: 42.007919, mean_q: 0.791767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2383/5000: episode: 2382, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.313272, mean_absolute_error: 42.014950, mean_q: 0.790544\n",
      " 2384/5000: episode: 2383, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.377, 1.000], loss: 0.312181, mean_absolute_error: 42.011761, mean_q: 0.789164\n",
      " 2385/5000: episode: 2384, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 0.311114, mean_absolute_error: 42.011475, mean_q: 0.787813\n",
      " 2386/5000: episode: 2385, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.329, 1.000], loss: 0.310064, mean_absolute_error: 42.012680, mean_q: 0.786480\n",
      " 2387/5000: episode: 2386, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 0.308997, mean_absolute_error: 42.011780, mean_q: 0.785124\n",
      " 2388/5000: episode: 2387, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 0.307805, mean_absolute_error: 41.999771, mean_q: 0.783598\n",
      " 2389/5000: episode: 2388, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.740], loss: 0.306851, mean_absolute_error: 42.012367, mean_q: 0.782390\n",
      " 2390/5000: episode: 2389, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.856], loss: 0.305734, mean_absolute_error: 42.002277, mean_q: 0.780961\n",
      " 2391/5000: episode: 2390, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.046, 1.000], loss: 0.304764, mean_absolute_error: 42.015549, mean_q: 0.779722\n",
      " 2392/5000: episode: 2391, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.368, 1.000], loss: 0.303651, mean_absolute_error: 42.007481, mean_q: 0.778294\n",
      " 2393/5000: episode: 2392, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.398, 1.000], loss: 0.302539, mean_absolute_error: 42.006950, mean_q: 0.776863\n",
      " 2394/5000: episode: 2393, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.003, 1.000], loss: 0.301513, mean_absolute_error: 42.006989, mean_q: 0.775545\n",
      " 2395/5000: episode: 2394, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.009, 1.000], loss: 0.300502, mean_absolute_error: 42.005493, mean_q: 0.774242\n",
      " 2396/5000: episode: 2395, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 0.299233, mean_absolute_error: 41.998985, mean_q: 0.772599\n",
      " 2397/5000: episode: 2396, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.750], loss: 0.298443, mean_absolute_error: 42.006500, mean_q: 0.771582\n",
      " 2398/5000: episode: 2397, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.759], loss: 0.297340, mean_absolute_error: 42.004879, mean_q: 0.770152\n",
      " 2399/5000: episode: 2398, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.780], loss: 0.296368, mean_absolute_error: 42.009956, mean_q: 0.768892\n",
      " 2400/5000: episode: 2399, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.013, 1.000], loss: 0.295302, mean_absolute_error: 42.001152, mean_q: 0.767506\n",
      " 2401/5000: episode: 2400, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.124, 1.000], loss: 0.294091, mean_absolute_error: 42.003010, mean_q: 0.765928\n",
      " 2402/5000: episode: 2401, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.000, 0.968], loss: 0.293540, mean_absolute_error: 41.994949, mean_q: 0.765193\n",
      " 2403/5000: episode: 2402, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.528], loss: 0.292220, mean_absolute_error: 42.003872, mean_q: 0.763485\n",
      " 2404/5000: episode: 2403, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 0.291083, mean_absolute_error: 41.989777, mean_q: 0.761993\n",
      " 2405/5000: episode: 2404, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.000, 0.972], loss: 0.290141, mean_absolute_error: 41.993935, mean_q: 0.760757\n",
      " 2406/5000: episode: 2405, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.170, 1.000], loss: 0.289188, mean_absolute_error: 42.007843, mean_q: 0.759509\n",
      " 2407/5000: episode: 2406, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.022, 1.000], loss: 0.288107, mean_absolute_error: 41.997383, mean_q: 0.758085\n",
      " 2408/5000: episode: 2407, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.432 [0.000, 0.986], loss: 0.287125, mean_absolute_error: 42.003128, mean_q: 0.756792\n",
      " 2409/5000: episode: 2408, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.000, 1.000], loss: 0.286122, mean_absolute_error: 42.002541, mean_q: 0.755468\n",
      " 2410/5000: episode: 2409, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.194, 1.000], loss: 0.285084, mean_absolute_error: 41.998775, mean_q: 0.754094\n",
      " 2411/5000: episode: 2410, duration: 0.016s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.575 [0.017, 1.000], loss: 0.284045, mean_absolute_error: 41.995934, mean_q: 0.752713\n",
      " 2412/5000: episode: 2411, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.435, 1.000], loss: 0.283082, mean_absolute_error: 41.993820, mean_q: 0.751433\n",
      " 2413/5000: episode: 2412, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.754], loss: 0.282032, mean_absolute_error: 41.992096, mean_q: 0.750039\n",
      " 2414/5000: episode: 2413, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.388 [0.000, 0.964], loss: 0.281081, mean_absolute_error: 41.999321, mean_q: 0.748773\n",
      " 2415/5000: episode: 2414, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.555], loss: 0.280101, mean_absolute_error: 41.993607, mean_q: 0.747461\n",
      " 2416/5000: episode: 2415, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.364, 1.000], loss: 0.279116, mean_absolute_error: 41.997711, mean_q: 0.746148\n",
      " 2417/5000: episode: 2416, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.544 [0.007, 1.000], loss: 0.277996, mean_absolute_error: 41.992332, mean_q: 0.744645\n",
      " 2418/5000: episode: 2417, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.000, 0.996], loss: 0.277063, mean_absolute_error: 41.995644, mean_q: 0.743394\n",
      " 2419/5000: episode: 2418, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.276148, mean_absolute_error: 41.997200, mean_q: 0.742164\n",
      " 2420/5000: episode: 2419, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.543 [0.006, 1.000], loss: 0.275177, mean_absolute_error: 41.992302, mean_q: 0.740853\n",
      " 2421/5000: episode: 2420, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.492], loss: 0.274162, mean_absolute_error: 41.990303, mean_q: 0.739484\n",
      " 2422/5000: episode: 2421, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.000, 1.000], loss: 0.273143, mean_absolute_error: 41.988983, mean_q: 0.738109\n",
      " 2423/5000: episode: 2422, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.544], loss: 0.272186, mean_absolute_error: 41.993484, mean_q: 0.736814\n",
      " 2424/5000: episode: 2423, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.608], loss: 0.271253, mean_absolute_error: 41.992035, mean_q: 0.735547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2425/5000: episode: 2424, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.144, 1.000], loss: 0.270244, mean_absolute_error: 41.991570, mean_q: 0.734177\n",
      " 2426/5000: episode: 2425, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.457, 1.000], loss: 0.269267, mean_absolute_error: 41.989731, mean_q: 0.732845\n",
      " 2427/5000: episode: 2426, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.730 [0.163, 1.000], loss: 0.268284, mean_absolute_error: 41.991001, mean_q: 0.731505\n",
      " 2428/5000: episode: 2427, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.319, 1.000], loss: 0.267344, mean_absolute_error: 41.986477, mean_q: 0.730220\n",
      " 2429/5000: episode: 2428, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.634], loss: 0.266432, mean_absolute_error: 41.995331, mean_q: 0.728974\n",
      " 2430/5000: episode: 2429, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 0.265465, mean_absolute_error: 41.991436, mean_q: 0.727646\n",
      " 2431/5000: episode: 2430, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 0.264495, mean_absolute_error: 41.992714, mean_q: 0.726315\n",
      " 2432/5000: episode: 2431, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.563 [0.012, 1.000], loss: 0.263472, mean_absolute_error: 41.983665, mean_q: 0.724901\n",
      " 2433/5000: episode: 2432, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.374 [0.000, 0.954], loss: 0.262595, mean_absolute_error: 41.990150, mean_q: 0.723699\n",
      " 2434/5000: episode: 2433, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.648 [0.063, 1.000], loss: 0.261766, mean_absolute_error: 41.985641, mean_q: 0.722543\n",
      " 2435/5000: episode: 2434, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.199, 1.000], loss: 0.260722, mean_absolute_error: 41.992519, mean_q: 0.721110\n",
      " 2436/5000: episode: 2435, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.000, 0.819], loss: 0.259751, mean_absolute_error: 41.987900, mean_q: 0.719763\n",
      " 2437/5000: episode: 2436, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.672 [0.085, 1.000], loss: 0.258790, mean_absolute_error: 41.989090, mean_q: 0.718427\n",
      " 2438/5000: episode: 2437, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.909], loss: 0.257831, mean_absolute_error: 41.985649, mean_q: 0.717094\n",
      " 2439/5000: episode: 2438, duration: 0.021s, episode steps: 1, steps per second: 47, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.536], loss: 0.256955, mean_absolute_error: 41.985748, mean_q: 0.715872\n",
      " 2440/5000: episode: 2439, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.223, 1.000], loss: 0.255978, mean_absolute_error: 41.980286, mean_q: 0.714508\n",
      " 2441/5000: episode: 2440, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.505 [0.000, 1.000], loss: 0.254964, mean_absolute_error: 41.978531, mean_q: 0.713088\n",
      " 2442/5000: episode: 2441, duration: 0.017s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.294 [0.000, 0.873], loss: 0.254091, mean_absolute_error: 41.982662, mean_q: 0.711866\n",
      " 2443/5000: episode: 2442, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.125, 1.000], loss: 0.253190, mean_absolute_error: 41.984901, mean_q: 0.710600\n",
      " 2444/5000: episode: 2443, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.578], loss: 0.252316, mean_absolute_error: 41.986412, mean_q: 0.709373\n",
      " 2445/5000: episode: 2444, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.251358, mean_absolute_error: 41.987850, mean_q: 0.708022\n",
      " 2446/5000: episode: 2445, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 0.250420, mean_absolute_error: 41.980461, mean_q: 0.706698\n",
      " 2447/5000: episode: 2446, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.356, 1.000], loss: 0.249454, mean_absolute_error: 41.979515, mean_q: 0.705333\n",
      " 2448/5000: episode: 2447, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.357, 1.000], loss: 0.248582, mean_absolute_error: 41.984879, mean_q: 0.704097\n",
      " 2449/5000: episode: 2448, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 0.247724, mean_absolute_error: 41.985748, mean_q: 0.702879\n",
      " 2450/5000: episode: 2449, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.418, 1.000], loss: 0.246810, mean_absolute_error: 41.981995, mean_q: 0.701579\n",
      " 2451/5000: episode: 2450, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.748], loss: 0.244832, mean_absolute_error: 41.971947, mean_q: 0.698758\n",
      " 2452/5000: episode: 2451, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.000, 0.951], loss: 0.244990, mean_absolute_error: 41.981674, mean_q: 0.698984\n",
      " 2453/5000: episode: 2452, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 0.244141, mean_absolute_error: 41.974258, mean_q: 0.697767\n",
      " 2454/5000: episode: 2453, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.156, 1.000], loss: 0.243175, mean_absolute_error: 41.978638, mean_q: 0.696386\n",
      " 2455/5000: episode: 2454, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.019, 1.000], loss: 0.242232, mean_absolute_error: 41.972290, mean_q: 0.695026\n",
      " 2456/5000: episode: 2455, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.769], loss: 0.241382, mean_absolute_error: 41.978073, mean_q: 0.693809\n",
      " 2457/5000: episode: 2456, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.000, 0.885], loss: 0.240445, mean_absolute_error: 41.975815, mean_q: 0.692459\n",
      " 2458/5000: episode: 2457, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.456 [0.000, 0.993], loss: 0.239583, mean_absolute_error: 41.974232, mean_q: 0.691215\n",
      " 2459/5000: episode: 2458, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.045, 1.000], loss: 0.238731, mean_absolute_error: 41.982529, mean_q: 0.689985\n",
      " 2460/5000: episode: 2459, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.561 [0.012, 1.000], loss: 0.237778, mean_absolute_error: 41.968132, mean_q: 0.688603\n",
      " 2461/5000: episode: 2460, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.677], loss: 0.236915, mean_absolute_error: 41.976944, mean_q: 0.687351\n",
      " 2462/5000: episode: 2461, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.352, 1.000], loss: 0.236075, mean_absolute_error: 41.982224, mean_q: 0.686131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2463/5000: episode: 2462, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.000, 0.697], loss: 0.235182, mean_absolute_error: 41.971672, mean_q: 0.684826\n",
      " 2464/5000: episode: 2463, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 0.233851, mean_absolute_error: 41.960014, mean_q: 0.682873\n",
      " 2465/5000: episode: 2464, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.223, 1.000], loss: 0.233443, mean_absolute_error: 41.975483, mean_q: 0.682288\n",
      " 2466/5000: episode: 2465, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.341, 1.000], loss: 0.232578, mean_absolute_error: 41.967072, mean_q: 0.681017\n",
      " 2467/5000: episode: 2466, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.000, 0.953], loss: 0.231621, mean_absolute_error: 41.969986, mean_q: 0.679614\n",
      " 2468/5000: episode: 2467, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.617], loss: 0.230848, mean_absolute_error: 41.974163, mean_q: 0.678481\n",
      " 2469/5000: episode: 2468, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.000, 0.976], loss: 0.229948, mean_absolute_error: 41.973591, mean_q: 0.677153\n",
      " 2470/5000: episode: 2469, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.030, 1.000], loss: 0.229107, mean_absolute_error: 41.977196, mean_q: 0.675912\n",
      " 2471/5000: episode: 2470, duration: 0.017s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 0.228228, mean_absolute_error: 41.960861, mean_q: 0.674607\n",
      " 2472/5000: episode: 2471, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.081, 1.000], loss: 0.227374, mean_absolute_error: 41.972187, mean_q: 0.673346\n",
      " 2473/5000: episode: 2472, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.203, 1.000], loss: 0.226525, mean_absolute_error: 41.974022, mean_q: 0.672088\n",
      " 2474/5000: episode: 2473, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 0.225651, mean_absolute_error: 41.966331, mean_q: 0.670786\n",
      " 2475/5000: episode: 2474, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.594], loss: 0.224820, mean_absolute_error: 41.967312, mean_q: 0.669549\n",
      " 2476/5000: episode: 2475, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 0.223990, mean_absolute_error: 41.973133, mean_q: 0.668311\n",
      " 2477/5000: episode: 2476, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.731], loss: 0.223121, mean_absolute_error: 41.973846, mean_q: 0.667012\n",
      " 2478/5000: episode: 2477, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.223, 1.000], loss: 0.222262, mean_absolute_error: 41.965897, mean_q: 0.665724\n",
      " 2479/5000: episode: 2478, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 0.221438, mean_absolute_error: 41.965263, mean_q: 0.664486\n",
      " 2480/5000: episode: 2479, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 0.220552, mean_absolute_error: 41.966274, mean_q: 0.663152\n",
      " 2481/5000: episode: 2480, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.861], loss: 0.219718, mean_absolute_error: 41.958019, mean_q: 0.661895\n",
      " 2482/5000: episode: 2481, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.621 [0.042, 1.000], loss: 0.218874, mean_absolute_error: 41.962845, mean_q: 0.660623\n",
      " 2483/5000: episode: 2482, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.169, 1.000], loss: 0.218005, mean_absolute_error: 41.955051, mean_q: 0.659303\n",
      " 2484/5000: episode: 2483, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.379, 1.000], loss: 0.217249, mean_absolute_error: 41.967609, mean_q: 0.658162\n",
      " 2485/5000: episode: 2484, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 0.216398, mean_absolute_error: 41.959599, mean_q: 0.656868\n",
      " 2486/5000: episode: 2485, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.674], loss: 0.215590, mean_absolute_error: 41.964024, mean_q: 0.655640\n",
      " 2487/5000: episode: 2486, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.000, 0.929], loss: 0.214759, mean_absolute_error: 41.954502, mean_q: 0.654371\n",
      " 2488/5000: episode: 2487, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.641 [0.057, 1.000], loss: 0.214305, mean_absolute_error: 41.965950, mean_q: 0.653651\n",
      " 2489/5000: episode: 2488, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.570], loss: 0.213119, mean_absolute_error: 41.961910, mean_q: 0.651866\n",
      " 2490/5000: episode: 2489, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 0.211916, mean_absolute_error: 41.910011, mean_q: 0.649745\n",
      " 2491/5000: episode: 2490, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.009, 1.000], loss: 0.211489, mean_absolute_error: 41.958900, mean_q: 0.649361\n",
      " 2492/5000: episode: 2491, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.381, 1.000], loss: 0.210690, mean_absolute_error: 41.964813, mean_q: 0.648136\n",
      " 2493/5000: episode: 2492, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.440, 1.000], loss: 0.209894, mean_absolute_error: 41.961327, mean_q: 0.646906\n",
      " 2494/5000: episode: 2493, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.312, 1.000], loss: 0.209031, mean_absolute_error: 41.958191, mean_q: 0.645574\n",
      " 2495/5000: episode: 2494, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.445, 1.000], loss: 0.208302, mean_absolute_error: 41.965767, mean_q: 0.644446\n",
      " 2496/5000: episode: 2495, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.061, 1.000], loss: 0.207402, mean_absolute_error: 41.950050, mean_q: 0.643043\n",
      " 2497/5000: episode: 2496, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.524 [0.003, 1.000], loss: 0.206716, mean_absolute_error: 41.959225, mean_q: 0.641982\n",
      " 2498/5000: episode: 2497, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.440 [0.000, 0.989], loss: 0.205861, mean_absolute_error: 41.957245, mean_q: 0.640650\n",
      " 2499/5000: episode: 2498, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.381, 1.000], loss: 0.205015, mean_absolute_error: 41.953621, mean_q: 0.639331\n",
      " 2500/5000: episode: 2499, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.246, 1.000], loss: 0.204296, mean_absolute_error: 41.962158, mean_q: 0.638211\n",
      " 2501/5000: episode: 2500, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 0.870], loss: 0.203553, mean_absolute_error: 41.958336, mean_q: 0.637046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2502/5000: episode: 2501, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.638], loss: 0.202737, mean_absolute_error: 41.952621, mean_q: 0.635764\n",
      " 2503/5000: episode: 2502, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.123, 1.000], loss: 0.201905, mean_absolute_error: 41.954453, mean_q: 0.634457\n",
      " 2504/5000: episode: 2503, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.061, 1.000], loss: 0.201145, mean_absolute_error: 41.950562, mean_q: 0.633258\n",
      " 2505/5000: episode: 2504, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.243, 1.000], loss: 0.200365, mean_absolute_error: 41.958397, mean_q: 0.632030\n",
      " 2506/5000: episode: 2505, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.185, 1.000], loss: 0.199517, mean_absolute_error: 41.947594, mean_q: 0.630682\n",
      " 2507/5000: episode: 2506, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.310, 1.000], loss: 0.198811, mean_absolute_error: 41.958008, mean_q: 0.629571\n",
      " 2508/5000: episode: 2507, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.472, 1.000], loss: 0.198047, mean_absolute_error: 41.960602, mean_q: 0.628359\n",
      " 2509/5000: episode: 2508, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.512], loss: 0.197239, mean_absolute_error: 41.953663, mean_q: 0.627072\n",
      " 2510/5000: episode: 2509, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.196457, mean_absolute_error: 41.950413, mean_q: 0.625823\n",
      " 2511/5000: episode: 2510, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.154, 1.000], loss: 0.195708, mean_absolute_error: 41.950630, mean_q: 0.624630\n",
      " 2512/5000: episode: 2511, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.554 [0.009, 1.000], loss: 0.194989, mean_absolute_error: 41.956573, mean_q: 0.623481\n",
      " 2513/5000: episode: 2512, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.677], loss: 0.194208, mean_absolute_error: 41.954788, mean_q: 0.622229\n",
      " 2514/5000: episode: 2513, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.003, 1.000], loss: 0.193446, mean_absolute_error: 41.954514, mean_q: 0.621004\n",
      " 2515/5000: episode: 2514, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.000, 0.987], loss: 0.192602, mean_absolute_error: 41.945850, mean_q: 0.619644\n",
      " 2516/5000: episode: 2515, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.053, 1.000], loss: 0.191921, mean_absolute_error: 41.952293, mean_q: 0.618547\n",
      " 2517/5000: episode: 2516, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 0.191179, mean_absolute_error: 41.946884, mean_q: 0.617343\n",
      " 2518/5000: episode: 2517, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.489], loss: 0.190423, mean_absolute_error: 41.950882, mean_q: 0.616124\n",
      " 2519/5000: episode: 2518, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.491, 1.000], loss: 0.189666, mean_absolute_error: 41.954041, mean_q: 0.614896\n",
      " 2520/5000: episode: 2519, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.647 [0.062, 1.000], loss: 0.188936, mean_absolute_error: 41.946602, mean_q: 0.613707\n",
      " 2521/5000: episode: 2520, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.482, 1.000], loss: 0.188159, mean_absolute_error: 41.948349, mean_q: 0.612441\n",
      " 2522/5000: episode: 2521, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.187382, mean_absolute_error: 41.951038, mean_q: 0.611176\n",
      " 2523/5000: episode: 2522, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.000, 0.935], loss: 0.186640, mean_absolute_error: 41.947536, mean_q: 0.609964\n",
      " 2524/5000: episode: 2523, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.484, 1.000], loss: 0.185940, mean_absolute_error: 41.948769, mean_q: 0.608817\n",
      " 2525/5000: episode: 2524, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.185210, mean_absolute_error: 41.947296, mean_q: 0.607618\n",
      " 2526/5000: episode: 2525, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.102, 1.000], loss: 0.184502, mean_absolute_error: 41.943302, mean_q: 0.606440\n",
      " 2527/5000: episode: 2526, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.545], loss: 0.183752, mean_absolute_error: 41.952003, mean_q: 0.605221\n",
      " 2528/5000: episode: 2527, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.018, 1.000], loss: 0.182999, mean_absolute_error: 41.947990, mean_q: 0.603974\n",
      " 2529/5000: episode: 2528, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.009, 1.000], loss: 0.182273, mean_absolute_error: 41.948471, mean_q: 0.602773\n",
      " 2530/5000: episode: 2529, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.813], loss: 0.181555, mean_absolute_error: 41.946220, mean_q: 0.601584\n",
      " 2531/5000: episode: 2530, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.290 [0.000, 0.868], loss: 0.180845, mean_absolute_error: 41.953178, mean_q: 0.600406\n",
      " 2532/5000: episode: 2531, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 0.180051, mean_absolute_error: 41.940445, mean_q: 0.599081\n",
      " 2533/5000: episode: 2532, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.118, 1.000], loss: 0.179422, mean_absolute_error: 41.943939, mean_q: 0.598030\n",
      " 2534/5000: episode: 2533, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 0.178691, mean_absolute_error: 41.950073, mean_q: 0.596812\n",
      " 2535/5000: episode: 2534, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.047, 1.000], loss: 0.177865, mean_absolute_error: 41.938057, mean_q: 0.595426\n",
      " 2536/5000: episode: 2535, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.852], loss: 0.177232, mean_absolute_error: 41.934425, mean_q: 0.594362\n",
      " 2537/5000: episode: 2536, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.414 [0.000, 0.978], loss: 0.176486, mean_absolute_error: 41.938976, mean_q: 0.593109\n",
      " 2538/5000: episode: 2537, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 0.175842, mean_absolute_error: 41.946629, mean_q: 0.592027\n",
      " 2539/5000: episode: 2538, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.300, 1.000], loss: 0.175069, mean_absolute_error: 41.937607, mean_q: 0.590722\n",
      " 2540/5000: episode: 2539, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.123, 1.000], loss: 0.174354, mean_absolute_error: 41.939590, mean_q: 0.589512\n",
      " 2541/5000: episode: 2540, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.194, 1.000], loss: 0.173668, mean_absolute_error: 41.934578, mean_q: 0.588345\n",
      " 2542/5000: episode: 2541, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.000, 0.888], loss: 0.173004, mean_absolute_error: 41.940544, mean_q: 0.587220\n",
      " 2543/5000: episode: 2542, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.505 [0.000, 1.000], loss: 0.172313, mean_absolute_error: 41.945965, mean_q: 0.586048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2544/5000: episode: 2543, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.856], loss: 0.171606, mean_absolute_error: 41.941444, mean_q: 0.584842\n",
      " 2545/5000: episode: 2544, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.170905, mean_absolute_error: 41.941521, mean_q: 0.583642\n",
      " 2546/5000: episode: 2545, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.672], loss: 0.170242, mean_absolute_error: 41.940426, mean_q: 0.582508\n",
      " 2547/5000: episode: 2546, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.371 [0.000, 0.952], loss: 0.169540, mean_absolute_error: 41.943321, mean_q: 0.581305\n",
      " 2548/5000: episode: 2547, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.016, 1.000], loss: 0.168864, mean_absolute_error: 41.941631, mean_q: 0.580140\n",
      " 2549/5000: episode: 2548, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.200, 1.000], loss: 0.168185, mean_absolute_error: 41.932014, mean_q: 0.578968\n",
      " 2550/5000: episode: 2549, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.000, 0.942], loss: 0.167471, mean_absolute_error: 41.937370, mean_q: 0.577738\n",
      " 2551/5000: episode: 2550, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.323, 1.000], loss: 0.166801, mean_absolute_error: 41.930260, mean_q: 0.576575\n",
      " 2552/5000: episode: 2551, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.385 [0.000, 0.962], loss: 0.165754, mean_absolute_error: 41.928398, mean_q: 0.574764\n",
      " 2553/5000: episode: 2552, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.061, 1.000], loss: 0.165420, mean_absolute_error: 41.935932, mean_q: 0.574184\n",
      " 2554/5000: episode: 2553, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.164768, mean_absolute_error: 41.943481, mean_q: 0.573052\n",
      " 2555/5000: episode: 2554, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.498 [0.000, 1.000], loss: 0.164044, mean_absolute_error: 41.932102, mean_q: 0.571785\n",
      " 2556/5000: episode: 2555, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.502, 1.000], loss: 0.163439, mean_absolute_error: 41.934326, mean_q: 0.570726\n",
      " 2557/5000: episode: 2556, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.099, 1.000], loss: 0.162772, mean_absolute_error: 41.933788, mean_q: 0.569560\n",
      " 2558/5000: episode: 2557, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.515], loss: 0.162038, mean_absolute_error: 41.932335, mean_q: 0.568271\n",
      " 2559/5000: episode: 2558, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.544], loss: 0.161425, mean_absolute_error: 41.935555, mean_q: 0.567196\n",
      " 2560/5000: episode: 2559, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.368 [0.000, 0.950], loss: 0.160723, mean_absolute_error: 41.933495, mean_q: 0.565958\n",
      " 2561/5000: episode: 2560, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.507], loss: 0.160073, mean_absolute_error: 41.926575, mean_q: 0.564810\n",
      " 2562/5000: episode: 2561, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.752], loss: 0.159374, mean_absolute_error: 41.927593, mean_q: 0.563570\n",
      " 2563/5000: episode: 2562, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.613], loss: 0.158799, mean_absolute_error: 41.932808, mean_q: 0.562557\n",
      " 2564/5000: episode: 2563, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.590], loss: 0.158097, mean_absolute_error: 41.929417, mean_q: 0.561307\n",
      " 2565/5000: episode: 2564, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.416, 1.000], loss: 0.157412, mean_absolute_error: 41.927216, mean_q: 0.560083\n",
      " 2566/5000: episode: 2565, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.352, 1.000], loss: 0.156793, mean_absolute_error: 41.928841, mean_q: 0.558985\n",
      " 2567/5000: episode: 2566, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.227, 1.000], loss: 0.156149, mean_absolute_error: 41.936825, mean_q: 0.557835\n",
      " 2568/5000: episode: 2567, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.155527, mean_absolute_error: 41.926933, mean_q: 0.556714\n",
      " 2569/5000: episode: 2568, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.485 [0.000, 0.999], loss: 0.154868, mean_absolute_error: 41.934883, mean_q: 0.555539\n",
      " 2570/5000: episode: 2569, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.300, 1.000], loss: 0.154218, mean_absolute_error: 41.930534, mean_q: 0.554368\n",
      " 2571/5000: episode: 2570, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.157, 1.000], loss: 0.153588, mean_absolute_error: 41.933502, mean_q: 0.553232\n",
      " 2572/5000: episode: 2571, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.508], loss: 0.152947, mean_absolute_error: 41.931969, mean_q: 0.552075\n",
      " 2573/5000: episode: 2572, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 0.152307, mean_absolute_error: 41.925804, mean_q: 0.550913\n",
      " 2574/5000: episode: 2573, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.000, 0.991], loss: 0.151639, mean_absolute_error: 41.917885, mean_q: 0.549698\n",
      " 2575/5000: episode: 2574, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.105, 1.000], loss: 0.151036, mean_absolute_error: 41.925491, mean_q: 0.548607\n",
      " 2576/5000: episode: 2575, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.696 [0.114, 1.000], loss: 0.150414, mean_absolute_error: 41.923065, mean_q: 0.547467\n",
      " 2577/5000: episode: 2576, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.231, 1.000], loss: 0.149779, mean_absolute_error: 41.930916, mean_q: 0.546318\n",
      " 2578/5000: episode: 2577, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.000, 0.964], loss: 0.149162, mean_absolute_error: 41.930710, mean_q: 0.545187\n",
      " 2579/5000: episode: 2578, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.298, 1.000], loss: 0.148509, mean_absolute_error: 41.919388, mean_q: 0.543984\n",
      " 2580/5000: episode: 2579, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.147933, mean_absolute_error: 41.920139, mean_q: 0.542921\n",
      " 2581/5000: episode: 2580, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.570], loss: 0.147291, mean_absolute_error: 41.924362, mean_q: 0.541751\n",
      " 2582/5000: episode: 2581, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 0.146680, mean_absolute_error: 41.923061, mean_q: 0.540623\n",
      " 2583/5000: episode: 2582, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.376 [0.000, 0.956], loss: 0.146077, mean_absolute_error: 41.925476, mean_q: 0.539508\n",
      " 2584/5000: episode: 2583, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.357, 1.000], loss: 0.145414, mean_absolute_error: 41.924622, mean_q: 0.538280\n",
      " 2585/5000: episode: 2584, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.207, 1.000], loss: 0.144838, mean_absolute_error: 41.927040, mean_q: 0.537213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2586/5000: episode: 2585, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.144224, mean_absolute_error: 41.926498, mean_q: 0.536072\n",
      " 2587/5000: episode: 2586, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.495], loss: 0.143611, mean_absolute_error: 41.930096, mean_q: 0.534930\n",
      " 2588/5000: episode: 2587, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.000, 0.899], loss: 0.142980, mean_absolute_error: 41.919861, mean_q: 0.533749\n",
      " 2589/5000: episode: 2588, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.656], loss: 0.142397, mean_absolute_error: 41.924561, mean_q: 0.532655\n",
      " 2590/5000: episode: 2589, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.585], loss: 0.141792, mean_absolute_error: 41.919498, mean_q: 0.531522\n",
      " 2591/5000: episode: 2590, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.016, 1.000], loss: 0.141183, mean_absolute_error: 41.924519, mean_q: 0.530379\n",
      " 2592/5000: episode: 2591, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 0.140638, mean_absolute_error: 41.903080, mean_q: 0.529176\n",
      " 2593/5000: episode: 2592, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.601], loss: 0.139999, mean_absolute_error: 41.927723, mean_q: 0.528147\n",
      " 2594/5000: episode: 2593, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.373, 1.000], loss: 0.139424, mean_absolute_error: 41.921371, mean_q: 0.527058\n",
      " 2595/5000: episode: 2594, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.022, 1.000], loss: 0.138806, mean_absolute_error: 41.918816, mean_q: 0.525884\n",
      " 2596/5000: episode: 2595, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.348, 1.000], loss: 0.138187, mean_absolute_error: 41.914692, mean_q: 0.524704\n",
      " 2597/5000: episode: 2596, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.262, 1.000], loss: 0.137658, mean_absolute_error: 41.921360, mean_q: 0.523703\n",
      " 2598/5000: episode: 2597, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.769], loss: 0.137056, mean_absolute_error: 41.916134, mean_q: 0.522552\n",
      " 2599/5000: episode: 2598, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.749], loss: 0.136449, mean_absolute_error: 41.919666, mean_q: 0.521393\n",
      " 2600/5000: episode: 2599, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.166, 1.000], loss: 0.136300, mean_absolute_error: 41.906868, mean_q: 0.520864\n",
      " 2601/5000: episode: 2600, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.047, 1.000], loss: 0.135291, mean_absolute_error: 41.919464, mean_q: 0.519173\n",
      " 2602/5000: episode: 2601, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.000, 0.840], loss: 0.134705, mean_absolute_error: 41.917503, mean_q: 0.518044\n",
      " 2603/5000: episode: 2602, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 0.134124, mean_absolute_error: 41.919876, mean_q: 0.516925\n",
      " 2604/5000: episode: 2603, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.767], loss: 0.133612, mean_absolute_error: 41.907654, mean_q: 0.515906\n",
      " 2605/5000: episode: 2604, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.614], loss: 0.132983, mean_absolute_error: 41.915031, mean_q: 0.514714\n",
      " 2606/5000: episode: 2605, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.305, 1.000], loss: 0.132387, mean_absolute_error: 41.911354, mean_q: 0.513559\n",
      " 2607/5000: episode: 2606, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.190, 1.000], loss: 0.131846, mean_absolute_error: 41.918701, mean_q: 0.512506\n",
      " 2608/5000: episode: 2607, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.131280, mean_absolute_error: 41.912315, mean_q: 0.511398\n",
      " 2609/5000: episode: 2608, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 0.130703, mean_absolute_error: 41.920670, mean_q: 0.510279\n",
      " 2610/5000: episode: 2609, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.000, 0.793], loss: 0.130150, mean_absolute_error: 41.910660, mean_q: 0.509191\n",
      " 2611/5000: episode: 2610, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.326, 1.000], loss: 0.129580, mean_absolute_error: 41.914589, mean_q: 0.508075\n",
      " 2612/5000: episode: 2611, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.695 [0.112, 1.000], loss: 0.128992, mean_absolute_error: 41.915928, mean_q: 0.506919\n",
      " 2613/5000: episode: 2612, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.723], loss: 0.128444, mean_absolute_error: 41.914158, mean_q: 0.505836\n",
      " 2614/5000: episode: 2613, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.787], loss: 0.127883, mean_absolute_error: 41.912746, mean_q: 0.504730\n",
      " 2615/5000: episode: 2614, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.277, 1.000], loss: 0.127323, mean_absolute_error: 41.913536, mean_q: 0.503621\n",
      " 2616/5000: episode: 2615, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.511], loss: 0.126767, mean_absolute_error: 41.911716, mean_q: 0.502518\n",
      " 2617/5000: episode: 2616, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.683 [0.098, 1.000], loss: 0.126217, mean_absolute_error: 41.908127, mean_q: 0.501419\n",
      " 2618/5000: episode: 2617, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.564], loss: 0.125660, mean_absolute_error: 41.914074, mean_q: 0.500315\n",
      " 2619/5000: episode: 2618, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.512, 1.000], loss: 0.125093, mean_absolute_error: 41.907467, mean_q: 0.499169\n",
      " 2620/5000: episode: 2619, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 0.124583, mean_absolute_error: 41.912552, mean_q: 0.498161\n",
      " 2621/5000: episode: 2620, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.589], loss: 0.124050, mean_absolute_error: 41.908615, mean_q: 0.497092\n",
      " 2622/5000: episode: 2621, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.000, 0.972], loss: 0.123516, mean_absolute_error: 41.914116, mean_q: 0.496021\n",
      " 2623/5000: episode: 2622, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.371 [0.000, 0.952], loss: 0.122982, mean_absolute_error: 41.913898, mean_q: 0.494946\n",
      " 2624/5000: episode: 2623, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.493], loss: 0.122426, mean_absolute_error: 41.909279, mean_q: 0.493823\n",
      " 2625/5000: episode: 2624, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.000, 0.913], loss: 0.121851, mean_absolute_error: 41.911644, mean_q: 0.492658\n",
      " 2626/5000: episode: 2625, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.199, 1.000], loss: 0.121363, mean_absolute_error: 41.912056, mean_q: 0.491671\n",
      " 2627/5000: episode: 2626, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.158, 1.000], loss: 0.120815, mean_absolute_error: 41.913200, mean_q: 0.490558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2628/5000: episode: 2627, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.198, 1.000], loss: 0.120270, mean_absolute_error: 41.904625, mean_q: 0.489443\n",
      " 2629/5000: episode: 2628, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 0.119732, mean_absolute_error: 41.905270, mean_q: 0.488347\n",
      " 2630/5000: episode: 2629, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 0.119237, mean_absolute_error: 41.914558, mean_q: 0.487338\n",
      " 2631/5000: episode: 2630, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.769], loss: 0.118700, mean_absolute_error: 41.911137, mean_q: 0.486235\n",
      " 2632/5000: episode: 2631, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.388 [0.000, 0.963], loss: 0.118169, mean_absolute_error: 41.907471, mean_q: 0.485144\n",
      " 2633/5000: episode: 2632, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.000, 0.884], loss: 0.117629, mean_absolute_error: 41.899189, mean_q: 0.484027\n",
      " 2634/5000: episode: 2633, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.221, 1.000], loss: 0.117126, mean_absolute_error: 41.898750, mean_q: 0.482990\n",
      " 2635/5000: episode: 2634, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.345, 1.000], loss: 0.116615, mean_absolute_error: 41.904499, mean_q: 0.481933\n",
      " 2636/5000: episode: 2635, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.492, 1.000], loss: 0.116077, mean_absolute_error: 41.908798, mean_q: 0.480821\n",
      " 2637/5000: episode: 2636, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.039, 1.000], loss: 0.115601, mean_absolute_error: 41.907852, mean_q: 0.479832\n",
      " 2638/5000: episode: 2637, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.615], loss: 0.115055, mean_absolute_error: 41.909706, mean_q: 0.478696\n",
      " 2639/5000: episode: 2638, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.000, 0.962], loss: 0.114544, mean_absolute_error: 41.908440, mean_q: 0.477629\n",
      " 2640/5000: episode: 2639, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.186, 1.000], loss: 0.114051, mean_absolute_error: 41.908684, mean_q: 0.476598\n",
      " 2641/5000: episode: 2640, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.009, 1.000], loss: 0.113532, mean_absolute_error: 41.907909, mean_q: 0.475510\n",
      " 2642/5000: episode: 2641, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.907], loss: 0.113037, mean_absolute_error: 41.905319, mean_q: 0.474469\n",
      " 2643/5000: episode: 2642, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.147, 1.000], loss: 0.112520, mean_absolute_error: 41.903038, mean_q: 0.473380\n",
      " 2644/5000: episode: 2643, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.474, 1.000], loss: 0.112025, mean_absolute_error: 41.901360, mean_q: 0.472335\n",
      " 2645/5000: episode: 2644, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 0.112077, mean_absolute_error: 41.896675, mean_q: 0.472252\n",
      " 2646/5000: episode: 2645, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.430 [0.000, 0.985], loss: 0.111002, mean_absolute_error: 41.890663, mean_q: 0.470162\n",
      " 2647/5000: episode: 2646, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.159, 1.000], loss: 0.110512, mean_absolute_error: 41.907131, mean_q: 0.469131\n",
      " 2648/5000: episode: 2647, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.453, 1.000], loss: 0.110047, mean_absolute_error: 41.899258, mean_q: 0.468138\n",
      " 2649/5000: episode: 2648, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.708], loss: 0.109537, mean_absolute_error: 41.899937, mean_q: 0.467047\n",
      " 2650/5000: episode: 2649, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 0.109035, mean_absolute_error: 41.902348, mean_q: 0.465977\n",
      " 2651/5000: episode: 2650, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.723], loss: 0.108528, mean_absolute_error: 41.897823, mean_q: 0.464885\n",
      " 2652/5000: episode: 2651, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.514], loss: 0.108043, mean_absolute_error: 41.899414, mean_q: 0.463847\n",
      " 2653/5000: episode: 2652, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.030, 1.000], loss: 0.107827, mean_absolute_error: 41.884727, mean_q: 0.463342\n",
      " 2654/5000: episode: 2653, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.018, 1.000], loss: 0.107051, mean_absolute_error: 41.897957, mean_q: 0.461707\n",
      " 2655/5000: episode: 2654, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.371, 1.000], loss: 0.106595, mean_absolute_error: 41.905289, mean_q: 0.460725\n",
      " 2656/5000: episode: 2655, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.359, 1.000], loss: 0.106092, mean_absolute_error: 41.898193, mean_q: 0.459631\n",
      " 2657/5000: episode: 2656, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.061, 1.000], loss: 0.105606, mean_absolute_error: 41.897366, mean_q: 0.458573\n",
      " 2658/5000: episode: 2657, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.490 [0.000, 0.999], loss: 0.105145, mean_absolute_error: 41.897263, mean_q: 0.457571\n",
      " 2659/5000: episode: 2658, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.000, 0.940], loss: 0.104668, mean_absolute_error: 41.900829, mean_q: 0.456531\n",
      " 2660/5000: episode: 2659, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.587], loss: 0.104201, mean_absolute_error: 41.892948, mean_q: 0.455505\n",
      " 2661/5000: episode: 2660, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.000, 0.953], loss: 0.103714, mean_absolute_error: 41.899048, mean_q: 0.454441\n",
      " 2662/5000: episode: 2661, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.844], loss: 0.103250, mean_absolute_error: 41.897583, mean_q: 0.453418\n",
      " 2663/5000: episode: 2662, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.013, 1.000], loss: 0.102774, mean_absolute_error: 41.899380, mean_q: 0.452373\n",
      " 2664/5000: episode: 2663, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.479, 1.000], loss: 0.102309, mean_absolute_error: 41.897152, mean_q: 0.451345\n",
      " 2665/5000: episode: 2664, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 0.101845, mean_absolute_error: 41.880882, mean_q: 0.450302\n",
      " 2666/5000: episode: 2665, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.314, 1.000], loss: 0.101365, mean_absolute_error: 41.881569, mean_q: 0.449242\n",
      " 2667/5000: episode: 2666, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.686], loss: 0.100921, mean_absolute_error: 41.895325, mean_q: 0.448267\n",
      " 2668/5000: episode: 2667, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.000, 0.846], loss: 0.100449, mean_absolute_error: 41.892052, mean_q: 0.447211\n",
      " 2669/5000: episode: 2668, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.099964, mean_absolute_error: 41.891777, mean_q: 0.446129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2670/5000: episode: 2669, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.845], loss: 0.099540, mean_absolute_error: 41.892593, mean_q: 0.445179\n",
      " 2671/5000: episode: 2670, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 0.099071, mean_absolute_error: 41.891296, mean_q: 0.444125\n",
      " 2672/5000: episode: 2671, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.008, 1.000], loss: 0.098622, mean_absolute_error: 41.893253, mean_q: 0.443119\n",
      " 2673/5000: episode: 2672, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.812], loss: 0.098146, mean_absolute_error: 41.886799, mean_q: 0.442040\n",
      " 2674/5000: episode: 2673, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.000, 0.999], loss: 0.097705, mean_absolute_error: 41.894234, mean_q: 0.441050\n",
      " 2675/5000: episode: 2674, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.564], loss: 0.096996, mean_absolute_error: 41.887886, mean_q: 0.439442\n",
      " 2676/5000: episode: 2675, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 0.096799, mean_absolute_error: 41.895515, mean_q: 0.438997\n",
      " 2677/5000: episode: 2676, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 0.096352, mean_absolute_error: 41.892899, mean_q: 0.437978\n",
      " 2678/5000: episode: 2677, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.686], loss: 0.095909, mean_absolute_error: 41.887100, mean_q: 0.436965\n",
      " 2679/5000: episode: 2678, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.000, 0.777], loss: 0.095468, mean_absolute_error: 41.884949, mean_q: 0.435944\n",
      " 2680/5000: episode: 2679, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.181, 1.000], loss: 0.095017, mean_absolute_error: 41.893063, mean_q: 0.434927\n",
      " 2681/5000: episode: 2680, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.360, 1.000], loss: 0.094569, mean_absolute_error: 41.889523, mean_q: 0.433897\n",
      " 2682/5000: episode: 2681, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.472, 1.000], loss: 0.094143, mean_absolute_error: 41.883446, mean_q: 0.432912\n",
      " 2683/5000: episode: 2682, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.656 [0.070, 1.000], loss: 0.093702, mean_absolute_error: 41.884979, mean_q: 0.431898\n",
      " 2684/5000: episode: 2683, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.325, 1.000], loss: 0.093278, mean_absolute_error: 41.886227, mean_q: 0.430917\n",
      " 2685/5000: episode: 2684, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.221, 1.000], loss: 0.092844, mean_absolute_error: 41.888794, mean_q: 0.429909\n",
      " 2686/5000: episode: 2685, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.132, 1.000], loss: 0.092397, mean_absolute_error: 41.884560, mean_q: 0.428873\n",
      " 2687/5000: episode: 2686, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.524 [0.002, 1.000], loss: 0.091967, mean_absolute_error: 41.887699, mean_q: 0.427872\n",
      " 2688/5000: episode: 2687, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.632 [0.050, 1.000], loss: 0.091526, mean_absolute_error: 41.886967, mean_q: 0.426839\n",
      " 2689/5000: episode: 2688, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.148, 1.000], loss: 0.091117, mean_absolute_error: 41.889473, mean_q: 0.425887\n",
      " 2690/5000: episode: 2689, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.020, 1.000], loss: 0.090676, mean_absolute_error: 41.889938, mean_q: 0.424853\n",
      " 2691/5000: episode: 2690, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.000, 0.903], loss: 0.090248, mean_absolute_error: 41.885338, mean_q: 0.423842\n",
      " 2692/5000: episode: 2691, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.196, 1.000], loss: 0.089804, mean_absolute_error: 41.878372, mean_q: 0.422792\n",
      " 2693/5000: episode: 2692, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.600], loss: 0.089411, mean_absolute_error: 41.888996, mean_q: 0.421870\n",
      " 2694/5000: episode: 2693, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.263, 1.000], loss: 0.088972, mean_absolute_error: 41.883232, mean_q: 0.420826\n",
      " 2695/5000: episode: 2694, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.648], loss: 0.088687, mean_absolute_error: 41.873989, mean_q: 0.420091\n",
      " 2696/5000: episode: 2695, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.416, 1.000], loss: 0.088146, mean_absolute_error: 41.885864, mean_q: 0.418870\n",
      " 2697/5000: episode: 2696, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.624], loss: 0.087718, mean_absolute_error: 41.883045, mean_q: 0.417846\n",
      " 2698/5000: episode: 2697, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.001, 1.000], loss: 0.087290, mean_absolute_error: 41.874969, mean_q: 0.416814\n",
      " 2699/5000: episode: 2698, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.357, 1.000], loss: 0.086887, mean_absolute_error: 41.871399, mean_q: 0.415848\n",
      " 2700/5000: episode: 2699, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.336, 1.000], loss: 0.086487, mean_absolute_error: 41.883743, mean_q: 0.414899\n",
      " 2701/5000: episode: 2700, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.803], loss: 0.086080, mean_absolute_error: 41.885132, mean_q: 0.413919\n",
      " 2702/5000: episode: 2701, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 0.085647, mean_absolute_error: 41.882545, mean_q: 0.412872\n",
      " 2703/5000: episode: 2702, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 0.085260, mean_absolute_error: 41.881638, mean_q: 0.411936\n",
      " 2704/5000: episode: 2703, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.792], loss: 0.084881, mean_absolute_error: 41.879536, mean_q: 0.411015\n",
      " 2705/5000: episode: 2704, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.000, 0.964], loss: 0.084450, mean_absolute_error: 41.884106, mean_q: 0.409970\n",
      " 2706/5000: episode: 2705, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.681], loss: 0.084056, mean_absolute_error: 41.878227, mean_q: 0.409003\n",
      " 2707/5000: episode: 2706, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.312, 1.000], loss: 0.083660, mean_absolute_error: 41.881531, mean_q: 0.408044\n",
      " 2708/5000: episode: 2707, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.000, 0.999], loss: 0.083248, mean_absolute_error: 41.872894, mean_q: 0.407033\n",
      " 2709/5000: episode: 2708, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.000, 1.000], loss: 0.082847, mean_absolute_error: 41.876701, mean_q: 0.406047\n",
      " 2710/5000: episode: 2709, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.486, 1.000], loss: 0.082460, mean_absolute_error: 41.875290, mean_q: 0.405096\n",
      " 2711/5000: episode: 2710, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.215, 1.000], loss: 0.082064, mean_absolute_error: 41.879364, mean_q: 0.404123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2712/5000: episode: 2711, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 0.081659, mean_absolute_error: 41.879494, mean_q: 0.403123\n",
      " 2713/5000: episode: 2712, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.000, 0.987], loss: 0.081281, mean_absolute_error: 41.877670, mean_q: 0.402183\n",
      " 2714/5000: episode: 2713, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.323, 1.000], loss: 0.080886, mean_absolute_error: 41.877220, mean_q: 0.401206\n",
      " 2715/5000: episode: 2714, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.000, 0.925], loss: 0.080495, mean_absolute_error: 41.874332, mean_q: 0.400227\n",
      " 2716/5000: episode: 2715, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.812], loss: 0.080121, mean_absolute_error: 41.873566, mean_q: 0.399295\n",
      " 2717/5000: episode: 2716, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.005, 1.000], loss: 0.079737, mean_absolute_error: 41.878414, mean_q: 0.398337\n",
      " 2718/5000: episode: 2717, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.081, 1.000], loss: 0.079348, mean_absolute_error: 41.876434, mean_q: 0.397363\n",
      " 2719/5000: episode: 2718, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.196, 1.000], loss: 0.078952, mean_absolute_error: 41.873238, mean_q: 0.396366\n",
      " 2720/5000: episode: 2719, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 0.078561, mean_absolute_error: 41.870495, mean_q: 0.395378\n",
      " 2721/5000: episode: 2720, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 0.078194, mean_absolute_error: 41.873398, mean_q: 0.394454\n",
      " 2722/5000: episode: 2721, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.724], loss: 0.077819, mean_absolute_error: 41.877213, mean_q: 0.393507\n",
      " 2723/5000: episode: 2722, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.000, 0.999], loss: 0.077430, mean_absolute_error: 41.875854, mean_q: 0.392519\n",
      " 2724/5000: episode: 2723, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 0.077077, mean_absolute_error: 41.872025, mean_q: 0.391620\n",
      " 2725/5000: episode: 2724, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.444, 1.000], loss: 0.076700, mean_absolute_error: 41.874638, mean_q: 0.390659\n",
      " 2726/5000: episode: 2725, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.032, 1.000], loss: 0.076318, mean_absolute_error: 41.868473, mean_q: 0.389674\n",
      " 2727/5000: episode: 2726, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.954], loss: 0.075960, mean_absolute_error: 41.871948, mean_q: 0.388764\n",
      " 2728/5000: episode: 2727, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.130, 1.000], loss: 0.075584, mean_absolute_error: 41.876339, mean_q: 0.387800\n",
      " 2729/5000: episode: 2728, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.619 [0.041, 1.000], loss: 0.075194, mean_absolute_error: 41.876511, mean_q: 0.386796\n",
      " 2730/5000: episode: 2729, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.111, 1.000], loss: 0.074823, mean_absolute_error: 41.873993, mean_q: 0.385839\n",
      " 2731/5000: episode: 2730, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.414 [0.000, 0.978], loss: 0.074469, mean_absolute_error: 41.864948, mean_q: 0.384899\n",
      " 2732/5000: episode: 2731, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.046, 1.000], loss: 0.074083, mean_absolute_error: 41.874229, mean_q: 0.383919\n",
      " 2733/5000: episode: 2732, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.760], loss: 0.073766, mean_absolute_error: 41.863464, mean_q: 0.383086\n",
      " 2734/5000: episode: 2733, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.389, 1.000], loss: 0.073355, mean_absolute_error: 41.864693, mean_q: 0.382018\n",
      " 2735/5000: episode: 2734, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.703], loss: 0.073022, mean_absolute_error: 41.874256, mean_q: 0.381153\n",
      " 2736/5000: episode: 2735, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.858], loss: 0.072651, mean_absolute_error: 41.863853, mean_q: 0.380178\n",
      " 2737/5000: episode: 2736, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: 0.072308, mean_absolute_error: 41.870934, mean_q: 0.379280\n",
      " 2738/5000: episode: 2737, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.345, 1.000], loss: 0.071962, mean_absolute_error: 41.870991, mean_q: 0.378371\n",
      " 2739/5000: episode: 2738, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.009, 1.000], loss: 0.071604, mean_absolute_error: 41.869694, mean_q: 0.377425\n",
      " 2740/5000: episode: 2739, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.269 [0.000, 0.834], loss: 0.071265, mean_absolute_error: 41.868748, mean_q: 0.376526\n",
      " 2741/5000: episode: 2740, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.070956, mean_absolute_error: 41.868931, mean_q: 0.375700\n",
      " 2742/5000: episode: 2741, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 0.070574, mean_absolute_error: 41.856758, mean_q: 0.374671\n",
      " 2743/5000: episode: 2742, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.200, 1.000], loss: 0.070368, mean_absolute_error: 41.842926, mean_q: 0.373982\n",
      " 2744/5000: episode: 2743, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.756 [0.213, 1.000], loss: 0.069838, mean_absolute_error: 41.874550, mean_q: 0.372733\n",
      " 2745/5000: episode: 2744, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.730 [0.163, 1.000], loss: 0.069532, mean_absolute_error: 41.859310, mean_q: 0.371902\n",
      " 2746/5000: episode: 2745, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.002, 1.000], loss: 0.069154, mean_absolute_error: 41.873779, mean_q: 0.370896\n",
      " 2747/5000: episode: 2746, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.915], loss: 0.068800, mean_absolute_error: 41.866753, mean_q: 0.369940\n",
      " 2748/5000: episode: 2747, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.016, 1.000], loss: 0.068460, mean_absolute_error: 41.869614, mean_q: 0.369023\n",
      " 2749/5000: episode: 2748, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.000, 0.776], loss: 0.068105, mean_absolute_error: 41.865913, mean_q: 0.368060\n",
      " 2750/5000: episode: 2749, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.371, 1.000], loss: 0.067795, mean_absolute_error: 41.864738, mean_q: 0.367220\n",
      " 2751/5000: episode: 2750, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.000, 0.827], loss: 0.067439, mean_absolute_error: 41.870415, mean_q: 0.366253\n",
      " 2752/5000: episode: 2751, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 0.067120, mean_absolute_error: 41.865547, mean_q: 0.365383\n",
      " 2753/5000: episode: 2752, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.686], loss: 0.066797, mean_absolute_error: 41.862991, mean_q: 0.364498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2754/5000: episode: 2753, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 0.066452, mean_absolute_error: 41.868443, mean_q: 0.363555\n",
      " 2755/5000: episode: 2754, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.723], loss: 0.066119, mean_absolute_error: 41.864716, mean_q: 0.362636\n",
      " 2756/5000: episode: 2755, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.000, 0.940], loss: 0.065803, mean_absolute_error: 41.860790, mean_q: 0.361766\n",
      " 2757/5000: episode: 2756, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.336, 1.000], loss: 0.065462, mean_absolute_error: 41.861359, mean_q: 0.360825\n",
      " 2758/5000: episode: 2757, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.526 [0.003, 1.000], loss: 0.065127, mean_absolute_error: 41.856041, mean_q: 0.359894\n",
      " 2759/5000: episode: 2758, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.664 [0.078, 1.000], loss: 0.064803, mean_absolute_error: 41.862122, mean_q: 0.359004\n",
      " 2760/5000: episode: 2759, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.569 [0.014, 1.000], loss: 0.064486, mean_absolute_error: 41.860733, mean_q: 0.358116\n",
      " 2761/5000: episode: 2760, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.297, 1.000], loss: 0.064160, mean_absolute_error: 41.853119, mean_q: 0.357203\n",
      " 2762/5000: episode: 2761, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.314, 1.000], loss: 0.063821, mean_absolute_error: 41.868584, mean_q: 0.356268\n",
      " 2763/5000: episode: 2762, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.456, 1.000], loss: 0.063494, mean_absolute_error: 41.866302, mean_q: 0.355350\n",
      " 2764/5000: episode: 2763, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.115, 1.000], loss: 0.063173, mean_absolute_error: 41.861862, mean_q: 0.354445\n",
      " 2765/5000: episode: 2764, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.632], loss: 0.062848, mean_absolute_error: 41.861626, mean_q: 0.353530\n",
      " 2766/5000: episode: 2765, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.000, 1.000], loss: 0.062513, mean_absolute_error: 41.856106, mean_q: 0.352577\n",
      " 2767/5000: episode: 2766, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.168, 1.000], loss: 0.062230, mean_absolute_error: 41.854862, mean_q: 0.351780\n",
      " 2768/5000: episode: 2767, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.399, 1.000], loss: 0.061908, mean_absolute_error: 41.859695, mean_q: 0.350869\n",
      " 2769/5000: episode: 2768, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 0.061587, mean_absolute_error: 41.857887, mean_q: 0.349958\n",
      " 2770/5000: episode: 2769, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 0.061286, mean_absolute_error: 41.855995, mean_q: 0.349087\n",
      " 2771/5000: episode: 2770, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.340 [0.000, 0.926], loss: 0.060962, mean_absolute_error: 41.857033, mean_q: 0.348168\n",
      " 2772/5000: episode: 2771, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.646], loss: 0.060646, mean_absolute_error: 41.860809, mean_q: 0.347267\n",
      " 2773/5000: episode: 2772, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.871], loss: 0.060350, mean_absolute_error: 41.863537, mean_q: 0.346417\n",
      " 2774/5000: episode: 2773, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.345, 1.000], loss: 0.060048, mean_absolute_error: 41.854408, mean_q: 0.345535\n",
      " 2775/5000: episode: 2774, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 0.059734, mean_absolute_error: 41.861633, mean_q: 0.344639\n",
      " 2776/5000: episode: 2775, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.465, 1.000], loss: 0.059417, mean_absolute_error: 41.862572, mean_q: 0.343719\n",
      " 2777/5000: episode: 2776, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 0.059124, mean_absolute_error: 41.854836, mean_q: 0.342867\n",
      " 2778/5000: episode: 2777, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.000, 0.957], loss: 0.058825, mean_absolute_error: 41.858994, mean_q: 0.341998\n",
      " 2779/5000: episode: 2778, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.671], loss: 0.058515, mean_absolute_error: 41.847458, mean_q: 0.341083\n",
      " 2780/5000: episode: 2779, duration: 0.021s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.389, 1.000], loss: 0.058202, mean_absolute_error: 41.860252, mean_q: 0.340176\n",
      " 2781/5000: episode: 2780, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 0.057939, mean_absolute_error: 41.851295, mean_q: 0.339400\n",
      " 2782/5000: episode: 2781, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.000, 0.945], loss: 0.057602, mean_absolute_error: 41.858154, mean_q: 0.338413\n",
      " 2783/5000: episode: 2782, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.002, 1.000], loss: 0.057312, mean_absolute_error: 41.858299, mean_q: 0.337558\n",
      " 2784/5000: episode: 2783, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 0.057025, mean_absolute_error: 41.854462, mean_q: 0.336707\n",
      " 2785/5000: episode: 2784, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.906], loss: 0.056723, mean_absolute_error: 41.851212, mean_q: 0.335802\n",
      " 2786/5000: episode: 2785, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.804], loss: 0.056447, mean_absolute_error: 41.854889, mean_q: 0.334993\n",
      " 2787/5000: episode: 2786, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.769], loss: 0.056141, mean_absolute_error: 41.855186, mean_q: 0.334079\n",
      " 2788/5000: episode: 2787, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.580], loss: 0.055865, mean_absolute_error: 41.854614, mean_q: 0.333255\n",
      " 2789/5000: episode: 2788, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.371, 1.000], loss: 0.055584, mean_absolute_error: 41.851299, mean_q: 0.332413\n",
      " 2790/5000: episode: 2789, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.424 [0.000, 0.982], loss: 0.055281, mean_absolute_error: 41.856560, mean_q: 0.331504\n",
      " 2791/5000: episode: 2790, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.175, 1.000], loss: 0.054982, mean_absolute_error: 41.856815, mean_q: 0.330604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2792/5000: episode: 2791, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.226 [0.000, 0.744], loss: 0.054710, mean_absolute_error: 41.853012, mean_q: 0.329780\n",
      " 2793/5000: episode: 2792, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.584 [0.021, 1.000], loss: 0.054419, mean_absolute_error: 41.854950, mean_q: 0.328904\n",
      " 2794/5000: episode: 2793, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.395, 1.000], loss: 0.054137, mean_absolute_error: 41.853741, mean_q: 0.328045\n",
      " 2795/5000: episode: 2794, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.788], loss: 0.053865, mean_absolute_error: 41.847816, mean_q: 0.327211\n",
      " 2796/5000: episode: 2795, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.792], loss: 0.053565, mean_absolute_error: 41.848923, mean_q: 0.326302\n",
      " 2797/5000: episode: 2796, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.907], loss: 0.053289, mean_absolute_error: 41.851006, mean_q: 0.325456\n",
      " 2798/5000: episode: 2797, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.000, 0.965], loss: 0.053032, mean_absolute_error: 41.851646, mean_q: 0.324670\n",
      " 2799/5000: episode: 2798, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.229, 1.000], loss: 0.052746, mean_absolute_error: 41.846302, mean_q: 0.323778\n",
      " 2800/5000: episode: 2799, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.483, 1.000], loss: 0.052460, mean_absolute_error: 41.850410, mean_q: 0.322902\n",
      " 2801/5000: episode: 2800, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.000, 0.957], loss: 0.052189, mean_absolute_error: 41.850685, mean_q: 0.322067\n",
      " 2802/5000: episode: 2801, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.089, 1.000], loss: 0.051919, mean_absolute_error: 41.845959, mean_q: 0.321231\n",
      " 2803/5000: episode: 2802, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.658 [0.072, 1.000], loss: 0.051632, mean_absolute_error: 41.854126, mean_q: 0.320344\n",
      " 2804/5000: episode: 2803, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.751], loss: 0.051378, mean_absolute_error: 41.843430, mean_q: 0.319544\n",
      " 2805/5000: episode: 2804, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.125, 1.000], loss: 0.051094, mean_absolute_error: 41.848194, mean_q: 0.318658\n",
      " 2806/5000: episode: 2805, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.209 [0.000, 0.692], loss: 0.050824, mean_absolute_error: 41.848263, mean_q: 0.317815\n",
      " 2807/5000: episode: 2806, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.229, 1.000], loss: 0.050548, mean_absolute_error: 41.853806, mean_q: 0.316954\n",
      " 2808/5000: episode: 2807, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.045, 1.000], loss: 0.050311, mean_absolute_error: 41.847893, mean_q: 0.316202\n",
      " 2809/5000: episode: 2808, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.431 [0.000, 0.985], loss: 0.050019, mean_absolute_error: 41.850636, mean_q: 0.315284\n",
      " 2810/5000: episode: 2809, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 0.049719, mean_absolute_error: 41.846687, mean_q: 0.314335\n",
      " 2811/5000: episode: 2810, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.299, 1.000], loss: 0.049492, mean_absolute_error: 41.850864, mean_q: 0.313613\n",
      " 2812/5000: episode: 2811, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.068, 1.000], loss: 0.049226, mean_absolute_error: 41.846710, mean_q: 0.312767\n",
      " 2813/5000: episode: 2812, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.031, 1.000], loss: 0.048993, mean_absolute_error: 41.847252, mean_q: 0.312016\n",
      " 2814/5000: episode: 2813, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.477, 1.000], loss: 0.048745, mean_absolute_error: 41.847115, mean_q: 0.311227\n",
      " 2815/5000: episode: 2814, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.217, 1.000], loss: 0.048461, mean_absolute_error: 41.849045, mean_q: 0.310312\n",
      " 2816/5000: episode: 2815, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.000, 0.927], loss: 0.048193, mean_absolute_error: 41.845943, mean_q: 0.309455\n",
      " 2817/5000: episode: 2816, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.448, 1.000], loss: 0.047931, mean_absolute_error: 41.842491, mean_q: 0.308600\n",
      " 2818/5000: episode: 2817, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 0.047678, mean_absolute_error: 41.849586, mean_q: 0.307795\n",
      " 2819/5000: episode: 2818, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 0.047421, mean_absolute_error: 41.847702, mean_q: 0.306957\n",
      " 2820/5000: episode: 2819, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.660], loss: 0.047156, mean_absolute_error: 41.837845, mean_q: 0.306090\n",
      " 2821/5000: episode: 2820, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.160, 1.000], loss: 0.046955, mean_absolute_error: 41.841560, mean_q: 0.305430\n",
      " 2822/5000: episode: 2821, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.607], loss: 0.046671, mean_absolute_error: 41.840164, mean_q: 0.304500\n",
      " 2823/5000: episode: 2822, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.938], loss: 0.046411, mean_absolute_error: 41.849964, mean_q: 0.303665\n",
      " 2824/5000: episode: 2823, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.001, 1.000], loss: 0.046165, mean_absolute_error: 41.846954, mean_q: 0.302855\n",
      " 2825/5000: episode: 2824, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.502, 1.000], loss: 0.045917, mean_absolute_error: 41.846313, mean_q: 0.302037\n",
      " 2826/5000: episode: 2825, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.760], loss: 0.045711, mean_absolute_error: 41.838531, mean_q: 0.301335\n",
      " 2827/5000: episode: 2826, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.045419, mean_absolute_error: 41.840019, mean_q: 0.300387\n",
      " 2828/5000: episode: 2827, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.533], loss: 0.045196, mean_absolute_error: 41.841431, mean_q: 0.299640\n",
      " 2829/5000: episode: 2828, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.724], loss: 0.044934, mean_absolute_error: 41.839279, mean_q: 0.298775\n",
      " 2830/5000: episode: 2829, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.723 [0.152, 1.000], loss: 0.044693, mean_absolute_error: 41.834904, mean_q: 0.297963\n",
      " 2831/5000: episode: 2830, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.780], loss: 0.044453, mean_absolute_error: 41.833408, mean_q: 0.297161\n",
      " 2832/5000: episode: 2831, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.374, 1.000], loss: 0.044214, mean_absolute_error: 41.837917, mean_q: 0.296361\n",
      " 2833/5000: episode: 2832, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.121, 1.000], loss: 0.043981, mean_absolute_error: 41.840515, mean_q: 0.295577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2834/5000: episode: 2833, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 0.043728, mean_absolute_error: 41.841019, mean_q: 0.294725\n",
      " 2835/5000: episode: 2834, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.825], loss: 0.043487, mean_absolute_error: 41.836258, mean_q: 0.293900\n",
      " 2836/5000: episode: 2835, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.069, 1.000], loss: 0.043254, mean_absolute_error: 41.846771, mean_q: 0.293121\n",
      " 2837/5000: episode: 2836, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.658], loss: 0.043032, mean_absolute_error: 41.839668, mean_q: 0.292361\n",
      " 2838/5000: episode: 2837, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.423, 1.000], loss: 0.042799, mean_absolute_error: 41.842854, mean_q: 0.291569\n",
      " 2839/5000: episode: 2838, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.124, 1.000], loss: 0.042581, mean_absolute_error: 41.837715, mean_q: 0.290817\n",
      " 2840/5000: episode: 2839, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 0.042317, mean_absolute_error: 41.832039, mean_q: 0.289905\n",
      " 2841/5000: episode: 2840, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 0.042090, mean_absolute_error: 41.836327, mean_q: 0.289127\n",
      " 2842/5000: episode: 2841, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.041866, mean_absolute_error: 41.838829, mean_q: 0.288358\n",
      " 2843/5000: episode: 2842, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.385, 1.000], loss: 0.041620, mean_absolute_error: 41.835266, mean_q: 0.287499\n",
      " 2844/5000: episode: 2843, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.000, 0.940], loss: 0.041413, mean_absolute_error: 41.835152, mean_q: 0.286784\n",
      " 2845/5000: episode: 2844, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.497], loss: 0.041186, mean_absolute_error: 41.834148, mean_q: 0.285996\n",
      " 2846/5000: episode: 2845, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.860], loss: 0.040949, mean_absolute_error: 41.837910, mean_q: 0.285170\n",
      " 2847/5000: episode: 2846, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.741 [0.182, 1.000], loss: 0.040735, mean_absolute_error: 41.833252, mean_q: 0.284418\n",
      " 2848/5000: episode: 2847, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.070, 1.000], loss: 0.040498, mean_absolute_error: 41.840454, mean_q: 0.283592\n",
      " 2849/5000: episode: 2848, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.014, 1.000], loss: 0.040289, mean_absolute_error: 41.842484, mean_q: 0.282861\n",
      " 2850/5000: episode: 2849, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.853], loss: 0.040077, mean_absolute_error: 41.834290, mean_q: 0.282106\n",
      " 2851/5000: episode: 2850, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.374, 1.000], loss: 0.039839, mean_absolute_error: 41.837753, mean_q: 0.281266\n",
      " 2852/5000: episode: 2851, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 0.039629, mean_absolute_error: 41.831348, mean_q: 0.280514\n",
      " 2853/5000: episode: 2852, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.000, 1.000], loss: 0.039393, mean_absolute_error: 41.839012, mean_q: 0.279684\n",
      " 2854/5000: episode: 2853, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 0.039182, mean_absolute_error: 41.827942, mean_q: 0.278924\n",
      " 2855/5000: episode: 2854, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.667], loss: 0.038956, mean_absolute_error: 41.837944, mean_q: 0.278123\n",
      " 2856/5000: episode: 2855, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.000, 0.847], loss: 0.038754, mean_absolute_error: 41.834076, mean_q: 0.277392\n",
      " 2857/5000: episode: 2856, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 0.038549, mean_absolute_error: 41.832710, mean_q: 0.276654\n",
      " 2858/5000: episode: 2857, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 0.038328, mean_absolute_error: 41.836723, mean_q: 0.275866\n",
      " 2859/5000: episode: 2858, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.456 [0.000, 0.993], loss: 0.038103, mean_absolute_error: 41.838463, mean_q: 0.275047\n",
      " 2860/5000: episode: 2859, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.000, 0.891], loss: 0.037888, mean_absolute_error: 41.837681, mean_q: 0.274271\n",
      " 2861/5000: episode: 2860, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.942], loss: 0.037680, mean_absolute_error: 41.834858, mean_q: 0.273513\n",
      " 2862/5000: episode: 2861, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.603], loss: 0.037477, mean_absolute_error: 41.824516, mean_q: 0.272757\n",
      " 2863/5000: episode: 2862, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.468, 1.000], loss: 0.037256, mean_absolute_error: 41.834770, mean_q: 0.271963\n",
      " 2864/5000: episode: 2863, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.220, 1.000], loss: 0.037044, mean_absolute_error: 41.835793, mean_q: 0.271185\n",
      " 2865/5000: episode: 2864, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.665], loss: 0.036836, mean_absolute_error: 41.837372, mean_q: 0.270422\n",
      " 2866/5000: episode: 2865, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.322, 1.000], loss: 0.036610, mean_absolute_error: 41.830948, mean_q: 0.269584\n",
      " 2867/5000: episode: 2866, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.393, 1.000], loss: 0.036438, mean_absolute_error: 41.833603, mean_q: 0.268948\n",
      " 2868/5000: episode: 2867, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.030, 1.000], loss: 0.036233, mean_absolute_error: 41.829205, mean_q: 0.268189\n",
      " 2869/5000: episode: 2868, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.321, 1.000], loss: 0.036027, mean_absolute_error: 41.827358, mean_q: 0.267420\n",
      " 2870/5000: episode: 2869, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.490, 1.000], loss: 0.035832, mean_absolute_error: 41.827034, mean_q: 0.266690\n",
      " 2871/5000: episode: 2870, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 0.035555, mean_absolute_error: 41.832863, mean_q: 0.265661\n",
      " 2872/5000: episode: 2871, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.569 [0.015, 1.000], loss: 0.035411, mean_absolute_error: 41.823299, mean_q: 0.265114\n",
      " 2873/5000: episode: 2872, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.004, 1.000], loss: 0.035218, mean_absolute_error: 41.836189, mean_q: 0.264395\n",
      " 2874/5000: episode: 2873, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.938], loss: 0.035013, mean_absolute_error: 41.832550, mean_q: 0.263620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2875/5000: episode: 2874, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.187, 1.000], loss: 0.034807, mean_absolute_error: 41.832581, mean_q: 0.262836\n",
      " 2876/5000: episode: 2875, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.506, 1.000], loss: 0.034628, mean_absolute_error: 41.820053, mean_q: 0.262149\n",
      " 2877/5000: episode: 2876, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.201, 1.000], loss: 0.034435, mean_absolute_error: 41.834450, mean_q: 0.261424\n",
      " 2878/5000: episode: 2877, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.000, 0.819], loss: 0.034239, mean_absolute_error: 41.830490, mean_q: 0.260679\n",
      " 2879/5000: episode: 2878, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.501, 1.000], loss: 0.034050, mean_absolute_error: 41.830635, mean_q: 0.259953\n",
      " 2880/5000: episode: 2879, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.382, 1.000], loss: 0.033841, mean_absolute_error: 41.832111, mean_q: 0.259155\n",
      " 2881/5000: episode: 2880, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.502], loss: 0.033662, mean_absolute_error: 41.823410, mean_q: 0.258460\n",
      " 2882/5000: episode: 2881, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.338, 1.000], loss: 0.033454, mean_absolute_error: 41.830856, mean_q: 0.257659\n",
      " 2883/5000: episode: 2882, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.762], loss: 0.033277, mean_absolute_error: 41.820061, mean_q: 0.256962\n",
      " 2884/5000: episode: 2883, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.391 [0.000, 0.966], loss: 0.033083, mean_absolute_error: 41.821766, mean_q: 0.256209\n",
      " 2885/5000: episode: 2884, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 0.032882, mean_absolute_error: 41.832687, mean_q: 0.255441\n",
      " 2886/5000: episode: 2885, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.000, 0.996], loss: 0.032713, mean_absolute_error: 41.826912, mean_q: 0.254779\n",
      " 2887/5000: episode: 2886, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.672 [0.085, 1.000], loss: 0.032506, mean_absolute_error: 41.834614, mean_q: 0.253975\n",
      " 2888/5000: episode: 2887, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.000, 0.915], loss: 0.032314, mean_absolute_error: 41.824955, mean_q: 0.253205\n",
      " 2889/5000: episode: 2888, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.111, 1.000], loss: 0.032144, mean_absolute_error: 41.823364, mean_q: 0.252540\n",
      " 2890/5000: episode: 2889, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.585], loss: 0.031950, mean_absolute_error: 41.825073, mean_q: 0.251768\n",
      " 2891/5000: episode: 2890, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.000, 0.999], loss: 0.031762, mean_absolute_error: 41.828442, mean_q: 0.251030\n",
      " 2892/5000: episode: 2891, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 0.031567, mean_absolute_error: 41.825562, mean_q: 0.250256\n",
      " 2893/5000: episode: 2892, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.537], loss: 0.031403, mean_absolute_error: 41.826675, mean_q: 0.249599\n",
      " 2894/5000: episode: 2893, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.130, 1.000], loss: 0.031222, mean_absolute_error: 41.824463, mean_q: 0.248880\n",
      " 2895/5000: episode: 2894, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.128, 1.000], loss: 0.031059, mean_absolute_error: 41.814613, mean_q: 0.248219\n",
      " 2896/5000: episode: 2895, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.519], loss: 0.030863, mean_absolute_error: 41.826679, mean_q: 0.247442\n",
      " 2897/5000: episode: 2896, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.000, 0.959], loss: 0.030692, mean_absolute_error: 41.818321, mean_q: 0.246743\n",
      " 2898/5000: episode: 2897, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.541], loss: 0.030507, mean_absolute_error: 41.825974, mean_q: 0.246004\n",
      " 2899/5000: episode: 2898, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.701], loss: 0.030334, mean_absolute_error: 41.827927, mean_q: 0.245306\n",
      " 2900/5000: episode: 2899, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.432, 1.000], loss: 0.030183, mean_absolute_error: 41.821793, mean_q: 0.244684\n",
      " 2901/5000: episode: 2900, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.192, 1.000], loss: 0.029994, mean_absolute_error: 41.814346, mean_q: 0.243909\n",
      " 2902/5000: episode: 2901, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.290, 1.000], loss: 0.029806, mean_absolute_error: 41.820724, mean_q: 0.243146\n",
      " 2903/5000: episode: 2902, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 0.029630, mean_absolute_error: 41.824409, mean_q: 0.242427\n",
      " 2904/5000: episode: 2903, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.000, 0.940], loss: 0.029471, mean_absolute_error: 41.821449, mean_q: 0.241763\n",
      " 2905/5000: episode: 2904, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.625], loss: 0.029282, mean_absolute_error: 41.820427, mean_q: 0.240991\n",
      " 2906/5000: episode: 2905, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.658 [0.072, 1.000], loss: 0.029134, mean_absolute_error: 41.820930, mean_q: 0.240380\n",
      " 2907/5000: episode: 2906, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.124, 1.000], loss: 0.028965, mean_absolute_error: 41.809681, mean_q: 0.239664\n",
      " 2908/5000: episode: 2907, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 0.028815, mean_absolute_error: 41.819542, mean_q: 0.239048\n",
      " 2909/5000: episode: 2908, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.363, 1.000], loss: 0.028608, mean_absolute_error: 41.817551, mean_q: 0.238181\n",
      " 2910/5000: episode: 2909, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.060, 1.000], loss: 0.028454, mean_absolute_error: 41.821297, mean_q: 0.237545\n",
      " 2911/5000: episode: 2910, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.391, 1.000], loss: 0.028345, mean_absolute_error: 41.812180, mean_q: 0.237067\n",
      " 2912/5000: episode: 2911, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.589], loss: 0.028116, mean_absolute_error: 41.821758, mean_q: 0.236124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2913/5000: episode: 2912, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.000, 0.970], loss: 0.027942, mean_absolute_error: 41.825493, mean_q: 0.235395\n",
      " 2914/5000: episode: 2913, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.490], loss: 0.027778, mean_absolute_error: 41.810314, mean_q: 0.234682\n",
      " 2915/5000: episode: 2914, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 0.027615, mean_absolute_error: 41.822998, mean_q: 0.234007\n",
      " 2916/5000: episode: 2915, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.081, 1.000], loss: 0.027445, mean_absolute_error: 41.818405, mean_q: 0.233275\n",
      " 2917/5000: episode: 2916, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.000, 0.888], loss: 0.027289, mean_absolute_error: 41.820091, mean_q: 0.232608\n",
      " 2918/5000: episode: 2917, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.503, 1.000], loss: 0.027138, mean_absolute_error: 41.822495, mean_q: 0.231967\n",
      " 2919/5000: episode: 2918, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.036, 1.000], loss: 0.026965, mean_absolute_error: 41.821377, mean_q: 0.231221\n",
      " 2920/5000: episode: 2919, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.434, 1.000], loss: 0.026814, mean_absolute_error: 41.821472, mean_q: 0.230572\n",
      " 2921/5000: episode: 2920, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.310, 1.000], loss: 0.026663, mean_absolute_error: 41.814308, mean_q: 0.229909\n",
      " 2922/5000: episode: 2921, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.596], loss: 0.026509, mean_absolute_error: 41.811211, mean_q: 0.229238\n",
      " 2923/5000: episode: 2922, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.133, 1.000], loss: 0.026345, mean_absolute_error: 41.816223, mean_q: 0.228531\n",
      " 2924/5000: episode: 2923, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.491, 1.000], loss: 0.026179, mean_absolute_error: 41.824360, mean_q: 0.227815\n",
      " 2925/5000: episode: 2924, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.942], loss: 0.026030, mean_absolute_error: 41.817188, mean_q: 0.227154\n",
      " 2926/5000: episode: 2925, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.294 [0.000, 0.873], loss: 0.025869, mean_absolute_error: 41.822533, mean_q: 0.226459\n",
      " 2927/5000: episode: 2926, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.190, 1.000], loss: 0.025703, mean_absolute_error: 41.818092, mean_q: 0.225720\n",
      " 2928/5000: episode: 2927, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.108, 1.000], loss: 0.025561, mean_absolute_error: 41.815720, mean_q: 0.225089\n",
      " 2929/5000: episode: 2928, duration: 0.022s, episode steps: 1, steps per second: 46, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.445 [0.000, 0.990], loss: 0.025420, mean_absolute_error: 41.818100, mean_q: 0.224468\n",
      " 2930/5000: episode: 2929, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.257, 1.000], loss: 0.025256, mean_absolute_error: 41.818581, mean_q: 0.223741\n",
      " 2931/5000: episode: 2930, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.393, 1.000], loss: 0.025123, mean_absolute_error: 41.809940, mean_q: 0.223140\n",
      " 2932/5000: episode: 2931, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.519], loss: 0.024957, mean_absolute_error: 41.813774, mean_q: 0.222403\n",
      " 2933/5000: episode: 2932, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 0.024815, mean_absolute_error: 41.811226, mean_q: 0.221766\n",
      " 2934/5000: episode: 2933, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.438 [0.000, 0.988], loss: 0.024668, mean_absolute_error: 41.809875, mean_q: 0.221105\n",
      " 2935/5000: episode: 2934, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.845], loss: 0.024509, mean_absolute_error: 41.820728, mean_q: 0.220394\n",
      " 2936/5000: episode: 2935, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.461, 1.000], loss: 0.024359, mean_absolute_error: 41.818157, mean_q: 0.219715\n",
      " 2937/5000: episode: 2936, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.000, 0.987], loss: 0.024215, mean_absolute_error: 41.817978, mean_q: 0.219062\n",
      " 2938/5000: episode: 2937, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 0.024078, mean_absolute_error: 41.811272, mean_q: 0.218433\n",
      " 2939/5000: episode: 2938, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.877], loss: 0.023924, mean_absolute_error: 41.812920, mean_q: 0.217733\n",
      " 2940/5000: episode: 2939, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.000, 0.822], loss: 0.023788, mean_absolute_error: 41.811939, mean_q: 0.217102\n",
      " 2941/5000: episode: 2940, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.563 [0.012, 1.000], loss: 0.023644, mean_absolute_error: 41.816341, mean_q: 0.216451\n",
      " 2942/5000: episode: 2941, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.528 [0.003, 1.000], loss: 0.023499, mean_absolute_error: 41.814949, mean_q: 0.215782\n",
      " 2943/5000: episode: 2942, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.360 [0.000, 0.944], loss: 0.023327, mean_absolute_error: 41.809017, mean_q: 0.214971\n",
      " 2944/5000: episode: 2943, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.023213, mean_absolute_error: 41.815674, mean_q: 0.214453\n",
      " 2945/5000: episode: 2944, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.604], loss: 0.023073, mean_absolute_error: 41.807915, mean_q: 0.213803\n",
      " 2946/5000: episode: 2945, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.521], loss: 0.022943, mean_absolute_error: 41.811264, mean_q: 0.213200\n",
      " 2947/5000: episode: 2946, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.741], loss: 0.022780, mean_absolute_error: 41.811314, mean_q: 0.212436\n",
      " 2948/5000: episode: 2947, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.004, 1.000], loss: 0.022659, mean_absolute_error: 41.806038, mean_q: 0.211864\n",
      " 2949/5000: episode: 2948, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.675], loss: 0.022511, mean_absolute_error: 41.815056, mean_q: 0.211179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2950/5000: episode: 2949, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.117, 1.000], loss: 0.022388, mean_absolute_error: 41.805298, mean_q: 0.210584\n",
      " 2951/5000: episode: 2950, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.000, 0.999], loss: 0.022242, mean_absolute_error: 41.810760, mean_q: 0.209894\n",
      " 2952/5000: episode: 2951, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.495, 1.000], loss: 0.022098, mean_absolute_error: 41.808784, mean_q: 0.209212\n",
      " 2953/5000: episode: 2952, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.000, 0.806], loss: 0.021970, mean_absolute_error: 41.809254, mean_q: 0.208605\n",
      " 2954/5000: episode: 2953, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 0.021824, mean_absolute_error: 41.812401, mean_q: 0.207913\n",
      " 2955/5000: episode: 2954, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.249, 1.000], loss: 0.021752, mean_absolute_error: 41.803970, mean_q: 0.207454\n",
      " 2956/5000: episode: 2955, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.411, 1.000], loss: 0.021573, mean_absolute_error: 41.809799, mean_q: 0.206709\n",
      " 2957/5000: episode: 2956, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.041, 1.000], loss: 0.021430, mean_absolute_error: 41.807915, mean_q: 0.206018\n",
      " 2958/5000: episode: 2957, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.000, 0.694], loss: 0.021309, mean_absolute_error: 41.811554, mean_q: 0.205436\n",
      " 2959/5000: episode: 2958, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 0.021163, mean_absolute_error: 41.811798, mean_q: 0.204723\n",
      " 2960/5000: episode: 2959, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.000, 0.970], loss: 0.021044, mean_absolute_error: 41.803528, mean_q: 0.204133\n",
      " 2961/5000: episode: 2960, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.145, 1.000], loss: 0.020895, mean_absolute_error: 41.814217, mean_q: 0.203421\n",
      " 2962/5000: episode: 2961, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.297, 1.000], loss: 0.020770, mean_absolute_error: 41.805244, mean_q: 0.202802\n",
      " 2963/5000: episode: 2962, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.468, 1.000], loss: 0.020661, mean_absolute_error: 41.802872, mean_q: 0.202264\n",
      " 2964/5000: episode: 2963, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.143, 1.000], loss: 0.020528, mean_absolute_error: 41.808716, mean_q: 0.201611\n",
      " 2965/5000: episode: 2964, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.020393, mean_absolute_error: 41.810432, mean_q: 0.200938\n",
      " 2966/5000: episode: 2965, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.786], loss: 0.020279, mean_absolute_error: 41.805180, mean_q: 0.200377\n",
      " 2967/5000: episode: 2966, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.081, 1.000], loss: 0.020143, mean_absolute_error: 41.806702, mean_q: 0.199702\n",
      " 2968/5000: episode: 2967, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.679 [0.093, 1.000], loss: 0.020009, mean_absolute_error: 41.810860, mean_q: 0.199039\n",
      " 2969/5000: episode: 2968, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.255, 1.000], loss: 0.019886, mean_absolute_error: 41.813755, mean_q: 0.198429\n",
      " 2970/5000: episode: 2969, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.456, 1.000], loss: 0.019761, mean_absolute_error: 41.812073, mean_q: 0.197795\n",
      " 2971/5000: episode: 2970, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.226, 1.000], loss: 0.019644, mean_absolute_error: 41.808453, mean_q: 0.197207\n",
      " 2972/5000: episode: 2971, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.007, 1.000], loss: 0.019522, mean_absolute_error: 41.808098, mean_q: 0.196584\n",
      " 2973/5000: episode: 2972, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.347, 1.000], loss: 0.019406, mean_absolute_error: 41.805458, mean_q: 0.195993\n",
      " 2974/5000: episode: 2973, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.513], loss: 0.019293, mean_absolute_error: 41.808083, mean_q: 0.195423\n",
      " 2975/5000: episode: 2974, duration: 0.021s, episode steps: 1, steps per second: 47, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.112, 1.000], loss: 0.019164, mean_absolute_error: 41.804245, mean_q: 0.194756\n",
      " 2976/5000: episode: 2975, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.480, 1.000], loss: 0.019032, mean_absolute_error: 41.810863, mean_q: 0.194091\n",
      " 2977/5000: episode: 2976, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.407 [0.000, 0.974], loss: 0.018907, mean_absolute_error: 41.814400, mean_q: 0.193459\n",
      " 2978/5000: episode: 2977, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.000, 0.751], loss: 0.018805, mean_absolute_error: 41.801208, mean_q: 0.192917\n",
      " 2979/5000: episode: 2978, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.796], loss: 0.018633, mean_absolute_error: 41.804111, mean_q: 0.192036\n",
      " 2980/5000: episode: 2979, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.000, 0.997], loss: 0.018565, mean_absolute_error: 41.806953, mean_q: 0.191685\n",
      " 2981/5000: episode: 2980, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.429, 1.000], loss: 0.018444, mean_absolute_error: 41.810402, mean_q: 0.191050\n",
      " 2982/5000: episode: 2981, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.575], loss: 0.018326, mean_absolute_error: 41.811172, mean_q: 0.190440\n",
      " 2983/5000: episode: 2982, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.000, 0.924], loss: 0.018221, mean_absolute_error: 41.796364, mean_q: 0.189877\n",
      " 2984/5000: episode: 2983, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.018118, mean_absolute_error: 41.803589, mean_q: 0.189336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2985/5000: episode: 2984, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.334, 1.000], loss: 0.017976, mean_absolute_error: 41.807907, mean_q: 0.188602\n",
      " 2986/5000: episode: 2985, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.151, 1.000], loss: 0.017877, mean_absolute_error: 41.803650, mean_q: 0.188069\n",
      " 2987/5000: episode: 2986, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.000, 0.999], loss: 0.017727, mean_absolute_error: 41.803993, mean_q: 0.187284\n",
      " 2988/5000: episode: 2987, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.177, 1.000], loss: 0.017645, mean_absolute_error: 41.801090, mean_q: 0.186836\n",
      " 2989/5000: episode: 2988, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.316, 1.000], loss: 0.017541, mean_absolute_error: 41.798771, mean_q: 0.186289\n",
      " 2990/5000: episode: 2989, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 0.017421, mean_absolute_error: 41.804070, mean_q: 0.185652\n",
      " 2991/5000: episode: 2990, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.017373, mean_absolute_error: 41.800602, mean_q: 0.185338\n",
      " 2992/5000: episode: 2991, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 0.017200, mean_absolute_error: 41.808990, mean_q: 0.184469\n",
      " 2993/5000: episode: 2992, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.145, 1.000], loss: 0.017085, mean_absolute_error: 41.805092, mean_q: 0.183847\n",
      " 2994/5000: episode: 2993, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.305, 1.000], loss: 0.016996, mean_absolute_error: 41.770401, mean_q: 0.182877\n",
      " 2995/5000: episode: 2994, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.671], loss: 0.016871, mean_absolute_error: 41.801521, mean_q: 0.182678\n",
      " 2996/5000: episode: 2995, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.667], loss: 0.016772, mean_absolute_error: 41.802887, mean_q: 0.182144\n",
      " 2997/5000: episode: 2996, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 0.016662, mean_absolute_error: 41.800964, mean_q: 0.181538\n",
      " 2998/5000: episode: 2997, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.000, 0.959], loss: 0.016555, mean_absolute_error: 41.798294, mean_q: 0.180949\n",
      " 2999/5000: episode: 2998, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 0.016447, mean_absolute_error: 41.800407, mean_q: 0.180346\n",
      " 3000/5000: episode: 2999, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.137, 1.000], loss: 0.016355, mean_absolute_error: 41.793476, mean_q: 0.179831\n",
      " 3001/5000: episode: 3000, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.784], loss: 0.016245, mean_absolute_error: 41.793678, mean_q: 0.179226\n",
      " 3002/5000: episode: 3001, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.616], loss: 0.016134, mean_absolute_error: 41.800613, mean_q: 0.178619\n",
      " 3003/5000: episode: 3002, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.007, 1.000], loss: 0.016037, mean_absolute_error: 41.796967, mean_q: 0.178075\n",
      " 3004/5000: episode: 3003, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.345, 1.000], loss: 0.015911, mean_absolute_error: 41.801231, mean_q: 0.177374\n",
      " 3005/5000: episode: 3004, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.104, 1.000], loss: 0.015833, mean_absolute_error: 41.794083, mean_q: 0.176934\n",
      " 3006/5000: episode: 3005, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.732], loss: 0.015704, mean_absolute_error: 41.787628, mean_q: 0.176169\n",
      " 3007/5000: episode: 3006, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.000, 0.997], loss: 0.015614, mean_absolute_error: 41.797356, mean_q: 0.175693\n",
      " 3008/5000: episode: 3007, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.069, 1.000], loss: 0.015519, mean_absolute_error: 41.799477, mean_q: 0.175163\n",
      " 3009/5000: episode: 3008, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 0.870], loss: 0.015435, mean_absolute_error: 41.794712, mean_q: 0.174684\n",
      " 3010/5000: episode: 3009, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.497], loss: 0.015322, mean_absolute_error: 41.796787, mean_q: 0.174042\n",
      " 3011/5000: episode: 3010, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.181, 1.000], loss: 0.015213, mean_absolute_error: 41.792503, mean_q: 0.173406\n",
      " 3012/5000: episode: 3011, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.504, 1.000], loss: 0.015113, mean_absolute_error: 41.800789, mean_q: 0.172847\n",
      " 3013/5000: episode: 3012, duration: 0.021s, episode steps: 1, steps per second: 47, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.000, 0.982], loss: 0.015008, mean_absolute_error: 41.806900, mean_q: 0.172251\n",
      " 3014/5000: episode: 3013, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.014914, mean_absolute_error: 41.804131, mean_q: 0.171706\n",
      " 3015/5000: episode: 3014, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.770], loss: 0.014817, mean_absolute_error: 41.803864, mean_q: 0.171145\n",
      " 3016/5000: episode: 3015, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.374, 1.000], loss: 0.014739, mean_absolute_error: 41.785656, mean_q: 0.170653\n",
      " 3017/5000: episode: 3016, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.575 [0.017, 1.000], loss: 0.014639, mean_absolute_error: 41.794079, mean_q: 0.170085\n",
      " 3018/5000: episode: 3017, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.000, 0.930], loss: 0.014529, mean_absolute_error: 41.794884, mean_q: 0.169443\n",
      " 3019/5000: episode: 3018, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.468, 1.000], loss: 0.014453, mean_absolute_error: 41.781837, mean_q: 0.168984\n",
      " 3020/5000: episode: 3019, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.209 [0.000, 0.690], loss: 0.014341, mean_absolute_error: 41.798775, mean_q: 0.168351\n",
      " 3021/5000: episode: 3020, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.772 [0.250, 1.000], loss: 0.014265, mean_absolute_error: 41.790894, mean_q: 0.167878\n",
      " 3022/5000: episode: 3021, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.489], loss: 0.014165, mean_absolute_error: 41.798119, mean_q: 0.167310\n",
      " 3023/5000: episode: 3022, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.000, 0.942], loss: 0.014062, mean_absolute_error: 41.797150, mean_q: 0.166690\n",
      " 3024/5000: episode: 3023, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.000, 0.972], loss: 0.013968, mean_absolute_error: 41.800228, mean_q: 0.166137\n",
      " 3025/5000: episode: 3024, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.340 [0.000, 0.926], loss: 0.013897, mean_absolute_error: 41.784554, mean_q: 0.165666\n",
      " 3026/5000: episode: 3025, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.838], loss: 0.013792, mean_absolute_error: 41.796013, mean_q: 0.165078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3027/5000: episode: 3026, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.396, 1.000], loss: 0.013701, mean_absolute_error: 41.793808, mean_q: 0.164520\n",
      " 3028/5000: episode: 3027, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.015, 1.000], loss: 0.013611, mean_absolute_error: 41.794098, mean_q: 0.163979\n",
      " 3029/5000: episode: 3028, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.446 [0.000, 0.991], loss: 0.013511, mean_absolute_error: 41.792267, mean_q: 0.163364\n",
      " 3030/5000: episode: 3029, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.635], loss: 0.013443, mean_absolute_error: 41.793739, mean_q: 0.162945\n",
      " 3031/5000: episode: 3030, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.347, 1.000], loss: 0.013341, mean_absolute_error: 41.795219, mean_q: 0.162339\n",
      " 3032/5000: episode: 3031, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 0.013249, mean_absolute_error: 41.798702, mean_q: 0.161774\n",
      " 3033/5000: episode: 3032, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.679 [0.094, 1.000], loss: 0.013171, mean_absolute_error: 41.791401, mean_q: 0.161282\n",
      " 3034/5000: episode: 3033, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 0.013072, mean_absolute_error: 41.796638, mean_q: 0.160677\n",
      " 3035/5000: episode: 3034, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 0.012986, mean_absolute_error: 41.797546, mean_q: 0.160149\n",
      " 3036/5000: episode: 3035, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.512, 1.000], loss: 0.012912, mean_absolute_error: 41.790276, mean_q: 0.159661\n",
      " 3037/5000: episode: 3036, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.384, 1.000], loss: 0.012821, mean_absolute_error: 41.786804, mean_q: 0.159102\n",
      " 3038/5000: episode: 3037, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.508 [0.001, 1.000], loss: 0.012729, mean_absolute_error: 41.787533, mean_q: 0.158531\n",
      " 3039/5000: episode: 3038, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.613], loss: 0.012632, mean_absolute_error: 41.778275, mean_q: 0.157882\n",
      " 3040/5000: episode: 3039, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.147, 1.000], loss: 0.012554, mean_absolute_error: 41.791649, mean_q: 0.157441\n",
      " 3041/5000: episode: 3040, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.715], loss: 0.012471, mean_absolute_error: 41.792061, mean_q: 0.156919\n",
      " 3042/5000: episode: 3041, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.118, 1.000], loss: 0.012408, mean_absolute_error: 41.792068, mean_q: 0.156503\n",
      " 3043/5000: episode: 3042, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.000, 0.987], loss: 0.012308, mean_absolute_error: 41.782051, mean_q: 0.155855\n",
      " 3044/5000: episode: 3043, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.603], loss: 0.012225, mean_absolute_error: 41.789227, mean_q: 0.155321\n",
      " 3045/5000: episode: 3044, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.647 [0.063, 1.000], loss: 0.012156, mean_absolute_error: 41.790592, mean_q: 0.154909\n",
      " 3046/5000: episode: 3045, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.257, 1.000], loss: 0.012063, mean_absolute_error: 41.792336, mean_q: 0.154307\n",
      " 3047/5000: episode: 3046, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.350, 1.000], loss: 0.011977, mean_absolute_error: 41.789646, mean_q: 0.153752\n",
      " 3048/5000: episode: 3047, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.730 [0.163, 1.000], loss: 0.011896, mean_absolute_error: 41.796967, mean_q: 0.153242\n",
      " 3049/5000: episode: 3048, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.228, 1.000], loss: 0.011825, mean_absolute_error: 41.788071, mean_q: 0.152763\n",
      " 3050/5000: episode: 3049, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.943], loss: 0.011741, mean_absolute_error: 41.790867, mean_q: 0.152221\n",
      " 3051/5000: episode: 3050, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.100, 1.000], loss: 0.011668, mean_absolute_error: 41.795021, mean_q: 0.151753\n",
      " 3052/5000: episode: 3051, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.301, 1.000], loss: 0.011580, mean_absolute_error: 41.791744, mean_q: 0.151169\n",
      " 3053/5000: episode: 3052, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.000, 0.984], loss: 0.011500, mean_absolute_error: 41.789635, mean_q: 0.150638\n",
      " 3054/5000: episode: 3053, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.387, 1.000], loss: 0.011421, mean_absolute_error: 41.793476, mean_q: 0.150127\n",
      " 3055/5000: episode: 3054, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.953], loss: 0.011362, mean_absolute_error: 41.780899, mean_q: 0.149724\n",
      " 3056/5000: episode: 3055, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.000, 0.927], loss: 0.011261, mean_absolute_error: 41.799156, mean_q: 0.149072\n",
      " 3057/5000: episode: 3056, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.319 [0.000, 0.904], loss: 0.011206, mean_absolute_error: 41.782589, mean_q: 0.148682\n",
      " 3058/5000: episode: 3057, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.282, 1.000], loss: 0.011114, mean_absolute_error: 41.792988, mean_q: 0.148081\n",
      " 3059/5000: episode: 3058, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.456 [0.000, 0.993], loss: 0.011045, mean_absolute_error: 41.789841, mean_q: 0.147607\n",
      " 3060/5000: episode: 3059, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.579 [0.019, 1.000], loss: 0.010968, mean_absolute_error: 41.793129, mean_q: 0.147099\n",
      " 3061/5000: episode: 3060, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.076, 1.000], loss: 0.010888, mean_absolute_error: 41.795197, mean_q: 0.146564\n",
      " 3062/5000: episode: 3061, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.002, 1.000], loss: 0.010824, mean_absolute_error: 41.789360, mean_q: 0.146116\n",
      " 3063/5000: episode: 3062, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.175, 1.000], loss: 0.010870, mean_absolute_error: 41.776085, mean_q: 0.146235\n",
      " 3064/5000: episode: 3063, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.000, 1.000], loss: 0.010684, mean_absolute_error: 41.783249, mean_q: 0.145153\n",
      " 3065/5000: episode: 3064, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.607 [0.033, 1.000], loss: 0.010611, mean_absolute_error: 41.786221, mean_q: 0.144659\n",
      " 3066/5000: episode: 3065, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.000, 0.999], loss: 0.010522, mean_absolute_error: 41.788849, mean_q: 0.144042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3067/5000: episode: 3066, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.260, 1.000], loss: 0.010511, mean_absolute_error: 41.770401, mean_q: 0.143509\n",
      " 3068/5000: episode: 3067, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.289, 1.000], loss: 0.010376, mean_absolute_error: 41.790672, mean_q: 0.143031\n",
      " 3069/5000: episode: 3068, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.000, 0.925], loss: 0.010309, mean_absolute_error: 41.789154, mean_q: 0.142570\n",
      " 3070/5000: episode: 3069, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 0.010234, mean_absolute_error: 41.792358, mean_q: 0.142057\n",
      " 3071/5000: episode: 3070, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.297, 1.000], loss: 0.010160, mean_absolute_error: 41.791405, mean_q: 0.141540\n",
      " 3072/5000: episode: 3071, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.556], loss: 0.010102, mean_absolute_error: 41.788841, mean_q: 0.141124\n",
      " 3073/5000: episode: 3072, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.429 [0.000, 0.985], loss: 0.010025, mean_absolute_error: 41.786919, mean_q: 0.140577\n",
      " 3074/5000: episode: 3073, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.277, 1.000], loss: 0.009956, mean_absolute_error: 41.790508, mean_q: 0.140101\n",
      " 3075/5000: episode: 3074, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.000, 0.891], loss: 0.009888, mean_absolute_error: 41.790634, mean_q: 0.139618\n",
      " 3076/5000: episode: 3075, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.016, 1.000], loss: 0.009819, mean_absolute_error: 41.789482, mean_q: 0.139126\n",
      " 3077/5000: episode: 3076, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.000, 0.778], loss: 0.009758, mean_absolute_error: 41.786865, mean_q: 0.138689\n",
      " 3078/5000: episode: 3077, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.035, 1.000], loss: 0.009682, mean_absolute_error: 41.787056, mean_q: 0.138129\n",
      " 3079/5000: episode: 3078, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 0.009618, mean_absolute_error: 41.785027, mean_q: 0.137685\n",
      " 3080/5000: episode: 3079, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.265, 1.000], loss: 0.009551, mean_absolute_error: 41.785583, mean_q: 0.137194\n",
      " 3081/5000: episode: 3080, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.281 [0.000, 0.855], loss: 0.009482, mean_absolute_error: 41.783981, mean_q: 0.136684\n",
      " 3082/5000: episode: 3081, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.519 [0.002, 1.000], loss: 0.009432, mean_absolute_error: 41.780724, mean_q: 0.136285\n",
      " 3083/5000: episode: 3082, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.009348, mean_absolute_error: 41.788063, mean_q: 0.135725\n",
      " 3084/5000: episode: 3083, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.492 [0.000, 0.999], loss: 0.009276, mean_absolute_error: 41.794617, mean_q: 0.135203\n",
      " 3085/5000: episode: 3084, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.000, 0.963], loss: 0.009221, mean_absolute_error: 41.786240, mean_q: 0.134793\n",
      " 3086/5000: episode: 3085, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 0.009162, mean_absolute_error: 41.785191, mean_q: 0.134344\n",
      " 3087/5000: episode: 3086, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.000, 0.883], loss: 0.009090, mean_absolute_error: 41.781509, mean_q: 0.133802\n",
      " 3088/5000: episode: 3087, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.944], loss: 0.009026, mean_absolute_error: 41.785400, mean_q: 0.133326\n",
      " 3089/5000: episode: 3088, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.456, 1.000], loss: 0.008971, mean_absolute_error: 41.783947, mean_q: 0.132894\n",
      " 3090/5000: episode: 3089, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 0.008903, mean_absolute_error: 41.784340, mean_q: 0.132419\n",
      " 3091/5000: episode: 3090, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.254, 1.000], loss: 0.008841, mean_absolute_error: 41.779842, mean_q: 0.131931\n",
      " 3092/5000: episode: 3091, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.487, 1.000], loss: 0.008772, mean_absolute_error: 41.787952, mean_q: 0.131442\n",
      " 3093/5000: episode: 3092, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.569], loss: 0.008729, mean_absolute_error: 41.774834, mean_q: 0.131057\n",
      " 3094/5000: episode: 3093, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.193, 1.000], loss: 0.008649, mean_absolute_error: 41.773319, mean_q: 0.130443\n",
      " 3095/5000: episode: 3094, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.763], loss: 0.008585, mean_absolute_error: 41.788986, mean_q: 0.130032\n",
      " 3096/5000: episode: 3095, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 0.008538, mean_absolute_error: 41.776947, mean_q: 0.129633\n",
      " 3097/5000: episode: 3096, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.008464, mean_absolute_error: 41.788048, mean_q: 0.129101\n",
      " 3098/5000: episode: 3097, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.492, 1.000], loss: 0.008409, mean_absolute_error: 41.779518, mean_q: 0.128656\n",
      " 3099/5000: episode: 3098, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.620], loss: 0.008344, mean_absolute_error: 41.785812, mean_q: 0.128173\n",
      " 3100/5000: episode: 3099, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.555], loss: 0.008295, mean_absolute_error: 41.781654, mean_q: 0.127783\n",
      " 3101/5000: episode: 3100, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.572 [0.016, 1.000], loss: 0.008240, mean_absolute_error: 41.776787, mean_q: 0.127333\n",
      " 3102/5000: episode: 3101, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.818], loss: 0.008176, mean_absolute_error: 41.785057, mean_q: 0.126865\n",
      " 3103/5000: episode: 3102, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.546], loss: 0.008110, mean_absolute_error: 41.786263, mean_q: 0.126346\n",
      " 3104/5000: episode: 3103, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.241, 1.000], loss: 0.008165, mean_absolute_error: 41.768124, mean_q: 0.126478\n",
      " 3105/5000: episode: 3104, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.912], loss: 0.008003, mean_absolute_error: 41.783684, mean_q: 0.125507\n",
      " 3106/5000: episode: 3105, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.746], loss: 0.007939, mean_absolute_error: 41.776909, mean_q: 0.124980\n",
      " 3107/5000: episode: 3106, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.846], loss: 0.007878, mean_absolute_error: 41.789310, mean_q: 0.124522\n",
      " 3108/5000: episode: 3107, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.858], loss: 0.007829, mean_absolute_error: 41.779030, mean_q: 0.124116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3109/5000: episode: 3108, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.582], loss: 0.007774, mean_absolute_error: 41.778526, mean_q: 0.123676\n",
      " 3110/5000: episode: 3109, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.880], loss: 0.007719, mean_absolute_error: 41.778511, mean_q: 0.123236\n",
      " 3111/5000: episode: 3110, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.593], loss: 0.007659, mean_absolute_error: 41.773834, mean_q: 0.122742\n",
      " 3112/5000: episode: 3111, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.000, 0.993], loss: 0.007610, mean_absolute_error: 41.772354, mean_q: 0.122312\n",
      " 3113/5000: episode: 3112, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.466 [0.000, 0.996], loss: 0.007549, mean_absolute_error: 41.783035, mean_q: 0.121863\n",
      " 3114/5000: episode: 3113, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.742], loss: 0.007493, mean_absolute_error: 41.784855, mean_q: 0.121408\n",
      " 3115/5000: episode: 3114, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 0.007440, mean_absolute_error: 41.780373, mean_q: 0.120966\n",
      " 3116/5000: episode: 3115, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.276, 1.000], loss: 0.007391, mean_absolute_error: 41.779022, mean_q: 0.120569\n",
      " 3117/5000: episode: 3116, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.333, 1.000], loss: 0.007326, mean_absolute_error: 41.787666, mean_q: 0.120040\n",
      " 3118/5000: episode: 3117, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 0.007104, mean_absolute_error: 41.740685, mean_q: 0.118013\n",
      " 3119/5000: episode: 3118, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 0.007232, mean_absolute_error: 41.778172, mean_q: 0.119245\n",
      " 3120/5000: episode: 3119, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.014, 1.000], loss: 0.007173, mean_absolute_error: 41.784721, mean_q: 0.118772\n",
      " 3121/5000: episode: 3120, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.387, 1.000], loss: 0.007130, mean_absolute_error: 41.777958, mean_q: 0.118379\n",
      " 3122/5000: episode: 3121, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.354, 1.000], loss: 0.007070, mean_absolute_error: 41.782921, mean_q: 0.117900\n",
      " 3123/5000: episode: 3122, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.563 [0.013, 1.000], loss: 0.007023, mean_absolute_error: 41.778919, mean_q: 0.117500\n",
      " 3124/5000: episode: 3123, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 0.006966, mean_absolute_error: 41.783798, mean_q: 0.117023\n",
      " 3125/5000: episode: 3124, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 0.006919, mean_absolute_error: 41.777641, mean_q: 0.116615\n",
      " 3126/5000: episode: 3125, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.756], loss: 0.006871, mean_absolute_error: 41.780167, mean_q: 0.116215\n",
      " 3127/5000: episode: 3126, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.000, 0.893], loss: 0.006816, mean_absolute_error: 41.782928, mean_q: 0.115744\n",
      " 3128/5000: episode: 3127, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.723 [0.152, 1.000], loss: 0.006777, mean_absolute_error: 41.777962, mean_q: 0.115403\n",
      " 3129/5000: episode: 3128, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.185, 1.000], loss: 0.006715, mean_absolute_error: 41.779915, mean_q: 0.114869\n",
      " 3130/5000: episode: 3129, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.297, 1.000], loss: 0.006666, mean_absolute_error: 41.781631, mean_q: 0.114451\n",
      " 3131/5000: episode: 3130, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.099, 1.000], loss: 0.006623, mean_absolute_error: 41.776253, mean_q: 0.114072\n",
      " 3132/5000: episode: 3131, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.064, 1.000], loss: 0.006578, mean_absolute_error: 41.772579, mean_q: 0.113675\n",
      " 3133/5000: episode: 3132, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.000, 0.789], loss: 0.006519, mean_absolute_error: 41.785576, mean_q: 0.113176\n",
      " 3134/5000: episode: 3133, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.539], loss: 0.006473, mean_absolute_error: 41.775917, mean_q: 0.112734\n",
      " 3135/5000: episode: 3134, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.535 [0.004, 1.000], loss: 0.006432, mean_absolute_error: 41.772636, mean_q: 0.112369\n",
      " 3136/5000: episode: 3135, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.065, 1.000], loss: 0.006377, mean_absolute_error: 41.774437, mean_q: 0.111893\n",
      " 3137/5000: episode: 3136, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 0.006339, mean_absolute_error: 41.777016, mean_q: 0.111566\n",
      " 3138/5000: episode: 3137, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.623], loss: 0.006282, mean_absolute_error: 41.782967, mean_q: 0.111074\n",
      " 3139/5000: episode: 3138, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.000, 0.968], loss: 0.006235, mean_absolute_error: 41.783897, mean_q: 0.110658\n",
      " 3140/5000: episode: 3139, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.571], loss: 0.006193, mean_absolute_error: 41.779636, mean_q: 0.110280\n",
      " 3141/5000: episode: 3140, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.407 [0.000, 0.975], loss: 0.006145, mean_absolute_error: 41.780514, mean_q: 0.109855\n",
      " 3142/5000: episode: 3141, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.144, 1.000], loss: 0.006105, mean_absolute_error: 41.776077, mean_q: 0.109479\n",
      " 3143/5000: episode: 3142, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.540], loss: 0.006057, mean_absolute_error: 41.779125, mean_q: 0.109057\n",
      " 3144/5000: episode: 3143, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 0.006011, mean_absolute_error: 41.779297, mean_q: 0.108633\n",
      " 3145/5000: episode: 3144, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.621 [0.042, 1.000], loss: 0.005960, mean_absolute_error: 41.768982, mean_q: 0.107954\n",
      " 3146/5000: episode: 3145, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.000, 0.891], loss: 0.005966, mean_absolute_error: 41.764786, mean_q: 0.108015\n",
      " 3147/5000: episode: 3146, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.422, 1.000], loss: 0.005885, mean_absolute_error: 41.773544, mean_q: 0.107470\n",
      " 3148/5000: episode: 3147, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.279, 1.000], loss: 0.005843, mean_absolute_error: 41.770805, mean_q: 0.107048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3149/5000: episode: 3148, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.574], loss: 0.005795, mean_absolute_error: 41.777260, mean_q: 0.106642\n",
      " 3150/5000: episode: 3149, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.626], loss: 0.005757, mean_absolute_error: 41.770584, mean_q: 0.106275\n",
      " 3151/5000: episode: 3150, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.299, 1.000], loss: 0.005719, mean_absolute_error: 41.772827, mean_q: 0.105930\n",
      " 3152/5000: episode: 3151, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 0.005669, mean_absolute_error: 41.774128, mean_q: 0.105456\n",
      " 3153/5000: episode: 3152, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.543], loss: 0.005625, mean_absolute_error: 41.774151, mean_q: 0.105036\n",
      " 3154/5000: episode: 3153, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 0.005582, mean_absolute_error: 41.774876, mean_q: 0.104638\n",
      " 3155/5000: episode: 3154, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.528], loss: 0.005540, mean_absolute_error: 41.770767, mean_q: 0.104225\n",
      " 3156/5000: episode: 3155, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.000, 0.930], loss: 0.005496, mean_absolute_error: 41.777699, mean_q: 0.103827\n",
      " 3157/5000: episode: 3156, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.124, 1.000], loss: 0.005460, mean_absolute_error: 41.769436, mean_q: 0.103449\n",
      " 3158/5000: episode: 3157, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 0.005427, mean_absolute_error: 41.774929, mean_q: 0.103146\n",
      " 3159/5000: episode: 3158, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.678 [0.092, 1.000], loss: 0.005392, mean_absolute_error: 41.769249, mean_q: 0.102738\n",
      " 3160/5000: episode: 3159, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.488], loss: 0.005333, mean_absolute_error: 41.774158, mean_q: 0.102256\n",
      " 3161/5000: episode: 3160, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.000, 0.945], loss: 0.005287, mean_absolute_error: 41.761791, mean_q: 0.101766\n",
      " 3162/5000: episode: 3161, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.695 [0.112, 1.000], loss: 0.005250, mean_absolute_error: 41.776791, mean_q: 0.101456\n",
      " 3163/5000: episode: 3162, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.568 [0.014, 1.000], loss: 0.005216, mean_absolute_error: 41.771397, mean_q: 0.101109\n",
      " 3164/5000: episode: 3163, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.313, 1.000], loss: 0.005169, mean_absolute_error: 41.772865, mean_q: 0.100653\n",
      " 3165/5000: episode: 3164, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.005151, mean_absolute_error: 41.766731, mean_q: 0.100424\n",
      " 3166/5000: episode: 3165, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.474, 1.000], loss: 0.005096, mean_absolute_error: 41.768356, mean_q: 0.099925\n",
      " 3167/5000: episode: 3166, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.191, 1.000], loss: 0.005060, mean_absolute_error: 41.770660, mean_q: 0.099540\n",
      " 3168/5000: episode: 3167, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.814], loss: 0.005018, mean_absolute_error: 41.772667, mean_q: 0.099152\n",
      " 3169/5000: episode: 3168, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.000, 0.776], loss: 0.004979, mean_absolute_error: 41.771378, mean_q: 0.098757\n",
      " 3170/5000: episode: 3169, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.000, 0.987], loss: 0.004938, mean_absolute_error: 41.774506, mean_q: 0.098366\n",
      " 3171/5000: episode: 3170, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.494, 1.000], loss: 0.004908, mean_absolute_error: 41.769012, mean_q: 0.098041\n",
      " 3172/5000: episode: 3171, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.000, 0.953], loss: 0.004880, mean_absolute_error: 41.767712, mean_q: 0.097703\n",
      " 3173/5000: episode: 3172, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.489, 1.000], loss: 0.004837, mean_absolute_error: 41.770187, mean_q: 0.097317\n",
      " 3174/5000: episode: 3173, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.004792, mean_absolute_error: 41.774117, mean_q: 0.096874\n",
      " 3175/5000: episode: 3174, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.288 [0.000, 0.865], loss: 0.004758, mean_absolute_error: 41.765877, mean_q: 0.096510\n",
      " 3176/5000: episode: 3175, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.666], loss: 0.004722, mean_absolute_error: 41.769341, mean_q: 0.096159\n",
      " 3177/5000: episode: 3176, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 0.004681, mean_absolute_error: 41.773689, mean_q: 0.095737\n",
      " 3178/5000: episode: 3177, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.710], loss: 0.004642, mean_absolute_error: 41.776657, mean_q: 0.095328\n",
      " 3179/5000: episode: 3178, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.262, 1.000], loss: 0.004613, mean_absolute_error: 41.769672, mean_q: 0.095028\n",
      " 3180/5000: episode: 3179, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.512], loss: 0.004585, mean_absolute_error: 41.759674, mean_q: 0.094670\n",
      " 3181/5000: episode: 3180, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.479, 1.000], loss: 0.004535, mean_absolute_error: 41.773952, mean_q: 0.094222\n",
      " 3182/5000: episode: 3181, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.537 [0.005, 1.000], loss: 0.004514, mean_absolute_error: 41.763657, mean_q: 0.093973\n",
      " 3183/5000: episode: 3182, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.704], loss: 0.004469, mean_absolute_error: 41.770279, mean_q: 0.093514\n",
      " 3184/5000: episode: 3183, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.197, 1.000], loss: 0.004435, mean_absolute_error: 41.772339, mean_q: 0.093155\n",
      " 3185/5000: episode: 3184, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.645], loss: 0.004402, mean_absolute_error: 41.773045, mean_q: 0.092814\n",
      " 3186/5000: episode: 3185, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 0.004371, mean_absolute_error: 41.768356, mean_q: 0.092474\n",
      " 3187/5000: episode: 3186, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.260, 1.000], loss: 0.004330, mean_absolute_error: 41.771477, mean_q: 0.092043\n",
      " 3188/5000: episode: 3187, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.128, 1.000], loss: 0.004296, mean_absolute_error: 41.767021, mean_q: 0.091659\n",
      " 3189/5000: episode: 3188, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.401 [0.000, 0.971], loss: 0.004265, mean_absolute_error: 41.769539, mean_q: 0.091305\n",
      " 3190/5000: episode: 3189, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.360 [0.000, 0.944], loss: 0.004229, mean_absolute_error: 41.770691, mean_q: 0.090951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3191/5000: episode: 3190, duration: 0.027s, episode steps: 1, steps per second: 38, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 0.004192, mean_absolute_error: 41.777004, mean_q: 0.090561\n",
      " 3192/5000: episode: 3191, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.377, 1.000], loss: 0.004169, mean_absolute_error: 41.767136, mean_q: 0.090273\n",
      " 3193/5000: episode: 3192, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.309, 1.000], loss: 0.004130, mean_absolute_error: 41.771339, mean_q: 0.089859\n",
      " 3194/5000: episode: 3193, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.469, 1.000], loss: 0.004101, mean_absolute_error: 41.772377, mean_q: 0.089551\n",
      " 3195/5000: episode: 3194, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.000, 0.913], loss: 0.004077, mean_absolute_error: 41.766834, mean_q: 0.089263\n",
      " 3196/5000: episode: 3195, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 0.004032, mean_absolute_error: 41.773857, mean_q: 0.088786\n",
      " 3197/5000: episode: 3196, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.267 [0.000, 0.831], loss: 0.004006, mean_absolute_error: 41.771130, mean_q: 0.088471\n",
      " 3198/5000: episode: 3197, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.120, 1.000], loss: 0.003978, mean_absolute_error: 41.768600, mean_q: 0.088157\n",
      " 3199/5000: episode: 3198, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.360, 1.000], loss: 0.003939, mean_absolute_error: 41.769112, mean_q: 0.087738\n",
      " 3200/5000: episode: 3199, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 0.003909, mean_absolute_error: 41.766857, mean_q: 0.087393\n",
      " 3201/5000: episode: 3200, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.003881, mean_absolute_error: 41.767075, mean_q: 0.087065\n",
      " 3202/5000: episode: 3201, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.030, 1.000], loss: 0.003854, mean_absolute_error: 41.765274, mean_q: 0.086753\n",
      " 3203/5000: episode: 3202, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.221, 1.000], loss: 0.003826, mean_absolute_error: 41.765526, mean_q: 0.086446\n",
      " 3204/5000: episode: 3203, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.368, 1.000], loss: 0.003785, mean_absolute_error: 41.775837, mean_q: 0.085997\n",
      " 3205/5000: episode: 3204, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.636], loss: 0.003767, mean_absolute_error: 41.762383, mean_q: 0.085727\n",
      " 3206/5000: episode: 3205, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.170, 1.000], loss: 0.003728, mean_absolute_error: 41.769703, mean_q: 0.085323\n",
      " 3207/5000: episode: 3206, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.000, 0.823], loss: 0.003697, mean_absolute_error: 41.766830, mean_q: 0.084951\n",
      " 3208/5000: episode: 3207, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.002, 1.000], loss: 0.003669, mean_absolute_error: 41.763054, mean_q: 0.084630\n",
      " 3209/5000: episode: 3208, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.584], loss: 0.003639, mean_absolute_error: 41.770412, mean_q: 0.084292\n",
      " 3210/5000: episode: 3209, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 0.003615, mean_absolute_error: 41.766556, mean_q: 0.084011\n",
      " 3211/5000: episode: 3210, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.540 [0.006, 1.000], loss: 0.003596, mean_absolute_error: 41.762047, mean_q: 0.083719\n",
      " 3212/5000: episode: 3211, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.003558, mean_absolute_error: 41.765312, mean_q: 0.083313\n",
      " 3213/5000: episode: 3212, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.328, 1.000], loss: 0.003543, mean_absolute_error: 41.756363, mean_q: 0.083055\n",
      " 3214/5000: episode: 3213, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.319 [0.000, 0.904], loss: 0.003500, mean_absolute_error: 41.766258, mean_q: 0.082636\n",
      " 3215/5000: episode: 3214, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.128, 1.000], loss: 0.003472, mean_absolute_error: 41.769958, mean_q: 0.082309\n",
      " 3216/5000: episode: 3215, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.642], loss: 0.003438, mean_absolute_error: 41.772484, mean_q: 0.081903\n",
      " 3217/5000: episode: 3216, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.588], loss: 0.003416, mean_absolute_error: 41.766392, mean_q: 0.081624\n",
      " 3218/5000: episode: 3217, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 0.003395, mean_absolute_error: 41.764446, mean_q: 0.081343\n",
      " 3219/5000: episode: 3218, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.488], loss: 0.003360, mean_absolute_error: 41.768761, mean_q: 0.080952\n",
      " 3220/5000: episode: 3219, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.193, 1.000], loss: 0.003335, mean_absolute_error: 41.762115, mean_q: 0.080609\n",
      " 3221/5000: episode: 3220, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.240, 1.000], loss: 0.003349, mean_absolute_error: 41.754665, mean_q: 0.080641\n",
      " 3222/5000: episode: 3221, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.201, 1.000], loss: 0.003285, mean_absolute_error: 41.763161, mean_q: 0.080009\n",
      " 3223/5000: episode: 3222, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.014, 1.000], loss: 0.003257, mean_absolute_error: 41.765892, mean_q: 0.079674\n",
      " 3224/5000: episode: 3223, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.001, 1.000], loss: 0.003229, mean_absolute_error: 41.766388, mean_q: 0.079342\n",
      " 3225/5000: episode: 3224, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.391 [0.000, 0.966], loss: 0.003208, mean_absolute_error: 41.763065, mean_q: 0.079067\n",
      " 3226/5000: episode: 3225, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.527], loss: 0.003179, mean_absolute_error: 41.763802, mean_q: 0.078689\n",
      " 3227/5000: episode: 3226, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.000, 0.856], loss: 0.003154, mean_absolute_error: 41.765160, mean_q: 0.078394\n",
      " 3228/5000: episode: 3227, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.068, 1.000], loss: 0.003124, mean_absolute_error: 41.766945, mean_q: 0.078028\n",
      " 3229/5000: episode: 3228, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.030, 1.000], loss: 0.003106, mean_absolute_error: 41.754120, mean_q: 0.077752\n",
      " 3230/5000: episode: 3229, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 0.003093, mean_absolute_error: 41.751434, mean_q: 0.077540\n",
      " 3231/5000: episode: 3230, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.032, 1.000], loss: 0.003044, mean_absolute_error: 41.769047, mean_q: 0.076997\n",
      " 3232/5000: episode: 3231, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.371 [0.000, 0.952], loss: 0.003030, mean_absolute_error: 41.759819, mean_q: 0.076803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3233/5000: episode: 3232, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.400, 1.000], loss: 0.002995, mean_absolute_error: 41.770828, mean_q: 0.076378\n",
      " 3234/5000: episode: 3233, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 0.002988, mean_absolute_error: 41.765034, mean_q: 0.076269\n",
      " 3235/5000: episode: 3234, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.118, 1.000], loss: 0.002955, mean_absolute_error: 41.759521, mean_q: 0.075828\n",
      " 3236/5000: episode: 3235, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.621], loss: 0.002933, mean_absolute_error: 41.765701, mean_q: 0.075542\n",
      " 3237/5000: episode: 3236, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.274, 1.000], loss: 0.002904, mean_absolute_error: 41.765137, mean_q: 0.075186\n",
      " 3238/5000: episode: 3237, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 0.002905, mean_absolute_error: 41.754704, mean_q: 0.075091\n",
      " 3239/5000: episode: 3238, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.000, 0.812], loss: 0.002857, mean_absolute_error: 41.765591, mean_q: 0.074565\n",
      " 3240/5000: episode: 3239, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.394, 1.000], loss: 0.002834, mean_absolute_error: 41.761223, mean_q: 0.074250\n",
      " 3241/5000: episode: 3240, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.708], loss: 0.002805, mean_absolute_error: 41.766655, mean_q: 0.073885\n",
      " 3242/5000: episode: 3241, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 0.002788, mean_absolute_error: 41.765095, mean_q: 0.073622\n",
      " 3243/5000: episode: 3242, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.687 [0.103, 1.000], loss: 0.002758, mean_absolute_error: 41.768517, mean_q: 0.073263\n",
      " 3244/5000: episode: 3243, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.735 [0.171, 1.000], loss: 0.002743, mean_absolute_error: 41.763317, mean_q: 0.073024\n",
      " 3245/5000: episode: 3244, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.748], loss: 0.002727, mean_absolute_error: 41.754105, mean_q: 0.072779\n",
      " 3246/5000: episode: 3245, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.507], loss: 0.002696, mean_absolute_error: 41.759754, mean_q: 0.072391\n",
      " 3247/5000: episode: 3246, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.473, 1.000], loss: 0.002675, mean_absolute_error: 41.764297, mean_q: 0.072121\n",
      " 3248/5000: episode: 3247, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.770], loss: 0.002649, mean_absolute_error: 41.766685, mean_q: 0.071771\n",
      " 3249/5000: episode: 3248, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.120, 1.000], loss: 0.002634, mean_absolute_error: 41.762749, mean_q: 0.071537\n",
      " 3250/5000: episode: 3249, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.000, 0.999], loss: 0.002612, mean_absolute_error: 41.760628, mean_q: 0.071245\n",
      " 3251/5000: episode: 3250, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 0.002589, mean_absolute_error: 41.760746, mean_q: 0.070909\n",
      " 3252/5000: episode: 3251, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.040, 1.000], loss: 0.002563, mean_absolute_error: 41.761997, mean_q: 0.070576\n",
      " 3253/5000: episode: 3252, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.385, 1.000], loss: 0.002536, mean_absolute_error: 41.760067, mean_q: 0.070189\n",
      " 3254/5000: episode: 3253, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.258, 1.000], loss: 0.002524, mean_absolute_error: 41.763538, mean_q: 0.070006\n",
      " 3255/5000: episode: 3254, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.309, 1.000], loss: 0.002502, mean_absolute_error: 41.763947, mean_q: 0.069699\n",
      " 3256/5000: episode: 3255, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.184, 1.000], loss: 0.002480, mean_absolute_error: 41.763905, mean_q: 0.069394\n",
      " 3257/5000: episode: 3256, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.666], loss: 0.002459, mean_absolute_error: 41.766029, mean_q: 0.069108\n",
      " 3258/5000: episode: 3257, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.436, 1.000], loss: 0.002442, mean_absolute_error: 41.756561, mean_q: 0.068817\n",
      " 3259/5000: episode: 3258, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.000, 0.987], loss: 0.002427, mean_absolute_error: 41.752373, mean_q: 0.068581\n",
      " 3260/5000: episode: 3259, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.329, 1.000], loss: 0.002393, mean_absolute_error: 41.767433, mean_q: 0.068170\n",
      " 3261/5000: episode: 3260, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 0.002379, mean_absolute_error: 41.763733, mean_q: 0.067949\n",
      " 3262/5000: episode: 3261, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.385, 1.000], loss: 0.002363, mean_absolute_error: 41.756462, mean_q: 0.067696\n",
      " 3263/5000: episode: 3262, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.515], loss: 0.002342, mean_absolute_error: 41.758598, mean_q: 0.067396\n",
      " 3264/5000: episode: 3263, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.509 [0.001, 1.000], loss: 0.002313, mean_absolute_error: 41.768929, mean_q: 0.067001\n",
      " 3265/5000: episode: 3264, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.677 [0.091, 1.000], loss: 0.002304, mean_absolute_error: 41.756126, mean_q: 0.066825\n",
      " 3266/5000: episode: 3265, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.143, 1.000], loss: 0.002300, mean_absolute_error: 41.749523, mean_q: 0.066706\n",
      " 3267/5000: episode: 3266, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 0.002268, mean_absolute_error: 41.755196, mean_q: 0.066249\n",
      " 3268/5000: episode: 3267, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.053, 1.000], loss: 0.002242, mean_absolute_error: 41.766224, mean_q: 0.065950\n",
      " 3269/5000: episode: 3268, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.310, 1.000], loss: 0.002226, mean_absolute_error: 41.760910, mean_q: 0.065679\n",
      " 3270/5000: episode: 3269, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.162, 1.000], loss: 0.002207, mean_absolute_error: 41.760002, mean_q: 0.065398\n",
      " 3271/5000: episode: 3270, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.000, 0.962], loss: 0.002183, mean_absolute_error: 41.765007, mean_q: 0.065055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3272/5000: episode: 3271, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.658], loss: 0.002168, mean_absolute_error: 41.763924, mean_q: 0.064827\n",
      " 3273/5000: episode: 3272, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.253, 1.000], loss: 0.002150, mean_absolute_error: 41.762524, mean_q: 0.064533\n",
      " 3274/5000: episode: 3273, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.000, 0.734], loss: 0.002141, mean_absolute_error: 41.751770, mean_q: 0.064369\n",
      " 3275/5000: episode: 3274, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.000, 1.000], loss: 0.002107, mean_absolute_error: 41.764793, mean_q: 0.063895\n",
      " 3276/5000: episode: 3275, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 0.002093, mean_absolute_error: 41.762337, mean_q: 0.063683\n",
      " 3277/5000: episode: 3276, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.008, 1.000], loss: 0.002077, mean_absolute_error: 41.765026, mean_q: 0.063422\n",
      " 3278/5000: episode: 3277, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.004, 1.000], loss: 0.002064, mean_absolute_error: 41.757954, mean_q: 0.063203\n",
      " 3279/5000: episode: 3278, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.492, 1.000], loss: 0.002045, mean_absolute_error: 41.759655, mean_q: 0.062904\n",
      " 3280/5000: episode: 3279, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.404 [0.000, 0.973], loss: 0.002024, mean_absolute_error: 41.754143, mean_q: 0.062583\n",
      " 3281/5000: episode: 3280, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.077, 1.000], loss: 0.002004, mean_absolute_error: 41.764431, mean_q: 0.062289\n",
      " 3282/5000: episode: 3281, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.374, 1.000], loss: 0.001993, mean_absolute_error: 41.755997, mean_q: 0.062090\n",
      " 3283/5000: episode: 3282, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.909], loss: 0.001973, mean_absolute_error: 41.757706, mean_q: 0.061777\n",
      " 3284/5000: episode: 3283, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.861], loss: 0.001954, mean_absolute_error: 41.766357, mean_q: 0.061496\n",
      " 3285/5000: episode: 3284, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.660 [0.074, 1.000], loss: 0.001940, mean_absolute_error: 41.760353, mean_q: 0.061255\n",
      " 3286/5000: episode: 3285, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.000, 0.884], loss: 0.001924, mean_absolute_error: 41.756611, mean_q: 0.060979\n",
      " 3287/5000: episode: 3286, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.215, 1.000], loss: 0.001914, mean_absolute_error: 41.755295, mean_q: 0.060736\n",
      " 3288/5000: episode: 3287, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.317, 1.000], loss: 0.001886, mean_absolute_error: 41.763027, mean_q: 0.060389\n",
      " 3289/5000: episode: 3288, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.410, 1.000], loss: 0.001871, mean_absolute_error: 41.764935, mean_q: 0.060156\n",
      " 3290/5000: episode: 3289, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.705], loss: 0.001856, mean_absolute_error: 41.754547, mean_q: 0.059855\n",
      " 3291/5000: episode: 3290, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.045, 1.000], loss: 0.001841, mean_absolute_error: 41.758095, mean_q: 0.059609\n",
      " 3292/5000: episode: 3291, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.204, 1.000], loss: 0.001840, mean_absolute_error: 41.752323, mean_q: 0.059550\n",
      " 3293/5000: episode: 3292, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.129, 1.000], loss: 0.001813, mean_absolute_error: 41.756855, mean_q: 0.059161\n",
      " 3294/5000: episode: 3293, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.211, 1.000], loss: 0.001796, mean_absolute_error: 41.754753, mean_q: 0.058887\n",
      " 3295/5000: episode: 3294, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.000, 0.924], loss: 0.001787, mean_absolute_error: 41.755558, mean_q: 0.058672\n",
      " 3296/5000: episode: 3295, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.420, 1.000], loss: 0.001763, mean_absolute_error: 41.757378, mean_q: 0.058354\n",
      " 3297/5000: episode: 3296, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.000, 0.849], loss: 0.001747, mean_absolute_error: 41.756760, mean_q: 0.058075\n",
      " 3298/5000: episode: 3297, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.000, 0.735], loss: 0.001730, mean_absolute_error: 41.758736, mean_q: 0.057780\n",
      " 3299/5000: episode: 3298, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.052, 1.000], loss: 0.001724, mean_absolute_error: 41.749634, mean_q: 0.057631\n",
      " 3300/5000: episode: 3299, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.000, 0.926], loss: 0.001706, mean_absolute_error: 41.755161, mean_q: 0.057340\n",
      " 3301/5000: episode: 3300, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.502], loss: 0.001687, mean_absolute_error: 41.761597, mean_q: 0.057066\n",
      " 3302/5000: episode: 3301, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.227, 1.000], loss: 0.001676, mean_absolute_error: 41.753399, mean_q: 0.056820\n",
      " 3303/5000: episode: 3302, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.817], loss: 0.001655, mean_absolute_error: 41.764732, mean_q: 0.056519\n",
      " 3304/5000: episode: 3303, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.565], loss: 0.001641, mean_absolute_error: 41.760670, mean_q: 0.056273\n",
      " 3305/5000: episode: 3304, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.408, 1.000], loss: 0.001633, mean_absolute_error: 41.754959, mean_q: 0.056084\n",
      " 3306/5000: episode: 3305, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.001621, mean_absolute_error: 41.755207, mean_q: 0.055878\n",
      " 3307/5000: episode: 3306, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 0.001607, mean_absolute_error: 41.752464, mean_q: 0.055620\n",
      " 3308/5000: episode: 3307, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.337, 1.000], loss: 0.001586, mean_absolute_error: 41.761372, mean_q: 0.055306\n",
      " 3309/5000: episode: 3308, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.367, 1.000], loss: 0.001580, mean_absolute_error: 41.750263, mean_q: 0.055115\n",
      " 3310/5000: episode: 3309, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.599 [0.029, 1.000], loss: 0.001567, mean_absolute_error: 41.750504, mean_q: 0.054905\n",
      " 3311/5000: episode: 3310, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.485 [0.000, 0.999], loss: 0.001547, mean_absolute_error: 41.757851, mean_q: 0.054587\n",
      " 3312/5000: episode: 3311, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.174, 1.000], loss: 0.001529, mean_absolute_error: 41.763077, mean_q: 0.054270\n",
      " 3313/5000: episode: 3312, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.159, 1.000], loss: 0.001517, mean_absolute_error: 41.759331, mean_q: 0.054047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3314/5000: episode: 3313, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.579], loss: 0.001501, mean_absolute_error: 41.757542, mean_q: 0.053763\n",
      " 3315/5000: episode: 3314, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.961], loss: 0.001490, mean_absolute_error: 41.758427, mean_q: 0.053560\n",
      " 3316/5000: episode: 3315, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.732], loss: 0.001474, mean_absolute_error: 41.761234, mean_q: 0.053278\n",
      " 3317/5000: episode: 3316, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.000, 1.000], loss: 0.001462, mean_absolute_error: 41.760880, mean_q: 0.053048\n",
      " 3318/5000: episode: 3317, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.281, 1.000], loss: 0.001455, mean_absolute_error: 41.752178, mean_q: 0.052881\n",
      " 3319/5000: episode: 3318, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.000, 1.000], loss: 0.001434, mean_absolute_error: 41.763992, mean_q: 0.052544\n",
      " 3320/5000: episode: 3319, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.000, 0.975], loss: 0.001426, mean_absolute_error: 41.758133, mean_q: 0.052360\n",
      " 3321/5000: episode: 3320, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.004, 1.000], loss: 0.001426, mean_absolute_error: 41.747849, mean_q: 0.052216\n",
      " 3322/5000: episode: 3321, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.735 [0.172, 1.000], loss: 0.001402, mean_absolute_error: 41.758469, mean_q: 0.051902\n",
      " 3323/5000: episode: 3322, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.774], loss: 0.001390, mean_absolute_error: 41.751534, mean_q: 0.051666\n",
      " 3324/5000: episode: 3323, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.356, 1.000], loss: 0.001378, mean_absolute_error: 41.755760, mean_q: 0.051440\n",
      " 3325/5000: episode: 3324, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.595], loss: 0.001367, mean_absolute_error: 41.754200, mean_q: 0.051226\n",
      " 3326/5000: episode: 3325, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 0.001350, mean_absolute_error: 41.750443, mean_q: 0.050901\n",
      " 3327/5000: episode: 3326, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.768], loss: 0.001338, mean_absolute_error: 41.760735, mean_q: 0.050710\n",
      " 3328/5000: episode: 3327, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.825], loss: 0.001326, mean_absolute_error: 41.760212, mean_q: 0.050474\n",
      " 3329/5000: episode: 3328, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.601], loss: 0.001284, mean_absolute_error: 41.745682, mean_q: 0.049586\n",
      " 3330/5000: episode: 3329, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.000, 0.999], loss: 0.001304, mean_absolute_error: 41.756508, mean_q: 0.050031\n",
      " 3331/5000: episode: 3330, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.225, 1.000], loss: 0.001290, mean_absolute_error: 41.760784, mean_q: 0.049764\n",
      " 3332/5000: episode: 3331, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.880], loss: 0.001286, mean_absolute_error: 41.749290, mean_q: 0.049598\n",
      " 3333/5000: episode: 3332, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.383, 1.000], loss: 0.001266, mean_absolute_error: 41.755844, mean_q: 0.049267\n",
      " 3334/5000: episode: 3333, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.051, 1.000], loss: 0.001256, mean_absolute_error: 41.759777, mean_q: 0.049102\n",
      " 3335/5000: episode: 3334, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.337, 1.000], loss: 0.001250, mean_absolute_error: 41.754444, mean_q: 0.048928\n",
      " 3336/5000: episode: 3335, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.526 [0.003, 1.000], loss: 0.001238, mean_absolute_error: 41.752144, mean_q: 0.048695\n",
      " 3337/5000: episode: 3336, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.938], loss: 0.001222, mean_absolute_error: 41.759869, mean_q: 0.048418\n",
      " 3338/5000: episode: 3337, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.000, 0.965], loss: 0.001218, mean_absolute_error: 41.754341, mean_q: 0.048293\n",
      " 3339/5000: episode: 3338, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.198, 1.000], loss: 0.001202, mean_absolute_error: 41.751778, mean_q: 0.047963\n",
      " 3340/5000: episode: 3339, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.670 [0.084, 1.000], loss: 0.001197, mean_absolute_error: 41.749638, mean_q: 0.047833\n",
      " 3341/5000: episode: 3340, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.649], loss: 0.001179, mean_absolute_error: 41.752373, mean_q: 0.047499\n",
      " 3342/5000: episode: 3341, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.755], loss: 0.001169, mean_absolute_error: 41.757126, mean_q: 0.047320\n",
      " 3343/5000: episode: 3342, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.000, 0.992], loss: 0.001169, mean_absolute_error: 41.742943, mean_q: 0.047209\n",
      " 3344/5000: episode: 3343, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.522 [0.002, 1.000], loss: 0.001148, mean_absolute_error: 41.757561, mean_q: 0.046864\n",
      " 3345/5000: episode: 3344, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.060, 1.000], loss: 0.001146, mean_absolute_error: 41.747276, mean_q: 0.046742\n",
      " 3346/5000: episode: 3345, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.731], loss: 0.001126, mean_absolute_error: 41.758366, mean_q: 0.046423\n",
      " 3347/5000: episode: 3346, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.001116, mean_absolute_error: 41.757645, mean_q: 0.046192\n",
      " 3348/5000: episode: 3347, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.079, 1.000], loss: 0.001108, mean_absolute_error: 41.754776, mean_q: 0.046003\n",
      " 3349/5000: episode: 3348, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.803], loss: 0.001097, mean_absolute_error: 41.752937, mean_q: 0.045767\n",
      " 3350/5000: episode: 3349, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.307, 1.000], loss: 0.001091, mean_absolute_error: 41.743095, mean_q: 0.045531\n",
      " 3351/5000: episode: 3350, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.599], loss: 0.001077, mean_absolute_error: 41.754555, mean_q: 0.045361\n",
      " 3352/5000: episode: 3351, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.676], loss: 0.001072, mean_absolute_error: 41.748520, mean_q: 0.045220\n",
      " 3353/5000: episode: 3352, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.695 [0.112, 1.000], loss: 0.001049, mean_absolute_error: 41.750301, mean_q: 0.044754\n",
      " 3354/5000: episode: 3353, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.288 [0.000, 0.864], loss: 0.001044, mean_absolute_error: 41.760422, mean_q: 0.044675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3355/5000: episode: 3354, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.001035, mean_absolute_error: 41.760677, mean_q: 0.044471\n",
      " 3356/5000: episode: 3355, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.027, 1.000], loss: 0.001017, mean_absolute_error: 41.743416, mean_q: 0.043932\n",
      " 3357/5000: episode: 3356, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.038, 1.000], loss: 0.001020, mean_absolute_error: 41.754364, mean_q: 0.044117\n",
      " 3358/5000: episode: 3357, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.592], loss: 0.001014, mean_absolute_error: 41.751694, mean_q: 0.043965\n",
      " 3359/5000: episode: 3358, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.644], loss: 0.000999, mean_absolute_error: 41.759323, mean_q: 0.043653\n",
      " 3360/5000: episode: 3359, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 0.000990, mean_absolute_error: 41.757942, mean_q: 0.043472\n",
      " 3361/5000: episode: 3360, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.000, 0.976], loss: 0.000982, mean_absolute_error: 41.756416, mean_q: 0.043246\n",
      " 3362/5000: episode: 3361, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.877], loss: 0.000973, mean_absolute_error: 41.756794, mean_q: 0.043065\n",
      " 3363/5000: episode: 3362, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.000, 0.940], loss: 0.000962, mean_absolute_error: 41.756477, mean_q: 0.042840\n",
      " 3364/5000: episode: 3363, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.357 [0.000, 0.941], loss: 0.000958, mean_absolute_error: 41.751812, mean_q: 0.042702\n",
      " 3365/5000: episode: 3364, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.498, 1.000], loss: 0.000944, mean_absolute_error: 41.754360, mean_q: 0.042382\n",
      " 3366/5000: episode: 3365, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.374, 1.000], loss: 0.000936, mean_absolute_error: 41.755325, mean_q: 0.042231\n",
      " 3367/5000: episode: 3366, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 0.000931, mean_absolute_error: 41.754044, mean_q: 0.042103\n",
      " 3368/5000: episode: 3367, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.570 [0.015, 1.000], loss: 0.000921, mean_absolute_error: 41.754131, mean_q: 0.041885\n",
      " 3369/5000: episode: 3368, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.416 [0.000, 0.979], loss: 0.000916, mean_absolute_error: 41.746635, mean_q: 0.041699\n",
      " 3370/5000: episode: 3369, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.035, 1.000], loss: 0.000901, mean_absolute_error: 41.756393, mean_q: 0.041429\n",
      " 3371/5000: episode: 3370, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.472, 1.000], loss: 0.000901, mean_absolute_error: 41.742554, mean_q: 0.041264\n",
      " 3372/5000: episode: 3371, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.872], loss: 0.000884, mean_absolute_error: 41.756775, mean_q: 0.041015\n",
      " 3373/5000: episode: 3372, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.002, 1.000], loss: 0.000885, mean_absolute_error: 41.745693, mean_q: 0.040940\n",
      " 3374/5000: episode: 3373, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.027, 1.000], loss: 0.000876, mean_absolute_error: 41.749611, mean_q: 0.040758\n",
      " 3375/5000: episode: 3374, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 0.000863, mean_absolute_error: 41.749664, mean_q: 0.040466\n",
      " 3376/5000: episode: 3375, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.000, 0.903], loss: 0.000852, mean_absolute_error: 41.756474, mean_q: 0.040261\n",
      " 3377/5000: episode: 3376, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.366, 1.000], loss: 0.000845, mean_absolute_error: 41.754913, mean_q: 0.040070\n",
      " 3378/5000: episode: 3377, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.669], loss: 0.000836, mean_absolute_error: 41.753792, mean_q: 0.039832\n",
      " 3379/5000: episode: 3378, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 0.000832, mean_absolute_error: 41.753830, mean_q: 0.039696\n",
      " 3380/5000: episode: 3379, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 0.000822, mean_absolute_error: 41.751652, mean_q: 0.039495\n",
      " 3381/5000: episode: 3380, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.492], loss: 0.000814, mean_absolute_error: 41.752106, mean_q: 0.039312\n",
      " 3382/5000: episode: 3381, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.704], loss: 0.000808, mean_absolute_error: 41.751717, mean_q: 0.039148\n",
      " 3383/5000: episode: 3382, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.349 [0.000, 0.934], loss: 0.000802, mean_absolute_error: 41.748680, mean_q: 0.038919\n",
      " 3384/5000: episode: 3383, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.443 [0.000, 0.990], loss: 0.000792, mean_absolute_error: 41.752228, mean_q: 0.038723\n",
      " 3385/5000: episode: 3384, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.343, 1.000], loss: 0.000784, mean_absolute_error: 41.752586, mean_q: 0.038530\n",
      " 3386/5000: episode: 3385, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.248, 1.000], loss: 0.000780, mean_absolute_error: 41.746555, mean_q: 0.038351\n",
      " 3387/5000: episode: 3386, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.279, 1.000], loss: 0.000768, mean_absolute_error: 41.751766, mean_q: 0.038107\n",
      " 3388/5000: episode: 3387, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.431, 1.000], loss: 0.000765, mean_absolute_error: 41.747826, mean_q: 0.038015\n",
      " 3389/5000: episode: 3388, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.300, 1.000], loss: 0.000754, mean_absolute_error: 41.750530, mean_q: 0.037761\n",
      " 3390/5000: episode: 3389, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.000, 1.000], loss: 0.000746, mean_absolute_error: 41.754253, mean_q: 0.037585\n",
      " 3391/5000: episode: 3390, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.465, 1.000], loss: 0.000744, mean_absolute_error: 41.746536, mean_q: 0.037458\n",
      " 3392/5000: episode: 3391, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.465, 1.000], loss: 0.000733, mean_absolute_error: 41.749611, mean_q: 0.037235\n",
      " 3393/5000: episode: 3392, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.120, 1.000], loss: 0.000725, mean_absolute_error: 41.753616, mean_q: 0.037039\n",
      " 3394/5000: episode: 3393, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 0.000720, mean_absolute_error: 41.751068, mean_q: 0.036873\n",
      " 3395/5000: episode: 3394, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.105, 1.000], loss: 0.000712, mean_absolute_error: 41.753544, mean_q: 0.036668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3396/5000: episode: 3395, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.514], loss: 0.000713, mean_absolute_error: 41.746803, mean_q: 0.036581\n",
      " 3397/5000: episode: 3396, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.521], loss: 0.000700, mean_absolute_error: 41.750717, mean_q: 0.036348\n",
      " 3398/5000: episode: 3397, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.030, 1.000], loss: 0.000692, mean_absolute_error: 41.754620, mean_q: 0.036135\n",
      " 3399/5000: episode: 3398, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.675], loss: 0.000684, mean_absolute_error: 41.756226, mean_q: 0.035943\n",
      " 3400/5000: episode: 3399, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.543 [0.006, 1.000], loss: 0.000677, mean_absolute_error: 41.753326, mean_q: 0.035761\n",
      " 3401/5000: episode: 3400, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.707], loss: 0.000672, mean_absolute_error: 41.751564, mean_q: 0.035578\n",
      " 3402/5000: episode: 3401, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.387, 1.000], loss: 0.000664, mean_absolute_error: 41.752434, mean_q: 0.035368\n",
      " 3403/5000: episode: 3402, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.533], loss: 0.000670, mean_absolute_error: 41.745747, mean_q: 0.035430\n",
      " 3404/5000: episode: 3403, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.739], loss: 0.000653, mean_absolute_error: 41.749039, mean_q: 0.035049\n",
      " 3405/5000: episode: 3404, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.498], loss: 0.000649, mean_absolute_error: 41.751892, mean_q: 0.034953\n",
      " 3406/5000: episode: 3405, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.529 [0.003, 1.000], loss: 0.000640, mean_absolute_error: 41.751854, mean_q: 0.034712\n",
      " 3407/5000: episode: 3406, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.531], loss: 0.000662, mean_absolute_error: 41.744381, mean_q: 0.034895\n",
      " 3408/5000: episode: 3407, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.674], loss: 0.000628, mean_absolute_error: 41.738125, mean_q: 0.034276\n",
      " 3409/5000: episode: 3408, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.000623, mean_absolute_error: 41.753082, mean_q: 0.034198\n",
      " 3410/5000: episode: 3409, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.011, 1.000], loss: 0.000618, mean_absolute_error: 41.746082, mean_q: 0.034046\n",
      " 3411/5000: episode: 3410, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.000, 0.993], loss: 0.000612, mean_absolute_error: 41.745422, mean_q: 0.033890\n",
      " 3412/5000: episode: 3411, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 0.000604, mean_absolute_error: 41.751694, mean_q: 0.033682\n",
      " 3413/5000: episode: 3412, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.000599, mean_absolute_error: 41.749084, mean_q: 0.033542\n",
      " 3414/5000: episode: 3413, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.088, 1.000], loss: 0.000590, mean_absolute_error: 41.751762, mean_q: 0.033306\n",
      " 3415/5000: episode: 3414, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.444 [0.000, 0.990], loss: 0.000590, mean_absolute_error: 41.746231, mean_q: 0.033249\n",
      " 3416/5000: episode: 3415, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.000, 0.808], loss: 0.000579, mean_absolute_error: 41.754028, mean_q: 0.032959\n",
      " 3417/5000: episode: 3416, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.055, 1.000], loss: 0.000578, mean_absolute_error: 41.741631, mean_q: 0.032874\n",
      " 3418/5000: episode: 3417, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.129, 1.000], loss: 0.000577, mean_absolute_error: 41.738396, mean_q: 0.032778\n",
      " 3419/5000: episode: 3418, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.960], loss: 0.000566, mean_absolute_error: 41.746040, mean_q: 0.032554\n",
      " 3420/5000: episode: 3419, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.677 [0.091, 1.000], loss: 0.000558, mean_absolute_error: 41.747841, mean_q: 0.032331\n",
      " 3421/5000: episode: 3420, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.453 [0.000, 0.993], loss: 0.000553, mean_absolute_error: 41.751568, mean_q: 0.032155\n",
      " 3422/5000: episode: 3421, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.572 [0.016, 1.000], loss: 0.000546, mean_absolute_error: 41.751297, mean_q: 0.031994\n",
      " 3423/5000: episode: 3422, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.344, 1.000], loss: 0.000539, mean_absolute_error: 41.753078, mean_q: 0.031805\n",
      " 3424/5000: episode: 3423, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.730], loss: 0.000535, mean_absolute_error: 41.749706, mean_q: 0.031621\n",
      " 3425/5000: episode: 3424, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 0.000541, mean_absolute_error: 41.747391, mean_q: 0.031618\n",
      " 3426/5000: episode: 3425, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.879], loss: 0.000526, mean_absolute_error: 41.748245, mean_q: 0.031331\n",
      " 3427/5000: episode: 3426, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.120, 1.000], loss: 0.000524, mean_absolute_error: 41.740276, mean_q: 0.031225\n",
      " 3428/5000: episode: 3427, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 0.000516, mean_absolute_error: 41.748791, mean_q: 0.031060\n",
      " 3429/5000: episode: 3428, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.595 [0.026, 1.000], loss: 0.000508, mean_absolute_error: 41.754013, mean_q: 0.030847\n",
      " 3430/5000: episode: 3429, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.000, 0.923], loss: 0.000501, mean_absolute_error: 41.755516, mean_q: 0.030642\n",
      " 3431/5000: episode: 3430, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.000505, mean_absolute_error: 41.739632, mean_q: 0.030634\n",
      " 3432/5000: episode: 3431, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.334 [0.000, 0.920], loss: 0.000498, mean_absolute_error: 41.741539, mean_q: 0.030451\n",
      " 3433/5000: episode: 3432, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.608], loss: 0.000487, mean_absolute_error: 41.756447, mean_q: 0.030198\n",
      " 3434/5000: episode: 3433, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.000, 0.885], loss: 0.000486, mean_absolute_error: 41.751034, mean_q: 0.030083\n",
      " 3435/5000: episode: 3434, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.909], loss: 0.000481, mean_absolute_error: 41.745758, mean_q: 0.029926\n",
      " 3436/5000: episode: 3435, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.373, 1.000], loss: 0.000479, mean_absolute_error: 41.745075, mean_q: 0.029806\n",
      " 3437/5000: episode: 3436, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.000, 0.924], loss: 0.000472, mean_absolute_error: 41.748451, mean_q: 0.029601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3438/5000: episode: 3437, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.485 [0.000, 0.999], loss: 0.000467, mean_absolute_error: 41.746925, mean_q: 0.029502\n",
      " 3439/5000: episode: 3438, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.061, 1.000], loss: 0.000460, mean_absolute_error: 41.752689, mean_q: 0.029287\n",
      " 3440/5000: episode: 3439, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.000, 0.698], loss: 0.000458, mean_absolute_error: 41.749859, mean_q: 0.029207\n",
      " 3441/5000: episode: 3440, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.706], loss: 0.000457, mean_absolute_error: 41.744263, mean_q: 0.029025\n",
      " 3442/5000: episode: 3441, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.603], loss: 0.000449, mean_absolute_error: 41.744389, mean_q: 0.028883\n",
      " 3443/5000: episode: 3442, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.503], loss: 0.000445, mean_absolute_error: 41.748993, mean_q: 0.028751\n",
      " 3444/5000: episode: 3443, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.000, 1.000], loss: 0.000441, mean_absolute_error: 41.746162, mean_q: 0.028599\n",
      " 3445/5000: episode: 3444, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 0.000435, mean_absolute_error: 41.748383, mean_q: 0.028419\n",
      " 3446/5000: episode: 3445, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.496], loss: 0.000431, mean_absolute_error: 41.749081, mean_q: 0.028281\n",
      " 3447/5000: episode: 3446, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 0.000425, mean_absolute_error: 41.751007, mean_q: 0.028102\n",
      " 3448/5000: episode: 3447, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 0.000421, mean_absolute_error: 41.748959, mean_q: 0.027935\n",
      " 3449/5000: episode: 3448, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.000, 0.883], loss: 0.000415, mean_absolute_error: 41.752563, mean_q: 0.027780\n",
      " 3450/5000: episode: 3449, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.278, 1.000], loss: 0.000415, mean_absolute_error: 41.746101, mean_q: 0.027676\n",
      " 3451/5000: episode: 3450, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 0.000411, mean_absolute_error: 41.748367, mean_q: 0.027570\n",
      " 3452/5000: episode: 3451, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 0.000421, mean_absolute_error: 41.738636, mean_q: 0.027518\n",
      " 3453/5000: episode: 3452, duration: 0.016s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.948], loss: 0.000401, mean_absolute_error: 41.749062, mean_q: 0.027232\n",
      " 3454/5000: episode: 3453, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.480, 1.000], loss: 0.000397, mean_absolute_error: 41.741379, mean_q: 0.026988\n",
      " 3455/5000: episode: 3454, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.000, 0.987], loss: 0.000393, mean_absolute_error: 41.749115, mean_q: 0.026914\n",
      " 3456/5000: episode: 3455, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 0.000385, mean_absolute_error: 41.754570, mean_q: 0.026740\n",
      " 3457/5000: episode: 3456, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.039, 1.000], loss: 0.000388, mean_absolute_error: 41.742210, mean_q: 0.026597\n",
      " 3458/5000: episode: 3457, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.259, 1.000], loss: 0.000383, mean_absolute_error: 41.744507, mean_q: 0.026541\n",
      " 3459/5000: episode: 3458, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.756 [0.213, 1.000], loss: 0.000377, mean_absolute_error: 41.746117, mean_q: 0.026287\n",
      " 3460/5000: episode: 3459, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.064, 1.000], loss: 0.000372, mean_absolute_error: 41.750870, mean_q: 0.026207\n",
      " 3461/5000: episode: 3460, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.140, 1.000], loss: 0.000369, mean_absolute_error: 41.746056, mean_q: 0.026068\n",
      " 3462/5000: episode: 3461, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.070, 1.000], loss: 0.000366, mean_absolute_error: 41.738991, mean_q: 0.025669\n",
      " 3463/5000: episode: 3462, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.022, 1.000], loss: 0.000360, mean_absolute_error: 41.749542, mean_q: 0.025785\n",
      " 3464/5000: episode: 3463, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.446, 1.000], loss: 0.000359, mean_absolute_error: 41.745396, mean_q: 0.025703\n",
      " 3465/5000: episode: 3464, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.548 [0.008, 1.000], loss: 0.000357, mean_absolute_error: 41.747589, mean_q: 0.025595\n",
      " 3466/5000: episode: 3465, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.142, 1.000], loss: 0.000348, mean_absolute_error: 41.752037, mean_q: 0.025327\n",
      " 3467/5000: episode: 3466, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.591], loss: 0.000347, mean_absolute_error: 41.748981, mean_q: 0.025260\n",
      " 3468/5000: episode: 3467, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.677], loss: 0.000342, mean_absolute_error: 41.750679, mean_q: 0.025082\n",
      " 3469/5000: episode: 3468, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.518], loss: 0.000340, mean_absolute_error: 41.748154, mean_q: 0.025015\n",
      " 3470/5000: episode: 3469, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.294 [0.000, 0.874], loss: 0.000334, mean_absolute_error: 41.751625, mean_q: 0.024789\n",
      " 3471/5000: episode: 3470, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.438, 1.000], loss: 0.000336, mean_absolute_error: 41.737350, mean_q: 0.024732\n",
      " 3472/5000: episode: 3471, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.504, 1.000], loss: 0.000335, mean_absolute_error: 41.740932, mean_q: 0.024688\n",
      " 3473/5000: episode: 3472, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.295 [0.000, 0.875], loss: 0.000329, mean_absolute_error: 41.744652, mean_q: 0.024512\n",
      " 3474/5000: episode: 3473, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.134, 1.000], loss: 0.000325, mean_absolute_error: 41.746292, mean_q: 0.024358\n",
      " 3475/5000: episode: 3474, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.069, 1.000], loss: 0.000325, mean_absolute_error: 41.735977, mean_q: 0.024306\n",
      " 3476/5000: episode: 3475, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.327, 1.000], loss: 0.000315, mean_absolute_error: 41.748451, mean_q: 0.024026\n",
      " 3477/5000: episode: 3476, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 0.000312, mean_absolute_error: 41.749241, mean_q: 0.023886\n",
      " 3478/5000: episode: 3477, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.231, 1.000], loss: 0.000311, mean_absolute_error: 41.746181, mean_q: 0.023811\n",
      " 3479/5000: episode: 3478, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.012, 1.000], loss: 0.000307, mean_absolute_error: 41.744534, mean_q: 0.023702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3480/5000: episode: 3479, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.490 [0.000, 0.999], loss: 0.000303, mean_absolute_error: 41.748508, mean_q: 0.023545\n",
      " 3481/5000: episode: 3480, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.658], loss: 0.000305, mean_absolute_error: 41.742325, mean_q: 0.023498\n",
      " 3482/5000: episode: 3481, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.395, 1.000], loss: 0.000299, mean_absolute_error: 41.737232, mean_q: 0.023215\n",
      " 3483/5000: episode: 3482, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.238, 1.000], loss: 0.000297, mean_absolute_error: 41.745876, mean_q: 0.023226\n",
      " 3484/5000: episode: 3483, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.529], loss: 0.000289, mean_absolute_error: 41.749283, mean_q: 0.022951\n",
      " 3485/5000: episode: 3484, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.446 [0.000, 0.991], loss: 0.000286, mean_absolute_error: 41.749809, mean_q: 0.022877\n",
      " 3486/5000: episode: 3485, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.485, 1.000], loss: 0.000286, mean_absolute_error: 41.743057, mean_q: 0.022830\n",
      " 3487/5000: episode: 3486, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.762], loss: 0.000282, mean_absolute_error: 41.748051, mean_q: 0.022660\n",
      " 3488/5000: episode: 3487, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.543 [0.006, 1.000], loss: 0.000283, mean_absolute_error: 41.745361, mean_q: 0.022583\n",
      " 3489/5000: episode: 3488, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 0.000277, mean_absolute_error: 41.747169, mean_q: 0.022448\n",
      " 3490/5000: episode: 3489, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.000, 0.953], loss: 0.000274, mean_absolute_error: 41.745998, mean_q: 0.022298\n",
      " 3491/5000: episode: 3490, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 0.000269, mean_absolute_error: 41.751656, mean_q: 0.022146\n",
      " 3492/5000: episode: 3491, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.595], loss: 0.000268, mean_absolute_error: 41.748222, mean_q: 0.022068\n",
      " 3493/5000: episode: 3492, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 0.000263, mean_absolute_error: 41.750481, mean_q: 0.021886\n",
      " 3494/5000: episode: 3493, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.944], loss: 0.000264, mean_absolute_error: 41.743073, mean_q: 0.021756\n",
      " 3495/5000: episode: 3494, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.853], loss: 0.000259, mean_absolute_error: 41.747086, mean_q: 0.021639\n",
      " 3496/5000: episode: 3495, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.000, 0.930], loss: 0.000255, mean_absolute_error: 41.748497, mean_q: 0.021540\n",
      " 3497/5000: episode: 3496, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.458 [0.000, 0.994], loss: 0.000255, mean_absolute_error: 41.744209, mean_q: 0.021474\n",
      " 3498/5000: episode: 3497, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.502], loss: 0.000251, mean_absolute_error: 41.747417, mean_q: 0.021333\n",
      " 3499/5000: episode: 3498, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.647 [0.062, 1.000], loss: 0.000248, mean_absolute_error: 41.744854, mean_q: 0.021171\n",
      " 3500/5000: episode: 3499, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 0.000246, mean_absolute_error: 41.747108, mean_q: 0.021077\n",
      " 3501/5000: episode: 3500, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.249, 1.000], loss: 0.000243, mean_absolute_error: 41.745457, mean_q: 0.020981\n",
      " 3502/5000: episode: 3501, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.578], loss: 0.000240, mean_absolute_error: 41.746925, mean_q: 0.020807\n",
      " 3503/5000: episode: 3502, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 0.000237, mean_absolute_error: 41.748936, mean_q: 0.020694\n",
      " 3504/5000: episode: 3503, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.599], loss: 0.000240, mean_absolute_error: 41.739777, mean_q: 0.020599\n",
      " 3505/5000: episode: 3504, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.818], loss: 0.000233, mean_absolute_error: 41.747837, mean_q: 0.020480\n",
      " 3506/5000: episode: 3505, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.705], loss: 0.000228, mean_absolute_error: 41.748272, mean_q: 0.020285\n",
      " 3507/5000: episode: 3506, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.387, 1.000], loss: 0.000230, mean_absolute_error: 41.739437, mean_q: 0.020290\n",
      " 3508/5000: episode: 3507, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.000, 0.972], loss: 0.000222, mean_absolute_error: 41.753128, mean_q: 0.020063\n",
      " 3509/5000: episode: 3508, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.030, 1.000], loss: 0.000223, mean_absolute_error: 41.746521, mean_q: 0.019972\n",
      " 3510/5000: episode: 3509, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.193, 1.000], loss: 0.000223, mean_absolute_error: 41.738403, mean_q: 0.019955\n",
      " 3511/5000: episode: 3510, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.589], loss: 0.000219, mean_absolute_error: 41.747231, mean_q: 0.019793\n",
      " 3512/5000: episode: 3511, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 0.000216, mean_absolute_error: 41.744465, mean_q: 0.019682\n",
      " 3513/5000: episode: 3512, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.104, 1.000], loss: 0.000213, mean_absolute_error: 41.749474, mean_q: 0.019560\n",
      " 3514/5000: episode: 3513, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.000, 0.988], loss: 0.000211, mean_absolute_error: 41.746582, mean_q: 0.019451\n",
      " 3515/5000: episode: 3514, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.644], loss: 0.000207, mean_absolute_error: 41.747307, mean_q: 0.019278\n",
      " 3516/5000: episode: 3515, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.860], loss: 0.000211, mean_absolute_error: 41.740311, mean_q: 0.019292\n",
      " 3517/5000: episode: 3516, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.065, 1.000], loss: 0.000204, mean_absolute_error: 41.746700, mean_q: 0.019097\n",
      " 3518/5000: episode: 3517, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.528 [0.003, 1.000], loss: 0.000201, mean_absolute_error: 41.750469, mean_q: 0.018966\n",
      " 3519/5000: episode: 3518, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.149, 1.000], loss: 0.000203, mean_absolute_error: 41.741623, mean_q: 0.018966\n",
      " 3520/5000: episode: 3519, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.722], loss: 0.000207, mean_absolute_error: 41.743942, mean_q: 0.018880\n",
      " 3521/5000: episode: 3520, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.472, 1.000], loss: 0.000197, mean_absolute_error: 41.741219, mean_q: 0.018667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3522/5000: episode: 3521, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 0.000191, mean_absolute_error: 41.748184, mean_q: 0.018483\n",
      " 3523/5000: episode: 3522, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.404 [0.000, 0.973], loss: 0.000191, mean_absolute_error: 41.745972, mean_q: 0.018477\n",
      " 3524/5000: episode: 3523, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.236, 1.000], loss: 0.000187, mean_absolute_error: 41.751122, mean_q: 0.018320\n",
      " 3525/5000: episode: 3524, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.803], loss: 0.000189, mean_absolute_error: 41.744247, mean_q: 0.018291\n",
      " 3526/5000: episode: 3525, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.509 [0.001, 1.000], loss: 0.000185, mean_absolute_error: 41.746944, mean_q: 0.018161\n",
      " 3527/5000: episode: 3526, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 0.000185, mean_absolute_error: 41.739525, mean_q: 0.018063\n",
      " 3528/5000: episode: 3527, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.000, 0.997], loss: 0.000185, mean_absolute_error: 41.740181, mean_q: 0.017972\n",
      " 3529/5000: episode: 3528, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.002, 1.000], loss: 0.000178, mean_absolute_error: 41.749046, mean_q: 0.017814\n",
      " 3530/5000: episode: 3529, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.147, 1.000], loss: 0.000180, mean_absolute_error: 41.743675, mean_q: 0.017765\n",
      " 3531/5000: episode: 3530, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.526], loss: 0.000175, mean_absolute_error: 41.749275, mean_q: 0.017640\n",
      " 3532/5000: episode: 3531, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.731], loss: 0.000173, mean_absolute_error: 41.746670, mean_q: 0.017478\n",
      " 3533/5000: episode: 3532, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.572], loss: 0.000173, mean_absolute_error: 41.742805, mean_q: 0.017423\n",
      " 3534/5000: episode: 3533, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.266, 1.000], loss: 0.000171, mean_absolute_error: 41.742088, mean_q: 0.017355\n",
      " 3535/5000: episode: 3534, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.434 [0.000, 0.987], loss: 0.000168, mean_absolute_error: 41.745136, mean_q: 0.017214\n",
      " 3536/5000: episode: 3535, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.206, 1.000], loss: 0.000168, mean_absolute_error: 41.740726, mean_q: 0.017149\n",
      " 3537/5000: episode: 3536, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.497], loss: 0.000169, mean_absolute_error: 41.737587, mean_q: 0.017052\n",
      " 3538/5000: episode: 3537, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.108, 1.000], loss: 0.000160, mean_absolute_error: 41.751450, mean_q: 0.016882\n",
      " 3539/5000: episode: 3538, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.604], loss: 0.000163, mean_absolute_error: 41.740475, mean_q: 0.016893\n",
      " 3540/5000: episode: 3539, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 0.000162, mean_absolute_error: 41.740532, mean_q: 0.016712\n",
      " 3541/5000: episode: 3540, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.649], loss: 0.000162, mean_absolute_error: 41.734886, mean_q: 0.016733\n",
      " 3542/5000: episode: 3541, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.000156, mean_absolute_error: 41.744949, mean_q: 0.016547\n",
      " 3543/5000: episode: 3542, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.448, 1.000], loss: 0.000155, mean_absolute_error: 41.739578, mean_q: 0.016268\n",
      " 3544/5000: episode: 3543, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.757], loss: 0.000154, mean_absolute_error: 41.745106, mean_q: 0.016380\n",
      " 3545/5000: episode: 3544, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.000, 0.997], loss: 0.000155, mean_absolute_error: 41.740036, mean_q: 0.016352\n",
      " 3546/5000: episode: 3545, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.207, 1.000], loss: 0.000150, mean_absolute_error: 41.743958, mean_q: 0.016197\n",
      " 3547/5000: episode: 3546, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.485 [0.000, 0.999], loss: 0.000150, mean_absolute_error: 41.741577, mean_q: 0.016054\n",
      " 3548/5000: episode: 3547, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.356, 1.000], loss: 0.000146, mean_absolute_error: 41.745907, mean_q: 0.015959\n",
      " 3549/5000: episode: 3548, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.020, 1.000], loss: 0.000142, mean_absolute_error: 41.749374, mean_q: 0.015788\n",
      " 3550/5000: episode: 3549, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.000, 0.940], loss: 0.000145, mean_absolute_error: 41.736984, mean_q: 0.015774\n",
      " 3551/5000: episode: 3550, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 0.000141, mean_absolute_error: 41.748058, mean_q: 0.015668\n",
      " 3552/5000: episode: 3551, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 0.000147, mean_absolute_error: 41.735413, mean_q: 0.015658\n",
      " 3553/5000: episode: 3552, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.000141, mean_absolute_error: 41.741501, mean_q: 0.015445\n",
      " 3554/5000: episode: 3553, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.271, 1.000], loss: 0.000140, mean_absolute_error: 41.734230, mean_q: 0.015445\n",
      " 3555/5000: episode: 3554, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 0.000139, mean_absolute_error: 41.736404, mean_q: 0.015362\n",
      " 3556/5000: episode: 3555, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.319, 1.000], loss: 0.000135, mean_absolute_error: 41.744667, mean_q: 0.015193\n",
      " 3557/5000: episode: 3556, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.651], loss: 0.000131, mean_absolute_error: 41.746941, mean_q: 0.015042\n",
      " 3558/5000: episode: 3557, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.426 [0.000, 0.984], loss: 0.000136, mean_absolute_error: 41.735413, mean_q: 0.015062\n",
      " 3559/5000: episode: 3558, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.463, 1.000], loss: 0.000129, mean_absolute_error: 41.742981, mean_q: 0.014918\n",
      " 3560/5000: episode: 3559, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.090, 1.000], loss: 0.000128, mean_absolute_error: 41.744915, mean_q: 0.014825\n",
      " 3561/5000: episode: 3560, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.438, 1.000], loss: 0.000127, mean_absolute_error: 41.742081, mean_q: 0.014750\n",
      " 3562/5000: episode: 3561, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.000, 0.973], loss: 0.000126, mean_absolute_error: 41.742111, mean_q: 0.014673\n",
      " 3563/5000: episode: 3562, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.246, 1.000], loss: 0.000123, mean_absolute_error: 41.743690, mean_q: 0.014550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3564/5000: episode: 3563, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.327, 1.000], loss: 0.000126, mean_absolute_error: 41.742393, mean_q: 0.014584\n",
      " 3565/5000: episode: 3564, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.235, 1.000], loss: 0.000120, mean_absolute_error: 41.745899, mean_q: 0.014356\n",
      " 3566/5000: episode: 3565, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 0.000120, mean_absolute_error: 41.738735, mean_q: 0.014288\n",
      " 3567/5000: episode: 3566, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.076, 1.000], loss: 0.000115, mean_absolute_error: 41.749043, mean_q: 0.014135\n",
      " 3568/5000: episode: 3567, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.527], loss: 0.000125, mean_absolute_error: 41.728165, mean_q: 0.014318\n",
      " 3569/5000: episode: 3568, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.610], loss: 0.000113, mean_absolute_error: 41.748940, mean_q: 0.013960\n",
      " 3570/5000: episode: 3569, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.169, 1.000], loss: 0.000113, mean_absolute_error: 41.746784, mean_q: 0.013962\n",
      " 3571/5000: episode: 3570, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.000115, mean_absolute_error: 41.736485, mean_q: 0.013830\n",
      " 3572/5000: episode: 3571, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 0.729], loss: 0.000110, mean_absolute_error: 41.745552, mean_q: 0.013756\n",
      " 3573/5000: episode: 3572, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.556 [0.010, 1.000], loss: 0.000112, mean_absolute_error: 41.738586, mean_q: 0.013671\n",
      " 3574/5000: episode: 3573, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.385, 1.000], loss: 0.000107, mean_absolute_error: 41.743656, mean_q: 0.013474\n",
      " 3575/5000: episode: 3574, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.601], loss: 0.000108, mean_absolute_error: 41.742790, mean_q: 0.013511\n",
      " 3576/5000: episode: 3575, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.732], loss: 0.000104, mean_absolute_error: 41.747574, mean_q: 0.013368\n",
      " 3577/5000: episode: 3576, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.444 [0.000, 0.990], loss: 0.000105, mean_absolute_error: 41.742378, mean_q: 0.013369\n",
      " 3578/5000: episode: 3577, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.295 [0.000, 0.875], loss: 0.000107, mean_absolute_error: 41.741760, mean_q: 0.013317\n",
      " 3579/5000: episode: 3578, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.948], loss: 0.000102, mean_absolute_error: 41.745697, mean_q: 0.013181\n",
      " 3580/5000: episode: 3579, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.464, 1.000], loss: 0.000105, mean_absolute_error: 41.740345, mean_q: 0.013102\n",
      " 3581/5000: episode: 3580, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.622], loss: 0.000099, mean_absolute_error: 41.748581, mean_q: 0.012982\n",
      " 3582/5000: episode: 3581, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.319 [0.000, 0.904], loss: 0.000097, mean_absolute_error: 41.746887, mean_q: 0.012883\n",
      " 3583/5000: episode: 3582, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.543], loss: 0.000102, mean_absolute_error: 41.735596, mean_q: 0.012923\n",
      " 3584/5000: episode: 3583, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.000, 0.999], loss: 0.000097, mean_absolute_error: 41.743446, mean_q: 0.012746\n",
      " 3585/5000: episode: 3584, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.000, 0.777], loss: 0.000098, mean_absolute_error: 41.737820, mean_q: 0.012765\n",
      " 3586/5000: episode: 3585, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.391, 1.000], loss: 0.000098, mean_absolute_error: 41.738434, mean_q: 0.012696\n",
      " 3587/5000: episode: 3586, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.712], loss: 0.000092, mean_absolute_error: 41.745838, mean_q: 0.012506\n",
      " 3588/5000: episode: 3587, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.528], loss: 0.000094, mean_absolute_error: 41.741703, mean_q: 0.012505\n",
      " 3589/5000: episode: 3588, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.372, 1.000], loss: 0.000093, mean_absolute_error: 41.738655, mean_q: 0.012386\n",
      " 3590/5000: episode: 3589, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.196, 1.000], loss: 0.000092, mean_absolute_error: 41.740940, mean_q: 0.012309\n",
      " 3591/5000: episode: 3590, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.567], loss: 0.000089, mean_absolute_error: 41.743233, mean_q: 0.012183\n",
      " 3592/5000: episode: 3591, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.396, 1.000], loss: 0.000088, mean_absolute_error: 41.746872, mean_q: 0.012141\n",
      " 3593/5000: episode: 3592, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.245, 1.000], loss: 0.000086, mean_absolute_error: 41.746250, mean_q: 0.012019\n",
      " 3594/5000: episode: 3593, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.000, 0.963], loss: 0.000089, mean_absolute_error: 41.741627, mean_q: 0.012039\n",
      " 3595/5000: episode: 3594, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.109, 1.000], loss: 0.000087, mean_absolute_error: 41.744984, mean_q: 0.011914\n",
      " 3596/5000: episode: 3595, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.273, 1.000], loss: 0.000085, mean_absolute_error: 41.742767, mean_q: 0.011855\n",
      " 3597/5000: episode: 3596, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.057, 1.000], loss: 0.000082, mean_absolute_error: 41.750870, mean_q: 0.011745\n",
      " 3598/5000: episode: 3597, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.617], loss: 0.000085, mean_absolute_error: 41.740223, mean_q: 0.011780\n",
      " 3599/5000: episode: 3598, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.495, 1.000], loss: 0.000086, mean_absolute_error: 41.743896, mean_q: 0.011697\n",
      " 3600/5000: episode: 3599, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.000081, mean_absolute_error: 41.742317, mean_q: 0.011510\n",
      " 3601/5000: episode: 3600, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.000, 0.921], loss: 0.000079, mean_absolute_error: 41.743767, mean_q: 0.011420\n",
      " 3602/5000: episode: 3601, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.619], loss: 0.000078, mean_absolute_error: 41.746685, mean_q: 0.011383\n",
      " 3603/5000: episode: 3602, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.756 [0.213, 1.000], loss: 0.000083, mean_absolute_error: 41.736286, mean_q: 0.011402\n",
      " 3604/5000: episode: 3603, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.584 [0.021, 1.000], loss: 0.000078, mean_absolute_error: 41.741776, mean_q: 0.011216\n",
      " 3605/5000: episode: 3604, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.715], loss: 0.000076, mean_absolute_error: 41.745033, mean_q: 0.011188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3606/5000: episode: 3605, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.701], loss: 0.000074, mean_absolute_error: 41.746056, mean_q: 0.011068\n",
      " 3607/5000: episode: 3606, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.762], loss: 0.000074, mean_absolute_error: 41.745522, mean_q: 0.011056\n",
      " 3608/5000: episode: 3607, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.391 [0.000, 0.966], loss: 0.000073, mean_absolute_error: 41.746750, mean_q: 0.010949\n",
      " 3609/5000: episode: 3608, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.739 [0.179, 1.000], loss: 0.000072, mean_absolute_error: 41.728664, mean_q: 0.010544\n",
      " 3610/5000: episode: 3609, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 0.000072, mean_absolute_error: 41.746536, mean_q: 0.010802\n",
      " 3611/5000: episode: 3610, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 0.000070, mean_absolute_error: 41.745583, mean_q: 0.010675\n",
      " 3612/5000: episode: 3611, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 0.000069, mean_absolute_error: 41.743008, mean_q: 0.010609\n",
      " 3613/5000: episode: 3612, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.000, 0.947], loss: 0.000070, mean_absolute_error: 41.741959, mean_q: 0.010635\n",
      " 3614/5000: episode: 3613, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.796], loss: 0.000069, mean_absolute_error: 41.740921, mean_q: 0.010548\n",
      " 3615/5000: episode: 3614, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.612], loss: 0.000069, mean_absolute_error: 41.742458, mean_q: 0.010470\n",
      " 3616/5000: episode: 3615, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.032, 1.000], loss: 0.000067, mean_absolute_error: 41.740921, mean_q: 0.010379\n",
      " 3617/5000: episode: 3616, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.000069, mean_absolute_error: 41.740189, mean_q: 0.010284\n",
      " 3618/5000: episode: 3617, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.853], loss: 0.000068, mean_absolute_error: 41.737877, mean_q: 0.010261\n",
      " 3619/5000: episode: 3618, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.233, 1.000], loss: 0.000072, mean_absolute_error: 41.736679, mean_q: 0.010262\n",
      " 3620/5000: episode: 3619, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.000063, mean_absolute_error: 41.742088, mean_q: 0.010107\n",
      " 3621/5000: episode: 3620, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.472, 1.000], loss: 0.000067, mean_absolute_error: 41.732105, mean_q: 0.010115\n",
      " 3622/5000: episode: 3621, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.628], loss: 0.000063, mean_absolute_error: 41.741203, mean_q: 0.010005\n",
      " 3623/5000: episode: 3622, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.161, 1.000], loss: 0.000062, mean_absolute_error: 41.739529, mean_q: 0.009870\n",
      " 3624/5000: episode: 3623, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.522 [0.002, 1.000], loss: 0.000060, mean_absolute_error: 41.746616, mean_q: 0.009779\n",
      " 3625/5000: episode: 3624, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.000, 0.913], loss: 0.000060, mean_absolute_error: 41.743530, mean_q: 0.009711\n",
      " 3626/5000: episode: 3625, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.016, 1.000], loss: 0.000062, mean_absolute_error: 41.741177, mean_q: 0.009719\n",
      " 3627/5000: episode: 3626, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.020, 1.000], loss: 0.000060, mean_absolute_error: 41.740364, mean_q: 0.009628\n",
      " 3628/5000: episode: 3627, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 0.000086, mean_absolute_error: 41.730240, mean_q: 0.009761\n",
      " 3629/5000: episode: 3628, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.585], loss: 0.000059, mean_absolute_error: 41.741364, mean_q: 0.009444\n",
      " 3630/5000: episode: 3629, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.374 [0.000, 0.954], loss: 0.000052, mean_absolute_error: 41.741188, mean_q: 0.009017\n",
      " 3631/5000: episode: 3630, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.672], loss: 0.000059, mean_absolute_error: 41.739120, mean_q: 0.009365\n",
      " 3632/5000: episode: 3631, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.657 [0.071, 1.000], loss: 0.000064, mean_absolute_error: 41.742249, mean_q: 0.009541\n",
      " 3633/5000: episode: 3632, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 0.000052, mean_absolute_error: 41.749111, mean_q: 0.009170\n",
      " 3634/5000: episode: 3633, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.638 [0.055, 1.000], loss: 0.000057, mean_absolute_error: 41.742668, mean_q: 0.009176\n",
      " 3635/5000: episode: 3634, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.825], loss: 0.000054, mean_absolute_error: 41.739029, mean_q: 0.009087\n",
      " 3636/5000: episode: 3635, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 0.000053, mean_absolute_error: 41.739620, mean_q: 0.009033\n",
      " 3637/5000: episode: 3636, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 0.000052, mean_absolute_error: 41.743958, mean_q: 0.008952\n",
      " 3638/5000: episode: 3637, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.000, 0.840], loss: 0.000049, mean_absolute_error: 41.749329, mean_q: 0.008860\n",
      " 3639/5000: episode: 3638, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.813], loss: 0.000048, mean_absolute_error: 41.751945, mean_q: 0.008771\n",
      " 3640/5000: episode: 3639, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.000, 0.977], loss: 0.000049, mean_absolute_error: 41.746620, mean_q: 0.008794\n",
      " 3641/5000: episode: 3640, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.000050, mean_absolute_error: 41.744419, mean_q: 0.008743\n",
      " 3642/5000: episode: 3641, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 0.000050, mean_absolute_error: 41.739464, mean_q: 0.008779\n",
      " 3643/5000: episode: 3642, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.418, 1.000], loss: 0.000063, mean_absolute_error: 41.741493, mean_q: 0.008762\n",
      " 3644/5000: episode: 3643, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.000047, mean_absolute_error: 41.745285, mean_q: 0.008550\n",
      " 3645/5000: episode: 3644, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.416, 1.000], loss: 0.000048, mean_absolute_error: 41.741356, mean_q: 0.008526\n",
      " 3646/5000: episode: 3645, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.492], loss: 0.000052, mean_absolute_error: 41.737411, mean_q: 0.008472\n",
      " 3647/5000: episode: 3646, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 0.000047, mean_absolute_error: 41.741791, mean_q: 0.008365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3648/5000: episode: 3647, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.244, 1.000], loss: 0.000046, mean_absolute_error: 41.744431, mean_q: 0.008341\n",
      " 3649/5000: episode: 3648, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.720], loss: 0.000044, mean_absolute_error: 41.745159, mean_q: 0.008241\n",
      " 3650/5000: episode: 3649, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.608 [0.034, 1.000], loss: 0.000045, mean_absolute_error: 41.741150, mean_q: 0.008261\n",
      " 3651/5000: episode: 3650, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.000, 0.945], loss: 0.000045, mean_absolute_error: 41.740517, mean_q: 0.008171\n",
      " 3652/5000: episode: 3651, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.177, 1.000], loss: 0.000047, mean_absolute_error: 41.739693, mean_q: 0.008169\n",
      " 3653/5000: episode: 3652, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.954], loss: 0.000042, mean_absolute_error: 41.744259, mean_q: 0.007995\n",
      " 3654/5000: episode: 3653, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.168, 1.000], loss: 0.000047, mean_absolute_error: 41.738014, mean_q: 0.007995\n",
      " 3655/5000: episode: 3654, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.570], loss: 0.000043, mean_absolute_error: 41.744202, mean_q: 0.007953\n",
      " 3656/5000: episode: 3655, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.582 [0.020, 1.000], loss: 0.000044, mean_absolute_error: 41.738239, mean_q: 0.007818\n",
      " 3657/5000: episode: 3656, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 0.000043, mean_absolute_error: 41.743279, mean_q: 0.007860\n",
      " 3658/5000: episode: 3657, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.166, 1.000], loss: 0.000040, mean_absolute_error: 41.742821, mean_q: 0.007745\n",
      " 3659/5000: episode: 3658, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.468 [0.000, 0.996], loss: 0.000043, mean_absolute_error: 41.738827, mean_q: 0.007809\n",
      " 3660/5000: episode: 3659, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.477, 1.000], loss: 0.000040, mean_absolute_error: 41.741745, mean_q: 0.007716\n",
      " 3661/5000: episode: 3660, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 0.000040, mean_absolute_error: 41.737946, mean_q: 0.007577\n",
      " 3662/5000: episode: 3661, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 0.000038, mean_absolute_error: 41.744015, mean_q: 0.007519\n",
      " 3663/5000: episode: 3662, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.632 [0.050, 1.000], loss: 0.000038, mean_absolute_error: 41.741253, mean_q: 0.007538\n",
      " 3664/5000: episode: 3663, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.347, 1.000], loss: 0.000039, mean_absolute_error: 41.738552, mean_q: 0.007514\n",
      " 3665/5000: episode: 3664, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 0.000039, mean_absolute_error: 41.737160, mean_q: 0.007492\n",
      " 3666/5000: episode: 3665, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 0.000038, mean_absolute_error: 41.737228, mean_q: 0.007406\n",
      " 3667/5000: episode: 3666, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.203, 1.000], loss: 0.000038, mean_absolute_error: 41.738453, mean_q: 0.007346\n",
      " 3668/5000: episode: 3667, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.193, 1.000], loss: 0.000036, mean_absolute_error: 41.744820, mean_q: 0.007234\n",
      " 3669/5000: episode: 3668, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.237, 1.000], loss: 0.000034, mean_absolute_error: 41.740059, mean_q: 0.006935\n",
      " 3670/5000: episode: 3669, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 0.000037, mean_absolute_error: 41.740398, mean_q: 0.007159\n",
      " 3671/5000: episode: 3670, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.288 [0.000, 0.865], loss: 0.000035, mean_absolute_error: 41.740860, mean_q: 0.007116\n",
      " 3672/5000: episode: 3671, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 0.000038, mean_absolute_error: 41.732468, mean_q: 0.007135\n",
      " 3673/5000: episode: 3672, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.477, 1.000], loss: 0.000034, mean_absolute_error: 41.741596, mean_q: 0.007004\n",
      " 3674/5000: episode: 3673, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.478 [0.000, 0.998], loss: 0.000034, mean_absolute_error: 41.741653, mean_q: 0.006949\n",
      " 3675/5000: episode: 3674, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.000, 0.851], loss: 0.000032, mean_absolute_error: 41.744576, mean_q: 0.006865\n",
      " 3676/5000: episode: 3675, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.647], loss: 0.000037, mean_absolute_error: 41.734390, mean_q: 0.006868\n",
      " 3677/5000: episode: 3676, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.267 [0.000, 0.831], loss: 0.000060, mean_absolute_error: 41.728783, mean_q: 0.007163\n",
      " 3678/5000: episode: 3677, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.413 [0.000, 0.977], loss: 0.000031, mean_absolute_error: 41.743496, mean_q: 0.006714\n",
      " 3679/5000: episode: 3678, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.053, 1.000], loss: 0.000037, mean_absolute_error: 41.733181, mean_q: 0.006761\n",
      " 3680/5000: episode: 3679, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.370, 1.000], loss: 0.000032, mean_absolute_error: 41.740105, mean_q: 0.006656\n",
      " 3681/5000: episode: 3680, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 0.868], loss: 0.000030, mean_absolute_error: 41.744747, mean_q: 0.006570\n",
      " 3682/5000: episode: 3681, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.578], loss: 0.000034, mean_absolute_error: 41.736073, mean_q: 0.006640\n",
      " 3683/5000: episode: 3682, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.000, 0.883], loss: 0.000029, mean_absolute_error: 41.745872, mean_q: 0.006473\n",
      " 3684/5000: episode: 3683, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.543 [0.006, 1.000], loss: 0.000030, mean_absolute_error: 41.741432, mean_q: 0.006478\n",
      " 3685/5000: episode: 3684, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.364, 1.000], loss: 0.000031, mean_absolute_error: 41.738472, mean_q: 0.006428\n",
      " 3686/5000: episode: 3685, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.138, 1.000], loss: 0.000028, mean_absolute_error: 41.744282, mean_q: 0.006390\n",
      " 3687/5000: episode: 3686, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.687 [0.102, 1.000], loss: 0.000032, mean_absolute_error: 41.733864, mean_q: 0.006084\n",
      " 3688/5000: episode: 3687, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.419, 1.000], loss: 0.000029, mean_absolute_error: 41.738724, mean_q: 0.006246\n",
      " 3689/5000: episode: 3688, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.529 [0.003, 1.000], loss: 0.000035, mean_absolute_error: 41.737926, mean_q: 0.006341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3690/5000: episode: 3689, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.544 [0.007, 1.000], loss: 0.000030, mean_absolute_error: 41.740135, mean_q: 0.006214\n",
      " 3691/5000: episode: 3690, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.601], loss: 0.000026, mean_absolute_error: 41.747040, mean_q: 0.006036\n",
      " 3692/5000: episode: 3691, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.150, 1.000], loss: 0.000030, mean_absolute_error: 41.740025, mean_q: 0.006078\n",
      " 3693/5000: episode: 3692, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.506 [0.000, 1.000], loss: 0.000025, mean_absolute_error: 41.744392, mean_q: 0.005953\n",
      " 3694/5000: episode: 3693, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.000, 0.849], loss: 0.000026, mean_absolute_error: 41.742970, mean_q: 0.005965\n",
      " 3695/5000: episode: 3694, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.000, 0.790], loss: 0.000027, mean_absolute_error: 41.738194, mean_q: 0.005906\n",
      " 3696/5000: episode: 3695, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.000, 0.999], loss: 0.000025, mean_absolute_error: 41.744156, mean_q: 0.005854\n",
      " 3697/5000: episode: 3696, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.303, 1.000], loss: 0.000027, mean_absolute_error: 41.737328, mean_q: 0.005890\n",
      " 3698/5000: episode: 3697, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.168, 1.000], loss: 0.000025, mean_absolute_error: 41.741180, mean_q: 0.005785\n",
      " 3699/5000: episode: 3698, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.138, 1.000], loss: 0.000178, mean_absolute_error: 41.709522, mean_q: 0.006227\n",
      " 3700/5000: episode: 3699, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.133, 1.000], loss: 0.000022, mean_absolute_error: 41.748817, mean_q: 0.005595\n",
      " 3701/5000: episode: 3700, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.265, 1.000], loss: 0.000025, mean_absolute_error: 41.739418, mean_q: 0.005699\n",
      " 3702/5000: episode: 3701, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.683 [0.098, 1.000], loss: 0.000027, mean_absolute_error: 41.737751, mean_q: 0.005668\n",
      " 3703/5000: episode: 3702, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.575 [0.017, 1.000], loss: 0.000026, mean_absolute_error: 41.734482, mean_q: 0.005549\n",
      " 3704/5000: episode: 3703, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.248, 1.000], loss: 0.000024, mean_absolute_error: 41.738701, mean_q: 0.005479\n",
      " 3705/5000: episode: 3704, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 0.000022, mean_absolute_error: 41.743614, mean_q: 0.005410\n",
      " 3706/5000: episode: 3705, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.503], loss: 0.000022, mean_absolute_error: 41.744820, mean_q: 0.005384\n",
      " 3707/5000: episode: 3706, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.476, 1.000], loss: 0.000024, mean_absolute_error: 41.740524, mean_q: 0.005380\n",
      " 3708/5000: episode: 3707, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.638 [0.055, 1.000], loss: 0.000023, mean_absolute_error: 41.737923, mean_q: 0.005364\n",
      " 3709/5000: episode: 3708, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.421, 1.000], loss: 0.000024, mean_absolute_error: 41.738213, mean_q: 0.005315\n",
      " 3710/5000: episode: 3709, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.464, 1.000], loss: 0.000020, mean_absolute_error: 41.744732, mean_q: 0.005168\n",
      " 3711/5000: episode: 3710, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 0.000021, mean_absolute_error: 41.742302, mean_q: 0.005160\n",
      " 3712/5000: episode: 3711, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.000, 0.820], loss: 0.000021, mean_absolute_error: 41.743729, mean_q: 0.005126\n",
      " 3713/5000: episode: 3712, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.563], loss: 0.000020, mean_absolute_error: 41.742432, mean_q: 0.005082\n",
      " 3714/5000: episode: 3713, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 0.000024, mean_absolute_error: 41.735405, mean_q: 0.005165\n",
      " 3715/5000: episode: 3714, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.634], loss: 0.000020, mean_absolute_error: 41.741623, mean_q: 0.005039\n",
      " 3716/5000: episode: 3715, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.032, 1.000], loss: 0.000023, mean_absolute_error: 41.737747, mean_q: 0.004973\n",
      " 3717/5000: episode: 3716, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.199, 1.000], loss: 0.000022, mean_absolute_error: 41.735153, mean_q: 0.004929\n",
      " 3718/5000: episode: 3717, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.014, 1.000], loss: 0.000030, mean_absolute_error: 41.736404, mean_q: 0.005015\n",
      " 3719/5000: episode: 3718, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.423, 1.000], loss: 0.000023, mean_absolute_error: 41.738346, mean_q: 0.004849\n",
      " 3720/5000: episode: 3719, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.388, 1.000], loss: 0.000020, mean_absolute_error: 41.737030, mean_q: 0.004816\n",
      " 3721/5000: episode: 3720, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.013, 1.000], loss: 0.000021, mean_absolute_error: 41.731560, mean_q: 0.004815\n",
      " 3722/5000: episode: 3721, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.180, 1.000], loss: 0.000019, mean_absolute_error: 41.738014, mean_q: 0.004740\n",
      " 3723/5000: episode: 3722, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.392 [0.000, 0.966], loss: 0.000021, mean_absolute_error: 41.734505, mean_q: 0.004734\n",
      " 3724/5000: episode: 3723, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.000, 0.993], loss: 0.000019, mean_absolute_error: 41.738335, mean_q: 0.004659\n",
      " 3725/5000: episode: 3724, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.391, 1.000], loss: 0.000020, mean_absolute_error: 41.736618, mean_q: 0.004609\n",
      " 3726/5000: episode: 3725, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.000017, mean_absolute_error: 41.740498, mean_q: 0.004559\n",
      " 3727/5000: episode: 3726, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.420, 1.000], loss: 0.000016, mean_absolute_error: 41.743107, mean_q: 0.004471\n",
      " 3728/5000: episode: 3727, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.198, 1.000], loss: 0.000021, mean_absolute_error: 41.730591, mean_q: 0.004498\n",
      " 3729/5000: episode: 3728, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.013, 1.000], loss: 0.000015, mean_absolute_error: 41.746239, mean_q: 0.004395\n",
      " 3730/5000: episode: 3729, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.253, 1.000], loss: 0.000019, mean_absolute_error: 41.737972, mean_q: 0.004437\n",
      " 3731/5000: episode: 3730, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.000, 0.816], loss: 0.000019, mean_absolute_error: 41.740952, mean_q: 0.004380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3732/5000: episode: 3731, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 0.000016, mean_absolute_error: 41.738289, mean_q: 0.004322\n",
      " 3733/5000: episode: 3732, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.334 [0.000, 0.920], loss: 0.000031, mean_absolute_error: 41.729950, mean_q: 0.004495\n",
      " 3734/5000: episode: 3733, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.556], loss: 0.000020, mean_absolute_error: 41.735676, mean_q: 0.004374\n",
      " 3735/5000: episode: 3734, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.320 [0.000, 0.905], loss: 0.000015, mean_absolute_error: 41.742027, mean_q: 0.004170\n",
      " 3736/5000: episode: 3735, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.331, 1.000], loss: 0.000015, mean_absolute_error: 41.744118, mean_q: 0.004182\n",
      " 3737/5000: episode: 3736, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.861], loss: 0.000015, mean_absolute_error: 41.741890, mean_q: 0.004148\n",
      " 3738/5000: episode: 3737, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.596], loss: 0.000017, mean_absolute_error: 41.739361, mean_q: 0.004175\n",
      " 3739/5000: episode: 3738, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.280, 1.000], loss: 0.000018, mean_absolute_error: 41.737064, mean_q: 0.004147\n",
      " 3740/5000: episode: 3739, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.666], loss: 0.000015, mean_absolute_error: 41.742828, mean_q: 0.004100\n",
      " 3741/5000: episode: 3740, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.151, 1.000], loss: 0.000019, mean_absolute_error: 41.735447, mean_q: 0.004083\n",
      " 3742/5000: episode: 3741, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.154, 1.000], loss: 0.000014, mean_absolute_error: 41.742893, mean_q: 0.004026\n",
      " 3743/5000: episode: 3742, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.571], loss: 0.000014, mean_absolute_error: 41.742867, mean_q: 0.004003\n",
      " 3744/5000: episode: 3743, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.285, 1.000], loss: 0.000015, mean_absolute_error: 41.738548, mean_q: 0.003973\n",
      " 3745/5000: episode: 3744, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.000014, mean_absolute_error: 41.743393, mean_q: 0.003863\n",
      " 3746/5000: episode: 3745, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 0.000017, mean_absolute_error: 41.735558, mean_q: 0.003944\n",
      " 3747/5000: episode: 3746, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.747], loss: 0.000013, mean_absolute_error: 41.744183, mean_q: 0.003852\n",
      " 3748/5000: episode: 3747, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.748], loss: 0.000013, mean_absolute_error: 41.743099, mean_q: 0.003802\n",
      " 3749/5000: episode: 3748, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 0.000013, mean_absolute_error: 41.744896, mean_q: 0.003800\n",
      " 3750/5000: episode: 3749, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.707], loss: 0.000032, mean_absolute_error: 41.732246, mean_q: 0.003766\n",
      " 3751/5000: episode: 3750, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 0.000022, mean_absolute_error: 41.734158, mean_q: 0.004025\n",
      " 3752/5000: episode: 3751, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.323, 1.000], loss: 0.000011, mean_absolute_error: 41.745644, mean_q: 0.003662\n",
      " 3753/5000: episode: 3752, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.614], loss: 0.000016, mean_absolute_error: 41.736202, mean_q: 0.003666\n",
      " 3754/5000: episode: 3753, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.331 [0.000, 0.917], loss: 0.000013, mean_absolute_error: 41.740273, mean_q: 0.003624\n",
      " 3755/5000: episode: 3754, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.767], loss: 0.000013, mean_absolute_error: 41.737038, mean_q: 0.003580\n",
      " 3756/5000: episode: 3755, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.030, 1.000], loss: 0.000013, mean_absolute_error: 41.738720, mean_q: 0.003569\n",
      " 3757/5000: episode: 3756, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.031, 1.000], loss: 0.000011, mean_absolute_error: 41.747147, mean_q: 0.003474\n",
      " 3758/5000: episode: 3757, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 0.000013, mean_absolute_error: 41.737740, mean_q: 0.003539\n",
      " 3759/5000: episode: 3758, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.020, 1.000], loss: 0.000015, mean_absolute_error: 41.736721, mean_q: 0.003550\n",
      " 3760/5000: episode: 3759, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.279, 1.000], loss: 0.000012, mean_absolute_error: 41.743370, mean_q: 0.003422\n",
      " 3761/5000: episode: 3760, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.081, 1.000], loss: 0.000014, mean_absolute_error: 41.732277, mean_q: 0.003461\n",
      " 3762/5000: episode: 3761, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.417, 1.000], loss: 0.000012, mean_absolute_error: 41.739861, mean_q: 0.003389\n",
      " 3763/5000: episode: 3762, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.519], loss: 0.000011, mean_absolute_error: 41.745342, mean_q: 0.003288\n",
      " 3764/5000: episode: 3763, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 0.000011, mean_absolute_error: 41.741280, mean_q: 0.003361\n",
      " 3765/5000: episode: 3764, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.076, 1.000], loss: 0.000012, mean_absolute_error: 41.740154, mean_q: 0.003275\n",
      " 3766/5000: episode: 3765, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.623 [0.044, 1.000], loss: 0.000011, mean_absolute_error: 41.738640, mean_q: 0.003273\n",
      " 3767/5000: episode: 3766, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.350, 1.000], loss: 0.000018, mean_absolute_error: 41.735241, mean_q: 0.003349\n",
      " 3768/5000: episode: 3767, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.583], loss: 0.000012, mean_absolute_error: 41.738503, mean_q: 0.003262\n",
      " 3769/5000: episode: 3768, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 0.000011, mean_absolute_error: 41.739204, mean_q: 0.003148\n",
      " 3770/5000: episode: 3769, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.433, 1.000], loss: 0.000011, mean_absolute_error: 41.738838, mean_q: 0.003134\n",
      " 3771/5000: episode: 3770, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.438 [0.000, 0.988], loss: 0.000012, mean_absolute_error: 41.740490, mean_q: 0.003179\n",
      " 3772/5000: episode: 3771, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.126, 1.000], loss: 0.000012, mean_absolute_error: 41.736320, mean_q: 0.003105\n",
      " 3773/5000: episode: 3772, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.000, 0.931], loss: 0.000011, mean_absolute_error: 41.742409, mean_q: 0.003098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3774/5000: episode: 3773, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.271, 1.000], loss: 0.000011, mean_absolute_error: 41.738457, mean_q: 0.003108\n",
      " 3775/5000: episode: 3774, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.753], loss: 0.000009, mean_absolute_error: 41.745049, mean_q: 0.003023\n",
      " 3776/5000: episode: 3775, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.000, 0.937], loss: 0.000011, mean_absolute_error: 41.738586, mean_q: 0.002997\n",
      " 3777/5000: episode: 3776, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.533 [0.004, 1.000], loss: 0.000012, mean_absolute_error: 41.737259, mean_q: 0.003061\n",
      " 3778/5000: episode: 3777, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.000, 0.903], loss: 0.000010, mean_absolute_error: 41.742424, mean_q: 0.002886\n",
      " 3779/5000: episode: 3778, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.352, 1.000], loss: 0.000014, mean_absolute_error: 41.733025, mean_q: 0.002960\n",
      " 3780/5000: episode: 3779, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.041, 1.000], loss: 0.000012, mean_absolute_error: 41.736343, mean_q: 0.002876\n",
      " 3781/5000: episode: 3780, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.430 [0.000, 0.985], loss: 0.000011, mean_absolute_error: 41.737526, mean_q: 0.002906\n",
      " 3782/5000: episode: 3781, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.344, 1.000], loss: 0.000009, mean_absolute_error: 41.741478, mean_q: 0.002860\n",
      " 3783/5000: episode: 3782, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.462, 1.000], loss: 0.000015, mean_absolute_error: 41.734123, mean_q: 0.002820\n",
      " 3784/5000: episode: 3783, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.002, 1.000], loss: 0.000011, mean_absolute_error: 41.735970, mean_q: 0.002853\n",
      " 3785/5000: episode: 3784, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.186, 1.000], loss: 0.000008, mean_absolute_error: 41.745289, mean_q: 0.002743\n",
      " 3786/5000: episode: 3785, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.274 [0.000, 0.843], loss: 0.000008, mean_absolute_error: 41.744705, mean_q: 0.002741\n",
      " 3787/5000: episode: 3786, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.192, 1.000], loss: 0.000008, mean_absolute_error: 41.744556, mean_q: 0.002692\n",
      " 3788/5000: episode: 3787, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.000, 0.996], loss: 0.000010, mean_absolute_error: 41.739273, mean_q: 0.002719\n",
      " 3789/5000: episode: 3788, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.000, 0.948], loss: 0.000010, mean_absolute_error: 41.737968, mean_q: 0.002724\n",
      " 3790/5000: episode: 3789, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.912], loss: 0.000008, mean_absolute_error: 41.741631, mean_q: 0.002590\n",
      " 3791/5000: episode: 3790, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.000, 0.921], loss: 0.000010, mean_absolute_error: 41.740650, mean_q: 0.002608\n",
      " 3792/5000: episode: 3791, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.488], loss: 0.000010, mean_absolute_error: 41.736259, mean_q: 0.002612\n",
      " 3793/5000: episode: 3792, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.060, 1.000], loss: 0.000008, mean_absolute_error: 41.744717, mean_q: 0.002555\n",
      " 3794/5000: episode: 3793, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.569], loss: 0.000010, mean_absolute_error: 41.738670, mean_q: 0.002573\n",
      " 3795/5000: episode: 3794, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.000009, mean_absolute_error: 41.739285, mean_q: 0.002519\n",
      " 3796/5000: episode: 3795, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.443, 1.000], loss: 0.000010, mean_absolute_error: 41.735886, mean_q: 0.002530\n",
      " 3797/5000: episode: 3796, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.406, 1.000], loss: 0.000007, mean_absolute_error: 41.745266, mean_q: 0.002429\n",
      " 3798/5000: episode: 3797, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.780], loss: 0.000007, mean_absolute_error: 41.743961, mean_q: 0.002429\n",
      " 3799/5000: episode: 3798, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.908], loss: 0.000008, mean_absolute_error: 41.741806, mean_q: 0.002441\n",
      " 3800/5000: episode: 3799, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.687 [0.103, 1.000], loss: 0.000009, mean_absolute_error: 41.732635, mean_q: 0.002422\n",
      " 3801/5000: episode: 3800, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.000, 0.879], loss: 0.000006, mean_absolute_error: 41.747639, mean_q: 0.002320\n",
      " 3802/5000: episode: 3801, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.592], loss: 0.000024, mean_absolute_error: 41.725838, mean_q: 0.002477\n",
      " 3803/5000: episode: 3802, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.088, 1.000], loss: 0.000089, mean_absolute_error: 41.722248, mean_q: 0.003304\n",
      " 3804/5000: episode: 3803, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.517 [0.001, 1.000], loss: 0.000008, mean_absolute_error: 41.739681, mean_q: 0.002267\n",
      " 3805/5000: episode: 3804, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.001, 1.000], loss: 0.000008, mean_absolute_error: 41.740952, mean_q: 0.002279\n",
      " 3806/5000: episode: 3805, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.000, 0.847], loss: 0.000008, mean_absolute_error: 41.741371, mean_q: 0.002257\n",
      " 3807/5000: episode: 3806, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.499], loss: 0.000007, mean_absolute_error: 41.744331, mean_q: 0.002205\n",
      " 3808/5000: episode: 3807, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.504, 1.000], loss: 0.000035, mean_absolute_error: 41.721592, mean_q: 0.002526\n",
      " 3809/5000: episode: 3808, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.474, 1.000], loss: 0.000012, mean_absolute_error: 41.736309, mean_q: 0.002135\n",
      " 3810/5000: episode: 3809, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.000, 0.826], loss: 0.000009, mean_absolute_error: 41.736897, mean_q: 0.002246\n",
      " 3811/5000: episode: 3810, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.000, 0.987], loss: 0.000008, mean_absolute_error: 41.736969, mean_q: 0.002167\n",
      " 3812/5000: episode: 3811, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.316, 1.000], loss: 0.000006, mean_absolute_error: 41.742573, mean_q: 0.002080\n",
      " 3813/5000: episode: 3812, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.937], loss: 0.000014, mean_absolute_error: 41.728806, mean_q: 0.002223\n",
      " 3814/5000: episode: 3813, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 0.000006, mean_absolute_error: 41.741962, mean_q: 0.002071\n",
      " 3815/5000: episode: 3814, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.000, 0.827], loss: 0.000012, mean_absolute_error: 41.730515, mean_q: 0.002173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3816/5000: episode: 3815, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.283 [0.000, 0.858], loss: 0.000008, mean_absolute_error: 41.739662, mean_q: 0.001935\n",
      " 3817/5000: episode: 3816, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.502], loss: 0.000008, mean_absolute_error: 41.737114, mean_q: 0.002033\n",
      " 3818/5000: episode: 3817, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.077, 1.000], loss: 0.000007, mean_absolute_error: 41.739403, mean_q: 0.001999\n",
      " 3819/5000: episode: 3818, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.246, 1.000], loss: 0.000007, mean_absolute_error: 41.736252, mean_q: 0.001969\n",
      " 3820/5000: episode: 3819, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.000, 0.988], loss: 0.000006, mean_absolute_error: 41.740532, mean_q: 0.001915\n",
      " 3821/5000: episode: 3820, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.000005, mean_absolute_error: 41.744545, mean_q: 0.001844\n",
      " 3822/5000: episode: 3821, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.000, 0.818], loss: 0.000008, mean_absolute_error: 41.736317, mean_q: 0.001954\n",
      " 3823/5000: episode: 3822, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.006, 1.000], loss: 0.000007, mean_absolute_error: 41.732780, mean_q: 0.001791\n",
      " 3824/5000: episode: 3823, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.878], loss: 0.000006, mean_absolute_error: 41.741634, mean_q: 0.001748\n",
      " 3825/5000: episode: 3824, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.701], loss: 0.000006, mean_absolute_error: 41.739063, mean_q: 0.001644\n",
      " 3826/5000: episode: 3825, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.670], loss: 0.000009, mean_absolute_error: 41.730762, mean_q: 0.001793\n",
      " 3827/5000: episode: 3826, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.695 [0.112, 1.000], loss: 0.000005, mean_absolute_error: 41.742973, mean_q: 0.001708\n",
      " 3828/5000: episode: 3827, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.388 [0.000, 0.963], loss: 0.000008, mean_absolute_error: 41.734932, mean_q: 0.001754\n",
      " 3829/5000: episode: 3828, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 0.000009, mean_absolute_error: 41.735146, mean_q: 0.001698\n",
      " 3830/5000: episode: 3829, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.419, 1.000], loss: 0.000006, mean_absolute_error: 41.742752, mean_q: 0.001670\n",
      " 3831/5000: episode: 3830, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.443, 1.000], loss: 0.000008, mean_absolute_error: 41.735878, mean_q: 0.001676\n",
      " 3832/5000: episode: 3831, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.000, 0.999], loss: 0.000006, mean_absolute_error: 41.737846, mean_q: 0.001637\n",
      " 3833/5000: episode: 3832, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.588], loss: 0.000007, mean_absolute_error: 41.734459, mean_q: 0.001678\n",
      " 3834/5000: episode: 3833, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 0.000006, mean_absolute_error: 41.733898, mean_q: 0.001668\n",
      " 3835/5000: episode: 3834, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.706], loss: 0.000006, mean_absolute_error: 41.739540, mean_q: 0.001601\n",
      " 3836/5000: episode: 3835, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.000, 0.959], loss: 0.000005, mean_absolute_error: 41.739197, mean_q: 0.001549\n",
      " 3837/5000: episode: 3836, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.354 [0.000, 0.939], loss: 0.000006, mean_absolute_error: 41.739635, mean_q: 0.001505\n",
      " 3838/5000: episode: 3837, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 0.000006, mean_absolute_error: 41.737015, mean_q: 0.001438\n",
      " 3839/5000: episode: 3838, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.190, 1.000], loss: 0.000004, mean_absolute_error: 41.742729, mean_q: 0.001433\n",
      " 3840/5000: episode: 3839, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.000, 0.914], loss: 0.000009, mean_absolute_error: 41.732552, mean_q: 0.001539\n",
      " 3841/5000: episode: 3840, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.031, 1.000], loss: 0.000003, mean_absolute_error: 41.745438, mean_q: 0.001403\n",
      " 3842/5000: episode: 3841, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.660 [0.074, 1.000], loss: 0.000005, mean_absolute_error: 41.740093, mean_q: 0.001379\n",
      " 3843/5000: episode: 3842, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.012, 1.000], loss: 0.000006, mean_absolute_error: 41.731510, mean_q: 0.001294\n",
      " 3844/5000: episode: 3843, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 0.000004, mean_absolute_error: 41.740997, mean_q: 0.001371\n",
      " 3845/5000: episode: 3844, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.602], loss: 0.000006, mean_absolute_error: 41.738934, mean_q: 0.001391\n",
      " 3846/5000: episode: 3845, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.595 [0.026, 1.000], loss: 0.000005, mean_absolute_error: 41.742790, mean_q: 0.001271\n",
      " 3847/5000: episode: 3846, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.289, 1.000], loss: 0.000007, mean_absolute_error: 41.736816, mean_q: 0.001351\n",
      " 3848/5000: episode: 3847, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.007, 1.000], loss: 0.000004, mean_absolute_error: 41.742531, mean_q: 0.001314\n",
      " 3849/5000: episode: 3848, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.000, 0.851], loss: 0.000006, mean_absolute_error: 41.741261, mean_q: 0.001214\n",
      " 3850/5000: episode: 3849, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.482, 1.000], loss: 0.000005, mean_absolute_error: 41.740547, mean_q: 0.001263\n",
      " 3851/5000: episode: 3850, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.572 [0.016, 1.000], loss: 0.000004, mean_absolute_error: 41.737362, mean_q: 0.001220\n",
      " 3852/5000: episode: 3851, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.102, 1.000], loss: 0.000004, mean_absolute_error: 41.740646, mean_q: 0.001195\n",
      " 3853/5000: episode: 3852, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.957], loss: 0.000004, mean_absolute_error: 41.741577, mean_q: 0.001142\n",
      " 3854/5000: episode: 3853, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.630], loss: 0.000004, mean_absolute_error: 41.739532, mean_q: 0.001183\n",
      " 3855/5000: episode: 3854, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.522 [0.002, 1.000], loss: 0.000004, mean_absolute_error: 41.741684, mean_q: 0.001143\n",
      " 3856/5000: episode: 3855, duration: 0.016s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.000, 0.963], loss: 0.000009, mean_absolute_error: 41.736931, mean_q: 0.001352\n",
      " 3857/5000: episode: 3856, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.000, 0.956], loss: 0.000004, mean_absolute_error: 41.737377, mean_q: 0.001169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3858/5000: episode: 3857, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.386, 1.000], loss: 0.000004, mean_absolute_error: 41.740395, mean_q: 0.001122\n",
      " 3859/5000: episode: 3858, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.032, 1.000], loss: 0.000005, mean_absolute_error: 41.737015, mean_q: 0.001134\n",
      " 3860/5000: episode: 3859, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.067, 1.000], loss: 0.000008, mean_absolute_error: 41.731564, mean_q: 0.001148\n",
      " 3861/5000: episode: 3860, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.000, 0.958], loss: 0.000005, mean_absolute_error: 41.737328, mean_q: 0.001070\n",
      " 3862/5000: episode: 3861, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.000006, mean_absolute_error: 41.739388, mean_q: 0.001046\n",
      " 3863/5000: episode: 3862, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.411, 1.000], loss: 0.000005, mean_absolute_error: 41.739471, mean_q: 0.001050\n",
      " 3864/5000: episode: 3863, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.000, 0.893], loss: 0.000005, mean_absolute_error: 41.736313, mean_q: 0.001041\n",
      " 3865/5000: episode: 3864, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.782], loss: 0.000004, mean_absolute_error: 41.740574, mean_q: 0.000998\n",
      " 3866/5000: episode: 3865, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.804], loss: 0.000003, mean_absolute_error: 41.744392, mean_q: 0.000897\n",
      " 3867/5000: episode: 3866, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 0.000037, mean_absolute_error: 41.733025, mean_q: 0.001279\n",
      " 3868/5000: episode: 3867, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.760], loss: 0.000006, mean_absolute_error: 41.731392, mean_q: 0.001019\n",
      " 3869/5000: episode: 3868, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 0.000009, mean_absolute_error: 41.726654, mean_q: 0.001054\n",
      " 3870/5000: episode: 3869, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.352, 1.000], loss: 0.000007, mean_absolute_error: 41.738728, mean_q: 0.000919\n",
      " 3871/5000: episode: 3870, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.878], loss: 0.000003, mean_absolute_error: 41.742630, mean_q: 0.000872\n",
      " 3872/5000: episode: 3871, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.000, 0.992], loss: 0.000004, mean_absolute_error: 41.739555, mean_q: 0.000863\n",
      " 3873/5000: episode: 3872, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.000, 0.984], loss: 0.000003, mean_absolute_error: 41.742226, mean_q: 0.000822\n",
      " 3874/5000: episode: 3873, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.455 [0.000, 0.993], loss: 0.000002, mean_absolute_error: 41.743862, mean_q: 0.000820\n",
      " 3875/5000: episode: 3874, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.085, 1.000], loss: 0.000003, mean_absolute_error: 41.740685, mean_q: 0.000790\n",
      " 3876/5000: episode: 3875, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.221, 1.000], loss: 0.000006, mean_absolute_error: 41.733963, mean_q: 0.000847\n",
      " 3877/5000: episode: 3876, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.314, 1.000], loss: 0.000003, mean_absolute_error: 41.741184, mean_q: 0.000805\n",
      " 3878/5000: episode: 3877, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.472, 1.000], loss: 0.000005, mean_absolute_error: 41.738693, mean_q: 0.000770\n",
      " 3879/5000: episode: 3878, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.818], loss: 0.000004, mean_absolute_error: 41.741516, mean_q: 0.000732\n",
      " 3880/5000: episode: 3879, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.757], loss: 0.000004, mean_absolute_error: 41.742058, mean_q: 0.000739\n",
      " 3881/5000: episode: 3880, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.772 [0.251, 1.000], loss: 0.000005, mean_absolute_error: 41.737209, mean_q: 0.000733\n",
      " 3882/5000: episode: 3881, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.000003, mean_absolute_error: 41.738525, mean_q: 0.000718\n",
      " 3883/5000: episode: 3882, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.559 [0.011, 1.000], loss: 0.000002, mean_absolute_error: 41.743431, mean_q: 0.000668\n",
      " 3884/5000: episode: 3883, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.712], loss: 0.000006, mean_absolute_error: 41.735081, mean_q: 0.000741\n",
      " 3885/5000: episode: 3884, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.000, 0.785], loss: 0.000003, mean_absolute_error: 41.742626, mean_q: 0.000618\n",
      " 3886/5000: episode: 3885, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.000, 0.877], loss: 0.000005, mean_absolute_error: 41.735001, mean_q: 0.000639\n",
      " 3887/5000: episode: 3886, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.521], loss: 0.000013, mean_absolute_error: 41.730682, mean_q: 0.000853\n",
      " 3888/5000: episode: 3887, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.000, 0.956], loss: 0.000004, mean_absolute_error: 41.740463, mean_q: 0.000583\n",
      " 3889/5000: episode: 3888, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.440 [0.000, 0.989], loss: 0.000003, mean_absolute_error: 41.741234, mean_q: 0.000581\n",
      " 3890/5000: episode: 3889, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 0.000003, mean_absolute_error: 41.743492, mean_q: 0.000580\n",
      " 3891/5000: episode: 3890, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.000, 0.737], loss: 0.000004, mean_absolute_error: 41.734116, mean_q: 0.000641\n",
      " 3892/5000: episode: 3891, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.000, 0.921], loss: 0.000003, mean_absolute_error: 41.740662, mean_q: 0.000506\n",
      " 3893/5000: episode: 3892, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.718], loss: 0.000004, mean_absolute_error: 41.740448, mean_q: 0.000523\n",
      " 3894/5000: episode: 3893, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.133, 1.000], loss: 0.000004, mean_absolute_error: 41.738728, mean_q: 0.000522\n",
      " 3895/5000: episode: 3894, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.200, 1.000], loss: 0.000004, mean_absolute_error: 41.739746, mean_q: 0.000487\n",
      " 3896/5000: episode: 3895, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.321, 1.000], loss: 0.000002, mean_absolute_error: 41.743767, mean_q: 0.000466\n",
      " 3897/5000: episode: 3896, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.343, 1.000], loss: 0.000003, mean_absolute_error: 41.744041, mean_q: 0.000477\n",
      " 3898/5000: episode: 3897, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.572], loss: 0.000005, mean_absolute_error: 41.735107, mean_q: 0.000467\n",
      " 3899/5000: episode: 3898, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.000, 0.903], loss: 0.000006, mean_absolute_error: 41.736694, mean_q: 0.000503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3900/5000: episode: 3899, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.148, 1.000], loss: 0.000004, mean_absolute_error: 41.739811, mean_q: 0.000525\n",
      " 3901/5000: episode: 3900, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 0.000004, mean_absolute_error: 41.738266, mean_q: 0.000519\n",
      " 3902/5000: episode: 3901, duration: 0.016s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.735 [0.172, 1.000], loss: 0.000003, mean_absolute_error: 41.741692, mean_q: 0.000450\n",
      " 3903/5000: episode: 3902, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.000, 0.823], loss: 0.000005, mean_absolute_error: 41.739731, mean_q: 0.000438\n",
      " 3904/5000: episode: 3903, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.000, 0.970], loss: 0.000003, mean_absolute_error: 41.739033, mean_q: 0.000467\n",
      " 3905/5000: episode: 3904, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.515 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.743149, mean_q: 0.000393\n",
      " 3906/5000: episode: 3905, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.775 [0.260, 1.000], loss: 0.000006, mean_absolute_error: 41.732811, mean_q: 0.000443\n",
      " 3907/5000: episode: 3906, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.708], loss: 0.000003, mean_absolute_error: 41.743351, mean_q: 0.000383\n",
      " 3908/5000: episode: 3907, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.570 [0.015, 1.000], loss: 0.000004, mean_absolute_error: 41.737499, mean_q: 0.000450\n",
      " 3909/5000: episode: 3908, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.314, 1.000], loss: 0.000003, mean_absolute_error: 41.739182, mean_q: 0.000383\n",
      " 3910/5000: episode: 3909, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.000, 1.000], loss: 0.000005, mean_absolute_error: 41.732506, mean_q: 0.000386\n",
      " 3911/5000: episode: 3910, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.689], loss: 0.000003, mean_absolute_error: 41.739861, mean_q: 0.000369\n",
      " 3912/5000: episode: 3911, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.425, 1.000], loss: 0.000005, mean_absolute_error: 41.739098, mean_q: 0.000374\n",
      " 3913/5000: episode: 3912, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 0.000004, mean_absolute_error: 41.736500, mean_q: 0.000351\n",
      " 3914/5000: episode: 3913, duration: 0.022s, episode steps: 1, steps per second: 46, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.000, 0.885], loss: 0.000003, mean_absolute_error: 41.739487, mean_q: 0.000373\n",
      " 3915/5000: episode: 3914, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 0.000003, mean_absolute_error: 41.739143, mean_q: 0.000374\n",
      " 3916/5000: episode: 3915, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.053, 1.000], loss: 0.000003, mean_absolute_error: 41.738808, mean_q: 0.000320\n",
      " 3917/5000: episode: 3916, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.791], loss: 0.000003, mean_absolute_error: 41.738037, mean_q: 0.000349\n",
      " 3918/5000: episode: 3917, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.000, 0.993], loss: 0.000002, mean_absolute_error: 41.743713, mean_q: 0.000308\n",
      " 3919/5000: episode: 3918, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.297, 1.000], loss: 0.000002, mean_absolute_error: 41.740227, mean_q: 0.000282\n",
      " 3920/5000: episode: 3919, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.595 [0.026, 1.000], loss: 0.000004, mean_absolute_error: 41.739326, mean_q: 0.000327\n",
      " 3921/5000: episode: 3920, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.000, 0.992], loss: 0.000003, mean_absolute_error: 41.741402, mean_q: 0.000264\n",
      " 3922/5000: episode: 3921, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.413 [0.000, 0.977], loss: 0.000002, mean_absolute_error: 41.741760, mean_q: 0.000250\n",
      " 3923/5000: episode: 3922, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.161, 1.000], loss: 0.000003, mean_absolute_error: 41.740372, mean_q: 0.000270\n",
      " 3924/5000: episode: 3923, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.749], loss: 0.000004, mean_absolute_error: 41.736511, mean_q: 0.000329\n",
      " 3925/5000: episode: 3924, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 0.000003, mean_absolute_error: 41.741295, mean_q: 0.000300\n",
      " 3926/5000: episode: 3925, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.000, 0.996], loss: 0.000003, mean_absolute_error: 41.738728, mean_q: 0.000346\n",
      " 3927/5000: episode: 3926, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 0.000003, mean_absolute_error: 41.739086, mean_q: 0.000280\n",
      " 3928/5000: episode: 3927, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.033, 1.000], loss: 0.000003, mean_absolute_error: 41.740154, mean_q: 0.000248\n",
      " 3929/5000: episode: 3928, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.226, 1.000], loss: 0.000004, mean_absolute_error: 41.734627, mean_q: 0.000248\n",
      " 3930/5000: episode: 3929, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 0.000003, mean_absolute_error: 41.743118, mean_q: 0.000210\n",
      " 3931/5000: episode: 3930, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.510, 1.000], loss: 0.000004, mean_absolute_error: 41.739494, mean_q: 0.000237\n",
      " 3932/5000: episode: 3931, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 0.000004, mean_absolute_error: 41.735794, mean_q: 0.000260\n",
      " 3933/5000: episode: 3932, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.517], loss: 0.000002, mean_absolute_error: 41.742355, mean_q: 0.000185\n",
      " 3934/5000: episode: 3933, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.000, 0.949], loss: 0.000003, mean_absolute_error: 41.740318, mean_q: 0.000194\n",
      " 3935/5000: episode: 3934, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.283, 1.000], loss: 0.000004, mean_absolute_error: 41.739941, mean_q: 0.000232\n",
      " 3936/5000: episode: 3935, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.011, 1.000], loss: 0.000006, mean_absolute_error: 41.735054, mean_q: 0.000239\n",
      " 3937/5000: episode: 3936, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.453 [0.000, 0.993], loss: 0.000003, mean_absolute_error: 41.742504, mean_q: 0.000184\n",
      " 3938/5000: episode: 3937, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 0.000005, mean_absolute_error: 41.739372, mean_q: 0.000335\n",
      " 3939/5000: episode: 3938, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.400, 1.000], loss: 0.000002, mean_absolute_error: 41.744370, mean_q: 0.000098\n",
      " 3940/5000: episode: 3939, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.497, 1.000], loss: 0.000003, mean_absolute_error: 41.738068, mean_q: 0.000158\n",
      " 3941/5000: episode: 3940, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.238, 1.000], loss: 0.000070, mean_absolute_error: 41.740025, mean_q: 0.000879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3942/5000: episode: 3941, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.447 [0.000, 0.991], loss: 0.000002, mean_absolute_error: 41.741821, mean_q: 0.000098\n",
      " 3943/5000: episode: 3942, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.437, 1.000], loss: 0.000003, mean_absolute_error: 41.740108, mean_q: 0.000135\n",
      " 3944/5000: episode: 3943, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.479 [0.000, 0.998], loss: 0.000003, mean_absolute_error: 41.738235, mean_q: 0.000112\n",
      " 3945/5000: episode: 3944, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.943], loss: 0.000003, mean_absolute_error: 41.741386, mean_q: 0.000027\n",
      " 3946/5000: episode: 3945, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 0.868], loss: 0.000003, mean_absolute_error: 41.737621, mean_q: 0.000133\n",
      " 3947/5000: episode: 3946, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.010, 1.000], loss: 0.000002, mean_absolute_error: 41.744339, mean_q: 0.000074\n",
      " 3948/5000: episode: 3947, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.185, 1.000], loss: 0.000004, mean_absolute_error: 41.734394, mean_q: 0.000070\n",
      " 3949/5000: episode: 3948, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.559], loss: 0.000001, mean_absolute_error: 41.744778, mean_q: -0.000007\n",
      " 3950/5000: episode: 3949, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.598 [0.028, 1.000], loss: 0.000004, mean_absolute_error: 41.735413, mean_q: 0.000089\n",
      " 3951/5000: episode: 3950, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.033, 1.000], loss: 0.000004, mean_absolute_error: 41.738235, mean_q: -0.000127\n",
      " 3952/5000: episode: 3951, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.567], loss: 0.000002, mean_absolute_error: 41.741501, mean_q: -0.000008\n",
      " 3953/5000: episode: 3952, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.497, 1.000], loss: 0.000003, mean_absolute_error: 41.739189, mean_q: 0.000002\n",
      " 3954/5000: episode: 3953, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.047, 1.000], loss: 0.000006, mean_absolute_error: 41.732624, mean_q: 0.000137\n",
      " 3955/5000: episode: 3954, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.381, 1.000], loss: 0.000006, mean_absolute_error: 41.730278, mean_q: 0.000056\n",
      " 3956/5000: episode: 3955, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.509 [0.001, 1.000], loss: 0.000001, mean_absolute_error: 41.743370, mean_q: -0.000016\n",
      " 3957/5000: episode: 3956, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.863], loss: 0.000004, mean_absolute_error: 41.736298, mean_q: 0.000053\n",
      " 3958/5000: episode: 3957, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.622 [0.043, 1.000], loss: 0.000003, mean_absolute_error: 41.736725, mean_q: 0.000062\n",
      " 3959/5000: episode: 3958, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.000, 0.939], loss: 0.000011, mean_absolute_error: 41.726517, mean_q: -0.000274\n",
      " 3960/5000: episode: 3959, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.024, 1.000], loss: 0.000004, mean_absolute_error: 41.740299, mean_q: 0.000047\n",
      " 3961/5000: episode: 3960, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 0.000002, mean_absolute_error: 41.738792, mean_q: -0.000030\n",
      " 3962/5000: episode: 3961, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.000, 0.888], loss: 0.000002, mean_absolute_error: 41.740387, mean_q: -0.000019\n",
      " 3963/5000: episode: 3962, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.000, 0.696], loss: 0.000008, mean_absolute_error: 41.739239, mean_q: -0.000031\n",
      " 3964/5000: episode: 3963, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.619], loss: 0.000003, mean_absolute_error: 41.734821, mean_q: 0.000022\n",
      " 3965/5000: episode: 3964, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.742], loss: 0.000003, mean_absolute_error: 41.738430, mean_q: -0.000022\n",
      " 3966/5000: episode: 3965, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.565 [0.013, 1.000], loss: 0.000007, mean_absolute_error: 41.733932, mean_q: 0.000026\n",
      " 3967/5000: episode: 3966, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.548], loss: 0.000004, mean_absolute_error: 41.741211, mean_q: -0.000077\n",
      " 3968/5000: episode: 3967, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.000, 1.000], loss: 0.000006, mean_absolute_error: 41.732967, mean_q: -0.000015\n",
      " 3969/5000: episode: 3968, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.461, 1.000], loss: 0.000009, mean_absolute_error: 41.733810, mean_q: -0.000074\n",
      " 3970/5000: episode: 3969, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.507 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.743660, mean_q: -0.000125\n",
      " 3971/5000: episode: 3970, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.191, 1.000], loss: 0.000006, mean_absolute_error: 41.732292, mean_q: -0.000110\n",
      " 3972/5000: episode: 3971, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.283, 1.000], loss: 0.000001, mean_absolute_error: 41.743893, mean_q: -0.000162\n",
      " 3973/5000: episode: 3972, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.615], loss: 0.000017, mean_absolute_error: 41.732063, mean_q: -0.000040\n",
      " 3974/5000: episode: 3973, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 0.000002, mean_absolute_error: 41.742939, mean_q: -0.000136\n",
      " 3975/5000: episode: 3974, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.357 [0.000, 0.941], loss: 0.000005, mean_absolute_error: 41.740868, mean_q: -0.000194\n",
      " 3976/5000: episode: 3975, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.000, 0.884], loss: 0.000003, mean_absolute_error: 41.739662, mean_q: -0.000131\n",
      " 3977/5000: episode: 3976, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.133, 1.000], loss: 0.000002, mean_absolute_error: 41.739311, mean_q: -0.000170\n",
      " 3978/5000: episode: 3977, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.578], loss: 0.000002, mean_absolute_error: 41.742519, mean_q: -0.000156\n",
      " 3979/5000: episode: 3978, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.000, 0.963], loss: 0.000007, mean_absolute_error: 41.729008, mean_q: -0.000095\n",
      " 3980/5000: episode: 3979, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.747], loss: 0.000009, mean_absolute_error: 41.728260, mean_q: -0.000134\n",
      " 3981/5000: episode: 3980, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.787], loss: 0.000004, mean_absolute_error: 41.733818, mean_q: -0.000101\n",
      " 3982/5000: episode: 3981, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.292, 1.000], loss: 0.000002, mean_absolute_error: 41.739304, mean_q: -0.000181\n",
      " 3983/5000: episode: 3982, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.227, 1.000], loss: 0.000003, mean_absolute_error: 41.736168, mean_q: -0.000227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3984/5000: episode: 3983, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.004, 1.000], loss: 0.000002, mean_absolute_error: 41.741287, mean_q: -0.000198\n",
      " 3985/5000: episode: 3984, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.001, 1.000], loss: 0.000006, mean_absolute_error: 41.735813, mean_q: -0.000123\n",
      " 3986/5000: episode: 3985, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.001, 1.000], loss: 0.000001, mean_absolute_error: 41.742100, mean_q: -0.000239\n",
      " 3987/5000: episode: 3986, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.313, 1.000], loss: 0.000002, mean_absolute_error: 41.742741, mean_q: -0.000258\n",
      " 3988/5000: episode: 3987, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.214, 1.000], loss: 0.000002, mean_absolute_error: 41.742058, mean_q: -0.000223\n",
      " 3989/5000: episode: 3988, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.618 [0.040, 1.000], loss: 0.000004, mean_absolute_error: 41.738129, mean_q: -0.000174\n",
      " 3990/5000: episode: 3989, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.872], loss: 0.000003, mean_absolute_error: 41.737473, mean_q: -0.000191\n",
      " 3991/5000: episode: 3990, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.565 [0.013, 1.000], loss: 0.000006, mean_absolute_error: 41.736115, mean_q: -0.000175\n",
      " 3992/5000: episode: 3991, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.506, 1.000], loss: 0.000003, mean_absolute_error: 41.736679, mean_q: -0.000240\n",
      " 3993/5000: episode: 3992, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.128, 1.000], loss: 0.000001, mean_absolute_error: 41.742668, mean_q: -0.000241\n",
      " 3994/5000: episode: 3993, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.778 [0.266, 1.000], loss: 0.000001, mean_absolute_error: 41.744804, mean_q: -0.000334\n",
      " 3995/5000: episode: 3994, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 0.868], loss: 0.000013, mean_absolute_error: 41.726913, mean_q: -0.000221\n",
      " 3996/5000: episode: 3995, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.453 [0.000, 0.993], loss: 0.000005, mean_absolute_error: 41.736534, mean_q: -0.000261\n",
      " 3997/5000: episode: 3996, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.730 [0.163, 1.000], loss: 0.000001, mean_absolute_error: 41.742634, mean_q: -0.000284\n",
      " 3998/5000: episode: 3997, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.507 [0.001, 1.000], loss: 0.000003, mean_absolute_error: 41.737099, mean_q: -0.000290\n",
      " 3999/5000: episode: 3998, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.476 [0.000, 0.997], loss: 0.000003, mean_absolute_error: 41.740414, mean_q: -0.000328\n",
      " 4000/5000: episode: 3999, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.429, 1.000], loss: 0.000002, mean_absolute_error: 41.743202, mean_q: -0.000335\n",
      " 4001/5000: episode: 4000, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.370 [0.000, 0.951], loss: 0.000003, mean_absolute_error: 41.737881, mean_q: -0.000278\n",
      " 4002/5000: episode: 4001, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.391 [0.000, 0.966], loss: 0.000021, mean_absolute_error: 41.722305, mean_q: -0.000039\n",
      " 4003/5000: episode: 4002, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.000, 0.858], loss: 0.000002, mean_absolute_error: 41.740608, mean_q: -0.000337\n",
      " 4004/5000: episode: 4003, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.466 [0.000, 0.996], loss: 0.000003, mean_absolute_error: 41.740051, mean_q: -0.000364\n",
      " 4005/5000: episode: 4004, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.021, 1.000], loss: 0.000004, mean_absolute_error: 41.736809, mean_q: -0.000334\n",
      " 4006/5000: episode: 4005, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.863], loss: 0.000005, mean_absolute_error: 41.735916, mean_q: -0.000274\n",
      " 4007/5000: episode: 4006, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.000, 0.892], loss: 0.000003, mean_absolute_error: 41.741329, mean_q: -0.000360\n",
      " 4008/5000: episode: 4007, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.818], loss: 0.000004, mean_absolute_error: 41.734562, mean_q: -0.000376\n",
      " 4009/5000: episode: 4008, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.417 [0.000, 0.979], loss: 0.000006, mean_absolute_error: 41.736351, mean_q: -0.000354\n",
      " 4010/5000: episode: 4009, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.564], loss: 0.000003, mean_absolute_error: 41.739613, mean_q: -0.000369\n",
      " 4011/5000: episode: 4010, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.011, 1.000], loss: 0.000003, mean_absolute_error: 41.740562, mean_q: -0.000394\n",
      " 4012/5000: episode: 4011, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 0.000004, mean_absolute_error: 41.735271, mean_q: -0.000356\n",
      " 4013/5000: episode: 4012, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.393, 1.000], loss: 0.000001, mean_absolute_error: 41.741753, mean_q: -0.000412\n",
      " 4014/5000: episode: 4013, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.533], loss: 0.000004, mean_absolute_error: 41.731682, mean_q: -0.000259\n",
      " 4015/5000: episode: 4014, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.000, 0.830], loss: 0.000004, mean_absolute_error: 41.736729, mean_q: -0.000361\n",
      " 4016/5000: episode: 4015, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.000, 0.889], loss: 0.000003, mean_absolute_error: 41.737206, mean_q: -0.000344\n",
      " 4017/5000: episode: 4016, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.000, 0.830], loss: 0.000001, mean_absolute_error: 41.744423, mean_q: -0.000441\n",
      " 4018/5000: episode: 4017, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.052, 1.000], loss: 0.000007, mean_absolute_error: 41.727158, mean_q: -0.000521\n",
      " 4019/5000: episode: 4018, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.344, 1.000], loss: 0.000001, mean_absolute_error: 41.743176, mean_q: -0.000452\n",
      " 4020/5000: episode: 4019, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.023, 1.000], loss: 0.000010, mean_absolute_error: 41.734829, mean_q: -0.000183\n",
      " 4021/5000: episode: 4020, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.008, 1.000], loss: 0.000004, mean_absolute_error: 41.732265, mean_q: -0.000324\n",
      " 4022/5000: episode: 4021, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 0.000006, mean_absolute_error: 41.730354, mean_q: -0.000386\n",
      " 4023/5000: episode: 4022, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.357 [0.000, 0.941], loss: 0.000003, mean_absolute_error: 41.737419, mean_q: -0.000335\n",
      " 4024/5000: episode: 4023, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.906], loss: 0.000002, mean_absolute_error: 41.740780, mean_q: -0.000405\n",
      " 4025/5000: episode: 4024, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.055, 1.000], loss: 0.000004, mean_absolute_error: 41.737240, mean_q: -0.000375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4026/5000: episode: 4025, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.054, 1.000], loss: 0.000002, mean_absolute_error: 41.739449, mean_q: -0.000349\n",
      " 4027/5000: episode: 4026, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.565], loss: 0.000002, mean_absolute_error: 41.740841, mean_q: -0.000372\n",
      " 4028/5000: episode: 4027, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.891], loss: 0.000006, mean_absolute_error: 41.734879, mean_q: -0.000342\n",
      " 4029/5000: episode: 4028, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 0.000004, mean_absolute_error: 41.737358, mean_q: -0.000408\n",
      " 4030/5000: episode: 4029, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 0.000002, mean_absolute_error: 41.742470, mean_q: -0.000402\n",
      " 4031/5000: episode: 4030, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.000, 1.000], loss: 0.000003, mean_absolute_error: 41.736702, mean_q: -0.000367\n",
      " 4032/5000: episode: 4031, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.325 [0.000, 0.911], loss: 0.000002, mean_absolute_error: 41.739384, mean_q: -0.000402\n",
      " 4033/5000: episode: 4032, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.010, 1.000], loss: 0.000001, mean_absolute_error: 41.742294, mean_q: -0.000391\n",
      " 4034/5000: episode: 4033, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 0.000001, mean_absolute_error: 41.741001, mean_q: -0.000430\n",
      " 4035/5000: episode: 4034, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 0.000011, mean_absolute_error: 41.733055, mean_q: -0.000324\n",
      " 4036/5000: episode: 4035, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.312, 1.000], loss: 0.000003, mean_absolute_error: 41.735809, mean_q: -0.000403\n",
      " 4037/5000: episode: 4036, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.387, 1.000], loss: 0.000004, mean_absolute_error: 41.737679, mean_q: -0.000367\n",
      " 4038/5000: episode: 4037, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.599], loss: 0.000002, mean_absolute_error: 41.743279, mean_q: -0.000445\n",
      " 4039/5000: episode: 4038, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.410, 1.000], loss: 0.000003, mean_absolute_error: 41.737553, mean_q: -0.000386\n",
      " 4040/5000: episode: 4039, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.919], loss: 0.000008, mean_absolute_error: 41.727127, mean_q: -0.000260\n",
      " 4041/5000: episode: 4040, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.722], loss: 0.000002, mean_absolute_error: 41.740616, mean_q: -0.000450\n",
      " 4042/5000: episode: 4041, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.000, 0.977], loss: 0.000002, mean_absolute_error: 41.742340, mean_q: -0.000483\n",
      " 4043/5000: episode: 4042, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.619], loss: 0.000002, mean_absolute_error: 41.738136, mean_q: -0.000384\n",
      " 4044/5000: episode: 4043, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.552], loss: 0.000005, mean_absolute_error: 41.734077, mean_q: -0.000385\n",
      " 4045/5000: episode: 4044, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 0.000003, mean_absolute_error: 41.738991, mean_q: -0.000424\n",
      " 4046/5000: episode: 4045, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.433, 1.000], loss: 0.000002, mean_absolute_error: 41.741795, mean_q: -0.000470\n",
      " 4047/5000: episode: 4046, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.369, 1.000], loss: 0.000003, mean_absolute_error: 41.736984, mean_q: -0.000396\n",
      " 4048/5000: episode: 4047, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.590], loss: 0.000001, mean_absolute_error: 41.744125, mean_q: -0.000468\n",
      " 4049/5000: episode: 4048, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.503], loss: 0.000004, mean_absolute_error: 41.738312, mean_q: -0.000415\n",
      " 4050/5000: episode: 4049, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.115, 1.000], loss: 0.000001, mean_absolute_error: 41.742729, mean_q: -0.000453\n",
      " 4051/5000: episode: 4050, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.379, 1.000], loss: 0.000003, mean_absolute_error: 41.739262, mean_q: -0.000434\n",
      " 4052/5000: episode: 4051, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.000, 0.999], loss: 0.000004, mean_absolute_error: 41.728855, mean_q: -0.000345\n",
      " 4053/5000: episode: 4052, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.039, 1.000], loss: 0.000002, mean_absolute_error: 41.739029, mean_q: -0.000449\n",
      " 4054/5000: episode: 4053, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 0.000004, mean_absolute_error: 41.740040, mean_q: -0.000425\n",
      " 4055/5000: episode: 4054, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.143, 1.000], loss: 0.000003, mean_absolute_error: 41.741051, mean_q: -0.000466\n",
      " 4056/5000: episode: 4055, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.742], loss: 0.000005, mean_absolute_error: 41.733913, mean_q: -0.000332\n",
      " 4057/5000: episode: 4056, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.000, 0.977], loss: 0.000003, mean_absolute_error: 41.738701, mean_q: -0.000412\n",
      " 4058/5000: episode: 4057, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.464, 1.000], loss: 0.000003, mean_absolute_error: 41.738960, mean_q: -0.000418\n",
      " 4059/5000: episode: 4058, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.538], loss: 0.000003, mean_absolute_error: 41.743202, mean_q: -0.000462\n",
      " 4060/5000: episode: 4059, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.627], loss: 0.000001, mean_absolute_error: 41.742645, mean_q: -0.000468\n",
      " 4061/5000: episode: 4060, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.612 [0.036, 1.000], loss: 0.000007, mean_absolute_error: 41.734833, mean_q: -0.000434\n",
      " 4062/5000: episode: 4061, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.772], loss: 0.000005, mean_absolute_error: 41.735798, mean_q: -0.000428\n",
      " 4063/5000: episode: 4062, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.509], loss: 0.000002, mean_absolute_error: 41.737801, mean_q: -0.000435\n",
      " 4064/5000: episode: 4063, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.502], loss: 0.000005, mean_absolute_error: 41.733948, mean_q: -0.000417\n",
      " 4065/5000: episode: 4064, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.474 [0.000, 0.997], loss: 0.000002, mean_absolute_error: 41.743546, mean_q: -0.000534\n",
      " 4066/5000: episode: 4065, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.000, 0.851], loss: 0.000002, mean_absolute_error: 41.740967, mean_q: -0.000436\n",
      " 4067/5000: episode: 4066, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.000003, mean_absolute_error: 41.736347, mean_q: -0.000425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4068/5000: episode: 4067, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.324, 1.000], loss: 0.000002, mean_absolute_error: 41.737465, mean_q: -0.000453\n",
      " 4069/5000: episode: 4068, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.642], loss: 0.000007, mean_absolute_error: 41.729370, mean_q: -0.000461\n",
      " 4070/5000: episode: 4069, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.103, 1.000], loss: 0.000004, mean_absolute_error: 41.735565, mean_q: -0.000421\n",
      " 4071/5000: episode: 4070, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.689], loss: 0.000002, mean_absolute_error: 41.743439, mean_q: -0.000420\n",
      " 4072/5000: episode: 4071, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.294 [0.000, 0.874], loss: 0.000002, mean_absolute_error: 41.738754, mean_q: -0.000582\n",
      " 4073/5000: episode: 4072, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.771 [0.248, 1.000], loss: 0.000001, mean_absolute_error: 41.744652, mean_q: -0.000512\n",
      " 4074/5000: episode: 4073, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.528 [0.003, 1.000], loss: 0.000002, mean_absolute_error: 41.736835, mean_q: -0.000495\n",
      " 4075/5000: episode: 4074, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.871], loss: 0.000004, mean_absolute_error: 41.735432, mean_q: -0.000403\n",
      " 4076/5000: episode: 4075, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.564], loss: 0.000003, mean_absolute_error: 41.736588, mean_q: -0.000447\n",
      " 4077/5000: episode: 4076, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.444 [0.000, 0.990], loss: 0.000002, mean_absolute_error: 41.737896, mean_q: -0.000433\n",
      " 4078/5000: episode: 4077, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.410 [0.000, 0.976], loss: 0.000004, mean_absolute_error: 41.735779, mean_q: -0.000514\n",
      " 4079/5000: episode: 4078, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.000, 0.999], loss: 0.000002, mean_absolute_error: 41.738819, mean_q: -0.000464\n",
      " 4080/5000: episode: 4079, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.000, 0.646], loss: 0.000002, mean_absolute_error: 41.740330, mean_q: -0.000464\n",
      " 4081/5000: episode: 4080, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.592], loss: 0.000002, mean_absolute_error: 41.741310, mean_q: -0.000496\n",
      " 4082/5000: episode: 4081, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.000, 0.947], loss: 0.000004, mean_absolute_error: 41.741051, mean_q: -0.000507\n",
      " 4083/5000: episode: 4082, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.008, 1.000], loss: 0.000002, mean_absolute_error: 41.742004, mean_q: -0.000477\n",
      " 4084/5000: episode: 4083, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.317 [0.000, 0.902], loss: 0.000013, mean_absolute_error: 41.736485, mean_q: -0.000325\n",
      " 4085/5000: episode: 4084, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.760], loss: 0.000004, mean_absolute_error: 41.732349, mean_q: -0.000394\n",
      " 4086/5000: episode: 4085, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.000, 0.957], loss: 0.000003, mean_absolute_error: 41.736214, mean_q: -0.000483\n",
      " 4087/5000: episode: 4086, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.845], loss: 0.000002, mean_absolute_error: 41.735985, mean_q: -0.000511\n",
      " 4088/5000: episode: 4087, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.560], loss: 0.000002, mean_absolute_error: 41.741661, mean_q: -0.000504\n",
      " 4089/5000: episode: 4088, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.000, 0.892], loss: 0.000004, mean_absolute_error: 41.740791, mean_q: -0.000507\n",
      " 4090/5000: episode: 4089, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.000, 0.860], loss: 0.000003, mean_absolute_error: 41.737968, mean_q: -0.000319\n",
      " 4091/5000: episode: 4090, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.430 [0.000, 0.985], loss: 0.000002, mean_absolute_error: 41.741768, mean_q: -0.000518\n",
      " 4092/5000: episode: 4091, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.715], loss: 0.000003, mean_absolute_error: 41.738617, mean_q: -0.000433\n",
      " 4093/5000: episode: 4092, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.651], loss: 0.000004, mean_absolute_error: 41.735172, mean_q: -0.000446\n",
      " 4094/5000: episode: 4093, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.184, 1.000], loss: 0.000002, mean_absolute_error: 41.740082, mean_q: -0.000488\n",
      " 4095/5000: episode: 4094, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.960], loss: 0.000003, mean_absolute_error: 41.740326, mean_q: -0.000505\n",
      " 4096/5000: episode: 4095, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.666], loss: 0.000002, mean_absolute_error: 41.742470, mean_q: -0.000499\n",
      " 4097/5000: episode: 4096, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.045, 1.000], loss: 0.000004, mean_absolute_error: 41.734814, mean_q: -0.000383\n",
      " 4098/5000: episode: 4097, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.953], loss: 0.000028, mean_absolute_error: 41.727280, mean_q: -0.000254\n",
      " 4099/5000: episode: 4098, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.244, 1.000], loss: 0.000004, mean_absolute_error: 41.740570, mean_q: -0.000544\n",
      " 4100/5000: episode: 4099, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.366, 1.000], loss: 0.000003, mean_absolute_error: 41.735947, mean_q: -0.000565\n",
      " 4101/5000: episode: 4100, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.148, 1.000], loss: 0.000003, mean_absolute_error: 41.737846, mean_q: -0.000433\n",
      " 4102/5000: episode: 4101, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.484, 1.000], loss: 0.000003, mean_absolute_error: 41.735783, mean_q: -0.000440\n",
      " 4103/5000: episode: 4102, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.000, 1.000], loss: 0.000002, mean_absolute_error: 41.738228, mean_q: -0.000401\n",
      " 4104/5000: episode: 4103, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.589 [0.023, 1.000], loss: 0.000002, mean_absolute_error: 41.741516, mean_q: -0.000499\n",
      " 4105/5000: episode: 4104, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.174, 1.000], loss: 0.000004, mean_absolute_error: 41.735695, mean_q: -0.000520\n",
      " 4106/5000: episode: 4105, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.490, 1.000], loss: 0.000005, mean_absolute_error: 41.734444, mean_q: -0.000462\n",
      " 4107/5000: episode: 4106, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.000002, mean_absolute_error: 41.741158, mean_q: -0.000524\n",
      " 4108/5000: episode: 4107, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.066, 1.000], loss: 0.000001, mean_absolute_error: 41.741703, mean_q: -0.000491\n",
      " 4109/5000: episode: 4108, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.000, 0.779], loss: 0.000001, mean_absolute_error: 41.741547, mean_q: -0.000517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4110/5000: episode: 4109, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.296, 1.000], loss: 0.000004, mean_absolute_error: 41.736679, mean_q: -0.000456\n",
      " 4111/5000: episode: 4110, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 0.000005, mean_absolute_error: 41.733490, mean_q: -0.000471\n",
      " 4112/5000: episode: 4111, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.719], loss: 0.000002, mean_absolute_error: 41.741291, mean_q: -0.000511\n",
      " 4113/5000: episode: 4112, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.389, 1.000], loss: 0.000002, mean_absolute_error: 41.740044, mean_q: -0.000540\n",
      " 4114/5000: episode: 4113, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.002, 1.000], loss: 0.000001, mean_absolute_error: 41.743706, mean_q: -0.000526\n",
      " 4115/5000: episode: 4114, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 0.000003, mean_absolute_error: 41.740131, mean_q: -0.000506\n",
      " 4116/5000: episode: 4115, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.498, 1.000], loss: 0.000001, mean_absolute_error: 41.743652, mean_q: -0.000530\n",
      " 4117/5000: episode: 4116, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.027, 1.000], loss: 0.000005, mean_absolute_error: 41.733894, mean_q: -0.000453\n",
      " 4118/5000: episode: 4117, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.687], loss: 0.000003, mean_absolute_error: 41.728767, mean_q: -0.000406\n",
      " 4119/5000: episode: 4118, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.026, 1.000], loss: 0.000003, mean_absolute_error: 41.738701, mean_q: -0.000555\n",
      " 4120/5000: episode: 4119, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.347 [0.000, 0.932], loss: 0.000012, mean_absolute_error: 41.735184, mean_q: -0.000499\n",
      " 4121/5000: episode: 4120, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 0.000003, mean_absolute_error: 41.733704, mean_q: -0.000479\n",
      " 4122/5000: episode: 4121, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.739 [0.179, 1.000], loss: 0.000005, mean_absolute_error: 41.737381, mean_q: -0.000453\n",
      " 4123/5000: episode: 4122, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.274 [0.000, 0.843], loss: 0.000002, mean_absolute_error: 41.739468, mean_q: -0.000502\n",
      " 4124/5000: episode: 4123, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.686], loss: 0.000002, mean_absolute_error: 41.735580, mean_q: -0.000458\n",
      " 4125/5000: episode: 4124, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.283 [0.000, 0.857], loss: 0.000003, mean_absolute_error: 41.735748, mean_q: -0.000491\n",
      " 4126/5000: episode: 4125, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.000, 0.876], loss: 0.000002, mean_absolute_error: 41.738903, mean_q: -0.000474\n",
      " 4127/5000: episode: 4126, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.162, 1.000], loss: 0.000003, mean_absolute_error: 41.739914, mean_q: -0.000575\n",
      " 4128/5000: episode: 4127, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.530], loss: 0.000002, mean_absolute_error: 41.739067, mean_q: -0.000573\n",
      " 4129/5000: episode: 4128, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.563 [0.012, 1.000], loss: 0.000010, mean_absolute_error: 41.731709, mean_q: -0.000458\n",
      " 4130/5000: episode: 4129, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 0.000001, mean_absolute_error: 41.743408, mean_q: -0.000558\n",
      " 4131/5000: episode: 4130, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 0.000001, mean_absolute_error: 41.744003, mean_q: -0.000577\n",
      " 4132/5000: episode: 4131, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.000, 0.913], loss: 0.000001, mean_absolute_error: 41.741264, mean_q: -0.000530\n",
      " 4133/5000: episode: 4132, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.611], loss: 0.000002, mean_absolute_error: 41.738598, mean_q: -0.000482\n",
      " 4134/5000: episode: 4133, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.838], loss: 0.000001, mean_absolute_error: 41.744797, mean_q: -0.000568\n",
      " 4135/5000: episode: 4134, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.000, 0.921], loss: 0.000005, mean_absolute_error: 41.734894, mean_q: -0.000535\n",
      " 4136/5000: episode: 4135, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.637], loss: 0.000003, mean_absolute_error: 41.737064, mean_q: -0.000481\n",
      " 4137/5000: episode: 4136, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.128, 1.000], loss: 0.000005, mean_absolute_error: 41.733658, mean_q: -0.000461\n",
      " 4138/5000: episode: 4137, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.270, 1.000], loss: 0.000034, mean_absolute_error: 41.709179, mean_q: -0.001805\n",
      " 4139/5000: episode: 4138, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.000, 0.959], loss: 0.000004, mean_absolute_error: 41.733227, mean_q: -0.000511\n",
      " 4140/5000: episode: 4139, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.703], loss: 0.000002, mean_absolute_error: 41.741669, mean_q: -0.000538\n",
      " 4141/5000: episode: 4140, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.000, 0.988], loss: 0.000003, mean_absolute_error: 41.740425, mean_q: -0.000546\n",
      " 4142/5000: episode: 4141, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.000, 0.997], loss: 0.000002, mean_absolute_error: 41.738789, mean_q: -0.000484\n",
      " 4143/5000: episode: 4142, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 0.000003, mean_absolute_error: 41.732391, mean_q: -0.000477\n",
      " 4144/5000: episode: 4143, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.611], loss: 0.000005, mean_absolute_error: 41.738876, mean_q: -0.000432\n",
      " 4145/5000: episode: 4144, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.579 [0.019, 1.000], loss: 0.000001, mean_absolute_error: 41.745377, mean_q: -0.000592\n",
      " 4146/5000: episode: 4145, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.000, 0.815], loss: 0.000002, mean_absolute_error: 41.738396, mean_q: -0.000555\n",
      " 4147/5000: episode: 4146, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.417 [0.000, 0.980], loss: 0.000001, mean_absolute_error: 41.741131, mean_q: -0.000537\n",
      " 4148/5000: episode: 4147, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.000006, mean_absolute_error: 41.735710, mean_q: -0.000461\n",
      " 4149/5000: episode: 4148, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.283, 1.000], loss: 0.000006, mean_absolute_error: 41.729225, mean_q: -0.000512\n",
      " 4150/5000: episode: 4149, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.557], loss: 0.000002, mean_absolute_error: 41.743191, mean_q: -0.000541\n",
      " 4151/5000: episode: 4150, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.554], loss: 0.000002, mean_absolute_error: 41.740593, mean_q: -0.000551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4152/5000: episode: 4151, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.416 [0.000, 0.979], loss: 0.000003, mean_absolute_error: 41.735542, mean_q: -0.000460\n",
      " 4153/5000: episode: 4152, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.011, 1.000], loss: 0.000005, mean_absolute_error: 41.731297, mean_q: -0.000441\n",
      " 4154/5000: episode: 4153, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.153, 1.000], loss: 0.000002, mean_absolute_error: 41.741272, mean_q: -0.000527\n",
      " 4155/5000: episode: 4154, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.432 [0.000, 0.986], loss: 0.000002, mean_absolute_error: 41.741081, mean_q: -0.000583\n",
      " 4156/5000: episode: 4155, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.433, 1.000], loss: 0.000003, mean_absolute_error: 41.736759, mean_q: -0.000468\n",
      " 4157/5000: episode: 4156, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.898], loss: 0.000003, mean_absolute_error: 41.739288, mean_q: -0.000516\n",
      " 4158/5000: episode: 4157, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.758], loss: 0.000002, mean_absolute_error: 41.741943, mean_q: -0.000531\n",
      " 4159/5000: episode: 4158, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.271, 1.000], loss: 0.000005, mean_absolute_error: 41.734577, mean_q: -0.000422\n",
      " 4160/5000: episode: 4159, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.000, 0.997], loss: 0.000005, mean_absolute_error: 41.734581, mean_q: -0.000519\n",
      " 4161/5000: episode: 4160, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.944], loss: 0.000002, mean_absolute_error: 41.738148, mean_q: -0.000538\n",
      " 4162/5000: episode: 4161, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.000, 1.000], loss: 0.000003, mean_absolute_error: 41.738930, mean_q: -0.000485\n",
      " 4163/5000: episode: 4162, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.000, 0.935], loss: 0.000003, mean_absolute_error: 41.736847, mean_q: -0.000517\n",
      " 4164/5000: episode: 4163, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.000, 0.846], loss: 0.000003, mean_absolute_error: 41.737400, mean_q: -0.000538\n",
      " 4165/5000: episode: 4164, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.754], loss: 0.000002, mean_absolute_error: 41.743340, mean_q: -0.000573\n",
      " 4166/5000: episode: 4165, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.813], loss: 0.000002, mean_absolute_error: 41.742592, mean_q: -0.000548\n",
      " 4167/5000: episode: 4166, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 0.000001, mean_absolute_error: 41.743408, mean_q: -0.000589\n",
      " 4168/5000: episode: 4167, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.089, 1.000], loss: 0.000003, mean_absolute_error: 41.734451, mean_q: -0.000438\n",
      " 4169/5000: episode: 4168, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.441, 1.000], loss: 0.000001, mean_absolute_error: 41.742134, mean_q: -0.000550\n",
      " 4170/5000: episode: 4169, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.049, 1.000], loss: 0.000008, mean_absolute_error: 41.731750, mean_q: -0.000464\n",
      " 4171/5000: episode: 4170, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.119, 1.000], loss: 0.000001, mean_absolute_error: 41.743385, mean_q: -0.000751\n",
      " 4172/5000: episode: 4171, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.739 [0.178, 1.000], loss: 0.000001, mean_absolute_error: 41.741394, mean_q: -0.000576\n",
      " 4173/5000: episode: 4172, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.106, 1.000], loss: 0.000002, mean_absolute_error: 41.738995, mean_q: -0.000516\n",
      " 4174/5000: episode: 4173, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.230, 1.000], loss: 0.000003, mean_absolute_error: 41.740948, mean_q: -0.000573\n",
      " 4175/5000: episode: 4174, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 0.000004, mean_absolute_error: 41.738411, mean_q: -0.000496\n",
      " 4176/5000: episode: 4175, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.000, 0.981], loss: 0.000002, mean_absolute_error: 41.740093, mean_q: -0.000522\n",
      " 4177/5000: episode: 4176, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.279, 1.000], loss: 0.000003, mean_absolute_error: 41.736084, mean_q: -0.000512\n",
      " 4178/5000: episode: 4177, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.454, 1.000], loss: 0.000003, mean_absolute_error: 41.737701, mean_q: -0.000548\n",
      " 4179/5000: episode: 4178, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.938], loss: 0.000001, mean_absolute_error: 41.743584, mean_q: -0.000624\n",
      " 4180/5000: episode: 4179, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.387, 1.000], loss: 0.000003, mean_absolute_error: 41.734367, mean_q: -0.000523\n",
      " 4181/5000: episode: 4180, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.036, 1.000], loss: 0.000004, mean_absolute_error: 41.741791, mean_q: -0.000536\n",
      " 4182/5000: episode: 4181, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.032, 1.000], loss: 0.000005, mean_absolute_error: 41.731796, mean_q: -0.000668\n",
      " 4183/5000: episode: 4182, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.047, 1.000], loss: 0.000003, mean_absolute_error: 41.737206, mean_q: -0.000526\n",
      " 4184/5000: episode: 4183, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.000001, mean_absolute_error: 41.744923, mean_q: -0.000604\n",
      " 4185/5000: episode: 4184, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.293, 1.000], loss: 0.000002, mean_absolute_error: 41.743347, mean_q: -0.000600\n",
      " 4186/5000: episode: 4185, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.436, 1.000], loss: 0.000001, mean_absolute_error: 41.741146, mean_q: -0.000569\n",
      " 4187/5000: episode: 4186, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.485, 1.000], loss: 0.000002, mean_absolute_error: 41.738914, mean_q: -0.000565\n",
      " 4188/5000: episode: 4187, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.194, 1.000], loss: 0.000008, mean_absolute_error: 41.728031, mean_q: -0.000485\n",
      " 4189/5000: episode: 4188, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.787], loss: 0.000007, mean_absolute_error: 41.737785, mean_q: -0.000519\n",
      " 4190/5000: episode: 4189, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.320, 1.000], loss: 0.000259, mean_absolute_error: 41.715000, mean_q: 0.000782\n",
      " 4191/5000: episode: 4190, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.301, 1.000], loss: 0.000002, mean_absolute_error: 41.741150, mean_q: -0.000573\n",
      " 4192/5000: episode: 4191, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 0.000004, mean_absolute_error: 41.738243, mean_q: -0.000582\n",
      " 4193/5000: episode: 4192, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.376, 1.000], loss: 0.000036, mean_absolute_error: 41.723564, mean_q: -0.000059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4194/5000: episode: 4193, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.197, 1.000], loss: 0.000003, mean_absolute_error: 41.734665, mean_q: -0.000499\n",
      " 4195/5000: episode: 4194, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.037, 1.000], loss: 0.000005, mean_absolute_error: 41.732933, mean_q: -0.000504\n",
      " 4196/5000: episode: 4195, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.282, 1.000], loss: 0.000003, mean_absolute_error: 41.738312, mean_q: -0.000539\n",
      " 4197/5000: episode: 4196, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.708], loss: 0.000004, mean_absolute_error: 41.730640, mean_q: -0.000496\n",
      " 4198/5000: episode: 4197, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.498 [0.000, 1.000], loss: 0.000003, mean_absolute_error: 41.735504, mean_q: -0.000517\n",
      " 4199/5000: episode: 4198, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.296, 1.000], loss: 0.000002, mean_absolute_error: 41.740402, mean_q: -0.000574\n",
      " 4200/5000: episode: 4199, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.538], loss: 0.000003, mean_absolute_error: 41.739555, mean_q: -0.000588\n",
      " 4201/5000: episode: 4200, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 0.000002, mean_absolute_error: 41.740303, mean_q: -0.000652\n",
      " 4202/5000: episode: 4201, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.578 [0.018, 1.000], loss: 0.000001, mean_absolute_error: 41.743252, mean_q: -0.000609\n",
      " 4203/5000: episode: 4202, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.000, 0.787], loss: 0.000012, mean_absolute_error: 41.731804, mean_q: -0.000574\n",
      " 4204/5000: episode: 4203, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.598], loss: 0.000002, mean_absolute_error: 41.737961, mean_q: -0.000550\n",
      " 4205/5000: episode: 4204, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.687 [0.102, 1.000], loss: 0.000002, mean_absolute_error: 41.739235, mean_q: -0.000648\n",
      " 4206/5000: episode: 4205, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.209, 1.000], loss: 0.000005, mean_absolute_error: 41.736382, mean_q: -0.000569\n",
      " 4207/5000: episode: 4206, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.674], loss: 0.000001, mean_absolute_error: 41.745506, mean_q: -0.000651\n",
      " 4208/5000: episode: 4207, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 0.000005, mean_absolute_error: 41.731335, mean_q: -0.000511\n",
      " 4209/5000: episode: 4208, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.000, 0.806], loss: 0.000004, mean_absolute_error: 41.737709, mean_q: -0.000552\n",
      " 4210/5000: episode: 4209, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.047, 1.000], loss: 0.000003, mean_absolute_error: 41.737717, mean_q: -0.000586\n",
      " 4211/5000: episode: 4210, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.174, 1.000], loss: 0.000002, mean_absolute_error: 41.742172, mean_q: -0.000651\n",
      " 4212/5000: episode: 4211, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.610], loss: 0.000001, mean_absolute_error: 41.744564, mean_q: -0.000643\n",
      " 4213/5000: episode: 4212, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.337, 1.000], loss: 0.000001, mean_absolute_error: 41.744411, mean_q: -0.000602\n",
      " 4214/5000: episode: 4213, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.000, 0.818], loss: 0.000007, mean_absolute_error: 41.732292, mean_q: -0.000445\n",
      " 4215/5000: episode: 4214, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.000, 0.840], loss: 0.000003, mean_absolute_error: 41.735657, mean_q: -0.000615\n",
      " 4216/5000: episode: 4215, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.088, 1.000], loss: 0.000005, mean_absolute_error: 41.730778, mean_q: -0.000544\n",
      " 4217/5000: episode: 4216, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.027, 1.000], loss: 0.000003, mean_absolute_error: 41.733574, mean_q: -0.000581\n",
      " 4218/5000: episode: 4217, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.954], loss: 0.000001, mean_absolute_error: 41.739559, mean_q: -0.000720\n",
      " 4219/5000: episode: 4218, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 0.000003, mean_absolute_error: 41.739902, mean_q: -0.000573\n",
      " 4220/5000: episode: 4219, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.244, 1.000], loss: 0.000001, mean_absolute_error: 41.743404, mean_q: -0.000610\n",
      " 4221/5000: episode: 4220, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 0.000084, mean_absolute_error: 41.726318, mean_q: 0.000416\n",
      " 4222/5000: episode: 4221, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.376 [0.000, 0.956], loss: 0.000003, mean_absolute_error: 41.738335, mean_q: -0.000597\n",
      " 4223/5000: episode: 4222, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.458, 1.000], loss: 0.000002, mean_absolute_error: 41.740246, mean_q: -0.000625\n",
      " 4224/5000: episode: 4223, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.100, 1.000], loss: 0.000005, mean_absolute_error: 41.734802, mean_q: -0.000629\n",
      " 4225/5000: episode: 4224, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.000, 0.800], loss: 0.000002, mean_absolute_error: 41.737579, mean_q: -0.000543\n",
      " 4226/5000: episode: 4225, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 0.000001, mean_absolute_error: 41.744217, mean_q: -0.000663\n",
      " 4227/5000: episode: 4226, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.182, 1.000], loss: 0.000005, mean_absolute_error: 41.735283, mean_q: -0.000598\n",
      " 4228/5000: episode: 4227, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.349, 1.000], loss: 0.000001, mean_absolute_error: 41.742489, mean_q: -0.000600\n",
      " 4229/5000: episode: 4228, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.261, 1.000], loss: 0.000003, mean_absolute_error: 41.737350, mean_q: -0.000808\n",
      " 4230/5000: episode: 4229, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.111, 1.000], loss: 0.000002, mean_absolute_error: 41.740982, mean_q: -0.000612\n",
      " 4231/5000: episode: 4230, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 0.000003, mean_absolute_error: 41.737411, mean_q: -0.000589\n",
      " 4232/5000: episode: 4231, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.007, 1.000], loss: 0.000002, mean_absolute_error: 41.738686, mean_q: -0.000596\n",
      " 4233/5000: episode: 4232, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.000, 0.992], loss: 0.000004, mean_absolute_error: 41.734833, mean_q: -0.000606\n",
      " 4234/5000: episode: 4233, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.000, 0.805], loss: 0.000004, mean_absolute_error: 41.733185, mean_q: -0.000597\n",
      " 4235/5000: episode: 4234, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.033, 1.000], loss: 0.000001, mean_absolute_error: 41.744080, mean_q: -0.000623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4236/5000: episode: 4235, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.389, 1.000], loss: 0.000004, mean_absolute_error: 41.732185, mean_q: -0.000921\n",
      " 4237/5000: episode: 4236, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.000, 0.921], loss: 0.000002, mean_absolute_error: 41.741070, mean_q: -0.000607\n",
      " 4238/5000: episode: 4237, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.351, 1.000], loss: 0.000003, mean_absolute_error: 41.736389, mean_q: -0.000634\n",
      " 4239/5000: episode: 4238, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.445, 1.000], loss: 0.000004, mean_absolute_error: 41.735210, mean_q: -0.000555\n",
      " 4240/5000: episode: 4239, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.144, 1.000], loss: 0.000006, mean_absolute_error: 41.732010, mean_q: -0.000725\n",
      " 4241/5000: episode: 4240, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.723], loss: 0.000005, mean_absolute_error: 41.735130, mean_q: -0.000592\n",
      " 4242/5000: episode: 4241, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.009, 1.000], loss: 0.000005, mean_absolute_error: 41.735481, mean_q: -0.000566\n",
      " 4243/5000: episode: 4242, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.578 [0.018, 1.000], loss: 0.000008, mean_absolute_error: 41.732494, mean_q: -0.000507\n",
      " 4244/5000: episode: 4243, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 0.000001, mean_absolute_error: 41.742771, mean_q: -0.000657\n",
      " 4245/5000: episode: 4244, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.010, 1.000], loss: 0.000001, mean_absolute_error: 41.742172, mean_q: -0.000602\n",
      " 4246/5000: episode: 4245, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.427 [0.000, 0.984], loss: 0.000004, mean_absolute_error: 41.739952, mean_q: -0.000579\n",
      " 4247/5000: episode: 4246, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.000, 0.699], loss: 0.000002, mean_absolute_error: 41.738396, mean_q: -0.000605\n",
      " 4248/5000: episode: 4247, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.318, 1.000], loss: 0.000004, mean_absolute_error: 41.737427, mean_q: -0.000624\n",
      " 4249/5000: episode: 4248, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 0.000003, mean_absolute_error: 41.740974, mean_q: -0.000607\n",
      " 4250/5000: episode: 4249, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.000004, mean_absolute_error: 41.735718, mean_q: -0.000558\n",
      " 4251/5000: episode: 4250, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.000, 0.958], loss: 0.000003, mean_absolute_error: 41.737404, mean_q: -0.000679\n",
      " 4252/5000: episode: 4251, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.265, 1.000], loss: 0.000002, mean_absolute_error: 41.740421, mean_q: -0.000658\n",
      " 4253/5000: episode: 4252, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.766], loss: 0.000003, mean_absolute_error: 41.732639, mean_q: -0.000614\n",
      " 4254/5000: episode: 4253, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.401 [0.000, 0.971], loss: 0.000002, mean_absolute_error: 41.741909, mean_q: -0.000671\n",
      " 4255/5000: episode: 4254, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.498, 1.000], loss: 0.000002, mean_absolute_error: 41.737610, mean_q: -0.000612\n",
      " 4256/5000: episode: 4255, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.000, 0.997], loss: 0.000001, mean_absolute_error: 41.742386, mean_q: -0.000632\n",
      " 4257/5000: episode: 4256, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.000, 0.960], loss: 0.000002, mean_absolute_error: 41.741058, mean_q: -0.000707\n",
      " 4258/5000: episode: 4257, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.006, 1.000], loss: 0.000002, mean_absolute_error: 41.739437, mean_q: -0.000610\n",
      " 4259/5000: episode: 4258, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.778 [0.268, 1.000], loss: 0.000004, mean_absolute_error: 41.733196, mean_q: -0.000587\n",
      " 4260/5000: episode: 4259, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.711], loss: 0.000002, mean_absolute_error: 41.738373, mean_q: -0.000620\n",
      " 4261/5000: episode: 4260, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.539 [0.005, 1.000], loss: 0.000003, mean_absolute_error: 41.737602, mean_q: -0.000591\n",
      " 4262/5000: episode: 4261, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.000, 0.849], loss: 0.000002, mean_absolute_error: 41.741047, mean_q: -0.000687\n",
      " 4263/5000: episode: 4262, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.498, 1.000], loss: 0.000002, mean_absolute_error: 41.739323, mean_q: -0.000685\n",
      " 4264/5000: episode: 4263, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.000003, mean_absolute_error: 41.732910, mean_q: -0.000684\n",
      " 4265/5000: episode: 4264, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.053, 1.000], loss: 0.000012, mean_absolute_error: 41.736328, mean_q: -0.000616\n",
      " 4266/5000: episode: 4265, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.257, 1.000], loss: 0.000004, mean_absolute_error: 41.735397, mean_q: -0.000571\n",
      " 4267/5000: episode: 4266, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.236, 1.000], loss: 0.000005, mean_absolute_error: 41.738281, mean_q: -0.000651\n",
      " 4268/5000: episode: 4267, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.000004, mean_absolute_error: 41.730820, mean_q: -0.000510\n",
      " 4269/5000: episode: 4268, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.563 [0.012, 1.000], loss: 0.000001, mean_absolute_error: 41.741528, mean_q: -0.000630\n",
      " 4270/5000: episode: 4269, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.478 [0.000, 0.998], loss: 0.000004, mean_absolute_error: 41.733597, mean_q: -0.000557\n",
      " 4271/5000: episode: 4270, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.103, 1.000], loss: 0.000002, mean_absolute_error: 41.742401, mean_q: -0.000691\n",
      " 4272/5000: episode: 4271, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.566], loss: 0.000003, mean_absolute_error: 41.739185, mean_q: -0.000653\n",
      " 4273/5000: episode: 4272, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.000, 0.718], loss: 0.000005, mean_absolute_error: 41.735298, mean_q: -0.000541\n",
      " 4274/5000: episode: 4273, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.366, 1.000], loss: 0.000002, mean_absolute_error: 41.741615, mean_q: -0.000675\n",
      " 4275/5000: episode: 4274, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.357, 1.000], loss: 0.000003, mean_absolute_error: 41.739330, mean_q: -0.000675\n",
      " 4276/5000: episode: 4275, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.328, 1.000], loss: 0.000002, mean_absolute_error: 41.744171, mean_q: -0.000674\n",
      " 4277/5000: episode: 4276, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.319, 1.000], loss: 0.000002, mean_absolute_error: 41.741211, mean_q: -0.000723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4278/5000: episode: 4277, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.124, 1.000], loss: 0.000002, mean_absolute_error: 41.741287, mean_q: -0.000669\n",
      " 4279/5000: episode: 4278, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.000, 0.915], loss: 0.000001, mean_absolute_error: 41.745106, mean_q: -0.000732\n",
      " 4280/5000: episode: 4279, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 0.000001, mean_absolute_error: 41.742023, mean_q: -0.000674\n",
      " 4281/5000: episode: 4280, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.557], loss: 0.000002, mean_absolute_error: 41.739540, mean_q: -0.000666\n",
      " 4282/5000: episode: 4281, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.287, 1.000], loss: 0.000003, mean_absolute_error: 41.735527, mean_q: -0.000653\n",
      " 4283/5000: episode: 4282, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.638 [0.055, 1.000], loss: 0.000002, mean_absolute_error: 41.739899, mean_q: -0.000692\n",
      " 4284/5000: episode: 4283, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.000, 0.794], loss: 0.000002, mean_absolute_error: 41.739586, mean_q: -0.000677\n",
      " 4285/5000: episode: 4284, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.607 [0.034, 1.000], loss: 0.000002, mean_absolute_error: 41.742867, mean_q: -0.000672\n",
      " 4286/5000: episode: 4285, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.825], loss: 0.000002, mean_absolute_error: 41.740376, mean_q: -0.000658\n",
      " 4287/5000: episode: 4286, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.000, 0.977], loss: 0.000005, mean_absolute_error: 41.732178, mean_q: -0.000550\n",
      " 4288/5000: episode: 4287, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.045, 1.000], loss: 0.000002, mean_absolute_error: 41.739342, mean_q: -0.000642\n",
      " 4289/5000: episode: 4288, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 0.000002, mean_absolute_error: 41.741940, mean_q: -0.000690\n",
      " 4290/5000: episode: 4289, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.607], loss: 0.000002, mean_absolute_error: 41.739761, mean_q: -0.000629\n",
      " 4291/5000: episode: 4290, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 0.000008, mean_absolute_error: 41.729168, mean_q: -0.000538\n",
      " 4292/5000: episode: 4291, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.385 [0.000, 0.962], loss: 0.000003, mean_absolute_error: 41.737423, mean_q: -0.000603\n",
      " 4293/5000: episode: 4292, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.723], loss: 0.000006, mean_absolute_error: 41.737625, mean_q: -0.000681\n",
      " 4294/5000: episode: 4293, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.000, 0.889], loss: 0.000002, mean_absolute_error: 41.740280, mean_q: -0.000663\n",
      " 4295/5000: episode: 4294, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.000, 0.999], loss: 0.000001, mean_absolute_error: 41.742226, mean_q: -0.000712\n",
      " 4296/5000: episode: 4295, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.000, 0.995], loss: 0.000003, mean_absolute_error: 41.740036, mean_q: -0.000704\n",
      " 4297/5000: episode: 4296, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.000, 0.889], loss: 0.000002, mean_absolute_error: 41.741901, mean_q: -0.000665\n",
      " 4298/5000: episode: 4297, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.209, 1.000], loss: 0.000008, mean_absolute_error: 41.734501, mean_q: -0.000630\n",
      " 4299/5000: episode: 4298, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.000, 0.972], loss: 0.000002, mean_absolute_error: 41.737617, mean_q: -0.000648\n",
      " 4300/5000: episode: 4299, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.443 [0.000, 0.990], loss: 0.000005, mean_absolute_error: 41.733864, mean_q: -0.000588\n",
      " 4301/5000: episode: 4300, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.539 [0.005, 1.000], loss: 0.000005, mean_absolute_error: 41.733459, mean_q: -0.000661\n",
      " 4302/5000: episode: 4301, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.334 [0.000, 0.920], loss: 0.000002, mean_absolute_error: 41.736870, mean_q: -0.000628\n",
      " 4303/5000: episode: 4302, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.543], loss: 0.000001, mean_absolute_error: 41.743061, mean_q: -0.000674\n",
      " 4304/5000: episode: 4303, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.395 [0.000, 0.968], loss: 0.000001, mean_absolute_error: 41.742203, mean_q: -0.000647\n",
      " 4305/5000: episode: 4304, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.101, 1.000], loss: 0.000015, mean_absolute_error: 41.732113, mean_q: -0.000074\n",
      " 4306/5000: episode: 4305, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.280, 1.000], loss: 0.000001, mean_absolute_error: 41.739925, mean_q: -0.000739\n",
      " 4307/5000: episode: 4306, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.470, 1.000], loss: 0.000003, mean_absolute_error: 41.741524, mean_q: -0.000697\n",
      " 4308/5000: episode: 4307, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.000, 0.840], loss: 0.000001, mean_absolute_error: 41.742657, mean_q: -0.000705\n",
      " 4309/5000: episode: 4308, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.000, 0.777], loss: 0.000000, mean_absolute_error: 41.746590, mean_q: -0.000744\n",
      " 4310/5000: episode: 4309, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.184, 1.000], loss: 0.000002, mean_absolute_error: 41.739315, mean_q: -0.000653\n",
      " 4311/5000: episode: 4310, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.281, 1.000], loss: 0.000004, mean_absolute_error: 41.736134, mean_q: -0.000620\n",
      " 4312/5000: episode: 4311, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.322, 1.000], loss: 0.000001, mean_absolute_error: 41.740780, mean_q: -0.000696\n",
      " 4313/5000: episode: 4312, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.102, 1.000], loss: 0.000003, mean_absolute_error: 41.737869, mean_q: -0.000662\n",
      " 4314/5000: episode: 4313, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.000, 0.827], loss: 0.000001, mean_absolute_error: 41.741280, mean_q: -0.000695\n",
      " 4315/5000: episode: 4314, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.157, 1.000], loss: 0.000008, mean_absolute_error: 41.729649, mean_q: -0.000682\n",
      " 4316/5000: episode: 4315, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.409, 1.000], loss: 0.000001, mean_absolute_error: 41.742832, mean_q: -0.000742\n",
      " 4317/5000: episode: 4316, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.080, 1.000], loss: 0.000002, mean_absolute_error: 41.741783, mean_q: -0.000689\n",
      " 4318/5000: episode: 4317, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.502, 1.000], loss: 0.000001, mean_absolute_error: 41.743626, mean_q: -0.000661\n",
      " 4319/5000: episode: 4318, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.771], loss: 0.000005, mean_absolute_error: 41.734249, mean_q: -0.000638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4320/5000: episode: 4319, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.216, 1.000], loss: 0.000002, mean_absolute_error: 41.739861, mean_q: -0.000703\n",
      " 4321/5000: episode: 4320, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.040, 1.000], loss: 0.000004, mean_absolute_error: 41.733795, mean_q: -0.000647\n",
      " 4322/5000: episode: 4321, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.223, 1.000], loss: 0.000001, mean_absolute_error: 41.741158, mean_q: -0.000686\n",
      " 4323/5000: episode: 4322, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.901], loss: 0.000004, mean_absolute_error: 41.735878, mean_q: -0.000651\n",
      " 4324/5000: episode: 4323, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.349 [0.000, 0.935], loss: 0.000002, mean_absolute_error: 41.739014, mean_q: -0.000639\n",
      " 4325/5000: episode: 4324, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.047, 1.000], loss: 0.000003, mean_absolute_error: 41.732460, mean_q: -0.000656\n",
      " 4326/5000: episode: 4325, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.115, 1.000], loss: 0.000002, mean_absolute_error: 41.738384, mean_q: -0.000647\n",
      " 4327/5000: episode: 4326, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.414 [0.000, 0.978], loss: 0.000003, mean_absolute_error: 41.736198, mean_q: -0.000661\n",
      " 4328/5000: episode: 4327, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.042, 1.000], loss: 0.000000, mean_absolute_error: 41.747749, mean_q: -0.000754\n",
      " 4329/5000: episode: 4328, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.677 [0.091, 1.000], loss: 0.000067, mean_absolute_error: 41.726078, mean_q: -0.000323\n",
      " 4330/5000: episode: 4329, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.147, 1.000], loss: 0.000003, mean_absolute_error: 41.737808, mean_q: -0.000696\n",
      " 4331/5000: episode: 4330, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.233, 1.000], loss: 0.000001, mean_absolute_error: 41.743095, mean_q: -0.000748\n",
      " 4332/5000: episode: 4331, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.139, 1.000], loss: 0.000002, mean_absolute_error: 41.740593, mean_q: -0.000704\n",
      " 4333/5000: episode: 4332, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.668 [0.082, 1.000], loss: 0.000003, mean_absolute_error: 41.739792, mean_q: -0.000655\n",
      " 4334/5000: episode: 4333, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.637], loss: 0.000003, mean_absolute_error: 41.736580, mean_q: -0.000664\n",
      " 4335/5000: episode: 4334, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.016, 1.000], loss: 0.000004, mean_absolute_error: 41.740479, mean_q: -0.000736\n",
      " 4336/5000: episode: 4335, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.647 [0.062, 1.000], loss: 0.000003, mean_absolute_error: 41.736588, mean_q: -0.000650\n",
      " 4337/5000: episode: 4336, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.539], loss: 0.000001, mean_absolute_error: 41.742725, mean_q: -0.000715\n",
      " 4338/5000: episode: 4337, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.447 [0.000, 0.991], loss: 0.000002, mean_absolute_error: 41.742088, mean_q: -0.000739\n",
      " 4339/5000: episode: 4338, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.912], loss: 0.000002, mean_absolute_error: 41.739647, mean_q: -0.000641\n",
      " 4340/5000: episode: 4339, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.166, 1.000], loss: 0.000004, mean_absolute_error: 41.738716, mean_q: -0.000649\n",
      " 4341/5000: episode: 4340, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.382, 1.000], loss: 0.000004, mean_absolute_error: 41.730736, mean_q: -0.000567\n",
      " 4342/5000: episode: 4341, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.319 [0.000, 0.905], loss: 0.000001, mean_absolute_error: 41.740692, mean_q: -0.000706\n",
      " 4343/5000: episode: 4342, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.187, 1.000], loss: 0.000003, mean_absolute_error: 41.737656, mean_q: -0.000699\n",
      " 4344/5000: episode: 4343, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.000, 0.674], loss: 0.000001, mean_absolute_error: 41.741600, mean_q: -0.000905\n",
      " 4345/5000: episode: 4344, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.467, 1.000], loss: 0.000003, mean_absolute_error: 41.736824, mean_q: -0.000712\n",
      " 4346/5000: episode: 4345, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.016, 1.000], loss: 0.000002, mean_absolute_error: 41.740784, mean_q: -0.000732\n",
      " 4347/5000: episode: 4346, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.068, 1.000], loss: 0.000002, mean_absolute_error: 41.741882, mean_q: -0.000704\n",
      " 4348/5000: episode: 4347, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.363, 1.000], loss: 0.000002, mean_absolute_error: 41.740089, mean_q: -0.000729\n",
      " 4349/5000: episode: 4348, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.061, 1.000], loss: 0.000003, mean_absolute_error: 41.740875, mean_q: -0.000646\n",
      " 4350/5000: episode: 4349, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.000, 0.942], loss: 0.000002, mean_absolute_error: 41.739807, mean_q: -0.000646\n",
      " 4351/5000: episode: 4350, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.474, 1.000], loss: 0.000002, mean_absolute_error: 41.743088, mean_q: -0.000710\n",
      " 4352/5000: episode: 4351, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.588 [0.023, 1.000], loss: 0.000003, mean_absolute_error: 41.739704, mean_q: -0.000651\n",
      " 4353/5000: episode: 4352, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 0.000004, mean_absolute_error: 41.735970, mean_q: -0.000587\n",
      " 4354/5000: episode: 4353, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.348 [0.000, 0.933], loss: 0.000002, mean_absolute_error: 41.741707, mean_q: -0.000738\n",
      " 4355/5000: episode: 4354, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.907], loss: 0.000001, mean_absolute_error: 41.742134, mean_q: -0.000800\n",
      " 4356/5000: episode: 4355, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.059, 1.000], loss: 0.000007, mean_absolute_error: 41.734715, mean_q: -0.000611\n",
      " 4357/5000: episode: 4356, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.000, 0.749], loss: 0.000002, mean_absolute_error: 41.739628, mean_q: -0.000721\n",
      " 4358/5000: episode: 4357, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.528 [0.003, 1.000], loss: 0.000002, mean_absolute_error: 41.739502, mean_q: -0.000699\n",
      " 4359/5000: episode: 4358, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.613], loss: 0.000002, mean_absolute_error: 41.742111, mean_q: -0.000767\n",
      " 4360/5000: episode: 4359, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.000, 0.968], loss: 0.000004, mean_absolute_error: 41.736374, mean_q: -0.000748\n",
      " 4361/5000: episode: 4360, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.453 [0.000, 0.993], loss: 0.000002, mean_absolute_error: 41.739250, mean_q: -0.000707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4362/5000: episode: 4361, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.177, 1.000], loss: 0.000002, mean_absolute_error: 41.737930, mean_q: -0.000713\n",
      " 4363/5000: episode: 4362, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.469, 1.000], loss: 0.000002, mean_absolute_error: 41.741943, mean_q: -0.000741\n",
      " 4364/5000: episode: 4363, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.493], loss: 0.000028, mean_absolute_error: 41.726910, mean_q: -0.000452\n",
      " 4365/5000: episode: 4364, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.576], loss: 0.000010, mean_absolute_error: 41.732674, mean_q: -0.000643\n",
      " 4366/5000: episode: 4365, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.623], loss: 0.000035, mean_absolute_error: 41.735420, mean_q: -0.000335\n",
      " 4367/5000: episode: 4366, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.000, 0.982], loss: 0.000003, mean_absolute_error: 41.732613, mean_q: -0.000621\n",
      " 4368/5000: episode: 4367, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.522], loss: 0.000004, mean_absolute_error: 41.735485, mean_q: -0.000574\n",
      " 4369/5000: episode: 4368, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.579 [0.019, 1.000], loss: 0.000004, mean_absolute_error: 41.734390, mean_q: -0.000661\n",
      " 4370/5000: episode: 4369, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.602], loss: 0.000001, mean_absolute_error: 41.744774, mean_q: -0.000777\n",
      " 4371/5000: episode: 4370, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.503], loss: 0.000007, mean_absolute_error: 41.733162, mean_q: -0.000610\n",
      " 4372/5000: episode: 4371, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.546], loss: 0.000002, mean_absolute_error: 41.738895, mean_q: -0.000722\n",
      " 4373/5000: episode: 4372, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.000, 0.924], loss: 0.000003, mean_absolute_error: 41.738873, mean_q: -0.000738\n",
      " 4374/5000: episode: 4373, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 0.000002, mean_absolute_error: 41.739124, mean_q: -0.000758\n",
      " 4375/5000: episode: 4374, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.529 [0.003, 1.000], loss: 0.000005, mean_absolute_error: 41.738930, mean_q: -0.000707\n",
      " 4376/5000: episode: 4375, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.512], loss: 0.000001, mean_absolute_error: 41.742668, mean_q: -0.000747\n",
      " 4377/5000: episode: 4376, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.000, 0.997], loss: 0.000002, mean_absolute_error: 41.740162, mean_q: -0.000725\n",
      " 4378/5000: episode: 4377, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.000, 0.984], loss: 0.000006, mean_absolute_error: 41.733620, mean_q: -0.000725\n",
      " 4379/5000: episode: 4378, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.000, 0.687], loss: 0.000002, mean_absolute_error: 41.740692, mean_q: -0.000744\n",
      " 4380/5000: episode: 4379, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.065, 1.000], loss: 0.000001, mean_absolute_error: 41.744347, mean_q: -0.000773\n",
      " 4381/5000: episode: 4380, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 0.000006, mean_absolute_error: 41.730083, mean_q: -0.000592\n",
      " 4382/5000: episode: 4381, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.000, 0.890], loss: 0.000002, mean_absolute_error: 41.739975, mean_q: -0.000718\n",
      " 4383/5000: episode: 4382, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.051, 1.000], loss: 0.000004, mean_absolute_error: 41.735931, mean_q: -0.000699\n",
      " 4384/5000: episode: 4383, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.660], loss: 0.000005, mean_absolute_error: 41.737614, mean_q: -0.000718\n",
      " 4385/5000: episode: 4384, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.000, 0.948], loss: 0.000003, mean_absolute_error: 41.734863, mean_q: -0.000657\n",
      " 4386/5000: episode: 4385, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.000002, mean_absolute_error: 41.740482, mean_q: -0.000717\n",
      " 4387/5000: episode: 4386, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.554 [0.009, 1.000], loss: 0.000003, mean_absolute_error: 41.742107, mean_q: -0.000761\n",
      " 4388/5000: episode: 4387, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.067, 1.000], loss: 0.000002, mean_absolute_error: 41.740982, mean_q: -0.000689\n",
      " 4389/5000: episode: 4388, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.027, 1.000], loss: 0.000003, mean_absolute_error: 41.737450, mean_q: -0.000696\n",
      " 4390/5000: episode: 4389, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.320, 1.000], loss: 0.000005, mean_absolute_error: 41.737091, mean_q: -0.000705\n",
      " 4391/5000: episode: 4390, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.396, 1.000], loss: 0.000002, mean_absolute_error: 41.737228, mean_q: -0.000675\n",
      " 4392/5000: episode: 4391, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.230, 1.000], loss: 0.000003, mean_absolute_error: 41.741264, mean_q: -0.000775\n",
      " 4393/5000: episode: 4392, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.879], loss: 0.000002, mean_absolute_error: 41.739864, mean_q: -0.000717\n",
      " 4394/5000: episode: 4393, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.052, 1.000], loss: 0.000003, mean_absolute_error: 41.738667, mean_q: -0.000677\n",
      " 4395/5000: episode: 4394, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.497, 1.000], loss: 0.000001, mean_absolute_error: 41.745274, mean_q: -0.000788\n",
      " 4396/5000: episode: 4395, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 0.000003, mean_absolute_error: 41.739506, mean_q: -0.000733\n",
      " 4397/5000: episode: 4396, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.490, 1.000], loss: 0.000016, mean_absolute_error: 41.727200, mean_q: -0.000787\n",
      " 4398/5000: episode: 4397, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.033, 1.000], loss: 0.000001, mean_absolute_error: 41.745972, mean_q: -0.000816\n",
      " 4399/5000: episode: 4398, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.620], loss: 0.000002, mean_absolute_error: 41.738972, mean_q: -0.000751\n",
      " 4400/5000: episode: 4399, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.505 [0.000, 1.000], loss: 0.000003, mean_absolute_error: 41.737801, mean_q: -0.000720\n",
      " 4401/5000: episode: 4400, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.000, 0.988], loss: 0.000003, mean_absolute_error: 41.739586, mean_q: -0.000806\n",
      " 4402/5000: episode: 4401, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.323, 1.000], loss: 0.000003, mean_absolute_error: 41.740356, mean_q: -0.000770\n",
      " 4403/5000: episode: 4402, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.842], loss: 0.000000, mean_absolute_error: 41.744537, mean_q: -0.000795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4404/5000: episode: 4403, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.224, 1.000], loss: 0.000001, mean_absolute_error: 41.742504, mean_q: -0.000769\n",
      " 4405/5000: episode: 4404, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.000003, mean_absolute_error: 41.736927, mean_q: -0.000775\n",
      " 4406/5000: episode: 4405, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.000, 0.795], loss: 0.000003, mean_absolute_error: 41.739407, mean_q: -0.000766\n",
      " 4407/5000: episode: 4406, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.000, 0.916], loss: 0.000002, mean_absolute_error: 41.739315, mean_q: -0.000734\n",
      " 4408/5000: episode: 4407, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.431, 1.000], loss: 0.000002, mean_absolute_error: 41.740719, mean_q: -0.000716\n",
      " 4409/5000: episode: 4408, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.003, 1.000], loss: 0.000004, mean_absolute_error: 41.733871, mean_q: -0.000642\n",
      " 4410/5000: episode: 4409, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.556], loss: 0.000003, mean_absolute_error: 41.736351, mean_q: -0.000744\n",
      " 4411/5000: episode: 4410, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.000, 0.999], loss: 0.000006, mean_absolute_error: 41.739079, mean_q: -0.000751\n",
      " 4412/5000: episode: 4411, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.000, 0.826], loss: 0.000001, mean_absolute_error: 41.745022, mean_q: -0.000804\n",
      " 4413/5000: episode: 4412, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.519 [0.002, 1.000], loss: 0.000002, mean_absolute_error: 41.739960, mean_q: -0.000777\n",
      " 4414/5000: episode: 4413, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.370 [0.000, 0.951], loss: 0.000001, mean_absolute_error: 41.742821, mean_q: -0.000807\n",
      " 4415/5000: episode: 4414, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.499], loss: 0.000006, mean_absolute_error: 41.733040, mean_q: -0.000626\n",
      " 4416/5000: episode: 4415, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.323, 1.000], loss: 0.000005, mean_absolute_error: 41.737568, mean_q: -0.000706\n",
      " 4417/5000: episode: 4416, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.052, 1.000], loss: 0.000002, mean_absolute_error: 41.737030, mean_q: -0.000736\n",
      " 4418/5000: episode: 4417, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 0.000003, mean_absolute_error: 41.741859, mean_q: -0.000773\n",
      " 4419/5000: episode: 4418, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.000004, mean_absolute_error: 41.733471, mean_q: -0.000722\n",
      " 4420/5000: episode: 4419, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.511, 1.000], loss: 0.000006, mean_absolute_error: 41.727879, mean_q: -0.001115\n",
      " 4421/5000: episode: 4420, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.000, 0.999], loss: 0.000004, mean_absolute_error: 41.736168, mean_q: -0.000723\n",
      " 4422/5000: episode: 4421, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.000, 0.910], loss: 0.000001, mean_absolute_error: 41.744881, mean_q: -0.000841\n",
      " 4423/5000: episode: 4422, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.879], loss: 0.000001, mean_absolute_error: 41.743839, mean_q: -0.000805\n",
      " 4424/5000: episode: 4423, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.196, 1.000], loss: 0.000025, mean_absolute_error: 41.728737, mean_q: -0.000248\n",
      " 4425/5000: episode: 4424, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.056, 1.000], loss: 0.000001, mean_absolute_error: 41.740669, mean_q: -0.000750\n",
      " 4426/5000: episode: 4425, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: 0.000002, mean_absolute_error: 41.741329, mean_q: -0.000764\n",
      " 4427/5000: episode: 4426, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.178, 1.000], loss: 0.000003, mean_absolute_error: 41.737270, mean_q: -0.000755\n",
      " 4428/5000: episode: 4427, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.000, 0.884], loss: 0.000002, mean_absolute_error: 41.741692, mean_q: -0.000760\n",
      " 4429/5000: episode: 4428, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.470, 1.000], loss: 0.000017, mean_absolute_error: 41.733990, mean_q: -0.000538\n",
      " 4430/5000: episode: 4429, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.000, 0.992], loss: 0.000005, mean_absolute_error: 41.739258, mean_q: -0.000771\n",
      " 4431/5000: episode: 4430, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.742], loss: 0.000003, mean_absolute_error: 41.734974, mean_q: -0.000708\n",
      " 4432/5000: episode: 4431, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.525], loss: 0.000003, mean_absolute_error: 41.734859, mean_q: -0.000680\n",
      " 4433/5000: episode: 4432, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.000, 0.991], loss: 0.000002, mean_absolute_error: 41.739998, mean_q: -0.000778\n",
      " 4434/5000: episode: 4433, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.000, 0.936], loss: 0.000003, mean_absolute_error: 41.738586, mean_q: -0.000765\n",
      " 4435/5000: episode: 4434, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.159, 1.000], loss: 0.000007, mean_absolute_error: 41.729317, mean_q: -0.000723\n",
      " 4436/5000: episode: 4435, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.123, 1.000], loss: 0.000009, mean_absolute_error: 41.731243, mean_q: -0.000672\n",
      " 4437/5000: episode: 4436, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.761], loss: 0.000001, mean_absolute_error: 41.743496, mean_q: -0.000778\n",
      " 4438/5000: episode: 4437, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.628], loss: 0.000003, mean_absolute_error: 41.736740, mean_q: -0.000722\n",
      " 4439/5000: episode: 4438, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 0.000000, mean_absolute_error: 41.745216, mean_q: -0.000826\n",
      " 4440/5000: episode: 4439, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.041, 1.000], loss: 0.000002, mean_absolute_error: 41.741135, mean_q: -0.000774\n",
      " 4441/5000: episode: 4440, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.317, 1.000], loss: 0.000002, mean_absolute_error: 41.739048, mean_q: -0.000782\n",
      " 4442/5000: episode: 4441, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.044, 1.000], loss: 0.000003, mean_absolute_error: 41.737663, mean_q: -0.000826\n",
      " 4443/5000: episode: 4442, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.000, 0.947], loss: 0.000002, mean_absolute_error: 41.739098, mean_q: -0.000780\n",
      " 4444/5000: episode: 4443, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.000, 0.888], loss: 0.000003, mean_absolute_error: 41.736740, mean_q: -0.000766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4445/5000: episode: 4444, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.398, 1.000], loss: 0.000005, mean_absolute_error: 41.736221, mean_q: -0.000757\n",
      " 4446/5000: episode: 4445, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.249 [0.000, 0.797], loss: 0.000001, mean_absolute_error: 41.741699, mean_q: -0.000794\n",
      " 4447/5000: episode: 4446, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.000, 0.799], loss: 0.000002, mean_absolute_error: 41.737663, mean_q: -0.000781\n",
      " 4448/5000: episode: 4447, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.528], loss: 0.000002, mean_absolute_error: 41.740307, mean_q: -0.000804\n",
      " 4449/5000: episode: 4448, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.203, 1.000], loss: 0.000001, mean_absolute_error: 41.741573, mean_q: -0.000805\n",
      " 4450/5000: episode: 4449, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.863], loss: 0.000001, mean_absolute_error: 41.743340, mean_q: -0.000817\n",
      " 4451/5000: episode: 4450, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.861], loss: 0.000003, mean_absolute_error: 41.737850, mean_q: -0.000775\n",
      " 4452/5000: episode: 4451, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.000, 0.923], loss: 0.000003, mean_absolute_error: 41.736740, mean_q: -0.000723\n",
      " 4453/5000: episode: 4452, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 0.000002, mean_absolute_error: 41.738873, mean_q: -0.000806\n",
      " 4454/5000: episode: 4453, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.556], loss: 0.000003, mean_absolute_error: 41.735264, mean_q: -0.000753\n",
      " 4455/5000: episode: 4454, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.845], loss: 0.000004, mean_absolute_error: 41.736328, mean_q: -0.000777\n",
      " 4456/5000: episode: 4455, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.034, 1.000], loss: 0.000001, mean_absolute_error: 41.743378, mean_q: -0.000816\n",
      " 4457/5000: episode: 4456, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.766], loss: 0.000002, mean_absolute_error: 41.740589, mean_q: -0.000811\n",
      " 4458/5000: episode: 4457, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 0.000002, mean_absolute_error: 41.737900, mean_q: -0.000770\n",
      " 4459/5000: episode: 4458, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.420, 1.000], loss: 0.000001, mean_absolute_error: 41.743492, mean_q: -0.000826\n",
      " 4460/5000: episode: 4459, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.576], loss: 0.000001, mean_absolute_error: 41.741089, mean_q: -0.000712\n",
      " 4461/5000: episode: 4460, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.075, 1.000], loss: 0.000006, mean_absolute_error: 41.731979, mean_q: -0.000693\n",
      " 4462/5000: episode: 4461, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.025, 1.000], loss: 0.000003, mean_absolute_error: 41.739990, mean_q: -0.000779\n",
      " 4463/5000: episode: 4462, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.432, 1.000], loss: 0.000001, mean_absolute_error: 41.741188, mean_q: -0.000797\n",
      " 4464/5000: episode: 4463, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.723 [0.152, 1.000], loss: 0.000001, mean_absolute_error: 41.743507, mean_q: -0.000825\n",
      " 4465/5000: episode: 4464, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.001, 1.000], loss: 0.000007, mean_absolute_error: 41.733421, mean_q: -0.000668\n",
      " 4466/5000: episode: 4465, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.000, 0.976], loss: 0.000001, mean_absolute_error: 41.743263, mean_q: -0.000818\n",
      " 4467/5000: episode: 4466, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.006, 1.000], loss: 0.000002, mean_absolute_error: 41.739452, mean_q: -0.000788\n",
      " 4468/5000: episode: 4467, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.565 [0.013, 1.000], loss: 0.000003, mean_absolute_error: 41.737320, mean_q: -0.000772\n",
      " 4469/5000: episode: 4468, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.000, 0.897], loss: 0.000005, mean_absolute_error: 41.737534, mean_q: -0.000878\n",
      " 4470/5000: episode: 4469, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.314 [0.000, 0.898], loss: 0.000002, mean_absolute_error: 41.740143, mean_q: -0.000766\n",
      " 4471/5000: episode: 4470, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.377, 1.000], loss: 0.000002, mean_absolute_error: 41.739197, mean_q: -0.000798\n",
      " 4472/5000: episode: 4471, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.415, 1.000], loss: 0.000007, mean_absolute_error: 41.740009, mean_q: -0.000516\n",
      " 4473/5000: episode: 4472, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.505, 1.000], loss: 0.000006, mean_absolute_error: 41.739971, mean_q: -0.000680\n",
      " 4474/5000: episode: 4473, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.942], loss: 0.000001, mean_absolute_error: 41.745041, mean_q: -0.000863\n",
      " 4475/5000: episode: 4474, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.000007, mean_absolute_error: 41.732803, mean_q: -0.000680\n",
      " 4476/5000: episode: 4475, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.217, 1.000], loss: 0.000005, mean_absolute_error: 41.737392, mean_q: -0.000796\n",
      " 4477/5000: episode: 4476, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.269 [0.000, 0.836], loss: 0.000002, mean_absolute_error: 41.738739, mean_q: -0.000725\n",
      " 4478/5000: episode: 4477, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.000005, mean_absolute_error: 41.732559, mean_q: -0.000693\n",
      " 4479/5000: episode: 4478, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.166, 1.000], loss: 0.000004, mean_absolute_error: 41.736492, mean_q: -0.000795\n",
      " 4480/5000: episode: 4479, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.551], loss: 0.000003, mean_absolute_error: 41.736153, mean_q: -0.000784\n",
      " 4481/5000: episode: 4480, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.552], loss: 0.000051, mean_absolute_error: 41.728195, mean_q: 0.000220\n",
      " 4482/5000: episode: 4481, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.000, 0.945], loss: 0.000002, mean_absolute_error: 41.740410, mean_q: -0.000820\n",
      " 4483/5000: episode: 4482, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.293, 1.000], loss: 0.000002, mean_absolute_error: 41.738697, mean_q: -0.000818\n",
      " 4484/5000: episode: 4483, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.000, 0.919], loss: 0.000001, mean_absolute_error: 41.744431, mean_q: -0.000840\n",
      " 4485/5000: episode: 4484, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.114, 1.000], loss: 0.000002, mean_absolute_error: 41.743889, mean_q: -0.000757\n",
      " 4486/5000: episode: 4485, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.563 [0.013, 1.000], loss: 0.000002, mean_absolute_error: 41.739716, mean_q: -0.000812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4487/5000: episode: 4486, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.217, 1.000], loss: 0.000001, mean_absolute_error: 41.741783, mean_q: -0.000870\n",
      " 4488/5000: episode: 4487, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.651], loss: 0.000002, mean_absolute_error: 41.740311, mean_q: -0.000808\n",
      " 4489/5000: episode: 4488, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.386, 1.000], loss: 0.000005, mean_absolute_error: 41.730942, mean_q: -0.000703\n",
      " 4490/5000: episode: 4489, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.206, 1.000], loss: 0.000002, mean_absolute_error: 41.741768, mean_q: -0.000837\n",
      " 4491/5000: episode: 4490, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.000, 0.967], loss: 0.000005, mean_absolute_error: 41.735561, mean_q: -0.000778\n",
      " 4492/5000: episode: 4491, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.539 [0.005, 1.000], loss: 0.000001, mean_absolute_error: 41.742104, mean_q: -0.000883\n",
      " 4493/5000: episode: 4492, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.079, 1.000], loss: 0.000004, mean_absolute_error: 41.739006, mean_q: -0.000801\n",
      " 4494/5000: episode: 4493, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 0.000002, mean_absolute_error: 41.743507, mean_q: -0.000830\n",
      " 4495/5000: episode: 4494, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.000, 0.959], loss: 0.000005, mean_absolute_error: 41.734566, mean_q: -0.000793\n",
      " 4496/5000: episode: 4495, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.920], loss: 0.000004, mean_absolute_error: 41.738457, mean_q: -0.000753\n",
      " 4497/5000: episode: 4496, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.007, 1.000], loss: 0.000004, mean_absolute_error: 41.738289, mean_q: -0.000830\n",
      " 4498/5000: episode: 4497, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.343 [0.000, 0.929], loss: 0.000001, mean_absolute_error: 41.742859, mean_q: -0.000841\n",
      " 4499/5000: episode: 4498, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.000, 0.840], loss: 0.000002, mean_absolute_error: 41.737854, mean_q: -0.000748\n",
      " 4500/5000: episode: 4499, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.039, 1.000], loss: 0.000001, mean_absolute_error: 41.739567, mean_q: -0.000827\n",
      " 4501/5000: episode: 4500, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.548 [0.008, 1.000], loss: 0.000002, mean_absolute_error: 41.738106, mean_q: -0.000811\n",
      " 4502/5000: episode: 4501, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.780], loss: 0.000001, mean_absolute_error: 41.743935, mean_q: -0.000856\n",
      " 4503/5000: episode: 4502, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.000003, mean_absolute_error: 41.738312, mean_q: -0.000817\n",
      " 4504/5000: episode: 4503, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 0.000001, mean_absolute_error: 41.740559, mean_q: -0.000800\n",
      " 4505/5000: episode: 4504, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.000, 0.791], loss: 0.000002, mean_absolute_error: 41.735104, mean_q: -0.000797\n",
      " 4506/5000: episode: 4505, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.000, 0.741], loss: 0.000002, mean_absolute_error: 41.741043, mean_q: -0.000834\n",
      " 4507/5000: episode: 4506, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.677], loss: 0.000002, mean_absolute_error: 41.738701, mean_q: -0.000792\n",
      " 4508/5000: episode: 4507, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.631], loss: 0.000003, mean_absolute_error: 41.739548, mean_q: -0.000823\n",
      " 4509/5000: episode: 4508, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.429 [0.000, 0.985], loss: 0.000002, mean_absolute_error: 41.734783, mean_q: -0.000784\n",
      " 4510/5000: episode: 4509, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.000, 0.948], loss: 0.000199, mean_absolute_error: 41.706455, mean_q: -0.000728\n",
      " 4511/5000: episode: 4510, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 0.000004, mean_absolute_error: 41.734787, mean_q: -0.000761\n",
      " 4512/5000: episode: 4511, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.845], loss: 0.000003, mean_absolute_error: 41.737892, mean_q: -0.000782\n",
      " 4513/5000: episode: 4512, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.000, 0.666], loss: 0.000007, mean_absolute_error: 41.733143, mean_q: -0.000712\n",
      " 4514/5000: episode: 4513, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.194, 1.000], loss: 0.000001, mean_absolute_error: 41.742950, mean_q: -0.000808\n",
      " 4515/5000: episode: 4514, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.000, 0.999], loss: 0.000002, mean_absolute_error: 41.738525, mean_q: -0.000811\n",
      " 4516/5000: episode: 4515, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.358, 1.000], loss: 0.000004, mean_absolute_error: 41.736927, mean_q: -0.000775\n",
      " 4517/5000: episode: 4516, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.056, 1.000], loss: 0.000002, mean_absolute_error: 41.736565, mean_q: -0.000732\n",
      " 4518/5000: episode: 4517, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.544 [0.007, 1.000], loss: 0.000003, mean_absolute_error: 41.730206, mean_q: -0.000916\n",
      " 4519/5000: episode: 4518, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.660], loss: 0.000007, mean_absolute_error: 41.731461, mean_q: -0.000664\n",
      " 4520/5000: episode: 4519, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.274, 1.000], loss: 0.000004, mean_absolute_error: 41.732788, mean_q: -0.000804\n",
      " 4521/5000: episode: 4520, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 0.000003, mean_absolute_error: 41.735607, mean_q: -0.000696\n",
      " 4522/5000: episode: 4521, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.003, 1.000], loss: 0.000008, mean_absolute_error: 41.723526, mean_q: -0.001210\n",
      " 4523/5000: episode: 4522, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.000, 0.961], loss: 0.000002, mean_absolute_error: 41.736671, mean_q: -0.000793\n",
      " 4524/5000: episode: 4523, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.611], loss: 0.000001, mean_absolute_error: 41.741940, mean_q: -0.000860\n",
      " 4525/5000: episode: 4524, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.341, 1.000], loss: 0.000002, mean_absolute_error: 41.740940, mean_q: -0.000827\n",
      " 4526/5000: episode: 4525, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.559 [0.011, 1.000], loss: 0.000002, mean_absolute_error: 41.741096, mean_q: -0.000813\n",
      " 4527/5000: episode: 4526, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.621], loss: 0.000005, mean_absolute_error: 41.735397, mean_q: -0.000760\n",
      " 4528/5000: episode: 4527, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.481 [0.000, 0.998], loss: 0.000003, mean_absolute_error: 41.739021, mean_q: -0.000783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4529/5000: episode: 4528, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.022, 1.000], loss: 0.000005, mean_absolute_error: 41.739288, mean_q: -0.000764\n",
      " 4530/5000: episode: 4529, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.521], loss: 0.000003, mean_absolute_error: 41.738625, mean_q: -0.000809\n",
      " 4531/5000: episode: 4530, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.351, 1.000], loss: 0.000003, mean_absolute_error: 41.738945, mean_q: -0.000801\n",
      " 4532/5000: episode: 4531, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.288 [0.000, 0.865], loss: 0.000001, mean_absolute_error: 41.744411, mean_q: -0.000842\n",
      " 4533/5000: episode: 4532, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.842], loss: 0.000003, mean_absolute_error: 41.738831, mean_q: -0.000826\n",
      " 4534/5000: episode: 4533, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.515 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.741066, mean_q: -0.000823\n",
      " 4535/5000: episode: 4534, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.461, 1.000], loss: 0.000002, mean_absolute_error: 41.736710, mean_q: -0.000774\n",
      " 4536/5000: episode: 4535, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.000, 0.983], loss: 0.000005, mean_absolute_error: 41.738434, mean_q: -0.001195\n",
      " 4537/5000: episode: 4536, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.135, 1.000], loss: 0.000002, mean_absolute_error: 41.740051, mean_q: -0.000804\n",
      " 4538/5000: episode: 4537, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.637], loss: 0.000001, mean_absolute_error: 41.742859, mean_q: -0.000829\n",
      " 4539/5000: episode: 4538, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.000, 0.754], loss: 0.000001, mean_absolute_error: 41.742989, mean_q: -0.000811\n",
      " 4540/5000: episode: 4539, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.780], loss: 0.000005, mean_absolute_error: 41.734825, mean_q: -0.000744\n",
      " 4541/5000: episode: 4540, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 0.000008, mean_absolute_error: 41.729485, mean_q: -0.000785\n",
      " 4542/5000: episode: 4541, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.000, 0.882], loss: 0.000002, mean_absolute_error: 41.738617, mean_q: -0.000782\n",
      " 4543/5000: episode: 4542, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.140, 1.000], loss: 0.000002, mean_absolute_error: 41.739571, mean_q: -0.000838\n",
      " 4544/5000: episode: 4543, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.263, 1.000], loss: 0.000003, mean_absolute_error: 41.738941, mean_q: -0.000778\n",
      " 4545/5000: episode: 4544, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.109, 1.000], loss: 0.000002, mean_absolute_error: 41.743210, mean_q: -0.000853\n",
      " 4546/5000: episode: 4545, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.548 [0.008, 1.000], loss: 0.000001, mean_absolute_error: 41.743057, mean_q: -0.000821\n",
      " 4547/5000: episode: 4546, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.842], loss: 0.000004, mean_absolute_error: 41.735321, mean_q: -0.000711\n",
      " 4548/5000: episode: 4547, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.766], loss: 0.000001, mean_absolute_error: 41.743694, mean_q: -0.000858\n",
      " 4549/5000: episode: 4548, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.583], loss: 0.000003, mean_absolute_error: 41.734219, mean_q: -0.000753\n",
      " 4550/5000: episode: 4549, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.568 [0.014, 1.000], loss: 0.000000, mean_absolute_error: 41.746204, mean_q: -0.000883\n",
      " 4551/5000: episode: 4550, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.770], loss: 0.000003, mean_absolute_error: 41.739399, mean_q: -0.000837\n",
      " 4552/5000: episode: 4551, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 0.000008, mean_absolute_error: 41.732811, mean_q: -0.000874\n",
      " 4553/5000: episode: 4552, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.370, 1.000], loss: 0.000001, mean_absolute_error: 41.743980, mean_q: -0.000847\n",
      " 4554/5000: episode: 4553, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.110, 1.000], loss: 0.000001, mean_absolute_error: 41.743752, mean_q: -0.000865\n",
      " 4555/5000: episode: 4554, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.090, 1.000], loss: 0.000001, mean_absolute_error: 41.740040, mean_q: -0.000855\n",
      " 4556/5000: episode: 4555, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.606], loss: 0.000003, mean_absolute_error: 41.739277, mean_q: -0.000836\n",
      " 4557/5000: episode: 4556, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.680 [0.095, 1.000], loss: 0.000002, mean_absolute_error: 41.740791, mean_q: -0.000860\n",
      " 4558/5000: episode: 4557, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.122, 1.000], loss: 0.000002, mean_absolute_error: 41.740814, mean_q: -0.000836\n",
      " 4559/5000: episode: 4558, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.000, 0.817], loss: 0.000002, mean_absolute_error: 41.741600, mean_q: -0.000836\n",
      " 4560/5000: episode: 4559, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.000, 0.790], loss: 0.000002, mean_absolute_error: 41.738564, mean_q: -0.000775\n",
      " 4561/5000: episode: 4560, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.462, 1.000], loss: 0.000002, mean_absolute_error: 41.734138, mean_q: -0.000764\n",
      " 4562/5000: episode: 4561, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.579], loss: 0.000003, mean_absolute_error: 41.736946, mean_q: -0.000796\n",
      " 4563/5000: episode: 4562, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.360, 1.000], loss: 0.000005, mean_absolute_error: 41.737354, mean_q: -0.000802\n",
      " 4564/5000: episode: 4563, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 0.000003, mean_absolute_error: 41.740448, mean_q: -0.000791\n",
      " 4565/5000: episode: 4564, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.000, 0.878], loss: 0.000001, mean_absolute_error: 41.739777, mean_q: -0.000834\n",
      " 4566/5000: episode: 4565, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.000, 0.939], loss: 0.000000, mean_absolute_error: 41.747490, mean_q: -0.000885\n",
      " 4567/5000: episode: 4566, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.274, 1.000], loss: 0.000004, mean_absolute_error: 41.731529, mean_q: -0.000737\n",
      " 4568/5000: episode: 4567, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.796], loss: 0.000002, mean_absolute_error: 41.740429, mean_q: -0.000840\n",
      " 4569/5000: episode: 4568, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.748], loss: 0.000003, mean_absolute_error: 41.739433, mean_q: -0.000806\n",
      " 4570/5000: episode: 4569, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.431 [0.000, 0.985], loss: 0.000004, mean_absolute_error: 41.733139, mean_q: -0.000730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4571/5000: episode: 4570, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.405 [0.000, 0.973], loss: 0.000001, mean_absolute_error: 41.739498, mean_q: -0.000787\n",
      " 4572/5000: episode: 4571, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.000, 0.809], loss: 0.000006, mean_absolute_error: 41.734867, mean_q: -0.000733\n",
      " 4573/5000: episode: 4572, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.000, 0.977], loss: 0.000003, mean_absolute_error: 41.733639, mean_q: -0.000711\n",
      " 4574/5000: episode: 4573, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.513 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.739689, mean_q: -0.000811\n",
      " 4575/5000: episode: 4574, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.740], loss: 0.000018, mean_absolute_error: 41.737556, mean_q: -0.000422\n",
      " 4576/5000: episode: 4575, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.544], loss: 0.000002, mean_absolute_error: 41.742035, mean_q: -0.000819\n",
      " 4577/5000: episode: 4576, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.008, 1.000], loss: 0.000003, mean_absolute_error: 41.736389, mean_q: -0.000780\n",
      " 4578/5000: episode: 4577, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.013, 1.000], loss: 0.000001, mean_absolute_error: 41.744507, mean_q: -0.000868\n",
      " 4579/5000: episode: 4578, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 0.000004, mean_absolute_error: 41.737885, mean_q: -0.000833\n",
      " 4580/5000: episode: 4579, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.019, 1.000], loss: 0.000004, mean_absolute_error: 41.730530, mean_q: -0.000669\n",
      " 4581/5000: episode: 4580, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.023, 1.000], loss: 0.000004, mean_absolute_error: 41.734375, mean_q: -0.000728\n",
      " 4582/5000: episode: 4581, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.000, 0.999], loss: 0.000001, mean_absolute_error: 41.744717, mean_q: -0.000874\n",
      " 4583/5000: episode: 4582, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.012, 1.000], loss: 0.000007, mean_absolute_error: 41.729370, mean_q: -0.000828\n",
      " 4584/5000: episode: 4583, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.388 [0.000, 0.963], loss: 0.000000, mean_absolute_error: 41.746651, mean_q: -0.000920\n",
      " 4585/5000: episode: 4584, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 0.000001, mean_absolute_error: 41.744240, mean_q: -0.000877\n",
      " 4586/5000: episode: 4585, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.119, 1.000], loss: 0.000006, mean_absolute_error: 41.730576, mean_q: -0.000818\n",
      " 4587/5000: episode: 4586, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.000, 0.681], loss: 0.000010, mean_absolute_error: 41.726711, mean_q: -0.000691\n",
      " 4588/5000: episode: 4587, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.738], loss: 0.000003, mean_absolute_error: 41.736240, mean_q: -0.000786\n",
      " 4589/5000: episode: 4588, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.357 [0.000, 0.941], loss: 0.000002, mean_absolute_error: 41.739532, mean_q: -0.000814\n",
      " 4590/5000: episode: 4589, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.772], loss: 0.000001, mean_absolute_error: 41.742172, mean_q: -0.000848\n",
      " 4591/5000: episode: 4590, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.301, 1.000], loss: 0.000007, mean_absolute_error: 41.732800, mean_q: -0.000899\n",
      " 4592/5000: episode: 4591, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.738960, mean_q: -0.000843\n",
      " 4593/5000: episode: 4592, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.193, 1.000], loss: 0.000007, mean_absolute_error: 41.732201, mean_q: -0.000816\n",
      " 4594/5000: episode: 4593, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.708], loss: 0.000001, mean_absolute_error: 41.742146, mean_q: -0.000823\n",
      " 4595/5000: episode: 4594, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.325, 1.000], loss: 0.000001, mean_absolute_error: 41.742077, mean_q: -0.000851\n",
      " 4596/5000: episode: 4595, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.591], loss: 0.000003, mean_absolute_error: 41.737709, mean_q: -0.000775\n",
      " 4597/5000: episode: 4596, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.770], loss: 0.000001, mean_absolute_error: 41.743282, mean_q: -0.000849\n",
      " 4598/5000: episode: 4597, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 0.000002, mean_absolute_error: 41.740936, mean_q: -0.000882\n",
      " 4599/5000: episode: 4598, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.274 [0.000, 0.843], loss: 0.000001, mean_absolute_error: 41.744377, mean_q: -0.000867\n",
      " 4600/5000: episode: 4599, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.000, 1.000], loss: 0.000002, mean_absolute_error: 41.740963, mean_q: -0.000847\n",
      " 4601/5000: episode: 4600, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.000, 0.650], loss: 0.000003, mean_absolute_error: 41.738808, mean_q: -0.000798\n",
      " 4602/5000: episode: 4601, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.586], loss: 0.000002, mean_absolute_error: 41.736431, mean_q: -0.000788\n",
      " 4603/5000: episode: 4602, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.156, 1.000], loss: 0.000001, mean_absolute_error: 41.743530, mean_q: -0.000893\n",
      " 4604/5000: episode: 4603, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.318, 1.000], loss: 0.000004, mean_absolute_error: 41.733948, mean_q: -0.000821\n",
      " 4605/5000: episode: 4604, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 0.000006, mean_absolute_error: 41.733257, mean_q: -0.000768\n",
      " 4606/5000: episode: 4605, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.170, 1.000], loss: 0.000001, mean_absolute_error: 41.740383, mean_q: -0.000896\n",
      " 4607/5000: episode: 4606, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.189, 1.000], loss: 0.000002, mean_absolute_error: 41.738640, mean_q: -0.000824\n",
      " 4608/5000: episode: 4607, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.756], loss: 0.000003, mean_absolute_error: 41.742268, mean_q: -0.000830\n",
      " 4609/5000: episode: 4608, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 0.000005, mean_absolute_error: 41.735062, mean_q: -0.000752\n",
      " 4610/5000: episode: 4609, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.424, 1.000], loss: 0.000003, mean_absolute_error: 41.738968, mean_q: -0.000788\n",
      " 4611/5000: episode: 4610, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.656], loss: 0.000003, mean_absolute_error: 41.738144, mean_q: -0.000825\n",
      " 4612/5000: episode: 4611, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.270, 1.000], loss: 0.000003, mean_absolute_error: 41.735306, mean_q: -0.000860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4613/5000: episode: 4612, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.000, 0.862], loss: 0.000003, mean_absolute_error: 41.732861, mean_q: -0.000812\n",
      " 4614/5000: episode: 4613, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.000, 0.921], loss: 0.000003, mean_absolute_error: 41.737179, mean_q: -0.000760\n",
      " 4615/5000: episode: 4614, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.537], loss: 0.000011, mean_absolute_error: 41.731674, mean_q: -0.000562\n",
      " 4616/5000: episode: 4615, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.007, 1.000], loss: 0.000002, mean_absolute_error: 41.736614, mean_q: -0.000852\n",
      " 4617/5000: episode: 4616, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.740], loss: 0.000002, mean_absolute_error: 41.740608, mean_q: -0.000824\n",
      " 4618/5000: episode: 4617, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.024, 1.000], loss: 0.000001, mean_absolute_error: 41.743496, mean_q: -0.000881\n",
      " 4619/5000: episode: 4618, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.000, 0.790], loss: 0.000005, mean_absolute_error: 41.733089, mean_q: -0.000776\n",
      " 4620/5000: episode: 4619, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.000, 0.733], loss: 0.000003, mean_absolute_error: 41.734482, mean_q: -0.000827\n",
      " 4621/5000: episode: 4620, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.000, 0.900], loss: 0.000006, mean_absolute_error: 41.734669, mean_q: -0.000777\n",
      " 4622/5000: episode: 4621, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 0.000002, mean_absolute_error: 41.740494, mean_q: -0.000850\n",
      " 4623/5000: episode: 4622, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.000006, mean_absolute_error: 41.738686, mean_q: -0.000776\n",
      " 4624/5000: episode: 4623, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.000, 0.784], loss: 0.000004, mean_absolute_error: 41.737625, mean_q: -0.000849\n",
      " 4625/5000: episode: 4624, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.444, 1.000], loss: 0.000003, mean_absolute_error: 41.740425, mean_q: -0.000838\n",
      " 4626/5000: episode: 4625, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.155, 1.000], loss: 0.000007, mean_absolute_error: 41.733467, mean_q: -0.000800\n",
      " 4627/5000: episode: 4626, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.000, 0.938], loss: 0.000001, mean_absolute_error: 41.742886, mean_q: -0.000861\n",
      " 4628/5000: episode: 4627, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.032, 1.000], loss: 0.000002, mean_absolute_error: 41.740509, mean_q: -0.000819\n",
      " 4629/5000: episode: 4628, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.000, 0.953], loss: 0.000017, mean_absolute_error: 41.733814, mean_q: -0.000812\n",
      " 4630/5000: episode: 4629, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.143, 1.000], loss: 0.000001, mean_absolute_error: 41.746208, mean_q: -0.000913\n",
      " 4631/5000: episode: 4630, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.395 [0.000, 0.968], loss: 0.000000, mean_absolute_error: 41.746723, mean_q: -0.000902\n",
      " 4632/5000: episode: 4631, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 0.000001, mean_absolute_error: 41.741867, mean_q: -0.000822\n",
      " 4633/5000: episode: 4632, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.700], loss: 0.000001, mean_absolute_error: 41.744797, mean_q: -0.000890\n",
      " 4634/5000: episode: 4633, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.000, 0.809], loss: 0.000001, mean_absolute_error: 41.743942, mean_q: -0.000890\n",
      " 4635/5000: episode: 4634, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.525], loss: 0.000001, mean_absolute_error: 41.742874, mean_q: -0.000882\n",
      " 4636/5000: episode: 4635, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.759], loss: 0.000002, mean_absolute_error: 41.741020, mean_q: -0.000868\n",
      " 4637/5000: episode: 4636, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 0.000007, mean_absolute_error: 41.728424, mean_q: -0.000780\n",
      " 4638/5000: episode: 4637, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.466 [0.000, 0.996], loss: 0.000002, mean_absolute_error: 41.740257, mean_q: -0.000816\n",
      " 4639/5000: episode: 4638, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.002, 1.000], loss: 0.000002, mean_absolute_error: 41.733818, mean_q: -0.000783\n",
      " 4640/5000: episode: 4639, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.000, 0.845], loss: 0.000001, mean_absolute_error: 41.744835, mean_q: -0.000890\n",
      " 4641/5000: episode: 4640, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.000, 0.982], loss: 0.000001, mean_absolute_error: 41.744560, mean_q: -0.000893\n",
      " 4642/5000: episode: 4641, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 0.000002, mean_absolute_error: 41.741058, mean_q: -0.000837\n",
      " 4643/5000: episode: 4642, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.290, 1.000], loss: 0.000002, mean_absolute_error: 41.739826, mean_q: -0.000831\n",
      " 4644/5000: episode: 4643, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.053, 1.000], loss: 0.000010, mean_absolute_error: 41.721626, mean_q: -0.000600\n",
      " 4645/5000: episode: 4644, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.515 [0.001, 1.000], loss: 0.000003, mean_absolute_error: 41.734772, mean_q: -0.000779\n",
      " 4646/5000: episode: 4645, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.000003, mean_absolute_error: 41.738281, mean_q: -0.000813\n",
      " 4647/5000: episode: 4646, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.413 [0.000, 0.978], loss: 0.000002, mean_absolute_error: 41.738716, mean_q: -0.000823\n",
      " 4648/5000: episode: 4647, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.637], loss: 0.000000, mean_absolute_error: 41.745415, mean_q: -0.000897\n",
      " 4649/5000: episode: 4648, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.000, 0.666], loss: 0.000001, mean_absolute_error: 41.743786, mean_q: -0.000854\n",
      " 4650/5000: episode: 4649, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.482, 1.000], loss: 0.000001, mean_absolute_error: 41.742813, mean_q: -0.000844\n",
      " 4651/5000: episode: 4650, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.176, 1.000], loss: 0.000001, mean_absolute_error: 41.742859, mean_q: -0.000891\n",
      " 4652/5000: episode: 4651, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.159, 1.000], loss: 0.000002, mean_absolute_error: 41.738483, mean_q: -0.000823\n",
      " 4653/5000: episode: 4652, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.504 [0.000, 1.000], loss: 0.000001, mean_absolute_error: 41.740955, mean_q: -0.000833\n",
      " 4654/5000: episode: 4653, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.656 [0.070, 1.000], loss: 0.000004, mean_absolute_error: 41.738045, mean_q: -0.000789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4655/5000: episode: 4654, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.633], loss: 0.000002, mean_absolute_error: 41.740578, mean_q: -0.000871\n",
      " 4656/5000: episode: 4655, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.000, 0.986], loss: 0.000006, mean_absolute_error: 41.734451, mean_q: -0.000936\n",
      " 4657/5000: episode: 4656, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.590], loss: 0.000003, mean_absolute_error: 41.737534, mean_q: -0.000789\n",
      " 4658/5000: episode: 4657, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.413 [0.000, 0.978], loss: 0.000006, mean_absolute_error: 41.733452, mean_q: -0.000808\n",
      " 4659/5000: episode: 4658, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.231, 1.000], loss: 0.000002, mean_absolute_error: 41.739708, mean_q: -0.000830\n",
      " 4660/5000: episode: 4659, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.664 [0.078, 1.000], loss: 0.000002, mean_absolute_error: 41.740540, mean_q: -0.000879\n",
      " 4661/5000: episode: 4660, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.340, 1.000], loss: 0.000001, mean_absolute_error: 41.742798, mean_q: -0.000901\n",
      " 4662/5000: episode: 4661, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.000001, mean_absolute_error: 41.742340, mean_q: -0.000900\n",
      " 4663/5000: episode: 4662, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 0.000002, mean_absolute_error: 41.740082, mean_q: -0.000846\n",
      " 4664/5000: episode: 4663, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.560], loss: 0.000002, mean_absolute_error: 41.739868, mean_q: -0.000893\n",
      " 4665/5000: episode: 4664, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.445 [0.000, 0.990], loss: 0.000002, mean_absolute_error: 41.738102, mean_q: -0.000839\n",
      " 4666/5000: episode: 4665, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.000, 0.784], loss: 0.000002, mean_absolute_error: 41.738602, mean_q: -0.000856\n",
      " 4667/5000: episode: 4666, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.000, 0.992], loss: 0.000001, mean_absolute_error: 41.743607, mean_q: -0.000902\n",
      " 4668/5000: episode: 4667, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.391, 1.000], loss: 0.000002, mean_absolute_error: 41.737782, mean_q: -0.000827\n",
      " 4669/5000: episode: 4668, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.530], loss: 0.000004, mean_absolute_error: 41.734070, mean_q: -0.000685\n",
      " 4670/5000: episode: 4669, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.418, 1.000], loss: 0.000003, mean_absolute_error: 41.736259, mean_q: -0.000783\n",
      " 4671/5000: episode: 4670, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.486, 1.000], loss: 0.000001, mean_absolute_error: 41.743237, mean_q: -0.000874\n",
      " 4672/5000: episode: 4671, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.588 [0.023, 1.000], loss: 0.000001, mean_absolute_error: 41.742775, mean_q: -0.000854\n",
      " 4673/5000: episode: 4672, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.000, 0.945], loss: 0.000003, mean_absolute_error: 41.738880, mean_q: -0.000873\n",
      " 4674/5000: episode: 4673, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.097, 1.000], loss: 0.000002, mean_absolute_error: 41.739044, mean_q: -0.000823\n",
      " 4675/5000: episode: 4674, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.002, 1.000], loss: 0.000002, mean_absolute_error: 41.736919, mean_q: -0.000832\n",
      " 4676/5000: episode: 4675, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.000, 0.716], loss: 0.000005, mean_absolute_error: 41.734814, mean_q: -0.000858\n",
      " 4677/5000: episode: 4676, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.547], loss: 0.000001, mean_absolute_error: 41.741390, mean_q: -0.000875\n",
      " 4678/5000: episode: 4677, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.871], loss: 0.000002, mean_absolute_error: 41.738777, mean_q: -0.000784\n",
      " 4679/5000: episode: 4678, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 0.000009, mean_absolute_error: 41.724422, mean_q: -0.000696\n",
      " 4680/5000: episode: 4679, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.660], loss: 0.000004, mean_absolute_error: 41.733418, mean_q: -0.000690\n",
      " 4681/5000: episode: 4680, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.000, 0.943], loss: 0.000002, mean_absolute_error: 41.740250, mean_q: -0.000841\n",
      " 4682/5000: episode: 4681, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.517], loss: 0.000003, mean_absolute_error: 41.735573, mean_q: -0.000849\n",
      " 4683/5000: episode: 4682, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.000, 0.806], loss: 0.000003, mean_absolute_error: 41.738670, mean_q: -0.000826\n",
      " 4684/5000: episode: 4683, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.153, 1.000], loss: 0.000006, mean_absolute_error: 41.727196, mean_q: -0.000708\n",
      " 4685/5000: episode: 4684, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.568], loss: 0.000006, mean_absolute_error: 41.738525, mean_q: -0.000789\n",
      " 4686/5000: episode: 4685, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 0.000001, mean_absolute_error: 41.742725, mean_q: -0.000870\n",
      " 4687/5000: episode: 4686, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 0.000002, mean_absolute_error: 41.737915, mean_q: -0.000819\n",
      " 4688/5000: episode: 4687, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.391 [0.000, 0.965], loss: 0.000003, mean_absolute_error: 41.737545, mean_q: -0.000814\n",
      " 4689/5000: episode: 4688, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.269 [0.000, 0.835], loss: 0.000002, mean_absolute_error: 41.737606, mean_q: -0.000832\n",
      " 4690/5000: episode: 4689, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.485, 1.000], loss: 0.000003, mean_absolute_error: 41.738907, mean_q: -0.000815\n",
      " 4691/5000: episode: 4690, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.295 [0.000, 0.874], loss: 0.000007, mean_absolute_error: 41.732803, mean_q: -0.000746\n",
      " 4692/5000: episode: 4691, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.220, 1.000], loss: 0.000002, mean_absolute_error: 41.740089, mean_q: -0.000881\n",
      " 4693/5000: episode: 4692, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.473, 1.000], loss: 0.000002, mean_absolute_error: 41.738415, mean_q: -0.000842\n",
      " 4694/5000: episode: 4693, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 0.000004, mean_absolute_error: 41.739182, mean_q: -0.000850\n",
      " 4695/5000: episode: 4694, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.000, 0.959], loss: 0.000002, mean_absolute_error: 41.739983, mean_q: -0.000862\n",
      " 4696/5000: episode: 4695, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.738], loss: 0.000002, mean_absolute_error: 41.737026, mean_q: -0.000816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4697/5000: episode: 4696, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.609], loss: 0.000002, mean_absolute_error: 41.741043, mean_q: -0.000897\n",
      " 4698/5000: episode: 4697, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.141, 1.000], loss: 0.000003, mean_absolute_error: 41.737751, mean_q: -0.000868\n",
      " 4699/5000: episode: 4698, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.682], loss: 0.000002, mean_absolute_error: 41.737343, mean_q: -0.000861\n",
      " 4700/5000: episode: 4699, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.633], loss: 0.000003, mean_absolute_error: 41.742210, mean_q: -0.000868\n",
      " 4701/5000: episode: 4700, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.598 [0.028, 1.000], loss: 0.000001, mean_absolute_error: 41.744209, mean_q: -0.000921\n",
      " 4702/5000: episode: 4701, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.658 [0.072, 1.000], loss: 0.000006, mean_absolute_error: 41.733646, mean_q: -0.000726\n",
      " 4703/5000: episode: 4702, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.000, 0.995], loss: 0.000002, mean_absolute_error: 41.740410, mean_q: -0.000925\n",
      " 4704/5000: episode: 4703, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 0.000001, mean_absolute_error: 41.743156, mean_q: -0.000894\n",
      " 4705/5000: episode: 4704, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.000, 0.908], loss: 0.000005, mean_absolute_error: 41.735073, mean_q: -0.000751\n",
      " 4706/5000: episode: 4705, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.168, 1.000], loss: 0.000003, mean_absolute_error: 41.738987, mean_q: -0.000831\n",
      " 4707/5000: episode: 4706, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.000, 0.759], loss: 0.000003, mean_absolute_error: 41.737576, mean_q: -0.000842\n",
      " 4708/5000: episode: 4707, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.300, 1.000], loss: 0.000003, mean_absolute_error: 41.734989, mean_q: -0.000828\n",
      " 4709/5000: episode: 4708, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.335, 1.000], loss: 0.000004, mean_absolute_error: 41.733696, mean_q: -0.000818\n",
      " 4710/5000: episode: 4709, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.039, 1.000], loss: 0.000002, mean_absolute_error: 41.740005, mean_q: -0.000856\n",
      " 4711/5000: episode: 4710, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.412, 1.000], loss: 0.000003, mean_absolute_error: 41.733124, mean_q: -0.000818\n",
      " 4712/5000: episode: 4711, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.723 [0.152, 1.000], loss: 0.000006, mean_absolute_error: 41.735672, mean_q: -0.000754\n",
      " 4713/5000: episode: 4712, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.504 [0.000, 1.000], loss: 0.000005, mean_absolute_error: 41.735447, mean_q: -0.000861\n",
      " 4714/5000: episode: 4713, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.478 [0.000, 0.998], loss: 0.000003, mean_absolute_error: 41.736324, mean_q: -0.000796\n",
      " 4715/5000: episode: 4714, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.553], loss: 0.000008, mean_absolute_error: 41.729893, mean_q: -0.000714\n",
      " 4716/5000: episode: 4715, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 0.000001, mean_absolute_error: 41.741928, mean_q: -0.000834\n",
      " 4717/5000: episode: 4716, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.376, 1.000], loss: 0.000002, mean_absolute_error: 41.740944, mean_q: -0.000877\n",
      " 4718/5000: episode: 4717, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 0.704], loss: 0.000006, mean_absolute_error: 41.733074, mean_q: -0.000766\n",
      " 4719/5000: episode: 4718, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.117, 1.000], loss: 0.000002, mean_absolute_error: 41.737209, mean_q: -0.001271\n",
      " 4720/5000: episode: 4719, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.373, 1.000], loss: 0.000002, mean_absolute_error: 41.742119, mean_q: -0.000867\n",
      " 4721/5000: episode: 4720, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.000, 0.980], loss: 0.000001, mean_absolute_error: 41.743469, mean_q: -0.000906\n",
      " 4722/5000: episode: 4721, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.000, 0.546], loss: 0.000007, mean_absolute_error: 41.731941, mean_q: -0.000795\n",
      " 4723/5000: episode: 4722, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 0.000005, mean_absolute_error: 41.732147, mean_q: -0.000777\n",
      " 4724/5000: episode: 4723, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.440 [0.000, 0.989], loss: 0.000001, mean_absolute_error: 41.741001, mean_q: -0.000885\n",
      " 4725/5000: episode: 4724, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.575], loss: 0.000002, mean_absolute_error: 41.740803, mean_q: -0.000862\n",
      " 4726/5000: episode: 4725, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.000, 0.707], loss: 0.000001, mean_absolute_error: 41.741669, mean_q: -0.000893\n",
      " 4727/5000: episode: 4726, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.314 [0.000, 0.898], loss: 0.000002, mean_absolute_error: 41.742832, mean_q: -0.000870\n",
      " 4728/5000: episode: 4727, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.051, 1.000], loss: 0.000002, mean_absolute_error: 41.736549, mean_q: -0.000860\n",
      " 4729/5000: episode: 4728, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.161, 1.000], loss: 0.000005, mean_absolute_error: 41.734871, mean_q: -0.000774\n",
      " 4730/5000: episode: 4729, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.149, 1.000], loss: 0.000003, mean_absolute_error: 41.738632, mean_q: -0.000806\n",
      " 4731/5000: episode: 4730, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.506 [0.000, 1.000], loss: 0.000006, mean_absolute_error: 41.734962, mean_q: -0.000789\n",
      " 4732/5000: episode: 4731, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.000, 0.747], loss: 0.000003, mean_absolute_error: 41.734123, mean_q: -0.000758\n",
      " 4733/5000: episode: 4732, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 0.880], loss: 0.000433, mean_absolute_error: 41.713043, mean_q: 0.000847\n",
      " 4734/5000: episode: 4733, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.842], loss: 0.000003, mean_absolute_error: 41.738609, mean_q: -0.000861\n",
      " 4735/5000: episode: 4734, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.089, 1.000], loss: 0.000005, mean_absolute_error: 41.730885, mean_q: -0.000806\n",
      " 4736/5000: episode: 4735, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.775], loss: 0.000009, mean_absolute_error: 41.731281, mean_q: -0.000709\n",
      " 4737/5000: episode: 4736, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.000, 0.998], loss: 0.000003, mean_absolute_error: 41.738647, mean_q: -0.000808\n",
      " 4738/5000: episode: 4737, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 0.000004, mean_absolute_error: 41.736404, mean_q: -0.000696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4739/5000: episode: 4738, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.017, 1.000], loss: 0.000003, mean_absolute_error: 41.733665, mean_q: -0.000873\n",
      " 4740/5000: episode: 4739, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 0.000002, mean_absolute_error: 41.739155, mean_q: -0.000835\n",
      " 4741/5000: episode: 4740, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.000, 0.762], loss: 0.000001, mean_absolute_error: 41.743019, mean_q: -0.000872\n",
      " 4742/5000: episode: 4741, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.000, 0.853], loss: 0.000004, mean_absolute_error: 41.733921, mean_q: -0.000872\n",
      " 4743/5000: episode: 4742, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.340 [0.000, 0.926], loss: 0.000003, mean_absolute_error: 41.734116, mean_q: -0.000850\n",
      " 4744/5000: episode: 4743, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.000, 0.839], loss: 0.000001, mean_absolute_error: 41.744484, mean_q: -0.000916\n",
      " 4745/5000: episode: 4744, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.027, 1.000], loss: 0.000003, mean_absolute_error: 41.739616, mean_q: -0.000875\n",
      " 4746/5000: episode: 4745, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.046, 1.000], loss: 0.000004, mean_absolute_error: 41.735481, mean_q: -0.000803\n",
      " 4747/5000: episode: 4746, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.635], loss: 0.000004, mean_absolute_error: 41.731865, mean_q: -0.000791\n",
      " 4748/5000: episode: 4747, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.711], loss: 0.000008, mean_absolute_error: 41.733501, mean_q: -0.000734\n",
      " 4749/5000: episode: 4748, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.573], loss: 0.000000, mean_absolute_error: 41.745705, mean_q: -0.000925\n",
      " 4750/5000: episode: 4749, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.016, 1.000], loss: 0.000006, mean_absolute_error: 41.732697, mean_q: -0.000684\n",
      " 4751/5000: episode: 4750, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.000, 0.960], loss: 0.000003, mean_absolute_error: 41.738808, mean_q: -0.000848\n",
      " 4752/5000: episode: 4751, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.116, 1.000], loss: 0.000003, mean_absolute_error: 41.739212, mean_q: -0.000928\n",
      " 4753/5000: episode: 4752, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.300, 1.000], loss: 0.000005, mean_absolute_error: 41.735199, mean_q: -0.000867\n",
      " 4754/5000: episode: 4753, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.000, 0.982], loss: 0.000000, mean_absolute_error: 41.745071, mean_q: -0.000908\n",
      " 4755/5000: episode: 4754, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.585], loss: 0.000003, mean_absolute_error: 41.739372, mean_q: -0.000857\n",
      " 4756/5000: episode: 4755, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.535 [0.005, 1.000], loss: 0.000004, mean_absolute_error: 41.737495, mean_q: -0.000907\n",
      " 4757/5000: episode: 4756, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.380, 1.000], loss: 0.000002, mean_absolute_error: 41.738628, mean_q: -0.000878\n",
      " 4758/5000: episode: 4757, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.679 [0.094, 1.000], loss: 0.000002, mean_absolute_error: 41.739365, mean_q: -0.000847\n",
      " 4759/5000: episode: 4758, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.000, 0.957], loss: 0.000003, mean_absolute_error: 41.734306, mean_q: -0.000854\n",
      " 4760/5000: episode: 4759, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.454, 1.000], loss: 0.000002, mean_absolute_error: 41.739716, mean_q: -0.000896\n",
      " 4761/5000: episode: 4760, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.197, 1.000], loss: 0.000003, mean_absolute_error: 41.736031, mean_q: -0.000835\n",
      " 4762/5000: episode: 4761, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.206, 1.000], loss: 0.000002, mean_absolute_error: 41.740746, mean_q: -0.000885\n",
      " 4763/5000: episode: 4762, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.000, 0.884], loss: 0.000003, mean_absolute_error: 41.737835, mean_q: -0.000834\n",
      " 4764/5000: episode: 4763, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.371, 1.000], loss: 0.000004, mean_absolute_error: 41.735641, mean_q: -0.000840\n",
      " 4765/5000: episode: 4764, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.079, 1.000], loss: 0.000001, mean_absolute_error: 41.741486, mean_q: -0.000914\n",
      " 4766/5000: episode: 4765, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.243, 1.000], loss: 0.000003, mean_absolute_error: 41.741104, mean_q: -0.000891\n",
      " 4767/5000: episode: 4766, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.159, 1.000], loss: 0.000001, mean_absolute_error: 41.742256, mean_q: -0.000906\n",
      " 4768/5000: episode: 4767, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.000, 0.682], loss: 0.000001, mean_absolute_error: 41.743423, mean_q: -0.000950\n",
      " 4769/5000: episode: 4768, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.037, 1.000], loss: 0.000002, mean_absolute_error: 41.737183, mean_q: -0.000906\n",
      " 4770/5000: episode: 4769, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 0.000001, mean_absolute_error: 41.743546, mean_q: -0.000914\n",
      " 4771/5000: episode: 4770, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.034, 1.000], loss: 0.000002, mean_absolute_error: 41.738007, mean_q: -0.000864\n",
      " 4772/5000: episode: 4771, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.619], loss: 0.000002, mean_absolute_error: 41.736961, mean_q: -0.000875\n",
      " 4773/5000: episode: 4772, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.000, 1.000], loss: 0.000003, mean_absolute_error: 41.739662, mean_q: -0.000822\n",
      " 4774/5000: episode: 4773, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.000, 1.000], loss: 0.000002, mean_absolute_error: 41.739326, mean_q: -0.000851\n",
      " 4775/5000: episode: 4774, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.620], loss: 0.000003, mean_absolute_error: 41.737236, mean_q: -0.000835\n",
      " 4776/5000: episode: 4775, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.131, 1.000], loss: 0.000001, mean_absolute_error: 41.740871, mean_q: -0.000951\n",
      " 4777/5000: episode: 4776, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.000, 0.989], loss: 0.000003, mean_absolute_error: 41.740173, mean_q: -0.000907\n",
      " 4778/5000: episode: 4777, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.173, 1.000], loss: 0.000002, mean_absolute_error: 41.739563, mean_q: -0.000849\n",
      " 4779/5000: episode: 4778, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.639], loss: 0.000006, mean_absolute_error: 41.736301, mean_q: -0.000865\n",
      " 4780/5000: episode: 4779, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.045, 1.000], loss: 0.000002, mean_absolute_error: 41.740765, mean_q: -0.000884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4781/5000: episode: 4780, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.004, 1.000], loss: 0.000005, mean_absolute_error: 41.730492, mean_q: -0.000772\n",
      " 4782/5000: episode: 4781, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.413, 1.000], loss: 0.000012, mean_absolute_error: 41.730499, mean_q: -0.000833\n",
      " 4783/5000: episode: 4782, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.038, 1.000], loss: 0.000002, mean_absolute_error: 41.741135, mean_q: -0.000918\n",
      " 4784/5000: episode: 4783, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 0.881], loss: 0.000008, mean_absolute_error: 41.731209, mean_q: -0.000863\n",
      " 4785/5000: episode: 4784, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.073, 1.000], loss: 0.000001, mean_absolute_error: 41.744022, mean_q: -0.000901\n",
      " 4786/5000: episode: 4785, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.024, 1.000], loss: 0.000003, mean_absolute_error: 41.735970, mean_q: -0.000863\n",
      " 4787/5000: episode: 4786, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.239, 1.000], loss: 0.000003, mean_absolute_error: 41.736607, mean_q: -0.000869\n",
      " 4788/5000: episode: 4787, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.000, 0.992], loss: 0.000001, mean_absolute_error: 41.743401, mean_q: -0.000942\n",
      " 4789/5000: episode: 4788, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.000, 0.756], loss: 0.000001, mean_absolute_error: 41.739464, mean_q: -0.000871\n",
      " 4790/5000: episode: 4789, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.000, 0.968], loss: 0.000004, mean_absolute_error: 41.734138, mean_q: -0.000795\n",
      " 4791/5000: episode: 4790, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.659], loss: 0.000003, mean_absolute_error: 41.734131, mean_q: -0.000831\n",
      " 4792/5000: episode: 4791, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.724], loss: 0.000004, mean_absolute_error: 41.736362, mean_q: -0.000814\n",
      " 4793/5000: episode: 4792, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.153, 1.000], loss: 0.000003, mean_absolute_error: 41.739189, mean_q: -0.000878\n",
      " 4794/5000: episode: 4793, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.000, 0.998], loss: 0.000004, mean_absolute_error: 41.733711, mean_q: -0.000881\n",
      " 4795/5000: episode: 4794, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.384, 1.000], loss: 0.000003, mean_absolute_error: 41.737797, mean_q: -0.000832\n",
      " 4796/5000: episode: 4795, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.432 [0.000, 0.986], loss: 0.000003, mean_absolute_error: 41.738289, mean_q: -0.000807\n",
      " 4797/5000: episode: 4796, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.000, 0.986], loss: 0.000001, mean_absolute_error: 41.745354, mean_q: -0.000922\n",
      " 4798/5000: episode: 4797, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.320, 1.000], loss: 0.000001, mean_absolute_error: 41.742336, mean_q: -0.000891\n",
      " 4799/5000: episode: 4798, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.000, 0.900], loss: 0.000004, mean_absolute_error: 41.739468, mean_q: -0.000952\n",
      " 4800/5000: episode: 4799, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.778 [0.267, 1.000], loss: 0.000002, mean_absolute_error: 41.740364, mean_q: -0.000901\n",
      " 4801/5000: episode: 4800, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.657 [0.071, 1.000], loss: 0.000004, mean_absolute_error: 41.736870, mean_q: -0.000854\n",
      " 4802/5000: episode: 4801, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.000, 0.999], loss: 0.000019, mean_absolute_error: 41.728786, mean_q: -0.000513\n",
      " 4803/5000: episode: 4802, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.000, 0.659], loss: 0.000003, mean_absolute_error: 41.739357, mean_q: -0.000939\n",
      " 4804/5000: episode: 4803, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.017, 1.000], loss: 0.000003, mean_absolute_error: 41.741444, mean_q: -0.000939\n",
      " 4805/5000: episode: 4804, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.234, 1.000], loss: 0.000003, mean_absolute_error: 41.737465, mean_q: -0.000886\n",
      " 4806/5000: episode: 4805, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.009, 1.000], loss: 0.000003, mean_absolute_error: 41.741585, mean_q: -0.000889\n",
      " 4807/5000: episode: 4806, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.462, 1.000], loss: 0.000002, mean_absolute_error: 41.741051, mean_q: -0.000893\n",
      " 4808/5000: episode: 4807, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.023, 1.000], loss: 0.000002, mean_absolute_error: 41.736794, mean_q: -0.000809\n",
      " 4809/5000: episode: 4808, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 0.000004, mean_absolute_error: 41.731457, mean_q: -0.000806\n",
      " 4810/5000: episode: 4809, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.361, 1.000], loss: 0.000004, mean_absolute_error: 41.738865, mean_q: -0.000874\n",
      " 4811/5000: episode: 4810, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.000, 0.638], loss: 0.000002, mean_absolute_error: 41.741318, mean_q: -0.000895\n",
      " 4812/5000: episode: 4811, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.000, 0.790], loss: 0.000004, mean_absolute_error: 41.731091, mean_q: -0.000767\n",
      " 4813/5000: episode: 4812, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.188, 1.000], loss: 0.000015, mean_absolute_error: 41.730927, mean_q: -0.000985\n",
      " 4814/5000: episode: 4813, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.453, 1.000], loss: 0.000002, mean_absolute_error: 41.740181, mean_q: -0.000861\n",
      " 4815/5000: episode: 4814, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.080, 1.000], loss: 0.000003, mean_absolute_error: 41.738533, mean_q: -0.000859\n",
      " 4816/5000: episode: 4815, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.153, 1.000], loss: 0.000002, mean_absolute_error: 41.741142, mean_q: -0.000892\n",
      " 4817/5000: episode: 4816, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.909], loss: 0.000001, mean_absolute_error: 41.744270, mean_q: -0.000942\n",
      " 4818/5000: episode: 4817, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.472 [0.000, 0.997], loss: 0.000001, mean_absolute_error: 41.745102, mean_q: -0.000922\n",
      " 4819/5000: episode: 4818, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.554], loss: 0.000003, mean_absolute_error: 41.737808, mean_q: -0.000794\n",
      " 4820/5000: episode: 4819, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 0.630], loss: 0.000002, mean_absolute_error: 41.739174, mean_q: -0.000887\n",
      " 4821/5000: episode: 4820, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.358, 1.000], loss: 0.000003, mean_absolute_error: 41.735962, mean_q: -0.000876\n",
      " 4822/5000: episode: 4821, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.474, 1.000], loss: 0.000001, mean_absolute_error: 41.743370, mean_q: -0.000877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4823/5000: episode: 4822, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.000, 0.722], loss: 0.000002, mean_absolute_error: 41.741936, mean_q: -0.000883\n",
      " 4824/5000: episode: 4823, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.459 [0.000, 0.994], loss: 0.000005, mean_absolute_error: 41.736801, mean_q: -0.000872\n",
      " 4825/5000: episode: 4824, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.308, 1.000], loss: 0.000002, mean_absolute_error: 41.735985, mean_q: -0.000851\n",
      " 4826/5000: episode: 4825, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.727], loss: 0.000002, mean_absolute_error: 41.734657, mean_q: -0.000846\n",
      " 4827/5000: episode: 4826, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.679 [0.094, 1.000], loss: 0.000002, mean_absolute_error: 41.740963, mean_q: -0.000888\n",
      " 4828/5000: episode: 4827, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.148, 1.000], loss: 0.000004, mean_absolute_error: 41.740879, mean_q: -0.000946\n",
      " 4829/5000: episode: 4828, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.481 [0.000, 0.998], loss: 0.000004, mean_absolute_error: 41.739380, mean_q: -0.000869\n",
      " 4830/5000: episode: 4829, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.589 [0.024, 1.000], loss: 0.000005, mean_absolute_error: 41.730629, mean_q: -0.000813\n",
      " 4831/5000: episode: 4830, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.456, 1.000], loss: 0.000021, mean_absolute_error: 41.721947, mean_q: -0.000973\n",
      " 4832/5000: episode: 4831, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.864], loss: 0.000005, mean_absolute_error: 41.735229, mean_q: -0.000798\n",
      " 4833/5000: episode: 4832, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.000, 0.626], loss: 0.000002, mean_absolute_error: 41.737301, mean_q: -0.000844\n",
      " 4834/5000: episode: 4833, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.000, 0.916], loss: 0.000000, mean_absolute_error: 41.745487, mean_q: -0.000874\n",
      " 4835/5000: episode: 4834, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.563 [0.012, 1.000], loss: 0.000002, mean_absolute_error: 41.739571, mean_q: -0.000850\n",
      " 4836/5000: episode: 4835, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.019, 1.000], loss: 0.000003, mean_absolute_error: 41.735210, mean_q: -0.000803\n",
      " 4837/5000: episode: 4836, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.037, 1.000], loss: 0.000001, mean_absolute_error: 41.742905, mean_q: -0.000893\n",
      " 4838/5000: episode: 4837, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.254, 1.000], loss: 0.000003, mean_absolute_error: 41.736435, mean_q: -0.000793\n",
      " 4839/5000: episode: 4838, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.462, 1.000], loss: 0.000001, mean_absolute_error: 41.742081, mean_q: -0.000954\n",
      " 4840/5000: episode: 4839, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.000, 0.725], loss: 0.000002, mean_absolute_error: 41.741238, mean_q: -0.000919\n",
      " 4841/5000: episode: 4840, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.598 [0.028, 1.000], loss: 0.000001, mean_absolute_error: 41.740204, mean_q: -0.000867\n",
      " 4842/5000: episode: 4841, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.480, 1.000], loss: 0.000003, mean_absolute_error: 41.735153, mean_q: -0.000821\n",
      " 4843/5000: episode: 4842, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.000, 0.963], loss: 0.000002, mean_absolute_error: 41.740158, mean_q: -0.000907\n",
      " 4844/5000: episode: 4843, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.517], loss: 0.000005, mean_absolute_error: 41.736813, mean_q: -0.000838\n",
      " 4845/5000: episode: 4844, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.540 [0.006, 1.000], loss: 0.000001, mean_absolute_error: 41.742706, mean_q: -0.000925\n",
      " 4846/5000: episode: 4845, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.000, 0.872], loss: 0.000002, mean_absolute_error: 41.737549, mean_q: -0.000864\n",
      " 4847/5000: episode: 4846, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.131, 1.000], loss: 0.000002, mean_absolute_error: 41.739544, mean_q: -0.000809\n",
      " 4848/5000: episode: 4847, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.498, 1.000], loss: 0.000001, mean_absolute_error: 41.744438, mean_q: -0.000933\n",
      " 4849/5000: episode: 4848, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 0.657], loss: 0.000002, mean_absolute_error: 41.739098, mean_q: -0.000840\n",
      " 4850/5000: episode: 4849, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.000001, mean_absolute_error: 41.744987, mean_q: -0.000983\n",
      " 4851/5000: episode: 4850, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.058, 1.000], loss: 0.000003, mean_absolute_error: 41.737793, mean_q: -0.000844\n",
      " 4852/5000: episode: 4851, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.017, 1.000], loss: 0.000004, mean_absolute_error: 41.735237, mean_q: -0.000818\n",
      " 4853/5000: episode: 4852, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.000, 0.781], loss: 0.000001, mean_absolute_error: 41.740700, mean_q: -0.000884\n",
      " 4854/5000: episode: 4853, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.311, 1.000], loss: 0.000003, mean_absolute_error: 41.737682, mean_q: -0.000896\n",
      " 4855/5000: episode: 4854, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.493, 1.000], loss: 0.000002, mean_absolute_error: 41.739311, mean_q: -0.000843\n",
      " 4856/5000: episode: 4855, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.000, 0.809], loss: 0.000001, mean_absolute_error: 41.742760, mean_q: -0.000950\n",
      " 4857/5000: episode: 4856, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.012, 1.000], loss: 0.000002, mean_absolute_error: 41.738785, mean_q: -0.000893\n",
      " 4858/5000: episode: 4857, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.000, 0.937], loss: 0.000005, mean_absolute_error: 41.736359, mean_q: -0.000864\n",
      " 4859/5000: episode: 4858, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.741772, mean_q: -0.000933\n",
      " 4860/5000: episode: 4859, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.210, 1.000], loss: 0.000003, mean_absolute_error: 41.739231, mean_q: -0.000831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4861/5000: episode: 4860, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.362, 1.000], loss: 0.000001, mean_absolute_error: 41.740776, mean_q: -0.000894\n",
      " 4862/5000: episode: 4861, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.000, 0.555], loss: 0.000005, mean_absolute_error: 41.733192, mean_q: -0.000843\n",
      " 4863/5000: episode: 4862, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.581], loss: 0.000005, mean_absolute_error: 41.735046, mean_q: -0.000819\n",
      " 4864/5000: episode: 4863, duration: 0.017s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.096, 1.000], loss: 0.000002, mean_absolute_error: 41.740826, mean_q: -0.000916\n",
      " 4865/5000: episode: 4864, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.437, 1.000], loss: 0.000002, mean_absolute_error: 41.735695, mean_q: -0.000844\n",
      " 4866/5000: episode: 4865, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.000, 0.919], loss: 0.000002, mean_absolute_error: 41.739098, mean_q: -0.000882\n",
      " 4867/5000: episode: 4866, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.000, 0.955], loss: 0.000006, mean_absolute_error: 41.727951, mean_q: -0.000770\n",
      " 4868/5000: episode: 4867, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.000, 0.909], loss: 0.000002, mean_absolute_error: 41.742317, mean_q: -0.000908\n",
      " 4869/5000: episode: 4868, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.576], loss: 0.000003, mean_absolute_error: 41.734818, mean_q: -0.000854\n",
      " 4870/5000: episode: 4869, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.334, 1.000], loss: 0.000003, mean_absolute_error: 41.739677, mean_q: -0.000898\n",
      " 4871/5000: episode: 4870, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.000, 0.608], loss: 0.000005, mean_absolute_error: 41.732407, mean_q: -0.000900\n",
      " 4872/5000: episode: 4871, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.458 [0.000, 0.994], loss: 0.000001, mean_absolute_error: 41.742851, mean_q: -0.000929\n",
      " 4873/5000: episode: 4872, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 0.772], loss: 0.000001, mean_absolute_error: 41.740726, mean_q: -0.000874\n",
      " 4874/5000: episode: 4873, duration: 0.020s, episode steps: 1, steps per second: 49, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.701], loss: 0.000003, mean_absolute_error: 41.738632, mean_q: -0.000841\n",
      " 4875/5000: episode: 4874, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.304, 1.000], loss: 0.000003, mean_absolute_error: 41.736820, mean_q: -0.000845\n",
      " 4876/5000: episode: 4875, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.000, 0.710], loss: 0.000002, mean_absolute_error: 41.739799, mean_q: -0.000914\n",
      " 4877/5000: episode: 4876, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.739475, mean_q: -0.000878\n",
      " 4878/5000: episode: 4877, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.406, 1.000], loss: 0.000001, mean_absolute_error: 41.742126, mean_q: -0.000887\n",
      " 4879/5000: episode: 4878, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.503], loss: 0.000002, mean_absolute_error: 41.738647, mean_q: -0.000927\n",
      " 4880/5000: episode: 4879, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.011, 1.000], loss: 0.000006, mean_absolute_error: 41.735023, mean_q: -0.000899\n",
      " 4881/5000: episode: 4880, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.459, 1.000], loss: 0.000003, mean_absolute_error: 41.737717, mean_q: -0.000908\n",
      " 4882/5000: episode: 4881, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.006, 1.000], loss: 0.000002, mean_absolute_error: 41.741386, mean_q: -0.000906\n",
      " 4883/5000: episode: 4882, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.620], loss: 0.000002, mean_absolute_error: 41.737587, mean_q: -0.000830\n",
      " 4884/5000: episode: 4883, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.000, 0.617], loss: 0.000001, mean_absolute_error: 41.743179, mean_q: -0.000884\n",
      " 4885/5000: episode: 4884, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.051, 1.000], loss: 0.000002, mean_absolute_error: 41.740704, mean_q: -0.000879\n",
      " 4886/5000: episode: 4885, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.000, 0.940], loss: 0.000003, mean_absolute_error: 41.739044, mean_q: -0.000877\n",
      " 4887/5000: episode: 4886, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.005, 1.000], loss: 0.000001, mean_absolute_error: 41.744370, mean_q: -0.000911\n",
      " 4888/5000: episode: 4887, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.895], loss: 0.000001, mean_absolute_error: 41.742668, mean_q: -0.000915\n",
      " 4889/5000: episode: 4888, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.358, 1.000], loss: 0.000004, mean_absolute_error: 41.735241, mean_q: -0.000902\n",
      " 4890/5000: episode: 4889, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.503], loss: 0.000004, mean_absolute_error: 41.734993, mean_q: -0.000796\n",
      " 4891/5000: episode: 4890, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.270 [0.000, 0.838], loss: 0.000004, mean_absolute_error: 41.736038, mean_q: -0.000889\n",
      " 4892/5000: episode: 4891, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.582 [0.020, 1.000], loss: 0.000003, mean_absolute_error: 41.733299, mean_q: -0.001163\n",
      " 4893/5000: episode: 4892, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.310, 1.000], loss: 0.000002, mean_absolute_error: 41.742287, mean_q: -0.000866\n",
      " 4894/5000: episode: 4893, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.413, 1.000], loss: 0.000004, mean_absolute_error: 41.734558, mean_q: -0.000883\n",
      " 4895/5000: episode: 4894, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.224, 1.000], loss: 0.000003, mean_absolute_error: 41.738815, mean_q: -0.000866\n",
      " 4896/5000: episode: 4895, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.013, 1.000], loss: 0.000001, mean_absolute_error: 41.740845, mean_q: -0.000882\n",
      " 4897/5000: episode: 4896, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.775], loss: 0.000005, mean_absolute_error: 41.733269, mean_q: -0.000764\n",
      " 4898/5000: episode: 4897, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.449, 1.000], loss: 0.000005, mean_absolute_error: 41.735874, mean_q: -0.000835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4899/5000: episode: 4898, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.000, 1.000], loss: 0.000002, mean_absolute_error: 41.738293, mean_q: -0.000864\n",
      " 4900/5000: episode: 4899, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.184, 1.000], loss: 0.000003, mean_absolute_error: 41.737694, mean_q: -0.000852\n",
      " 4901/5000: episode: 4900, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.121, 1.000], loss: 0.000003, mean_absolute_error: 41.738205, mean_q: -0.000848\n",
      " 4902/5000: episode: 4901, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.000, 0.573], loss: 0.000004, mean_absolute_error: 41.736984, mean_q: -0.000861\n",
      " 4903/5000: episode: 4902, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.536], loss: 0.000003, mean_absolute_error: 41.739044, mean_q: -0.000880\n",
      " 4904/5000: episode: 4903, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.206, 1.000], loss: 0.000006, mean_absolute_error: 41.730820, mean_q: -0.001243\n",
      " 4905/5000: episode: 4904, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.294 [0.000, 0.873], loss: 0.000006, mean_absolute_error: 41.735931, mean_q: -0.000781\n",
      " 4906/5000: episode: 4905, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.000, 0.596], loss: 0.000001, mean_absolute_error: 41.740662, mean_q: -0.000945\n",
      " 4907/5000: episode: 4906, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.000, 0.795], loss: 0.000001, mean_absolute_error: 41.744888, mean_q: -0.000925\n",
      " 4908/5000: episode: 4907, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.000, 0.814], loss: 0.000002, mean_absolute_error: 41.739677, mean_q: -0.000889\n",
      " 4909/5000: episode: 4908, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.498], loss: 0.000003, mean_absolute_error: 41.736824, mean_q: -0.000874\n",
      " 4910/5000: episode: 4909, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.019, 1.000], loss: 0.000003, mean_absolute_error: 41.737976, mean_q: -0.000878\n",
      " 4911/5000: episode: 4910, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.000, 0.769], loss: 0.000003, mean_absolute_error: 41.738152, mean_q: -0.000803\n",
      " 4912/5000: episode: 4911, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.000, 0.912], loss: 0.000004, mean_absolute_error: 41.737999, mean_q: -0.000875\n",
      " 4913/5000: episode: 4912, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.069, 1.000], loss: 0.000001, mean_absolute_error: 41.743412, mean_q: -0.000908\n",
      " 4914/5000: episode: 4913, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.079, 1.000], loss: 0.000003, mean_absolute_error: 41.736534, mean_q: -0.000828\n",
      " 4915/5000: episode: 4914, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.247, 1.000], loss: 0.000002, mean_absolute_error: 41.740425, mean_q: -0.000944\n",
      " 4916/5000: episode: 4915, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.000, 0.580], loss: 0.000003, mean_absolute_error: 41.739834, mean_q: -0.000897\n",
      " 4917/5000: episode: 4916, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.000, 0.640], loss: 0.000006, mean_absolute_error: 41.728695, mean_q: -0.000893\n",
      " 4918/5000: episode: 4917, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.000, 0.676], loss: 0.000003, mean_absolute_error: 41.738327, mean_q: -0.000850\n",
      " 4919/5000: episode: 4918, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 0.000006, mean_absolute_error: 41.734669, mean_q: -0.000845\n",
      " 4920/5000: episode: 4919, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.000, 0.776], loss: 0.000006, mean_absolute_error: 41.739342, mean_q: -0.000878\n",
      " 4921/5000: episode: 4920, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.421, 1.000], loss: 0.000004, mean_absolute_error: 41.735138, mean_q: -0.000856\n",
      " 4922/5000: episode: 4921, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.446, 1.000], loss: 0.000003, mean_absolute_error: 41.735638, mean_q: -0.000897\n",
      " 4923/5000: episode: 4922, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.408, 1.000], loss: 0.000003, mean_absolute_error: 41.737873, mean_q: -0.000830\n",
      " 4924/5000: episode: 4923, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.000, 0.622], loss: 0.000004, mean_absolute_error: 41.736565, mean_q: -0.000840\n",
      " 4925/5000: episode: 4924, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.000, 0.701], loss: 0.000005, mean_absolute_error: 41.736908, mean_q: -0.000839\n",
      " 4926/5000: episode: 4925, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.345, 1.000], loss: 0.000006, mean_absolute_error: 41.728436, mean_q: -0.000789\n",
      " 4927/5000: episode: 4926, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 0.803], loss: 0.000001, mean_absolute_error: 41.740707, mean_q: -0.000910\n",
      " 4928/5000: episode: 4927, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.424 [0.000, 0.982], loss: 0.000005, mean_absolute_error: 41.732491, mean_q: -0.000829\n",
      " 4929/5000: episode: 4928, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.350, 1.000], loss: 0.000004, mean_absolute_error: 41.738338, mean_q: -0.000843\n",
      " 4930/5000: episode: 4929, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.507, 1.000], loss: 0.000002, mean_absolute_error: 41.741016, mean_q: -0.000902\n",
      " 4931/5000: episode: 4930, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.036, 1.000], loss: 0.000002, mean_absolute_error: 41.742290, mean_q: -0.000874\n",
      " 4932/5000: episode: 4931, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.540], loss: 0.000005, mean_absolute_error: 41.738075, mean_q: -0.000856\n",
      " 4933/5000: episode: 4932, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.383, 1.000], loss: 0.000003, mean_absolute_error: 41.737816, mean_q: -0.000882\n",
      " 4934/5000: episode: 4933, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.000, 0.824], loss: 0.000008, mean_absolute_error: 41.727406, mean_q: -0.000761\n",
      " 4935/5000: episode: 4934, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.388 [0.000, 0.963], loss: 0.000001, mean_absolute_error: 41.743607, mean_q: -0.000924\n",
      " 4936/5000: episode: 4935, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.000, 0.885], loss: 0.000002, mean_absolute_error: 41.738026, mean_q: -0.000881\n",
      " 4937/5000: episode: 4936, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.000, 0.728], loss: 0.000002, mean_absolute_error: 41.738480, mean_q: -0.000863\n",
      " 4938/5000: episode: 4937, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.527], loss: 0.000002, mean_absolute_error: 41.738316, mean_q: -0.000884\n",
      " 4939/5000: episode: 4938, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.465 [0.000, 0.996], loss: 0.000004, mean_absolute_error: 41.734810, mean_q: -0.000840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4940/5000: episode: 4939, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.775], loss: 0.000002, mean_absolute_error: 41.742748, mean_q: -0.000954\n",
      " 4941/5000: episode: 4940, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.000, 0.937], loss: 0.000002, mean_absolute_error: 41.742893, mean_q: -0.000905\n",
      " 4942/5000: episode: 4941, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.000, 0.894], loss: 0.000001, mean_absolute_error: 41.745308, mean_q: -0.000948\n",
      " 4943/5000: episode: 4942, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.000, 0.981], loss: 0.000001, mean_absolute_error: 41.739723, mean_q: -0.000906\n",
      " 4944/5000: episode: 4943, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.533], loss: 0.000001, mean_absolute_error: 41.742165, mean_q: -0.000891\n",
      " 4945/5000: episode: 4944, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.000, 0.927], loss: 0.000005, mean_absolute_error: 41.738808, mean_q: -0.000883\n",
      " 4946/5000: episode: 4945, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.005, 1.000], loss: 0.000005, mean_absolute_error: 41.733383, mean_q: -0.000785\n",
      " 4947/5000: episode: 4946, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.381, 1.000], loss: 0.000002, mean_absolute_error: 41.740875, mean_q: -0.000960\n",
      " 4948/5000: episode: 4947, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.083, 1.000], loss: 0.000003, mean_absolute_error: 41.735119, mean_q: -0.000860\n",
      " 4949/5000: episode: 4948, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.000, 0.894], loss: 0.000004, mean_absolute_error: 41.734978, mean_q: -0.000836\n",
      " 4950/5000: episode: 4949, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.395 [0.000, 0.968], loss: 0.000016, mean_absolute_error: 41.731636, mean_q: -0.000832\n",
      " 4951/5000: episode: 4950, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.004, 1.000], loss: 0.000003, mean_absolute_error: 41.739639, mean_q: -0.000859\n",
      " 4952/5000: episode: 4951, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.000, 0.697], loss: 0.000001, mean_absolute_error: 41.743057, mean_q: -0.000902\n",
      " 4953/5000: episode: 4952, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.000, 0.534], loss: 0.000003, mean_absolute_error: 41.736694, mean_q: -0.000801\n",
      " 4954/5000: episode: 4953, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.405 [0.000, 0.973], loss: 0.000003, mean_absolute_error: 41.737144, mean_q: -0.000884\n",
      " 4955/5000: episode: 4954, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.000, 0.980], loss: 0.000002, mean_absolute_error: 41.734821, mean_q: -0.000736\n",
      " 4956/5000: episode: 4955, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.000, 0.517], loss: 0.000001, mean_absolute_error: 41.745136, mean_q: -0.000915\n",
      " 4957/5000: episode: 4956, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.000, 0.907], loss: 0.000003, mean_absolute_error: 41.737549, mean_q: -0.000883\n",
      " 4958/5000: episode: 4957, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.739], loss: 0.000003, mean_absolute_error: 41.739468, mean_q: -0.000881\n",
      " 4959/5000: episode: 4958, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.000, 0.767], loss: 0.000002, mean_absolute_error: 41.738094, mean_q: -0.000916\n",
      " 4960/5000: episode: 4959, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.087, 1.000], loss: 0.000002, mean_absolute_error: 41.740036, mean_q: -0.000935\n",
      " 4961/5000: episode: 4960, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.695 [0.112, 1.000], loss: 0.000002, mean_absolute_error: 41.739685, mean_q: -0.000854\n",
      " 4962/5000: episode: 4961, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.000, 0.779], loss: 0.000006, mean_absolute_error: 41.735332, mean_q: -0.000843\n",
      " 4963/5000: episode: 4962, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.035, 1.000], loss: 0.000001, mean_absolute_error: 41.739685, mean_q: -0.000928\n",
      " 4964/5000: episode: 4963, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.706 [0.127, 1.000], loss: 0.000002, mean_absolute_error: 41.741650, mean_q: -0.000912\n",
      " 4965/5000: episode: 4964, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.017, 1.000], loss: 0.000005, mean_absolute_error: 41.734642, mean_q: -0.001005\n",
      " 4966/5000: episode: 4965, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.444, 1.000], loss: 0.000001, mean_absolute_error: 41.742676, mean_q: -0.000936\n",
      " 4967/5000: episode: 4966, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.480, 1.000], loss: 0.000005, mean_absolute_error: 41.733940, mean_q: -0.000869\n",
      " 4968/5000: episode: 4967, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.001, 1.000], loss: 0.000002, mean_absolute_error: 41.740456, mean_q: -0.000898\n",
      " 4969/5000: episode: 4968, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.000, 0.599], loss: 0.000003, mean_absolute_error: 41.739506, mean_q: -0.000868\n",
      " 4970/5000: episode: 4969, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.422, 1.000], loss: 0.000003, mean_absolute_error: 41.740498, mean_q: -0.000943\n",
      " 4971/5000: episode: 4970, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.000, 0.992], loss: 0.000001, mean_absolute_error: 41.742340, mean_q: -0.000943\n",
      " 4972/5000: episode: 4971, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.504, 1.000], loss: 0.000003, mean_absolute_error: 41.737839, mean_q: -0.000932\n",
      " 4973/5000: episode: 4972, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.226 [0.000, 0.744], loss: 0.000002, mean_absolute_error: 41.742531, mean_q: -0.000929\n",
      " 4974/5000: episode: 4973, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.000, 0.784], loss: 0.000003, mean_absolute_error: 41.739468, mean_q: -0.000898\n",
      " 4975/5000: episode: 4974, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.000, 0.773], loss: 0.000002, mean_absolute_error: 41.737709, mean_q: -0.000822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4976/5000: episode: 4975, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.000, 0.999], loss: 0.000003, mean_absolute_error: 41.735519, mean_q: -0.000849\n",
      " 4977/5000: episode: 4976, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.022, 1.000], loss: 0.000008, mean_absolute_error: 41.734226, mean_q: -0.000958\n",
      " 4978/5000: episode: 4977, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.000, 0.961], loss: 0.000007, mean_absolute_error: 41.729748, mean_q: -0.000873\n",
      " 4979/5000: episode: 4978, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.000, 0.587], loss: 0.000003, mean_absolute_error: 41.734829, mean_q: -0.000799\n",
      " 4980/5000: episode: 4979, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.000, 0.738], loss: 0.000000, mean_absolute_error: 41.745514, mean_q: -0.000963\n",
      " 4981/5000: episode: 4980, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.000, 0.863], loss: 0.000006, mean_absolute_error: 41.733082, mean_q: -0.000918\n",
      " 4982/5000: episode: 4981, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.346, 1.000], loss: 0.000004, mean_absolute_error: 41.738892, mean_q: -0.000900\n",
      " 4983/5000: episode: 4982, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.395, 1.000], loss: 0.000002, mean_absolute_error: 41.741013, mean_q: -0.000883\n",
      " 4984/5000: episode: 4983, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.000, 0.951], loss: 0.000001, mean_absolute_error: 41.741257, mean_q: -0.000930\n",
      " 4985/5000: episode: 4984, duration: 0.017s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.121, 1.000], loss: 0.000001, mean_absolute_error: 41.742386, mean_q: -0.000915\n",
      " 4986/5000: episode: 4985, duration: 0.018s, episode steps: 1, steps per second: 57, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.000, 0.921], loss: 0.000002, mean_absolute_error: 41.738026, mean_q: -0.000879\n",
      " 4987/5000: episode: 4986, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.447 [0.000, 0.991], loss: 0.000002, mean_absolute_error: 41.741104, mean_q: -0.000915\n",
      " 4988/5000: episode: 4987, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 0.954], loss: 0.000006, mean_absolute_error: 41.730827, mean_q: -0.000659\n",
      " 4989/5000: episode: 4988, duration: 0.022s, episode steps: 1, steps per second: 46, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.000, 0.700], loss: 0.000002, mean_absolute_error: 41.738609, mean_q: -0.000860\n",
      " 4990/5000: episode: 4989, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.582 [0.020, 1.000], loss: 0.000002, mean_absolute_error: 41.739120, mean_q: -0.000871\n",
      " 4991/5000: episode: 4990, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.000, 0.947], loss: 0.000001, mean_absolute_error: 41.741455, mean_q: -0.000934\n",
      " 4992/5000: episode: 4991, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.470, 1.000], loss: 0.000007, mean_absolute_error: 41.733147, mean_q: -0.000867\n",
      " 4993/5000: episode: 4992, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.393 [0.000, 0.967], loss: 0.000003, mean_absolute_error: 41.741127, mean_q: -0.000938\n",
      " 4994/5000: episode: 4993, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.349, 1.000], loss: 0.000002, mean_absolute_error: 41.736782, mean_q: -0.000886\n",
      " 4995/5000: episode: 4994, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.125, 1.000], loss: 0.000008, mean_absolute_error: 41.731258, mean_q: -0.000856\n",
      " 4996/5000: episode: 4995, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.274 [0.000, 0.843], loss: 0.000002, mean_absolute_error: 41.742100, mean_q: -0.000929\n",
      " 4997/5000: episode: 4996, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.270 [0.000, 0.837], loss: 0.000003, mean_absolute_error: 41.733212, mean_q: -0.000856\n",
      " 4998/5000: episode: 4997, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.068, 1.000], loss: 0.000004, mean_absolute_error: 41.733299, mean_q: -0.000786\n",
      " 4999/5000: episode: 4998, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.271, 1.000], loss: 0.000002, mean_absolute_error: 41.737312, mean_q: -0.000920\n",
      " 5000/5000: episode: 4999, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: -0.001, mean reward: -0.001 [-0.001, -0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.000, 0.841], loss: 0.000001, mean_absolute_error: 41.739719, mean_q: -0.000855\n",
      "done, took 75.974 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('duel_dqn_{}_weights.h5f'.format('stock_bot'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -0.001, steps: 1\n",
      "Episode 2: reward: -0.001, steps: 1\n",
      "Episode 3: reward: -0.001, steps: 1\n",
      "Episode 4: reward: -0.001, steps: 1\n",
      "Episode 5: reward: -0.001, steps: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc38c8f7278>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
