{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-31 05:02:00</td>\n",
       "      <td>948.000</td>\n",
       "      <td>948.000</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.083403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-31 05:03:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-31 05:04:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-31 05:05:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-31 05:06:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     open     high      low    close    volume\n",
       "0  2016-12-31 05:02:00  948.000  948.000  942.899  942.899  0.083403\n",
       "1  2016-12-31 05:03:00  942.899  942.899  942.899  942.899  0.000000\n",
       "2  2016-12-31 05:04:00  942.899  942.899  942.899  942.899  0.000000\n",
       "3  2016-12-31 05:05:00  942.899  942.899  942.899  942.899  0.000000\n",
       "4  2016-12-31 05:06:00  942.899  942.899  942.899  942.899  0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exch = 'BTRX'\n",
    "pair = 'BTC/USDT'\n",
    "\n",
    "df = pd.read_csv(f\"{exch}_{pair.replace('/', '-')}_ohlcv.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp     object\n",
       "open         float64\n",
       "high         float64\n",
       "low          float64\n",
       "close        float64\n",
       "volume       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlarson/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/home/rlarson/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAE9CAYAAACiDN36AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAglss6uR5F2CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa26fdc48d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the stock prices for the last day\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plot_last_n_minutes = 60 * 12  # 1/2 day\n",
    "cs_frame = df.iloc[-1 * plot_last_n_minutes:].copy()  # Create the candlestick frame\n",
    "\n",
    "#if necessary convert to datetime\n",
    "cs_frame.timestamp = pd.to_datetime(cs_frame.timestamp)\n",
    "\n",
    "cs_frame = cs_frame[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "cs_frame[\"timestamp\"] = cs_frame[\"timestamp\"].apply(mdates.date2num)\n",
    "\n",
    "f1 = plt.subplot2grid((6, 1), (0, 0), rowspan=6, colspan=1, axisbg='#07000d')\n",
    "candlestick_ohlc(f1, cs_frame.values, width=.0001, colorup='#53c156', colordown='#ff1717', alpha=.75)\n",
    "f1.xaxis_date()\n",
    "f1.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d %H:%M'))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Date Hours:Minutes')\n",
    "plt.show()\n",
    "\n",
    "# Cleanup memory\n",
    "%reset_selective -f \"^cs_frame$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a minute moving average over period\n",
    "def add_moving_avg(df, period=30):\n",
    "    df[f\"{period}_ma\"] = pd.rolling_mean(df['close'], period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlarson/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=30,center=False).mean()\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>30_ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>532493</th>\n",
       "      <td>2018-01-04 23:55:00</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15200.00000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>10.446506</td>\n",
       "      <td>15063.411186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532494</th>\n",
       "      <td>2018-01-04 23:56:00</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15220.00000</td>\n",
       "      <td>15195.582639</td>\n",
       "      <td>15220.000000</td>\n",
       "      <td>5.457758</td>\n",
       "      <td>15070.777853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532495</th>\n",
       "      <td>2018-01-04 23:57:00</td>\n",
       "      <td>15220.000000</td>\n",
       "      <td>15238.00000</td>\n",
       "      <td>15200.000000</td>\n",
       "      <td>15201.000000</td>\n",
       "      <td>7.473745</td>\n",
       "      <td>15077.644520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532496</th>\n",
       "      <td>2018-01-04 23:58:00</td>\n",
       "      <td>15200.000000</td>\n",
       "      <td>15202.21529</td>\n",
       "      <td>15085.001000</td>\n",
       "      <td>15101.591266</td>\n",
       "      <td>7.258691</td>\n",
       "      <td>15081.030929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532497</th>\n",
       "      <td>2018-01-04 23:59:00</td>\n",
       "      <td>15101.591266</td>\n",
       "      <td>15199.00000</td>\n",
       "      <td>15085.001000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>1.777752</td>\n",
       "      <td>15087.697595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp          open         high           low  \\\n",
       "532493  2018-01-04 23:55:00  15199.000000  15200.00000  15199.000000   \n",
       "532494  2018-01-04 23:56:00  15199.000000  15220.00000  15195.582639   \n",
       "532495  2018-01-04 23:57:00  15220.000000  15238.00000  15200.000000   \n",
       "532496  2018-01-04 23:58:00  15200.000000  15202.21529  15085.001000   \n",
       "532497  2018-01-04 23:59:00  15101.591266  15199.00000  15085.001000   \n",
       "\n",
       "               close     volume         30_ma  \n",
       "532493  15199.000000  10.446506  15063.411186  \n",
       "532494  15220.000000   5.457758  15070.777853  \n",
       "532495  15201.000000   7.473745  15077.644520  \n",
       "532496  15101.591266   7.258691  15081.030929  \n",
       "532497  15199.000000   1.777752  15087.697595  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_moving_avg(df)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from stock_gym.envs import stocks\n",
    "import gym\n",
    "\n",
    "env = gym.make('FakeMarketEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, (1, 64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "(nb_actions, env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 1,635\n",
      "Trainable params: 1,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlarson/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   129/50000: episode: 1, duration: 1.357s, episode steps: 129, steps per second: 95, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000000, mean_absolute_error: 0.001086, mean_q: 0.001679\n",
      "   258/50000: episode: 2, duration: 0.598s, episode steps: 129, steps per second: 216, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000000, mean_absolute_error: 0.001942, mean_q: 0.002946\n",
      "   387/50000: episode: 3, duration: 0.601s, episode steps: 129, steps per second: 215, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.030 [0.013, 0.050], loss: 0.000001, mean_absolute_error: 0.002765, mean_q: 0.004331\n",
      "   516/50000: episode: 4, duration: 0.659s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.907 [0.000, 2.000], mean observation: 0.003 [0.002, 0.005], loss: 0.000000, mean_absolute_error: 0.002952, mean_q: 0.004513\n",
      "   645/50000: episode: 5, duration: 0.816s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.085 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000000, mean_absolute_error: 0.003516, mean_q: 0.005325\n",
      "   774/50000: episode: 6, duration: 0.665s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.915 [0.000, 2.000], mean observation: 0.014 [0.007, 0.034], loss: 0.000000, mean_absolute_error: 0.004306, mean_q: 0.006522\n",
      "   903/50000: episode: 7, duration: 0.639s, episode steps: 129, steps per second: 202, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.002 [0.001, 0.003], loss: 0.000000, mean_absolute_error: 0.005071, mean_q: 0.007687\n",
      "  1032/50000: episode: 8, duration: 0.674s, episode steps: 129, steps per second: 191, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000000, mean_absolute_error: 0.005811, mean_q: 0.008802\n",
      "  1161/50000: episode: 9, duration: 0.636s, episode steps: 129, steps per second: 203, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000000, mean_absolute_error: 0.006572, mean_q: 0.009961\n",
      "  1290/50000: episode: 10, duration: 0.625s, episode steps: 129, steps per second: 206, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000000, mean_absolute_error: 0.007224, mean_q: 0.010914\n",
      "  1419/50000: episode: 11, duration: 0.629s, episode steps: 129, steps per second: 205, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.003 [0.002, 0.004], loss: 0.000000, mean_absolute_error: 0.007841, mean_q: 0.011860\n",
      "  1548/50000: episode: 12, duration: 0.668s, episode steps: 129, steps per second: 193, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000001, mean_absolute_error: 0.008503, mean_q: 0.012916\n",
      "  1677/50000: episode: 13, duration: 0.630s, episode steps: 129, steps per second: 205, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.024 [0.012, 0.041], loss: 0.000001, mean_absolute_error: 0.009215, mean_q: 0.013903\n",
      "  1806/50000: episode: 14, duration: 0.607s, episode steps: 129, steps per second: 212, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.002 [0.001, 0.003], loss: 0.000000, mean_absolute_error: 0.009921, mean_q: 0.014972\n",
      "  1935/50000: episode: 15, duration: 0.832s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000001, mean_absolute_error: 0.010555, mean_q: 0.016002\n",
      "  2064/50000: episode: 16, duration: 0.605s, episode steps: 129, steps per second: 213, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000001, mean_absolute_error: 0.011161, mean_q: 0.016855\n",
      "  2193/50000: episode: 17, duration: 0.617s, episode steps: 129, steps per second: 209, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.032 [0.014, 0.061], loss: 0.000001, mean_absolute_error: 0.011877, mean_q: 0.017922\n",
      "  2322/50000: episode: 18, duration: 0.809s, episode steps: 129, steps per second: 159, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000001, mean_absolute_error: 0.012487, mean_q: 0.018932\n",
      "  2451/50000: episode: 19, duration: 0.630s, episode steps: 129, steps per second: 205, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000001, mean_absolute_error: 0.013071, mean_q: 0.019780\n",
      "  2580/50000: episode: 20, duration: 0.672s, episode steps: 129, steps per second: 192, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.023 [0.007, 0.051], loss: 0.000002, mean_absolute_error: 0.013643, mean_q: 0.020579\n",
      "  2709/50000: episode: 21, duration: 0.625s, episode steps: 129, steps per second: 206, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000002, mean_absolute_error: 0.014272, mean_q: 0.021481\n",
      "  2838/50000: episode: 22, duration: 0.666s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000002, mean_absolute_error: 0.014787, mean_q: 0.022451\n",
      "  2967/50000: episode: 23, duration: 0.694s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000002, mean_absolute_error: 0.015295, mean_q: 0.023066\n",
      "  3096/50000: episode: 24, duration: 0.665s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000003, mean_absolute_error: 0.015793, mean_q: 0.023910\n",
      "  3225/50000: episode: 25, duration: 1.085s, episode steps: 129, steps per second: 119, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000003, mean_absolute_error: 0.016252, mean_q: 0.024429\n",
      "  3354/50000: episode: 26, duration: 0.701s, episode steps: 129, steps per second: 184, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000002, mean_absolute_error: 0.016700, mean_q: 0.025178\n",
      "  3483/50000: episode: 27, duration: 0.658s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.829 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000003, mean_absolute_error: 0.017248, mean_q: 0.026014\n",
      "  3612/50000: episode: 28, duration: 1.033s, episode steps: 129, steps per second: 125, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000003, mean_absolute_error: 0.017695, mean_q: 0.026599\n",
      "  3741/50000: episode: 29, duration: 0.662s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000004, mean_absolute_error: 0.018203, mean_q: 0.027438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3870/50000: episode: 30, duration: 0.660s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.876 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000003, mean_absolute_error: 0.018545, mean_q: 0.028007\n",
      "  3999/50000: episode: 31, duration: 0.640s, episode steps: 129, steps per second: 202, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.922 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000003, mean_absolute_error: 0.018926, mean_q: 0.028565\n",
      "  4128/50000: episode: 32, duration: 0.655s, episode steps: 129, steps per second: 197, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.017 [0.012, 0.026], loss: 0.000003, mean_absolute_error: 0.019405, mean_q: 0.029191\n",
      "  4257/50000: episode: 33, duration: 0.602s, episode steps: 129, steps per second: 214, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.014 [0.005, 0.025], loss: 0.000002, mean_absolute_error: 0.019915, mean_q: 0.029992\n",
      "  4386/50000: episode: 34, duration: 0.692s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000003, mean_absolute_error: 0.020370, mean_q: 0.030620\n",
      "  4515/50000: episode: 35, duration: 0.653s, episode steps: 129, steps per second: 198, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000004, mean_absolute_error: 0.020828, mean_q: 0.031456\n",
      "  4644/50000: episode: 36, duration: 0.780s, episode steps: 129, steps per second: 165, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000004, mean_absolute_error: 0.021167, mean_q: 0.031876\n",
      "  4773/50000: episode: 37, duration: 1.071s, episode steps: 129, steps per second: 120, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.023 [0.013, 0.044], loss: 0.000004, mean_absolute_error: 0.021576, mean_q: 0.032421\n",
      "  4902/50000: episode: 38, duration: 0.677s, episode steps: 129, steps per second: 190, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000004, mean_absolute_error: 0.021974, mean_q: 0.033110\n",
      "  5031/50000: episode: 39, duration: 0.603s, episode steps: 129, steps per second: 214, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.001], loss: 0.000005, mean_absolute_error: 0.022385, mean_q: 0.033871\n",
      "  5160/50000: episode: 40, duration: 0.621s, episode steps: 129, steps per second: 208, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.042 [0.024, 0.060], loss: 0.000005, mean_absolute_error: 0.022673, mean_q: 0.034150\n",
      "  5289/50000: episode: 41, duration: 0.903s, episode steps: 129, steps per second: 143, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.042 [0.029, 0.061], loss: 0.000003, mean_absolute_error: 0.023077, mean_q: 0.034712\n",
      "  5418/50000: episode: 42, duration: 0.803s, episode steps: 129, steps per second: 161, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000005, mean_absolute_error: 0.023507, mean_q: 0.035501\n",
      "  5547/50000: episode: 43, duration: 0.813s, episode steps: 129, steps per second: 159, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000005, mean_absolute_error: 0.023797, mean_q: 0.035829\n",
      "  5676/50000: episode: 44, duration: 0.847s, episode steps: 129, steps per second: 152, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.186 [0.000, 2.000], mean observation: 0.390 [0.226, 0.584], loss: 0.000012, mean_absolute_error: 0.024559, mean_q: 0.037140\n",
      "  5805/50000: episode: 45, duration: 0.800s, episode steps: 129, steps per second: 161, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000007, mean_absolute_error: 0.024938, mean_q: 0.037978\n",
      "  5934/50000: episode: 46, duration: 0.827s, episode steps: 129, steps per second: 156, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000005, mean_absolute_error: 0.025144, mean_q: 0.037801\n",
      "  6063/50000: episode: 47, duration: 0.856s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.001 [0.001, 0.001], loss: 0.000007, mean_absolute_error: 0.025410, mean_q: 0.038208\n",
      "  6192/50000: episode: 48, duration: 0.856s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.907 [0.000, 2.000], mean observation: 0.001 [0.001, 0.002], loss: 0.000005, mean_absolute_error: 0.025691, mean_q: 0.038783\n",
      "  6321/50000: episode: 49, duration: 0.825s, episode steps: 129, steps per second: 156, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000004, mean_absolute_error: 0.026052, mean_q: 0.039127\n",
      "  6450/50000: episode: 50, duration: 0.822s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000006, mean_absolute_error: 0.026384, mean_q: 0.039708\n",
      "  6579/50000: episode: 51, duration: 0.922s, episode steps: 129, steps per second: 140, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000005, mean_absolute_error: 0.026597, mean_q: 0.040101\n",
      "  6708/50000: episode: 52, duration: 0.809s, episode steps: 129, steps per second: 160, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.009 [0.006, 0.015], loss: 0.000008, mean_absolute_error: 0.026881, mean_q: 0.040512\n",
      "  6837/50000: episode: 53, duration: 0.866s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000006, mean_absolute_error: 0.027103, mean_q: 0.040786\n",
      "  6966/50000: episode: 54, duration: 0.916s, episode steps: 129, steps per second: 141, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000005, mean_absolute_error: 0.027316, mean_q: 0.041089\n",
      "  7095/50000: episode: 55, duration: 0.905s, episode steps: 129, steps per second: 143, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000007, mean_absolute_error: 0.027657, mean_q: 0.041613\n",
      "  7224/50000: episode: 56, duration: 0.874s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.027912, mean_q: 0.042071\n",
      "  7353/50000: episode: 57, duration: 0.837s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000006, mean_absolute_error: 0.028184, mean_q: 0.042427\n",
      "  7482/50000: episode: 58, duration: 0.832s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.860 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000007, mean_absolute_error: 0.028411, mean_q: 0.042788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7611/50000: episode: 59, duration: 0.783s, episode steps: 129, steps per second: 165, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.028660, mean_q: 0.043117\n",
      "  7740/50000: episode: 60, duration: 1.148s, episode steps: 129, steps per second: 112, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.028737, mean_q: 0.043178\n",
      "  7869/50000: episode: 61, duration: 0.570s, episode steps: 129, steps per second: 226, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.876 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000006, mean_absolute_error: 0.028932, mean_q: 0.043593\n",
      "  7998/50000: episode: 62, duration: 0.890s, episode steps: 129, steps per second: 145, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.001], loss: 0.000006, mean_absolute_error: 0.029203, mean_q: 0.043925\n",
      "  8127/50000: episode: 63, duration: 0.652s, episode steps: 129, steps per second: 198, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.005 [0.002, 0.006], loss: 0.000006, mean_absolute_error: 0.029486, mean_q: 0.044392\n",
      "  8256/50000: episode: 64, duration: 0.623s, episode steps: 129, steps per second: 207, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.029773, mean_q: 0.044825\n",
      "  8385/50000: episode: 65, duration: 0.770s, episode steps: 129, steps per second: 167, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.029929, mean_q: 0.044937\n",
      "  8514/50000: episode: 66, duration: 0.745s, episode steps: 129, steps per second: 173, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000006, mean_absolute_error: 0.030156, mean_q: 0.045484\n",
      "  8643/50000: episode: 67, duration: 0.689s, episode steps: 129, steps per second: 187, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.155 [0.000, 2.000], mean observation: 0.001 [0.000, 0.001], loss: 0.000008, mean_absolute_error: 0.030418, mean_q: 0.045919\n",
      "  8772/50000: episode: 68, duration: 0.666s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.101 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000007, mean_absolute_error: 0.030520, mean_q: 0.045978\n",
      "  8901/50000: episode: 69, duration: 0.726s, episode steps: 129, steps per second: 178, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.081 [0.026, 0.146], loss: 0.000007, mean_absolute_error: 0.030771, mean_q: 0.046296\n",
      "  9030/50000: episode: 70, duration: 1.161s, episode steps: 129, steps per second: 111, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.030900, mean_q: 0.046541\n",
      "  9159/50000: episode: 71, duration: 0.914s, episode steps: 129, steps per second: 141, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.132 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.031030, mean_q: 0.046621\n",
      "  9288/50000: episode: 72, duration: 0.935s, episode steps: 129, steps per second: 138, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.860 [0.000, 2.000], mean observation: 0.286 [0.150, 0.512], loss: 0.000007, mean_absolute_error: 0.031320, mean_q: 0.047123\n",
      "  9417/50000: episode: 73, duration: 0.815s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000006, mean_absolute_error: 0.031582, mean_q: 0.047664\n",
      "  9546/50000: episode: 74, duration: 0.826s, episode steps: 129, steps per second: 156, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.031788, mean_q: 0.047645\n",
      "  9675/50000: episode: 75, duration: 0.793s, episode steps: 129, steps per second: 163, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.002 [0.001, 0.003], loss: 0.000009, mean_absolute_error: 0.031945, mean_q: 0.047974\n",
      "  9804/50000: episode: 76, duration: 0.824s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.907 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.032112, mean_q: 0.048297\n",
      "  9933/50000: episode: 77, duration: 0.822s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.907 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.032268, mean_q: 0.048633\n",
      " 10062/50000: episode: 78, duration: 0.876s, episode steps: 129, steps per second: 147, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.915 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.032469, mean_q: 0.048981\n",
      " 10191/50000: episode: 79, duration: 0.870s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.907 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.032554, mean_q: 0.048852\n",
      " 10320/50000: episode: 80, duration: 0.869s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.032748, mean_q: 0.049184\n",
      " 10449/50000: episode: 81, duration: 0.803s, episode steps: 129, steps per second: 161, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.002 [0.001, 0.003], loss: 0.000010, mean_absolute_error: 0.032786, mean_q: 0.049201\n",
      " 10578/50000: episode: 82, duration: 0.820s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.032974, mean_q: 0.049548\n",
      " 10707/50000: episode: 83, duration: 0.816s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.033009, mean_q: 0.049643\n",
      " 10836/50000: episode: 84, duration: 0.836s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.017 [0.009, 0.029], loss: 0.000008, mean_absolute_error: 0.033149, mean_q: 0.049761\n",
      " 10965/50000: episode: 85, duration: 0.820s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.003 [0.002, 0.004], loss: 0.000011, mean_absolute_error: 0.033299, mean_q: 0.049954\n",
      " 11094/50000: episode: 86, duration: 0.870s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000007, mean_absolute_error: 0.033441, mean_q: 0.050286\n",
      " 11223/50000: episode: 87, duration: 0.805s, episode steps: 129, steps per second: 160, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.004 [0.003, 0.005], loss: 0.000009, mean_absolute_error: 0.033639, mean_q: 0.050635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11352/50000: episode: 88, duration: 0.856s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.033775, mean_q: 0.050845\n",
      " 11481/50000: episode: 89, duration: 0.855s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.922 [0.000, 2.000], mean observation: 0.027 [0.017, 0.042], loss: 0.000011, mean_absolute_error: 0.033849, mean_q: 0.050920\n",
      " 11610/50000: episode: 90, duration: 0.836s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.104 [0.067, 0.162], loss: 0.000009, mean_absolute_error: 0.033970, mean_q: 0.051058\n",
      " 11739/50000: episode: 91, duration: 0.828s, episode steps: 129, steps per second: 156, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.044 [0.021, 0.067], loss: 0.000011, mean_absolute_error: 0.034076, mean_q: 0.051146\n",
      " 11868/50000: episode: 92, duration: 0.816s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.034175, mean_q: 0.051424\n",
      " 11997/50000: episode: 93, duration: 0.795s, episode steps: 129, steps per second: 162, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.034381, mean_q: 0.051637\n",
      " 12126/50000: episode: 94, duration: 0.879s, episode steps: 129, steps per second: 147, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.034464, mean_q: 0.051794\n",
      " 12255/50000: episode: 95, duration: 0.878s, episode steps: 129, steps per second: 147, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.034476, mean_q: 0.051880\n",
      " 12384/50000: episode: 96, duration: 0.880s, episode steps: 129, steps per second: 147, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.008 [0.004, 0.013], loss: 0.000014, mean_absolute_error: 0.034519, mean_q: 0.052096\n",
      " 12513/50000: episode: 97, duration: 0.815s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.034549, mean_q: 0.051975\n",
      " 12642/50000: episode: 98, duration: 0.801s, episode steps: 129, steps per second: 161, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.109 [0.000, 2.000], mean observation: 0.001 [0.000, 0.002], loss: 0.000010, mean_absolute_error: 0.034637, mean_q: 0.052073\n",
      " 12771/50000: episode: 99, duration: 0.851s, episode steps: 129, steps per second: 152, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.087 [0.031, 0.158], loss: 0.000007, mean_absolute_error: 0.034783, mean_q: 0.052300\n",
      " 12900/50000: episode: 100, duration: 0.804s, episode steps: 129, steps per second: 160, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.001 [0.001, 0.002], loss: 0.000007, mean_absolute_error: 0.034920, mean_q: 0.052495\n",
      " 13029/50000: episode: 101, duration: 0.784s, episode steps: 129, steps per second: 164, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.035186, mean_q: 0.053038\n",
      " 13158/50000: episode: 102, duration: 0.817s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.035144, mean_q: 0.052788\n",
      " 13287/50000: episode: 103, duration: 0.810s, episode steps: 129, steps per second: 159, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.035255, mean_q: 0.052996\n",
      " 13416/50000: episode: 104, duration: 0.881s, episode steps: 129, steps per second: 146, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.001 [0.000, 0.002], loss: 0.000011, mean_absolute_error: 0.035433, mean_q: 0.053237\n",
      " 13545/50000: episode: 105, duration: 0.840s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.035462, mean_q: 0.053321\n",
      " 13674/50000: episode: 106, duration: 0.843s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.005 [0.003, 0.008], loss: 0.000010, mean_absolute_error: 0.035541, mean_q: 0.053403\n",
      " 13803/50000: episode: 107, duration: 0.971s, episode steps: 129, steps per second: 133, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.035609, mean_q: 0.053648\n",
      " 13932/50000: episode: 108, duration: 0.826s, episode steps: 129, steps per second: 156, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.140 [0.000, 2.000], mean observation: 0.021 [0.014, 0.042], loss: 0.000013, mean_absolute_error: 0.035712, mean_q: 0.053714\n",
      " 14061/50000: episode: 109, duration: 0.854s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.035656, mean_q: 0.053633\n",
      " 14190/50000: episode: 110, duration: 0.844s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.035598, mean_q: 0.053637\n",
      " 14319/50000: episode: 111, duration: 0.867s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.035641, mean_q: 0.053586\n",
      " 14448/50000: episode: 112, duration: 0.848s, episode steps: 129, steps per second: 152, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.006 [0.003, 0.013], loss: 0.000010, mean_absolute_error: 0.035791, mean_q: 0.053880\n",
      " 14577/50000: episode: 113, duration: 0.839s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.380 [0.239, 1.274], loss: 0.000009, mean_absolute_error: 0.035961, mean_q: 0.054079\n",
      " 14706/50000: episode: 114, duration: 0.868s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.036042, mean_q: 0.054251\n",
      " 14835/50000: episode: 115, duration: 0.844s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.036111, mean_q: 0.054373\n",
      " 14964/50000: episode: 116, duration: 0.795s, episode steps: 129, steps per second: 162, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.155 [0.000, 2.000], mean observation: 0.012 [0.006, 0.026], loss: 0.000012, mean_absolute_error: 0.036115, mean_q: 0.054332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15093/50000: episode: 117, duration: 0.848s, episode steps: 129, steps per second: 152, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.036201, mean_q: 0.054457\n",
      " 15222/50000: episode: 118, duration: 0.817s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.036196, mean_q: 0.054313\n",
      " 15351/50000: episode: 119, duration: 0.829s, episode steps: 129, steps per second: 156, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.884 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.036389, mean_q: 0.054520\n",
      " 15480/50000: episode: 120, duration: 0.889s, episode steps: 129, steps per second: 145, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.036344, mean_q: 0.054592\n",
      " 15609/50000: episode: 121, duration: 0.865s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.036296, mean_q: 0.054510\n",
      " 15738/50000: episode: 122, duration: 0.859s, episode steps: 129, steps per second: 150, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.124 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.036285, mean_q: 0.054358\n",
      " 15867/50000: episode: 123, duration: 0.841s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.079 [0.043, 0.136], loss: 0.000009, mean_absolute_error: 0.036337, mean_q: 0.054660\n",
      " 15996/50000: episode: 124, duration: 0.844s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.001 [0.000, 0.001], loss: 0.000012, mean_absolute_error: 0.036551, mean_q: 0.055038\n",
      " 16125/50000: episode: 125, duration: 0.856s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.036584, mean_q: 0.055046\n",
      " 16254/50000: episode: 126, duration: 0.844s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.036711, mean_q: 0.055189\n",
      " 16383/50000: episode: 127, duration: 0.864s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.001 [0.001, 0.002], loss: 0.000016, mean_absolute_error: 0.036677, mean_q: 0.055080\n",
      " 16512/50000: episode: 128, duration: 0.864s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.005 [0.002, 0.007], loss: 0.000012, mean_absolute_error: 0.036684, mean_q: 0.055130\n",
      " 16641/50000: episode: 129, duration: 0.835s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.147 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.036763, mean_q: 0.055294\n",
      " 16770/50000: episode: 130, duration: 0.815s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.021 [0.009, 0.041], loss: 0.000013, mean_absolute_error: 0.036742, mean_q: 0.055241\n",
      " 16899/50000: episode: 131, duration: 0.822s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.036715, mean_q: 0.055294\n",
      " 17028/50000: episode: 132, duration: 0.790s, episode steps: 129, steps per second: 163, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.036569, mean_q: 0.054998\n",
      " 17157/50000: episode: 133, duration: 0.833s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.036668, mean_q: 0.054966\n",
      " 17286/50000: episode: 134, duration: 0.863s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.860 [0.000, 2.000], mean observation: 0.031 [0.018, 0.058], loss: 0.000014, mean_absolute_error: 0.036647, mean_q: 0.055056\n",
      " 17415/50000: episode: 135, duration: 0.835s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.361 [0.142, 0.558], loss: 0.000010, mean_absolute_error: 0.036693, mean_q: 0.055086\n",
      " 17544/50000: episode: 136, duration: 0.843s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.036877, mean_q: 0.055456\n",
      " 17673/50000: episode: 137, duration: 0.836s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.085 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.036782, mean_q: 0.055349\n",
      " 17802/50000: episode: 138, duration: 0.826s, episode steps: 129, steps per second: 156, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.036945, mean_q: 0.055632\n",
      " 17931/50000: episode: 139, duration: 0.850s, episode steps: 129, steps per second: 152, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.036941, mean_q: 0.055549\n",
      " 18060/50000: episode: 140, duration: 0.856s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.907 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.037188, mean_q: 0.055954\n",
      " 18189/50000: episode: 141, duration: 0.849s, episode steps: 129, steps per second: 152, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.005 [0.002, 0.008], loss: 0.000010, mean_absolute_error: 0.037353, mean_q: 0.056163\n",
      " 18318/50000: episode: 142, duration: 0.863s, episode steps: 129, steps per second: 149, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.037464, mean_q: 0.056204\n",
      " 18447/50000: episode: 143, duration: 0.853s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.037486, mean_q: 0.056347\n",
      " 18576/50000: episode: 144, duration: 0.873s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000018, mean_absolute_error: 0.037342, mean_q: 0.056217\n",
      " 18705/50000: episode: 145, duration: 0.870s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.037302, mean_q: 0.056025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18834/50000: episode: 146, duration: 0.875s, episode steps: 129, steps per second: 147, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.124 [0.000, 2.000], mean observation: 0.026 [0.020, 0.037], loss: 0.000010, mean_absolute_error: 0.037374, mean_q: 0.056076\n",
      " 18963/50000: episode: 147, duration: 0.822s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.037704, mean_q: 0.056689\n",
      " 19092/50000: episode: 148, duration: 0.799s, episode steps: 129, steps per second: 161, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.001 [0.000, 0.002], loss: 0.000010, mean_absolute_error: 0.037631, mean_q: 0.056677\n",
      " 19221/50000: episode: 149, duration: 0.824s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.006 [0.004, 0.010], loss: 0.000010, mean_absolute_error: 0.037690, mean_q: 0.056651\n",
      " 19350/50000: episode: 150, duration: 0.854s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.037826, mean_q: 0.056711\n",
      " 19479/50000: episode: 151, duration: 0.873s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000007, mean_absolute_error: 0.037801, mean_q: 0.056777\n",
      " 19608/50000: episode: 152, duration: 0.842s, episode steps: 129, steps per second: 153, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.284 [0.169, 0.510], loss: 0.000015, mean_absolute_error: 0.037971, mean_q: 0.057003\n",
      " 19737/50000: episode: 153, duration: 0.858s, episode steps: 129, steps per second: 150, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.101 [0.000, 2.000], mean observation: 0.492 [0.265, 0.767], loss: 0.000014, mean_absolute_error: 0.038090, mean_q: 0.057410\n",
      " 19866/50000: episode: 154, duration: 0.831s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.085 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038179, mean_q: 0.057348\n",
      " 19995/50000: episode: 155, duration: 0.818s, episode steps: 129, steps per second: 158, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038269, mean_q: 0.057479\n",
      " 20124/50000: episode: 156, duration: 0.853s, episode steps: 129, steps per second: 151, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.922 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000018, mean_absolute_error: 0.038292, mean_q: 0.057602\n",
      " 20253/50000: episode: 157, duration: 1.168s, episode steps: 129, steps per second: 110, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.002 [0.001, 0.004], loss: 0.000012, mean_absolute_error: 0.038152, mean_q: 0.057339\n",
      " 20382/50000: episode: 158, duration: 0.837s, episode steps: 129, steps per second: 154, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038361, mean_q: 0.057692\n",
      " 20511/50000: episode: 159, duration: 0.833s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038309, mean_q: 0.057681\n",
      " 20640/50000: episode: 160, duration: 0.781s, episode steps: 129, steps per second: 165, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038275, mean_q: 0.057724\n",
      " 20769/50000: episode: 161, duration: 0.788s, episode steps: 129, steps per second: 164, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038335, mean_q: 0.057499\n",
      " 20898/50000: episode: 162, duration: 0.718s, episode steps: 129, steps per second: 180, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038257, mean_q: 0.057531\n",
      " 21027/50000: episode: 163, duration: 0.644s, episode steps: 129, steps per second: 200, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038290, mean_q: 0.057501\n",
      " 21156/50000: episode: 164, duration: 0.682s, episode steps: 129, steps per second: 189, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038302, mean_q: 0.057687\n",
      " 21285/50000: episode: 165, duration: 0.788s, episode steps: 129, steps per second: 164, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000021, mean_absolute_error: 0.038150, mean_q: 0.057360\n",
      " 21414/50000: episode: 166, duration: 0.669s, episode steps: 129, steps per second: 193, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.006 [0.002, 0.011], loss: 0.000013, mean_absolute_error: 0.038192, mean_q: 0.057406\n",
      " 21543/50000: episode: 167, duration: 0.646s, episode steps: 129, steps per second: 200, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038017, mean_q: 0.057042\n",
      " 21672/50000: episode: 168, duration: 0.665s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038292, mean_q: 0.057429\n",
      " 21801/50000: episode: 169, duration: 0.724s, episode steps: 129, steps per second: 178, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038221, mean_q: 0.057545\n",
      " 21930/50000: episode: 170, duration: 0.638s, episode steps: 129, steps per second: 202, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.006 [0.003, 0.010], loss: 0.000017, mean_absolute_error: 0.038290, mean_q: 0.057455\n",
      " 22059/50000: episode: 171, duration: 0.669s, episode steps: 129, steps per second: 193, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038351, mean_q: 0.057623\n",
      " 22188/50000: episode: 172, duration: 0.725s, episode steps: 129, steps per second: 178, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038243, mean_q: 0.057598\n",
      " 22317/50000: episode: 173, duration: 0.642s, episode steps: 129, steps per second: 201, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038461, mean_q: 0.057876\n",
      " 22446/50000: episode: 174, duration: 0.650s, episode steps: 129, steps per second: 198, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038493, mean_q: 0.057856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22575/50000: episode: 175, duration: 0.675s, episode steps: 129, steps per second: 191, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000018, mean_absolute_error: 0.038561, mean_q: 0.057934\n",
      " 22704/50000: episode: 176, duration: 0.667s, episode steps: 129, steps per second: 193, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.085 [0.000, 2.000], mean observation: 0.031 [0.017, 0.046], loss: 0.000012, mean_absolute_error: 0.038504, mean_q: 0.057918\n",
      " 22833/50000: episode: 177, duration: 0.650s, episode steps: 129, steps per second: 198, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.003 [0.001, 0.004], loss: 0.000014, mean_absolute_error: 0.038562, mean_q: 0.057805\n",
      " 22962/50000: episode: 178, duration: 0.648s, episode steps: 129, steps per second: 199, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038568, mean_q: 0.058091\n",
      " 23091/50000: episode: 179, duration: 0.661s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038654, mean_q: 0.057996\n",
      " 23220/50000: episode: 180, duration: 0.746s, episode steps: 129, steps per second: 173, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.076 [0.053, 0.110], loss: 0.000015, mean_absolute_error: 0.038619, mean_q: 0.058025\n",
      " 23349/50000: episode: 181, duration: 0.769s, episode steps: 129, steps per second: 168, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038516, mean_q: 0.057904\n",
      " 23478/50000: episode: 182, duration: 0.811s, episode steps: 129, steps per second: 159, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038583, mean_q: 0.058020\n",
      " 23607/50000: episode: 183, duration: 0.682s, episode steps: 129, steps per second: 189, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.109 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038490, mean_q: 0.057891\n",
      " 23736/50000: episode: 184, duration: 0.693s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038615, mean_q: 0.057891\n",
      " 23865/50000: episode: 185, duration: 0.795s, episode steps: 129, steps per second: 162, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038529, mean_q: 0.057740\n",
      " 23994/50000: episode: 186, duration: 0.698s, episode steps: 129, steps per second: 185, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038775, mean_q: 0.058275\n",
      " 24123/50000: episode: 187, duration: 0.755s, episode steps: 129, steps per second: 171, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038733, mean_q: 0.058133\n",
      " 24252/50000: episode: 188, duration: 0.725s, episode steps: 129, steps per second: 178, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.915 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.038674, mean_q: 0.058137\n",
      " 24381/50000: episode: 189, duration: 0.731s, episode steps: 129, steps per second: 176, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.001 [0.000, 0.001], loss: 0.000013, mean_absolute_error: 0.038700, mean_q: 0.058095\n",
      " 24510/50000: episode: 190, duration: 0.761s, episode steps: 129, steps per second: 169, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.001], loss: 0.000012, mean_absolute_error: 0.038723, mean_q: 0.058062\n",
      " 24639/50000: episode: 191, duration: 0.710s, episode steps: 129, steps per second: 182, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038815, mean_q: 0.058333\n",
      " 24768/50000: episode: 192, duration: 0.725s, episode steps: 129, steps per second: 178, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.001], loss: 0.000012, mean_absolute_error: 0.038816, mean_q: 0.058401\n",
      " 24897/50000: episode: 193, duration: 0.738s, episode steps: 129, steps per second: 175, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.228 [0.090, 0.344], loss: 0.000011, mean_absolute_error: 0.038691, mean_q: 0.058149\n",
      " 25026/50000: episode: 194, duration: 0.687s, episode steps: 129, steps per second: 188, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.027 [0.015, 0.053], loss: 0.000009, mean_absolute_error: 0.038819, mean_q: 0.058289\n",
      " 25155/50000: episode: 195, duration: 0.714s, episode steps: 129, steps per second: 181, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038782, mean_q: 0.058227\n",
      " 25284/50000: episode: 196, duration: 0.617s, episode steps: 129, steps per second: 209, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038863, mean_q: 0.058426\n",
      " 25413/50000: episode: 197, duration: 0.582s, episode steps: 129, steps per second: 222, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.021 [0.006, 0.057], loss: 0.000010, mean_absolute_error: 0.038712, mean_q: 0.058186\n",
      " 25542/50000: episode: 198, duration: 0.607s, episode steps: 129, steps per second: 213, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.033 [0.017, 0.053], loss: 0.000017, mean_absolute_error: 0.038819, mean_q: 0.058347\n",
      " 25671/50000: episode: 199, duration: 0.628s, episode steps: 129, steps per second: 205, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038691, mean_q: 0.058138\n",
      " 25800/50000: episode: 200, duration: 0.695s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038614, mean_q: 0.057884\n",
      " 25929/50000: episode: 201, duration: 0.751s, episode steps: 129, steps per second: 172, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038750, mean_q: 0.058273\n",
      " 26058/50000: episode: 202, duration: 0.624s, episode steps: 129, steps per second: 207, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.085 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000007, mean_absolute_error: 0.038806, mean_q: 0.058347\n",
      " 26187/50000: episode: 203, duration: 0.771s, episode steps: 129, steps per second: 167, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.039042, mean_q: 0.058833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26316/50000: episode: 204, duration: 0.663s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038800, mean_q: 0.058355\n",
      " 26445/50000: episode: 205, duration: 0.616s, episode steps: 129, steps per second: 209, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038919, mean_q: 0.058575\n",
      " 26574/50000: episode: 206, duration: 0.567s, episode steps: 129, steps per second: 228, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.002 [0.001, 0.003], loss: 0.000015, mean_absolute_error: 0.038912, mean_q: 0.058397\n",
      " 26703/50000: episode: 207, duration: 0.697s, episode steps: 129, steps per second: 185, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038920, mean_q: 0.058504\n",
      " 26832/50000: episode: 208, duration: 0.619s, episode steps: 129, steps per second: 208, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.124 [0.050, 0.265], loss: 0.000016, mean_absolute_error: 0.038810, mean_q: 0.058227\n",
      " 26961/50000: episode: 209, duration: 0.632s, episode steps: 129, steps per second: 204, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038846, mean_q: 0.058326\n",
      " 27090/50000: episode: 210, duration: 0.760s, episode steps: 129, steps per second: 170, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038918, mean_q: 0.058468\n",
      " 27219/50000: episode: 211, duration: 0.600s, episode steps: 129, steps per second: 215, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.915 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038707, mean_q: 0.058213\n",
      " 27348/50000: episode: 212, duration: 0.697s, episode steps: 129, steps per second: 185, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.038921, mean_q: 0.058613\n",
      " 27477/50000: episode: 213, duration: 0.664s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.039011, mean_q: 0.058522\n",
      " 27606/50000: episode: 214, duration: 0.634s, episode steps: 129, steps per second: 203, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038990, mean_q: 0.058702\n",
      " 27735/50000: episode: 215, duration: 0.658s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.039069, mean_q: 0.058695\n",
      " 27864/50000: episode: 216, duration: 0.624s, episode steps: 129, steps per second: 207, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.039033, mean_q: 0.058687\n",
      " 27993/50000: episode: 217, duration: 0.713s, episode steps: 129, steps per second: 181, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.038997, mean_q: 0.058581\n",
      " 28122/50000: episode: 218, duration: 0.664s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.922 [0.000, 2.000], mean observation: 0.002 [0.001, 0.005], loss: 0.000012, mean_absolute_error: 0.039023, mean_q: 0.058708\n",
      " 28251/50000: episode: 219, duration: 0.681s, episode steps: 129, steps per second: 189, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.004 [0.002, 0.007], loss: 0.000012, mean_absolute_error: 0.038873, mean_q: 0.058274\n",
      " 28380/50000: episode: 220, duration: 0.641s, episode steps: 129, steps per second: 201, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.279 [0.116, 0.533], loss: 0.000013, mean_absolute_error: 0.039000, mean_q: 0.058569\n",
      " 28509/50000: episode: 221, duration: 0.669s, episode steps: 129, steps per second: 193, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.006 [0.003, 0.010], loss: 0.000016, mean_absolute_error: 0.038875, mean_q: 0.058401\n",
      " 28638/50000: episode: 222, duration: 0.703s, episode steps: 129, steps per second: 183, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.007 [0.003, 0.011], loss: 0.000013, mean_absolute_error: 0.039022, mean_q: 0.058671\n",
      " 28767/50000: episode: 223, duration: 0.618s, episode steps: 129, steps per second: 209, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.039024, mean_q: 0.058658\n",
      " 28896/50000: episode: 224, duration: 0.664s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038911, mean_q: 0.058544\n",
      " 29025/50000: episode: 225, duration: 0.663s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.032 [0.015, 0.068], loss: 0.000012, mean_absolute_error: 0.038905, mean_q: 0.058449\n",
      " 29154/50000: episode: 226, duration: 0.629s, episode steps: 129, steps per second: 205, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.003 [0.002, 0.005], loss: 0.000011, mean_absolute_error: 0.038947, mean_q: 0.058357\n",
      " 29283/50000: episode: 227, duration: 0.608s, episode steps: 129, steps per second: 212, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.053 [0.019, 0.123], loss: 0.000012, mean_absolute_error: 0.039012, mean_q: 0.058443\n",
      " 29412/50000: episode: 228, duration: 0.616s, episode steps: 129, steps per second: 209, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.039091, mean_q: 0.058843\n",
      " 29541/50000: episode: 229, duration: 0.604s, episode steps: 129, steps per second: 213, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038971, mean_q: 0.058640\n",
      " 29670/50000: episode: 230, duration: 0.616s, episode steps: 129, steps per second: 209, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.124 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038636, mean_q: 0.058043\n",
      " 29799/50000: episode: 231, duration: 0.610s, episode steps: 129, steps per second: 212, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038639, mean_q: 0.058046\n",
      " 29928/50000: episode: 232, duration: 0.605s, episode steps: 129, steps per second: 213, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.033 [0.022, 0.052], loss: 0.000014, mean_absolute_error: 0.038579, mean_q: 0.057796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30057/50000: episode: 233, duration: 0.624s, episode steps: 129, steps per second: 207, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038651, mean_q: 0.057985\n",
      " 30186/50000: episode: 234, duration: 0.622s, episode steps: 129, steps per second: 207, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.443 [0.261, 0.734], loss: 0.000014, mean_absolute_error: 0.038648, mean_q: 0.058068\n",
      " 30315/50000: episode: 235, duration: 0.642s, episode steps: 129, steps per second: 201, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.038711, mean_q: 0.058191\n",
      " 30444/50000: episode: 236, duration: 0.698s, episode steps: 129, steps per second: 185, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.002 [0.001, 0.004], loss: 0.000010, mean_absolute_error: 0.038846, mean_q: 0.058317\n",
      " 30573/50000: episode: 237, duration: 0.795s, episode steps: 129, steps per second: 162, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.132 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038923, mean_q: 0.058534\n",
      " 30702/50000: episode: 238, duration: 0.715s, episode steps: 129, steps per second: 180, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.101 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038848, mean_q: 0.058290\n",
      " 30831/50000: episode: 239, duration: 0.748s, episode steps: 129, steps per second: 173, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038889, mean_q: 0.058401\n",
      " 30960/50000: episode: 240, duration: 0.654s, episode steps: 129, steps per second: 197, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.035 [0.025, 0.074], loss: 0.000012, mean_absolute_error: 0.038960, mean_q: 0.058597\n",
      " 31089/50000: episode: 241, duration: 0.628s, episode steps: 129, steps per second: 205, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.000 [0.000, 0.001], loss: 0.000013, mean_absolute_error: 0.038889, mean_q: 0.058512\n",
      " 31218/50000: episode: 242, duration: 0.602s, episode steps: 129, steps per second: 214, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038795, mean_q: 0.058260\n",
      " 31347/50000: episode: 243, duration: 0.587s, episode steps: 129, steps per second: 220, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.009 [0.007, 0.013], loss: 0.000013, mean_absolute_error: 0.038869, mean_q: 0.058455\n",
      " 31476/50000: episode: 244, duration: 0.676s, episode steps: 129, steps per second: 191, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038564, mean_q: 0.057865\n",
      " 31605/50000: episode: 245, duration: 0.632s, episode steps: 129, steps per second: 204, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.038517, mean_q: 0.057825\n",
      " 31734/50000: episode: 246, duration: 0.579s, episode steps: 129, steps per second: 223, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.001 [0.001, 0.002], loss: 0.000010, mean_absolute_error: 0.038466, mean_q: 0.057830\n",
      " 31863/50000: episode: 247, duration: 0.776s, episode steps: 129, steps per second: 166, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038448, mean_q: 0.057787\n",
      " 31992/50000: episode: 248, duration: 0.797s, episode steps: 129, steps per second: 162, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.018 [0.009, 0.036], loss: 0.000014, mean_absolute_error: 0.038479, mean_q: 0.057688\n",
      " 32121/50000: episode: 249, duration: 0.746s, episode steps: 129, steps per second: 173, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.131 [0.069, 0.221], loss: 0.000011, mean_absolute_error: 0.038394, mean_q: 0.057892\n",
      " 32250/50000: episode: 250, duration: 0.758s, episode steps: 129, steps per second: 170, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.140 [0.000, 2.000], mean observation: 0.000 [0.000, 0.001], loss: 0.000011, mean_absolute_error: 0.038412, mean_q: 0.057736\n",
      " 32379/50000: episode: 251, duration: 0.732s, episode steps: 129, steps per second: 176, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038555, mean_q: 0.057760\n",
      " 32508/50000: episode: 252, duration: 0.682s, episode steps: 129, steps per second: 189, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.038466, mean_q: 0.057800\n",
      " 32637/50000: episode: 253, duration: 0.685s, episode steps: 129, steps per second: 188, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.101 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038499, mean_q: 0.057703\n",
      " 32766/50000: episode: 254, duration: 0.663s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.922 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038565, mean_q: 0.058136\n",
      " 32895/50000: episode: 255, duration: 0.729s, episode steps: 129, steps per second: 177, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038467, mean_q: 0.057894\n",
      " 33024/50000: episode: 256, duration: 0.717s, episode steps: 129, steps per second: 180, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.860 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038469, mean_q: 0.057841\n",
      " 33153/50000: episode: 257, duration: 0.770s, episode steps: 129, steps per second: 168, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038580, mean_q: 0.057986\n",
      " 33282/50000: episode: 258, duration: 0.781s, episode steps: 129, steps per second: 165, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038607, mean_q: 0.058043\n",
      " 33411/50000: episode: 259, duration: 0.792s, episode steps: 129, steps per second: 163, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038534, mean_q: 0.058075\n",
      " 33540/50000: episode: 260, duration: 0.751s, episode steps: 129, steps per second: 172, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038653, mean_q: 0.058111\n",
      " 33669/50000: episode: 261, duration: 1.036s, episode steps: 129, steps per second: 125, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.050 [0.022, 0.121], loss: 0.000015, mean_absolute_error: 0.038509, mean_q: 0.057972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33798/50000: episode: 262, duration: 0.920s, episode steps: 129, steps per second: 140, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.891 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038623, mean_q: 0.058192\n",
      " 33927/50000: episode: 263, duration: 0.679s, episode steps: 129, steps per second: 190, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038486, mean_q: 0.057811\n",
      " 34056/50000: episode: 264, duration: 0.772s, episode steps: 129, steps per second: 167, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.021 [0.010, 0.035], loss: 0.000013, mean_absolute_error: 0.038604, mean_q: 0.058059\n",
      " 34185/50000: episode: 265, duration: 0.707s, episode steps: 129, steps per second: 182, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038554, mean_q: 0.058000\n",
      " 34314/50000: episode: 266, duration: 0.698s, episode steps: 129, steps per second: 185, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.915 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.038545, mean_q: 0.057959\n",
      " 34443/50000: episode: 267, duration: 0.712s, episode steps: 129, steps per second: 181, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.089 [0.059, 0.128], loss: 0.000013, mean_absolute_error: 0.038722, mean_q: 0.058239\n",
      " 34572/50000: episode: 268, duration: 0.770s, episode steps: 129, steps per second: 167, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.024 [0.015, 0.035], loss: 0.000013, mean_absolute_error: 0.038559, mean_q: 0.057997\n",
      " 34701/50000: episode: 269, duration: 0.700s, episode steps: 129, steps per second: 184, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.005 [0.003, 0.009], loss: 0.000011, mean_absolute_error: 0.038528, mean_q: 0.057830\n",
      " 34830/50000: episode: 270, duration: 0.834s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038631, mean_q: 0.057999\n",
      " 34959/50000: episode: 271, duration: 0.785s, episode steps: 129, steps per second: 164, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 1.243 [0.321, 2.420], loss: 0.000027, mean_absolute_error: 0.038777, mean_q: 0.058443\n",
      " 35088/50000: episode: 272, duration: 0.800s, episode steps: 129, steps per second: 161, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.082 [0.046, 0.159], loss: 0.000022, mean_absolute_error: 0.038850, mean_q: 0.058238\n",
      " 35217/50000: episode: 273, duration: 0.762s, episode steps: 129, steps per second: 169, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.101 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038809, mean_q: 0.058260\n",
      " 35346/50000: episode: 274, duration: 0.780s, episode steps: 129, steps per second: 165, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038642, mean_q: 0.058066\n",
      " 35475/50000: episode: 275, duration: 0.764s, episode steps: 129, steps per second: 169, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038769, mean_q: 0.058310\n",
      " 35604/50000: episode: 276, duration: 0.666s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000023, mean_absolute_error: 0.038710, mean_q: 0.058211\n",
      " 35733/50000: episode: 277, duration: 0.774s, episode steps: 129, steps per second: 167, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.038542, mean_q: 0.058203\n",
      " 35862/50000: episode: 278, duration: 0.709s, episode steps: 129, steps per second: 182, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.001 [0.000, 0.001], loss: 0.000012, mean_absolute_error: 0.038367, mean_q: 0.057726\n",
      " 35991/50000: episode: 279, duration: 0.721s, episode steps: 129, steps per second: 179, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038292, mean_q: 0.057572\n",
      " 36120/50000: episode: 280, duration: 0.785s, episode steps: 129, steps per second: 164, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.085 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038368, mean_q: 0.057672\n",
      " 36249/50000: episode: 281, duration: 0.823s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038400, mean_q: 0.057666\n",
      " 36378/50000: episode: 282, duration: 0.754s, episode steps: 129, steps per second: 171, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.003 [0.002, 0.003], loss: 0.000015, mean_absolute_error: 0.038421, mean_q: 0.057730\n",
      " 36507/50000: episode: 283, duration: 0.694s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038279, mean_q: 0.057573\n",
      " 36636/50000: episode: 284, duration: 0.747s, episode steps: 129, steps per second: 173, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.037942, mean_q: 0.056951\n",
      " 36765/50000: episode: 285, duration: 0.702s, episode steps: 129, steps per second: 184, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.016 [0.008, 0.023], loss: 0.000009, mean_absolute_error: 0.038213, mean_q: 0.057519\n",
      " 36894/50000: episode: 286, duration: 0.699s, episode steps: 129, steps per second: 184, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.085 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038456, mean_q: 0.057806\n",
      " 37023/50000: episode: 287, duration: 0.730s, episode steps: 129, steps per second: 177, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000023, mean_absolute_error: 0.038611, mean_q: 0.058024\n",
      " 37152/50000: episode: 288, duration: 0.655s, episode steps: 129, steps per second: 197, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038455, mean_q: 0.057924\n",
      " 37281/50000: episode: 289, duration: 0.660s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.922 [0.000, 2.000], mean observation: 0.023 [0.011, 0.042], loss: 0.000009, mean_absolute_error: 0.038373, mean_q: 0.057721\n",
      " 37410/50000: episode: 290, duration: 0.751s, episode steps: 129, steps per second: 172, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038423, mean_q: 0.057734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37539/50000: episode: 291, duration: 0.723s, episode steps: 129, steps per second: 178, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038413, mean_q: 0.057637\n",
      " 37668/50000: episode: 292, duration: 0.662s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.051 [0.027, 0.102], loss: 0.000011, mean_absolute_error: 0.038654, mean_q: 0.058072\n",
      " 37797/50000: episode: 293, duration: 0.662s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.915 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038340, mean_q: 0.057577\n",
      " 37926/50000: episode: 294, duration: 0.667s, episode steps: 129, steps per second: 193, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038521, mean_q: 0.057974\n",
      " 38055/50000: episode: 295, duration: 0.659s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038482, mean_q: 0.057824\n",
      " 38184/50000: episode: 296, duration: 0.696s, episode steps: 129, steps per second: 185, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.040 [0.026, 0.103], loss: 0.000014, mean_absolute_error: 0.038548, mean_q: 0.058105\n",
      " 38313/50000: episode: 297, duration: 0.709s, episode steps: 129, steps per second: 182, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000023, mean_absolute_error: 0.038682, mean_q: 0.058187\n",
      " 38442/50000: episode: 298, duration: 0.720s, episode steps: 129, steps per second: 179, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.038286, mean_q: 0.057303\n",
      " 38571/50000: episode: 299, duration: 0.748s, episode steps: 129, steps per second: 172, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.037874, mean_q: 0.056875\n",
      " 38700/50000: episode: 300, duration: 0.657s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.868 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.037954, mean_q: 0.057163\n",
      " 38829/50000: episode: 301, duration: 0.748s, episode steps: 129, steps per second: 172, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.109 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.037787, mean_q: 0.056683\n",
      " 38958/50000: episode: 302, duration: 0.801s, episode steps: 129, steps per second: 161, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.037563, mean_q: 0.056543\n",
      " 39087/50000: episode: 303, duration: 0.739s, episode steps: 129, steps per second: 174, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.037581, mean_q: 0.056415\n",
      " 39216/50000: episode: 304, duration: 0.794s, episode steps: 129, steps per second: 162, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.062 [0.000, 2.000], mean observation: 0.002 [0.001, 0.004], loss: 0.000017, mean_absolute_error: 0.037619, mean_q: 0.056496\n",
      " 39345/50000: episode: 305, duration: 0.758s, episode steps: 129, steps per second: 170, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.037520, mean_q: 0.056313\n",
      " 39474/50000: episode: 306, duration: 0.656s, episode steps: 129, steps per second: 197, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.037625, mean_q: 0.056531\n",
      " 39603/50000: episode: 307, duration: 0.685s, episode steps: 129, steps per second: 188, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.037721, mean_q: 0.056786\n",
      " 39732/50000: episode: 308, duration: 0.716s, episode steps: 129, steps per second: 180, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.037851, mean_q: 0.056797\n",
      " 39861/50000: episode: 309, duration: 0.653s, episode steps: 129, steps per second: 197, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.037941, mean_q: 0.056900\n",
      " 39990/50000: episode: 310, duration: 0.693s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.009 [0.004, 0.018], loss: 0.000016, mean_absolute_error: 0.037720, mean_q: 0.056837\n",
      " 40119/50000: episode: 311, duration: 0.755s, episode steps: 129, steps per second: 171, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.009 [0.005, 0.015], loss: 0.000011, mean_absolute_error: 0.037723, mean_q: 0.056705\n",
      " 40248/50000: episode: 312, duration: 0.677s, episode steps: 129, steps per second: 190, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.037862, mean_q: 0.056993\n",
      " 40377/50000: episode: 313, duration: 0.763s, episode steps: 129, steps per second: 169, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.023 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038240, mean_q: 0.057464\n",
      " 40506/50000: episode: 314, duration: 0.633s, episode steps: 129, steps per second: 204, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038122, mean_q: 0.057288\n",
      " 40635/50000: episode: 315, duration: 0.703s, episode steps: 129, steps per second: 183, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000008, mean_absolute_error: 0.038281, mean_q: 0.057485\n",
      " 40764/50000: episode: 316, duration: 0.795s, episode steps: 129, steps per second: 162, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.027 [0.010, 0.061], loss: 0.000017, mean_absolute_error: 0.038221, mean_q: 0.057399\n",
      " 40893/50000: episode: 317, duration: 0.721s, episode steps: 129, steps per second: 179, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038306, mean_q: 0.057617\n",
      " 41022/50000: episode: 318, duration: 0.834s, episode steps: 129, steps per second: 155, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.868 [0.000, 2.000], mean observation: 0.004 [0.002, 0.006], loss: 0.000016, mean_absolute_error: 0.038285, mean_q: 0.057436\n",
      " 41151/50000: episode: 319, duration: 0.701s, episode steps: 129, steps per second: 184, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000029, mean_absolute_error: 0.038230, mean_q: 0.057411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41280/50000: episode: 320, duration: 0.703s, episode steps: 129, steps per second: 183, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.038222, mean_q: 0.057427\n",
      " 41409/50000: episode: 321, duration: 0.593s, episode steps: 129, steps per second: 218, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.109 [0.000, 2.000], mean observation: 0.813 [0.471, 1.240], loss: 0.000012, mean_absolute_error: 0.038312, mean_q: 0.057654\n",
      " 41538/50000: episode: 322, duration: 0.702s, episode steps: 129, steps per second: 184, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.069 [0.049, 0.115], loss: 0.000013, mean_absolute_error: 0.038707, mean_q: 0.058124\n",
      " 41667/50000: episode: 323, duration: 0.734s, episode steps: 129, steps per second: 176, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038583, mean_q: 0.058200\n",
      " 41796/50000: episode: 324, duration: 0.650s, episode steps: 129, steps per second: 198, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.140 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038580, mean_q: 0.057879\n",
      " 41925/50000: episode: 325, duration: 0.715s, episode steps: 129, steps per second: 180, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.031 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.038196, mean_q: 0.057491\n",
      " 42054/50000: episode: 326, duration: 0.728s, episode steps: 129, steps per second: 177, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038257, mean_q: 0.057425\n",
      " 42183/50000: episode: 327, duration: 0.707s, episode steps: 129, steps per second: 182, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.018 [0.009, 0.027], loss: 0.000013, mean_absolute_error: 0.038100, mean_q: 0.057285\n",
      " 42312/50000: episode: 328, duration: 0.659s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038239, mean_q: 0.057428\n",
      " 42441/50000: episode: 329, duration: 0.697s, episode steps: 129, steps per second: 185, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038250, mean_q: 0.057487\n",
      " 42570/50000: episode: 330, duration: 0.617s, episode steps: 129, steps per second: 209, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000025, mean_absolute_error: 0.038323, mean_q: 0.057526\n",
      " 42699/50000: episode: 331, duration: 0.640s, episode steps: 129, steps per second: 201, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038472, mean_q: 0.057776\n",
      " 42828/50000: episode: 332, duration: 0.785s, episode steps: 129, steps per second: 164, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.054 [0.000, 2.000], mean observation: 0.028 [0.012, 0.042], loss: 0.000012, mean_absolute_error: 0.038324, mean_q: 0.057509\n",
      " 42957/50000: episode: 333, duration: 0.876s, episode steps: 129, steps per second: 147, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038319, mean_q: 0.057699\n",
      " 43086/50000: episode: 334, duration: 0.755s, episode steps: 129, steps per second: 171, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.031 [0.015, 0.046], loss: 0.000010, mean_absolute_error: 0.038358, mean_q: 0.057674\n",
      " 43215/50000: episode: 335, duration: 0.719s, episode steps: 129, steps per second: 179, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.101 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038581, mean_q: 0.058004\n",
      " 43344/50000: episode: 336, duration: 0.812s, episode steps: 129, steps per second: 159, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.002 [0.001, 0.003], loss: 0.000011, mean_absolute_error: 0.038520, mean_q: 0.057937\n",
      " 43473/50000: episode: 337, duration: 0.723s, episode steps: 129, steps per second: 179, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.016 [0.000, 2.000], mean observation: 0.141 [0.076, 0.189], loss: 0.000013, mean_absolute_error: 0.038662, mean_q: 0.058124\n",
      " 43602/50000: episode: 338, duration: 0.821s, episode steps: 129, steps per second: 157, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038610, mean_q: 0.058106\n",
      " 43731/50000: episode: 339, duration: 0.677s, episode steps: 129, steps per second: 190, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.109 [0.000, 2.000], mean observation: 0.017 [0.010, 0.025], loss: 0.000011, mean_absolute_error: 0.038711, mean_q: 0.058329\n",
      " 43860/50000: episode: 340, duration: 0.655s, episode steps: 129, steps per second: 197, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038686, mean_q: 0.058135\n",
      " 43989/50000: episode: 341, duration: 0.746s, episode steps: 129, steps per second: 173, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.038342, mean_q: 0.057543\n",
      " 44118/50000: episode: 342, duration: 0.670s, episode steps: 129, steps per second: 193, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.024 [0.017, 0.041], loss: 0.000012, mean_absolute_error: 0.038728, mean_q: 0.058103\n",
      " 44247/50000: episode: 343, duration: 0.660s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.907 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038719, mean_q: 0.058226\n",
      " 44376/50000: episode: 344, duration: 0.694s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038677, mean_q: 0.058057\n",
      " 44505/50000: episode: 345, duration: 0.734s, episode steps: 129, steps per second: 176, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.148 [0.072, 0.235], loss: 0.000013, mean_absolute_error: 0.038532, mean_q: 0.057841\n",
      " 44634/50000: episode: 346, duration: 0.662s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038485, mean_q: 0.057724\n",
      " 44763/50000: episode: 347, duration: 0.975s, episode steps: 129, steps per second: 132, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.132 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000020, mean_absolute_error: 0.038728, mean_q: 0.058176\n",
      " 44892/50000: episode: 348, duration: 0.766s, episode steps: 129, steps per second: 168, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.645 [0.240, 1.261], loss: 0.000013, mean_absolute_error: 0.038963, mean_q: 0.058613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45021/50000: episode: 349, duration: 0.596s, episode steps: 129, steps per second: 216, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.003 [0.001, 0.007], loss: 0.000014, mean_absolute_error: 0.038753, mean_q: 0.058204\n",
      " 45150/50000: episode: 350, duration: 0.660s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.884 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.038854, mean_q: 0.058454\n",
      " 45279/50000: episode: 351, duration: 0.643s, episode steps: 129, steps per second: 201, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.961 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.038889, mean_q: 0.058365\n",
      " 45408/50000: episode: 352, duration: 0.612s, episode steps: 129, steps per second: 211, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.038573, mean_q: 0.058037\n",
      " 45537/50000: episode: 353, duration: 0.661s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.038939, mean_q: 0.058525\n",
      " 45666/50000: episode: 354, duration: 0.648s, episode steps: 129, steps per second: 199, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.039065, mean_q: 0.058885\n",
      " 45795/50000: episode: 355, duration: 0.607s, episode steps: 129, steps per second: 213, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.039045, mean_q: 0.058726\n",
      " 45924/50000: episode: 356, duration: 0.650s, episode steps: 129, steps per second: 199, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.039250, mean_q: 0.059038\n",
      " 46053/50000: episode: 357, duration: 0.609s, episode steps: 129, steps per second: 212, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000027, mean_absolute_error: 0.039312, mean_q: 0.059093\n",
      " 46182/50000: episode: 358, duration: 0.592s, episode steps: 129, steps per second: 218, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.039022, mean_q: 0.058713\n",
      " 46311/50000: episode: 359, duration: 0.623s, episode steps: 129, steps per second: 207, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.078 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038838, mean_q: 0.058407\n",
      " 46440/50000: episode: 360, duration: 0.664s, episode steps: 129, steps per second: 194, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038744, mean_q: 0.058187\n",
      " 46569/50000: episode: 361, duration: 0.760s, episode steps: 129, steps per second: 170, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.984 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.038970, mean_q: 0.058732\n",
      " 46698/50000: episode: 362, duration: 0.670s, episode steps: 129, steps per second: 192, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038742, mean_q: 0.058191\n",
      " 46827/50000: episode: 363, duration: 0.687s, episode steps: 129, steps per second: 188, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.070 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038765, mean_q: 0.058306\n",
      " 46956/50000: episode: 364, duration: 0.650s, episode steps: 129, steps per second: 198, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.109 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000010, mean_absolute_error: 0.038855, mean_q: 0.058508\n",
      " 47085/50000: episode: 365, duration: 0.639s, episode steps: 129, steps per second: 202, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.047 [0.000, 2.000], mean observation: 0.749 [0.597, 1.717], loss: 0.000015, mean_absolute_error: 0.039058, mean_q: 0.058752\n",
      " 47214/50000: episode: 366, duration: 0.692s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.004 [0.001, 0.006], loss: 0.000014, mean_absolute_error: 0.039140, mean_q: 0.059041\n",
      " 47343/50000: episode: 367, duration: 0.614s, episode steps: 129, steps per second: 210, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.946 [0.000, 2.000], mean observation: 0.030 [0.018, 0.049], loss: 0.000024, mean_absolute_error: 0.039111, mean_q: 0.058691\n",
      " 47472/50000: episode: 368, duration: 0.658s, episode steps: 129, steps per second: 196, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000019, mean_absolute_error: 0.039129, mean_q: 0.058788\n",
      " 47601/50000: episode: 369, duration: 0.765s, episode steps: 129, steps per second: 169, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.860 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.039096, mean_q: 0.058697\n",
      " 47730/50000: episode: 370, duration: 0.650s, episode steps: 129, steps per second: 198, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.101 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.038986, mean_q: 0.058692\n",
      " 47859/50000: episode: 371, duration: 0.675s, episode steps: 129, steps per second: 191, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000011, mean_absolute_error: 0.039104, mean_q: 0.058753\n",
      " 47988/50000: episode: 372, duration: 0.633s, episode steps: 129, steps per second: 204, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.038968, mean_q: 0.058587\n",
      " 48117/50000: episode: 373, duration: 0.776s, episode steps: 129, steps per second: 166, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.002 [0.001, 0.003], loss: 0.000015, mean_absolute_error: 0.039205, mean_q: 0.058879\n",
      " 48246/50000: episode: 374, duration: 0.873s, episode steps: 129, steps per second: 148, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.977 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000009, mean_absolute_error: 0.039332, mean_q: 0.059098\n",
      " 48375/50000: episode: 375, duration: 0.685s, episode steps: 129, steps per second: 188, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.116 [0.000, 2.000], mean observation: 0.101 [0.045, 0.138], loss: 0.000010, mean_absolute_error: 0.039532, mean_q: 0.059344\n",
      " 48504/50000: episode: 376, duration: 0.745s, episode steps: 129, steps per second: 173, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000016, mean_absolute_error: 0.039402, mean_q: 0.059286\n",
      " 48633/50000: episode: 377, duration: 0.755s, episode steps: 129, steps per second: 171, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.969 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000012, mean_absolute_error: 0.039397, mean_q: 0.059120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48762/50000: episode: 378, duration: 0.693s, episode steps: 129, steps per second: 186, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000017, mean_absolute_error: 0.039367, mean_q: 0.059204\n",
      " 48891/50000: episode: 379, duration: 0.767s, episode steps: 129, steps per second: 168, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.938 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000013, mean_absolute_error: 0.039384, mean_q: 0.059326\n",
      " 49020/50000: episode: 380, duration: 0.710s, episode steps: 129, steps per second: 182, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.930 [0.000, 2.000], mean observation: 0.017 [0.009, 0.022], loss: 0.000015, mean_absolute_error: 0.039309, mean_q: 0.059006\n",
      " 49149/50000: episode: 381, duration: 0.646s, episode steps: 129, steps per second: 200, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.992 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000015, mean_absolute_error: 0.039223, mean_q: 0.059127\n",
      " 49278/50000: episode: 382, duration: 0.720s, episode steps: 129, steps per second: 179, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.002 [0.002, 0.003], loss: 0.000009, mean_absolute_error: 0.039199, mean_q: 0.058961\n",
      " 49407/50000: episode: 383, duration: 0.914s, episode steps: 129, steps per second: 141, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.899 [0.000, 2.000], mean observation: 0.016 [0.009, 0.039], loss: 0.000017, mean_absolute_error: 0.039352, mean_q: 0.059091\n",
      " 49536/50000: episode: 384, duration: 1.204s, episode steps: 129, steps per second: 107, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.008 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.039334, mean_q: 0.059064\n",
      " 49665/50000: episode: 385, duration: 1.292s, episode steps: 129, steps per second: 100, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.093 [0.000, 2.000], mean observation: 0.003 [0.001, 0.007], loss: 0.000010, mean_absolute_error: 0.039408, mean_q: 0.059191\n",
      " 49794/50000: episode: 386, duration: 0.661s, episode steps: 129, steps per second: 195, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 0.953 [0.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.000014, mean_absolute_error: 0.039320, mean_q: 0.059118\n",
      " 49923/50000: episode: 387, duration: 0.732s, episode steps: 129, steps per second: 176, episode reward: 0.129, mean reward: 0.001 [0.001, 0.001], mean action: 1.039 [0.000, 2.000], mean observation: 0.010 [0.005, 0.025], loss: 0.000014, mean_absolute_error: 0.039475, mean_q: 0.059245\n",
      "done, took 290.457 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2658fdfd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('duel_dqn_{}_weights.h5f'.format('stock_bot'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 0.129, steps: 129\n",
      "Episode 2: reward: 0.129, steps: 129\n",
      "Episode 3: reward: 0.129, steps: 129\n",
      "Episode 4: reward: 0.129, steps: 129\n",
      "Episode 5: reward: 0.129, steps: 129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2658fd7b8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
