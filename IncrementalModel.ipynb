{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-31 05:02:00</td>\n",
       "      <td>948.000</td>\n",
       "      <td>948.000</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.083403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-31 05:03:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-31 05:04:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-31 05:05:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-31 05:06:00</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>942.899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     open     high      low    close    volume\n",
       "0  2016-12-31 05:02:00  948.000  948.000  942.899  942.899  0.083403\n",
       "1  2016-12-31 05:03:00  942.899  942.899  942.899  942.899  0.000000\n",
       "2  2016-12-31 05:04:00  942.899  942.899  942.899  942.899  0.000000\n",
       "3  2016-12-31 05:05:00  942.899  942.899  942.899  942.899  0.000000\n",
       "4  2016-12-31 05:06:00  942.899  942.899  942.899  942.899  0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exch = 'BTRX'\n",
    "pair = 'BTC/USDT'\n",
    "\n",
    "df = pd.read_csv(f\"{exch}_{pair.replace('/', '-')}_ohlcv.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp     object\n",
       "open         float64\n",
       "high         float64\n",
       "low          float64\n",
       "close        float64\n",
       "volume       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlarson/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/home/rlarson/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAE9CAYAAACiDN36AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAglss6uR5F2CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b905469b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the stock prices for the last day\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plot_last_n_minutes = 60 * 12  # 1/2 day\n",
    "cs_frame = df.iloc[-1 * plot_last_n_minutes:].copy()  # Create the candlestick frame\n",
    "\n",
    "#if necessary convert to datetime\n",
    "cs_frame.timestamp = pd.to_datetime(cs_frame.timestamp)\n",
    "\n",
    "cs_frame = cs_frame[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "cs_frame[\"timestamp\"] = cs_frame[\"timestamp\"].apply(mdates.date2num)\n",
    "\n",
    "f1 = plt.subplot2grid((6, 1), (0, 0), rowspan=6, colspan=1, axisbg='#07000d')\n",
    "candlestick_ohlc(f1, cs_frame.values, width=.0001, colorup='#53c156', colordown='#ff1717', alpha=.75)\n",
    "f1.xaxis_date()\n",
    "f1.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d %H:%M'))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Date Hours:Minutes')\n",
    "plt.show()\n",
    "\n",
    "# Cleanup memory\n",
    "%reset_selective -f \"^cs_frame$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add a minute moving average over period\n",
    "def add_moving_avg(df, period=30):\n",
    "    df[f\"{period}_ma\"] = pd.rolling_mean(df['close'], period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlarson/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=30,center=False).mean()\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>30_ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>532493</th>\n",
       "      <td>2018-01-04 23:55:00</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15200.00000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>10.446506</td>\n",
       "      <td>15063.411186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532494</th>\n",
       "      <td>2018-01-04 23:56:00</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>15220.00000</td>\n",
       "      <td>15195.582639</td>\n",
       "      <td>15220.000000</td>\n",
       "      <td>5.457758</td>\n",
       "      <td>15070.777853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532495</th>\n",
       "      <td>2018-01-04 23:57:00</td>\n",
       "      <td>15220.000000</td>\n",
       "      <td>15238.00000</td>\n",
       "      <td>15200.000000</td>\n",
       "      <td>15201.000000</td>\n",
       "      <td>7.473745</td>\n",
       "      <td>15077.644520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532496</th>\n",
       "      <td>2018-01-04 23:58:00</td>\n",
       "      <td>15200.000000</td>\n",
       "      <td>15202.21529</td>\n",
       "      <td>15085.001000</td>\n",
       "      <td>15101.591266</td>\n",
       "      <td>7.258691</td>\n",
       "      <td>15081.030929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532497</th>\n",
       "      <td>2018-01-04 23:59:00</td>\n",
       "      <td>15101.591266</td>\n",
       "      <td>15199.00000</td>\n",
       "      <td>15085.001000</td>\n",
       "      <td>15199.000000</td>\n",
       "      <td>1.777752</td>\n",
       "      <td>15087.697595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp          open         high           low  \\\n",
       "532493  2018-01-04 23:55:00  15199.000000  15200.00000  15199.000000   \n",
       "532494  2018-01-04 23:56:00  15199.000000  15220.00000  15195.582639   \n",
       "532495  2018-01-04 23:57:00  15220.000000  15238.00000  15200.000000   \n",
       "532496  2018-01-04 23:58:00  15200.000000  15202.21529  15085.001000   \n",
       "532497  2018-01-04 23:59:00  15101.591266  15199.00000  15085.001000   \n",
       "\n",
       "               close     volume         30_ma  \n",
       "532493  15199.000000  10.446506  15063.411186  \n",
       "532494  15220.000000   5.457758  15070.777853  \n",
       "532495  15201.000000   7.473745  15077.644520  \n",
       "532496  15101.591266   7.258691  15081.030929  \n",
       "532497  15199.000000   1.777752  15087.697595  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_moving_avg(df)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: Could not seed environment <SinMarketEnv instance>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from stock_gym.envs import stocks\n",
    "import gym\n",
    "\n",
    "env = gym.make('SinMarketEnv-v0')\n",
    "#env.add_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, GRU, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Could not seed environment <SinMarketEnv<SinMarketEnv-v0>>\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, (1, 128))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "(env.n_actions, env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 1, 128)            98688     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 1, 128)            98688     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 296,451\n",
      "Trainable params: 296,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(128,\n",
    "                input_shape=env.observation_space.shape,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.5,\n",
    "                return_sequences=True,\n",
    "               #stateful=False,\n",
    "             ))\n",
    "model.add(GRU(128,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.5,\n",
    "                return_sequences=True,\n",
    "             ))\n",
    "model.add(GRU(128,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.5,\n",
    "             ))\n",
    "#model.add(Dense(64))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(env.n_actions, kernel_initializer='lecun_uniform', activation='linear'))\n",
    "#model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "#model.add(Dense(env.n_actions, activation='linear'))\n",
    "\n",
    "# model.add(Flatten(input_shape=env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(env.n_actions, activation='linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=env.total_space_size, window_length=1)\n",
    "policy = BoltzmannQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=env.n_actions, memory=memory, nb_steps_warmup=100,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.compile(\n",
    "    #loss='mse',\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    metrics=['mae'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "    2/5000: episode: 1, duration: 0.407s, episode steps: 2, steps per second: 5, episode reward: -9999.989, mean reward: -4999.994 [-9999.990, 0.001], mean action: 1.000 [0.000, 2.000], mean observation: 0.959 [0.916, 0.990], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3/5000: episode: 2, duration: 0.005s, episode steps: 1, steps per second: 216, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.615 [0.519, 0.708], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4/5000: episode: 3, duration: 0.004s, episode steps: 1, steps per second: 235, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5/5000: episode: 4, duration: 0.004s, episode steps: 1, steps per second: 241, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.625 [0.530, 0.717], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6/5000: episode: 5, duration: 0.003s, episode steps: 1, steps per second: 302, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.681 [0.588, 0.769], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    7/5000: episode: 6, duration: 0.004s, episode steps: 1, steps per second: 232, episode reward: 2.896, mean reward: 2.896 [2.896, 2.896], mean action: 2.000 [2.000, 2.000], mean observation: 0.208 [0.133, 0.290], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    8/5000: episode: 7, duration: 0.004s, episode steps: 1, steps per second: 248, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.923 [0.867, 0.969], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    9/5000: episode: 8, duration: 0.003s, episode steps: 1, steps per second: 288, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.996 [0.983, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   10/5000: episode: 9, duration: 0.004s, episode steps: 1, steps per second: 264, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.348 [0.258, 0.442], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   11/5000: episode: 10, duration: 0.004s, episode steps: 1, steps per second: 244, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.623, 0.798], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   12/5000: episode: 11, duration: 0.004s, episode steps: 1, steps per second: 247, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.279 [0.195, 0.368], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   13/5000: episode: 12, duration: 0.004s, episode steps: 1, steps per second: 246, episode reward: 2.906, mean reward: 2.906 [2.906, 2.906], mean action: 2.000 [2.000, 2.000], mean observation: 0.209 [0.134, 0.291], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   14/5000: episode: 13, duration: 0.004s, episode steps: 1, steps per second: 269, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.005 [0.000, 0.021], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   15/5000: episode: 14, duration: 0.004s, episode steps: 1, steps per second: 237, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.016 [0.000, 0.044], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   16/5000: episode: 15, duration: 0.004s, episode steps: 1, steps per second: 240, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.106 [0.052, 0.170], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   17/5000: episode: 16, duration: 0.004s, episode steps: 1, steps per second: 260, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.268 [0.185, 0.356], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   18/5000: episode: 17, duration: 0.003s, episode steps: 1, steps per second: 289, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.962 [0.921, 0.992], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   19/5000: episode: 18, duration: 0.004s, episode steps: 1, steps per second: 240, episode reward: 0.937, mean reward: 0.937 [0.937, 0.937], mean action: 2.000 [2.000, 2.000], mean observation: 0.158 [0.092, 0.232], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   20/5000: episode: 19, duration: 0.004s, episode steps: 1, steps per second: 238, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.063 [0.022, 0.114], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   21/5000: episode: 20, duration: 0.004s, episode steps: 1, steps per second: 247, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.568 [0.471, 0.663], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   22/5000: episode: 21, duration: 0.004s, episode steps: 1, steps per second: 238, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.937, 0.996], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   23/5000: episode: 22, duration: 0.004s, episode steps: 1, steps per second: 242, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.331, 0.522], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   24/5000: episode: 23, duration: 0.004s, episode steps: 1, steps per second: 262, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.074 [0.030, 0.129], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   25/5000: episode: 24, duration: 0.004s, episode steps: 1, steps per second: 234, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   26/5000: episode: 25, duration: 0.006s, episode steps: 1, steps per second: 161, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.572 [0.476, 0.667], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   27/5000: episode: 26, duration: 0.004s, episode steps: 1, steps per second: 254, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   28/5000: episode: 27, duration: 0.004s, episode steps: 1, steps per second: 257, episode reward: 8.074, mean reward: 8.074 [8.074, 8.074], mean action: 2.000 [2.000, 2.000], mean observation: 0.724 [0.635, 0.808], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   29/5000: episode: 28, duration: 0.004s, episode steps: 1, steps per second: 232, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.731, 0.882], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   30/5000: episode: 29, duration: 0.005s, episode steps: 1, steps per second: 201, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.961 [0.919, 0.991], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   31/5000: episode: 30, duration: 0.004s, episode steps: 1, steps per second: 254, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.030 [0.004, 0.066], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   32/5000: episode: 31, duration: 0.004s, episode steps: 1, steps per second: 266, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.032 [0.006, 0.071], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   33/5000: episode: 32, duration: 0.004s, episode steps: 1, steps per second: 229, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.951 [0.904, 0.986], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   34/5000: episode: 33, duration: 0.004s, episode steps: 1, steps per second: 252, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.893 [0.829, 0.948], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   35/5000: episode: 34, duration: 0.003s, episode steps: 1, steps per second: 291, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.359 [0.268, 0.453], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   36/5000: episode: 35, duration: 0.004s, episode steps: 1, steps per second: 270, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.101 [0.049, 0.164], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   37/5000: episode: 36, duration: 0.005s, episode steps: 1, steps per second: 211, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.943 [0.893, 0.981], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   38/5000: episode: 37, duration: 0.004s, episode steps: 1, steps per second: 263, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.028 [0.004, 0.064], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   39/5000: episode: 38, duration: 0.004s, episode steps: 1, steps per second: 274, episode reward: 7.283, mean reward: 7.283 [7.283, 7.283], mean action: 2.000 [2.000, 2.000], mean observation: 0.806 [0.726, 0.878], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   40/5000: episode: 39, duration: 0.004s, episode steps: 1, steps per second: 240, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.271 [0.188, 0.359], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   41/5000: episode: 40, duration: 0.004s, episode steps: 1, steps per second: 261, episode reward: 0.212, mean reward: 0.212 [0.212, 0.212], mean action: 2.000 [2.000, 2.000], mean observation: 0.059 [0.020, 0.109], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   42/5000: episode: 41, duration: 0.004s, episode steps: 1, steps per second: 285, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.748 [0.661, 0.829], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   43/5000: episode: 42, duration: 0.005s, episode steps: 1, steps per second: 207, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.606, 0.784], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   44/5000: episode: 43, duration: 0.004s, episode steps: 1, steps per second: 225, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.926 [0.871, 0.971], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   45/5000: episode: 44, duration: 0.004s, episode steps: 1, steps per second: 278, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.631 [0.536, 0.723], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   46/5000: episode: 45, duration: 0.004s, episode steps: 1, steps per second: 238, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.990 [0.968, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47/5000: episode: 46, duration: 0.005s, episode steps: 1, steps per second: 206, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.559 [0.462, 0.655], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   48/5000: episode: 47, duration: 0.005s, episode steps: 1, steps per second: 201, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.364 [0.273, 0.459], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   49/5000: episode: 48, duration: 0.004s, episode steps: 1, steps per second: 271, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.421 [0.326, 0.518], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   50/5000: episode: 49, duration: 0.004s, episode steps: 1, steps per second: 228, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.242 [0.163, 0.328], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   51/5000: episode: 50, duration: 0.005s, episode steps: 1, steps per second: 200, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.893, 0.981], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   52/5000: episode: 51, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.758 [0.672, 0.838], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   53/5000: episode: 52, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 6.694, mean reward: 6.694 [6.694, 6.694], mean action: 2.000 [2.000, 2.000], mean observation: 0.753 [0.667, 0.833], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   54/5000: episode: 53, duration: 0.007s, episode steps: 1, steps per second: 153, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.283 [0.199, 0.373], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   55/5000: episode: 54, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.936 [0.884, 0.977], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   56/5000: episode: 55, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.714, mean reward: 0.714 [0.714, 0.714], mean action: 2.000 [2.000, 2.000], mean observation: 0.129 [0.070, 0.199], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   57/5000: episode: 56, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.064 [0.023, 0.116], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   58/5000: episode: 57, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.153 [0.088, 0.226], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   59/5000: episode: 58, duration: 0.005s, episode steps: 1, steps per second: 192, episode reward: 1.683, mean reward: 1.683 [1.683, 1.683], mean action: 2.000 [2.000, 2.000], mean observation: 0.105 [0.051, 0.168], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   60/5000: episode: 59, duration: 0.006s, episode steps: 1, steps per second: 155, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.790 [0.708, 0.865], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   61/5000: episode: 60, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.271 [0.188, 0.359], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   62/5000: episode: 61, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.004 [0.000, 0.016], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   63/5000: episode: 62, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.152], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   64/5000: episode: 63, duration: 0.022s, episode steps: 1, steps per second: 46, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.855 [0.783, 0.918], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   65/5000: episode: 64, duration: 0.004s, episode steps: 1, steps per second: 236, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.617, 0.793], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   66/5000: episode: 65, duration: 0.004s, episode steps: 1, steps per second: 284, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.457 [0.361, 0.554], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   67/5000: episode: 66, duration: 0.004s, episode steps: 1, steps per second: 250, episode reward: 9.966, mean reward: 9.966 [9.966, 9.966], mean action: 2.000 [2.000, 2.000], mean observation: 0.996 [0.984, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   68/5000: episode: 67, duration: 0.004s, episode steps: 1, steps per second: 231, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.051], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   69/5000: episode: 68, duration: 0.004s, episode steps: 1, steps per second: 271, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.291 [0.206, 0.382], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   70/5000: episode: 69, duration: 0.006s, episode steps: 1, steps per second: 181, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.912 [0.853, 0.961], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   71/5000: episode: 70, duration: 0.004s, episode steps: 1, steps per second: 262, episode reward: 9.533, mean reward: 9.533 [9.533, 9.533], mean action: 2.000 [2.000, 2.000], mean observation: 0.901 [0.839, 0.953], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   72/5000: episode: 71, duration: 0.004s, episode steps: 1, steps per second: 229, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.072, 0.202], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   73/5000: episode: 72, duration: 0.004s, episode steps: 1, steps per second: 233, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.975 [0.941, 0.997], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   74/5000: episode: 73, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.808 [0.729, 0.880], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   75/5000: episode: 74, duration: 0.008s, episode steps: 1, steps per second: 119, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.119, 0.271], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   76/5000: episode: 75, duration: 0.005s, episode steps: 1, steps per second: 190, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.768 [0.684, 0.847], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   77/5000: episode: 76, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 7.301, mean reward: 7.301 [7.301, 7.301], mean action: 2.000 [2.000, 2.000], mean observation: 0.639 [0.545, 0.730], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   78/5000: episode: 77, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.022 [0.002, 0.054], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   79/5000: episode: 78, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.017 [0.000, 0.046], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   80/5000: episode: 79, duration: 0.004s, episode steps: 1, steps per second: 243, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.198 [0.125, 0.279], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   81/5000: episode: 80, duration: 0.006s, episode steps: 1, steps per second: 171, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   82/5000: episode: 81, duration: 0.004s, episode steps: 1, steps per second: 285, episode reward: 9.839, mean reward: 9.839 [9.839, 9.839], mean action: 2.000 [2.000, 2.000], mean observation: 0.996 [0.982, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   83/5000: episode: 82, duration: 0.004s, episode steps: 1, steps per second: 281, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.909 [0.849, 0.959], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   84/5000: episode: 83, duration: 0.003s, episode steps: 1, steps per second: 329, episode reward: 9.712, mean reward: 9.712 [9.712, 9.712], mean action: 2.000 [2.000, 2.000], mean observation: 0.926 [0.871, 0.971], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   85/5000: episode: 84, duration: 0.004s, episode steps: 1, steps per second: 277, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.923, 0.993], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   86/5000: episode: 85, duration: 0.004s, episode steps: 1, steps per second: 273, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.914 [0.856, 0.962], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   87/5000: episode: 86, duration: 0.004s, episode steps: 1, steps per second: 265, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.863 [0.793, 0.925], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   88/5000: episode: 87, duration: 0.003s, episode steps: 1, steps per second: 290, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.783 [0.700, 0.859], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   89/5000: episode: 88, duration: 0.004s, episode steps: 1, steps per second: 257, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.754 [0.668, 0.834], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   90/5000: episode: 89, duration: 0.003s, episode steps: 1, steps per second: 290, episode reward: 1.785, mean reward: 1.785 [1.785, 1.785], mean action: 2.000 [2.000, 2.000], mean observation: 0.258 [0.176, 0.345], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   91/5000: episode: 90, duration: 0.004s, episode steps: 1, steps per second: 266, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.187 [0.116, 0.266], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   92/5000: episode: 91, duration: 0.004s, episode steps: 1, steps per second: 284, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.009 [0.000, 0.029], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   93/5000: episode: 92, duration: 0.005s, episode steps: 1, steps per second: 217, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.326 [0.238, 0.419], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   94/5000: episode: 93, duration: 0.004s, episode steps: 1, steps per second: 282, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.536 [0.439, 0.633], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   95/5000: episode: 94, duration: 0.004s, episode steps: 1, steps per second: 256, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   96/5000: episode: 95, duration: 0.004s, episode steps: 1, steps per second: 252, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   97/5000: episode: 96, duration: 0.004s, episode steps: 1, steps per second: 275, episode reward: 5.181, mean reward: 5.181 [5.181, 5.181], mean action: 2.000 [2.000, 2.000], mean observation: 0.611 [0.516, 0.704], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   98/5000: episode: 97, duration: 0.004s, episode steps: 1, steps per second: 250, episode reward: -9999.990, mean reward: -9999.990 [-9999.990, -9999.990], mean action: 2.000 [2.000, 2.000], mean observation: 0.314 [0.227, 0.406], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   99/5000: episode: 98, duration: 0.004s, episode steps: 1, steps per second: 253, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.270 [0.187, 0.359], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  100/5000: episode: 99, duration: 0.005s, episode steps: 1, steps per second: 214, episode reward: 10.010, mean reward: 10.010 [10.010, 10.010], mean action: 2.000 [2.000, 2.000], mean observation: 0.987 [0.961, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  101/5000: episode: 100, duration: 4.889s, episode steps: 1, steps per second: 0, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.079 [0.033, 0.136], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  102/5000: episode: 101, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.096 [0.045, 0.157], loss: 6452992.000000, mean_absolute_error: 552.357178, mean_q: -0.063400\n",
      "  103/5000: episode: 102, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 8061973.500000, mean_absolute_error: 688.050537, mean_q: -0.240484\n",
      "  104/5000: episode: 103, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.022, mean reward: 0.022 [0.022, 0.022], mean action: 2.000 [2.000, 2.000], mean observation: 0.020 [0.001, 0.051], loss: 6514489.500000, mean_absolute_error: 594.169067, mean_q: -0.467327\n",
      "  105/5000: episode: 104, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.266 [0.184, 0.355], loss: 8013806.000000, mean_absolute_error: 657.107422, mean_q: -0.760628\n",
      "  106/5000: episode: 105, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.049], loss: 8044159.000000, mean_absolute_error: 678.037598, mean_q: -1.142401\n",
      "  107/5000: episode: 106, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.931 [0.878, 0.974], loss: 4950138.500000, mean_absolute_error: 490.863159, mean_q: -1.639864\n",
      "  108/5000: episode: 107, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.830 [0.754, 0.898], loss: 4887335.500000, mean_absolute_error: 450.809387, mean_q: -2.544079\n",
      "  109/5000: episode: 108, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.014, mean reward: 0.014 [0.014, 0.014], mean action: 2.000 [2.000, 2.000], mean observation: 0.009 [0.000, 0.030], loss: 3370081.500000, mean_absolute_error: 377.187195, mean_q: -2.765535\n",
      "  110/5000: episode: 109, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.687 [0.596, 0.775], loss: 9537725.000000, mean_absolute_error: 742.732971, mean_q: -3.220037\n",
      "  111/5000: episode: 110, duration: 0.147s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.094 [0.044, 0.155], loss: 4896427.000000, mean_absolute_error: 462.660583, mean_q: -5.312231\n",
      "  112/5000: episode: 111, duration: 0.146s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.513, 0.702], loss: 3289845.000000, mean_absolute_error: 327.518097, mean_q: -5.029389\n",
      "  113/5000: episode: 112, duration: 0.118s, episode steps: 1, steps per second: 8, episode reward: -999.999, mean reward: -999.999 [-999.999, -999.999], mean action: 1.000 [1.000, 1.000], mean observation: 0.004 [0.000, 0.018], loss: 3335156.000000, mean_absolute_error: 360.243866, mean_q: -6.436492\n",
      "  114/5000: episode: 113, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.036], loss: 3396553.000000, mean_absolute_error: 401.478210, mean_q: -7.389779\n",
      "  115/5000: episode: 114, duration: 0.116s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.969, 1.000], loss: 6447013.000000, mean_absolute_error: 569.122742, mean_q: -8.196569\n",
      "  116/5000: episode: 115, duration: 0.159s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.878 [0.810, 0.936], loss: 3348982.250000, mean_absolute_error: 371.911072, mean_q: -8.388801\n",
      "  117/5000: episode: 116, duration: 0.139s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.084 [0.036, 0.142], loss: 6451129.500000, mean_absolute_error: 569.697937, mean_q: -8.100390\n",
      "  118/5000: episode: 117, duration: 0.125s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.115, 0.265], loss: 6404029.000000, mean_absolute_error: 540.055176, mean_q: -8.381405\n",
      "  119/5000: episode: 118, duration: 0.158s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.397 [0.303, 0.493], loss: 8002079.000000, mean_absolute_error: 674.419373, mean_q: -8.734159\n",
      "  120/5000: episode: 119, duration: 0.147s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.878 [0.810, 0.936], loss: 3343730.500000, mean_absolute_error: 373.719788, mean_q: -9.130033\n",
      "  121/5000: episode: 120, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.716 [0.626, 0.800], loss: 7983814.500000, mean_absolute_error: 664.881836, mean_q: -9.201876\n",
      "  122/5000: episode: 121, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.903, 0.985], loss: 6466548.500000, mean_absolute_error: 591.406738, mean_q: -9.286093\n",
      "  123/5000: episode: 122, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.038], loss: 8021540.500000, mean_absolute_error: 695.933960, mean_q: -10.103123\n",
      "  124/5000: episode: 123, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.917 [0.860, 0.965], loss: 3306146.000000, mean_absolute_error: 354.829254, mean_q: -10.237842\n",
      "  125/5000: episode: 124, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.516, 0.704], loss: 3382990.750000, mean_absolute_error: 404.731262, mean_q: -9.798132\n",
      "  126/5000: episode: 125, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 271058.062500, mean_absolute_error: 198.250488, mean_q: -9.980083\n",
      "  127/5000: episode: 126, duration: 0.087s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.184, 0.355], loss: 3294527.500000, mean_absolute_error: 345.400452, mean_q: -9.851733\n",
      "  128/5000: episode: 127, duration: 0.153s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.821, 0.942], loss: 3319445.500000, mean_absolute_error: 365.546539, mean_q: -9.957681\n",
      "  129/5000: episode: 128, duration: 0.089s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.934, 0.996], loss: 4858384.000000, mean_absolute_error: 459.230255, mean_q: -10.138743\n",
      "  130/5000: episode: 129, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 4860682.000000, mean_absolute_error: 459.552460, mean_q: -10.031771\n",
      "  131/5000: episode: 130, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.294, 0.483], loss: 7879863.000000, mean_absolute_error: 607.348389, mean_q: -10.015989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  132/5000: episode: 131, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 6396399.500000, mean_absolute_error: 553.199341, mean_q: -9.924541\n",
      "  133/5000: episode: 132, duration: 0.125s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [0.083, 0.219], loss: 4929961.000000, mean_absolute_error: 510.273621, mean_q: -10.502493\n",
      "  134/5000: episode: 133, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.048], loss: 6408213.000000, mean_absolute_error: 563.963257, mean_q: -10.273220\n",
      "  135/5000: episode: 134, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.887 [0.821, 0.943], loss: 4809571.000000, mean_absolute_error: 431.272461, mean_q: -10.393379\n",
      "  136/5000: episode: 135, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.247, 0.430], loss: 268670.843750, mean_absolute_error: 200.095703, mean_q: -10.337560\n",
      "  137/5000: episode: 136, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.832 [0.756, 0.900], loss: 3269535.500000, mean_absolute_error: 337.998352, mean_q: -10.596972\n",
      "  138/5000: episode: 137, duration: 0.103s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 3360183.500000, mean_absolute_error: 397.821045, mean_q: -10.518760\n",
      "  139/5000: episode: 138, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.909 [0.849, 0.959], loss: 4778020.000000, mean_absolute_error: 412.122498, mean_q: -10.514696\n",
      "  140/5000: episode: 139, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.472 [0.376, 0.569], loss: 3284463.250000, mean_absolute_error: 348.536346, mean_q: -10.468033\n",
      "  141/5000: episode: 140, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.030], loss: 3267903.250000, mean_absolute_error: 338.986511, mean_q: -10.567021\n",
      "  142/5000: episode: 141, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.036, 0.141], loss: 3311760.250000, mean_absolute_error: 368.592010, mean_q: -10.623217\n",
      "  143/5000: episode: 142, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.955, 1.000], loss: 6400653.500000, mean_absolute_error: 565.848145, mean_q: -10.552141\n",
      "  144/5000: episode: 143, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.906 [0.845, 0.956], loss: 3251687.000000, mean_absolute_error: 329.522034, mean_q: -10.594416\n",
      "  145/5000: episode: 144, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 3296086.000000, mean_absolute_error: 359.238342, mean_q: -10.573870\n",
      "  146/5000: episode: 145, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.937, 0.997], loss: 4876419.000000, mean_absolute_error: 482.692474, mean_q: -10.570721\n",
      "  147/5000: episode: 146, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.889 [0.823, 0.944], loss: 1728966.125000, mean_absolute_error: 246.317322, mean_q: -10.515741\n",
      "  148/5000: episode: 147, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.692, 0.853], loss: 3353321.500000, mean_absolute_error: 399.053406, mean_q: -10.612463\n",
      "  149/5000: episode: 148, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.557, 0.742], loss: 1772401.000000, mean_absolute_error: 276.288879, mean_q: -10.634650\n",
      "  150/5000: episode: 149, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 7857182.000000, mean_absolute_error: 611.509155, mean_q: -10.675478\n",
      "  151/5000: episode: 150, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.178], loss: 4858232.000000, mean_absolute_error: 473.734070, mean_q: -10.681561\n",
      "  152/5000: episode: 151, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.283 [0.199, 0.373], loss: 3277673.000000, mean_absolute_error: 350.809113, mean_q: -10.680201\n",
      "  153/5000: episode: 152, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.950, 0.999], loss: 4797880.000000, mean_absolute_error: 434.747437, mean_q: -10.768395\n",
      "  154/5000: episode: 153, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.079], loss: 4811988.000000, mean_absolute_error: 444.763000, mean_q: -10.797815\n",
      "  155/5000: episode: 154, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 4767166.000000, mean_absolute_error: 415.851074, mean_q: -10.858698\n",
      "  156/5000: episode: 155, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.907 [0.847, 0.958], loss: 7895421.500000, mean_absolute_error: 641.901123, mean_q: -10.868183\n",
      "  157/5000: episode: 156, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.064], loss: 3275096.000000, mean_absolute_error: 351.730164, mean_q: -10.937819\n",
      "  158/5000: episode: 157, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 3289426.500000, mean_absolute_error: 361.972656, mean_q: -10.988424\n",
      "  159/5000: episode: 158, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [0.087, 0.224], loss: 3303522.750000, mean_absolute_error: 371.673218, mean_q: -11.001723\n",
      "  160/5000: episode: 159, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.041], loss: 3200503.250000, mean_absolute_error: 303.772644, mean_q: -11.056858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  161/5000: episode: 160, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 4778166.500000, mean_absolute_error: 426.744080, mean_q: -11.061828\n",
      "  162/5000: episode: 161, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.939, 0.997], loss: 4865445.000000, mean_absolute_error: 485.460297, mean_q: -11.111400\n",
      "  163/5000: episode: 162, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.176, 0.345], loss: 161321.796875, mean_absolute_error: 136.880920, mean_q: -11.148272\n",
      "  164/5000: episode: 163, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.136, 0.294], loss: 4761775.500000, mean_absolute_error: 417.542236, mean_q: -11.160172\n",
      "  165/5000: episode: 164, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.147, 0.308], loss: 3184028.750000, mean_absolute_error: 295.126007, mean_q: -11.189686\n",
      "  166/5000: episode: 165, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [0.106, 0.252], loss: 4819087.000000, mean_absolute_error: 456.845520, mean_q: -11.217241\n",
      "  167/5000: episode: 166, duration: 0.130s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.869 [0.800, 0.930], loss: 7855205.500000, mean_absolute_error: 624.237183, mean_q: -11.240412\n",
      "  168/5000: episode: 167, duration: 0.111s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 1679319.625000, mean_absolute_error: 221.756042, mean_q: -11.296803\n",
      "  169/5000: episode: 168, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [0.065, 0.192], loss: 131779.437500, mean_absolute_error: 118.806656, mean_q: -11.311253\n",
      "  170/5000: episode: 169, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 3255083.000000, mean_absolute_error: 344.697876, mean_q: -11.349299\n",
      "  171/5000: episode: 170, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.721, 0.875], loss: 4831028.000000, mean_absolute_error: 467.729736, mean_q: -11.400594\n",
      "  172/5000: episode: 171, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.320 [0.232, 0.412], loss: 3254300.250000, mean_absolute_error: 345.226074, mean_q: -11.425568\n",
      "  173/5000: episode: 172, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.706, 0.863], loss: 9425851.000000, mean_absolute_error: 747.972046, mean_q: -11.445763\n",
      "  174/5000: episode: 173, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [0.069, 0.197], loss: 4785333.000000, mean_absolute_error: 438.700928, mean_q: -11.470594\n",
      "  175/5000: episode: 174, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.082], loss: 6331113.000000, mean_absolute_error: 542.054321, mean_q: -11.504165\n",
      "  176/5000: episode: 175, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 1691993.875000, mean_absolute_error: 233.080200, mean_q: -11.537270\n",
      "  177/5000: episode: 176, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.662, 0.830], loss: 3179585.000000, mean_absolute_error: 297.657410, mean_q: -11.579045\n",
      "  178/5000: episode: 177, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.940, 0.997], loss: 1691514.000000, mean_absolute_error: 233.431061, mean_q: -11.618096\n",
      "  179/5000: episode: 178, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.079, 0.213], loss: 4709899.500000, mean_absolute_error: 391.308838, mean_q: -11.650132\n",
      "  180/5000: episode: 179, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 1632975.625000, mean_absolute_error: 195.181137, mean_q: -11.679931\n",
      "  181/5000: episode: 180, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.728, 0.880], loss: 7843328.000000, mean_absolute_error: 626.852051, mean_q: -11.719607\n",
      "  182/5000: episode: 181, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.428, 0.621], loss: 1661544.750000, mean_absolute_error: 214.727036, mean_q: -11.767513\n",
      "  183/5000: episode: 182, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.721, 0.875], loss: 1646969.375000, mean_absolute_error: 205.556488, mean_q: -11.807836\n",
      "  184/5000: episode: 183, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.445, 0.638], loss: 4765419.500000, mean_absolute_error: 430.829193, mean_q: -11.830301\n",
      "  185/5000: episode: 184, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 4808378.000000, mean_absolute_error: 460.009460, mean_q: -11.882159\n",
      "  186/5000: episode: 185, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.079], loss: 1675474.125000, mean_absolute_error: 225.501480, mean_q: -11.914976\n",
      "  187/5000: episode: 186, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.161, 0.326], loss: 4720455.500000, mean_absolute_error: 402.406372, mean_q: -11.941158\n",
      "  188/5000: episode: 187, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.099 [0.047, 0.161], loss: 1689343.375000, mean_absolute_error: 235.350861, mean_q: -11.993839\n",
      "  189/5000: episode: 188, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.097], loss: 1703683.375000, mean_absolute_error: 245.201324, mean_q: -12.035931\n",
      "  190/5000: episode: 189, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 3218461.500000, mean_absolute_error: 329.071106, mean_q: -12.068998\n",
      "  191/5000: episode: 190, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.207, 0.382], loss: 4732939.000000, mean_absolute_error: 412.877747, mean_q: -12.104311\n",
      "  192/5000: episode: 191, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.158, 0.322], loss: 3159829.000000, mean_absolute_error: 290.898529, mean_q: -12.151109\n",
      "  193/5000: episode: 192, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.400 [0.307, 0.496], loss: 1616025.125000, mean_absolute_error: 188.169037, mean_q: -12.181463\n",
      "  194/5000: episode: 193, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [0.094, 0.236], loss: 3173756.500000, mean_absolute_error: 300.798767, mean_q: -12.207310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  195/5000: episode: 194, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 1630121.625000, mean_absolute_error: 198.070374, mean_q: -12.239726\n",
      "  196/5000: episode: 195, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 3245520.250000, mean_absolute_error: 349.473938, mean_q: -12.267780\n",
      "  197/5000: episode: 196, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.353, 0.546], loss: 4730489.500000, mean_absolute_error: 414.111664, mean_q: -12.280773\n",
      "  198/5000: episode: 197, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.177 [0.107, 0.254], loss: 7902252.500000, mean_absolute_error: 677.443970, mean_q: -12.298154\n",
      "  199/5000: episode: 198, duration: 0.112s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.619, 0.795], loss: 1629664.000000, mean_absolute_error: 198.842148, mean_q: -12.328344\n",
      "  200/5000: episode: 199, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.924, 0.993], loss: 1629529.625000, mean_absolute_error: 199.167130, mean_q: -12.355526\n",
      "  201/5000: episode: 200, duration: 0.135s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 1744805.500000, mean_absolute_error: 276.173279, mean_q: -12.379592\n",
      "  202/5000: episode: 201, duration: 0.138s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.664, 0.831], loss: 1643553.250000, mean_absolute_error: 209.108704, mean_q: -12.414704\n",
      "  203/5000: episode: 202, duration: 0.140s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 4828635.500000, mean_absolute_error: 482.025482, mean_q: -12.430743\n",
      "  204/5000: episode: 203, duration: 0.160s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.103 [0.050, 0.166], loss: 3156553.250000, mean_absolute_error: 292.985596, mean_q: -12.448762\n",
      "  205/5000: episode: 204, duration: 0.119s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.925, 0.993], loss: 4712298.000000, mean_absolute_error: 405.438202, mean_q: -12.471652\n",
      "  206/5000: episode: 205, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 86829.609375, mean_absolute_error: 97.423500, mean_q: -12.511925\n",
      "  207/5000: episode: 206, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.237, 0.419], loss: 144255.625000, mean_absolute_error: 135.888153, mean_q: -12.539631\n",
      "  208/5000: episode: 207, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.558, 0.742], loss: 115418.257812, mean_absolute_error: 116.911201, mean_q: -12.557426\n",
      "  209/5000: episode: 208, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.057], loss: 3212414.500000, mean_absolute_error: 332.074158, mean_q: -12.572802\n",
      "  210/5000: episode: 209, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.987], loss: 158734.250000, mean_absolute_error: 146.080521, mean_q: -12.587769\n",
      "  211/5000: episode: 210, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.060], loss: 4738932.000000, mean_absolute_error: 425.779724, mean_q: -12.572979\n",
      "  212/5000: episode: 211, duration: 0.107s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.948, 0.999], loss: 3168749.250000, mean_absolute_error: 303.893311, mean_q: -12.591352\n",
      "  213/5000: episode: 212, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.489, 0.680], loss: 143925.828125, mean_absolute_error: 136.600922, mean_q: -12.595434\n",
      "  214/5000: episode: 213, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.894 [0.830, 0.948], loss: 4694690.500000, mean_absolute_error: 397.314423, mean_q: -12.624318\n",
      "  215/5000: episode: 214, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [0.061, 0.184], loss: 1670209.375000, mean_absolute_error: 230.209900, mean_q: -12.654976\n",
      "  216/5000: episode: 215, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.043], loss: 3196220.500000, mean_absolute_error: 323.385071, mean_q: -12.676477\n",
      "  217/5000: episode: 216, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 3152684.750000, mean_absolute_error: 294.854919, mean_q: -12.692874\n",
      "  218/5000: episode: 217, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.445 [0.349, 0.542], loss: 1727057.125000, mean_absolute_error: 268.872620, mean_q: -12.712141\n",
      "  219/5000: episode: 218, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.306, 0.495], loss: 3152573.000000, mean_absolute_error: 295.326935, mean_q: -12.717474\n",
      "  220/5000: episode: 219, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.149], loss: 4706595.500000, mean_absolute_error: 407.485016, mean_q: -12.733407\n",
      "  221/5000: episode: 220, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.129, 0.284], loss: 4677680.000000, mean_absolute_error: 388.728027, mean_q: -12.759779\n",
      "  222/5000: episode: 221, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.844 [0.770, 0.909], loss: 3180328.000000, mean_absolute_error: 314.790833, mean_q: -12.801883\n",
      "  223/5000: episode: 222, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.269 [0.187, 0.358], loss: 1640330.500000, mean_absolute_error: 212.420013, mean_q: -12.838629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  224/5000: episode: 223, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.907 [0.846, 0.957], loss: 129052.648438, mean_absolute_error: 128.897522, mean_q: -12.873022\n",
      "  225/5000: episode: 224, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 1683066.125000, mean_absolute_error: 241.408463, mean_q: -12.894258\n",
      "  226/5000: episode: 225, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.840 [0.765, 0.906], loss: 4761259.500000, mean_absolute_error: 446.575256, mean_q: -12.919408\n",
      "  227/5000: episode: 226, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.698, 0.857], loss: 100559.406250, mean_absolute_error: 110.379677, mean_q: -12.938909\n",
      "  228/5000: episode: 227, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.947, 0.999], loss: 1625173.375000, mean_absolute_error: 203.663879, mean_q: -12.943707\n",
      "  229/5000: episode: 228, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.330, 0.522], loss: 3207013.500000, mean_absolute_error: 335.070251, mean_q: -12.962571\n",
      "  230/5000: episode: 229, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 1610389.500000, mean_absolute_error: 194.311188, mean_q: -12.977864\n",
      "  231/5000: episode: 230, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.361 [0.270, 0.456], loss: 3191876.500000, mean_absolute_error: 325.537781, mean_q: -12.987144\n",
      "  232/5000: episode: 231, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.726, 0.878], loss: 1653455.500000, mean_absolute_error: 223.337936, mean_q: -12.990827\n",
      "  233/5000: episode: 232, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.689, 0.851], loss: 1724229.875000, mean_absolute_error: 270.683533, mean_q: -12.994701\n",
      "  234/5000: episode: 233, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 3177024.000000, mean_absolute_error: 316.524994, mean_q: -12.990890\n",
      "  235/5000: episode: 234, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.545 [0.448, 0.640], loss: 1567501.625000, mean_absolute_error: 166.876938, mean_q: -13.004139\n",
      "  236/5000: episode: 235, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 1638350.250000, mean_absolute_error: 214.325928, mean_q: -13.009955\n",
      "  237/5000: episode: 236, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.044], loss: 1609864.750000, mean_absolute_error: 195.372284, mean_q: -13.023163\n",
      "  238/5000: episode: 237, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.036, 0.141], loss: 1638289.000000, mean_absolute_error: 214.558472, mean_q: -13.032045\n",
      "  239/5000: episode: 238, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 3190020.000000, mean_absolute_error: 326.570129, mean_q: -13.042079\n",
      "  240/5000: episode: 239, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 1623438.125000, mean_absolute_error: 205.253510, mean_q: -13.044405\n",
      "  241/5000: episode: 240, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.184, 0.355], loss: 1651775.250000, mean_absolute_error: 224.325165, mean_q: -13.032595\n",
      "  242/5000: episode: 241, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.130, 0.285], loss: 1666180.625000, mean_absolute_error: 234.011490, mean_q: -13.030163\n",
      "  243/5000: episode: 242, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.638 [0.543, 0.729], loss: 3118569.750000, mean_absolute_error: 279.915283, mean_q: -13.020027\n",
      "  244/5000: episode: 243, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.453 [0.357, 0.550], loss: 6250146.500000, mean_absolute_error: 522.770691, mean_q: -13.017911\n",
      "  245/5000: episode: 244, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.273, 0.459], loss: 1637176.500000, mean_absolute_error: 215.476883, mean_q: -13.025825\n",
      "  246/5000: episode: 245, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.402, 0.596], loss: 4725955.000000, mean_absolute_error: 430.018127, mean_q: -13.041487\n",
      "  247/5000: episode: 246, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.384, 0.577], loss: 3145757.500000, mean_absolute_error: 299.300781, mean_q: -13.051563\n",
      "  248/5000: episode: 247, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.163, 0.328], loss: 1650893.000000, mean_absolute_error: 225.301651, mean_q: -13.068615\n",
      "  249/5000: episode: 248, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.346 [0.256, 0.440], loss: 6219809.000000, mean_absolute_error: 504.943329, mean_q: -13.083862\n",
      "  250/5000: episode: 249, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 9292496.000000, mean_absolute_error: 709.562500, mean_q: -13.117499\n",
      "  251/5000: episode: 250, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.125, 0.278], loss: 3172960.000000, mean_absolute_error: 318.928772, mean_q: -13.169640\n",
      "  252/5000: episode: 251, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.695, 0.856], loss: 1622081.000000, mean_absolute_error: 207.321472, mean_q: -13.225428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  253/5000: episode: 252, duration: 0.145s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 4722835.500000, mean_absolute_error: 431.152222, mean_q: -13.281654\n",
      "  254/5000: episode: 253, duration: 0.117s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.072, 0.202], loss: 141890.546875, mean_absolute_error: 142.911255, mean_q: -13.332699\n",
      "  255/5000: episode: 254, duration: 0.152s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 1621277.000000, mean_absolute_error: 207.800690, mean_q: -13.372960\n",
      "  256/5000: episode: 255, duration: 0.133s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.096 [0.045, 0.158], loss: 3157354.750000, mean_absolute_error: 310.409210, mean_q: -13.411963\n",
      "  257/5000: episode: 256, duration: 0.118s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.251, 0.434], loss: 3129203.000000, mean_absolute_error: 292.087006, mean_q: -13.441244\n",
      "  258/5000: episode: 257, duration: 0.120s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.164, 0.329], loss: 127695.382812, mean_absolute_error: 134.230927, mean_q: -13.473472\n",
      "  259/5000: episode: 258, duration: 0.122s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.872, 0.971], loss: 3198457.500000, mean_absolute_error: 339.101624, mean_q: -13.490819\n",
      "  260/5000: episode: 259, duration: 0.142s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.176, 0.345], loss: 3169787.250000, mean_absolute_error: 320.362610, mean_q: -13.516363\n",
      "  261/5000: episode: 260, duration: 0.110s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.823 [0.745, 0.892], loss: 71326.812500, mean_absolute_error: 97.211288, mean_q: -13.546379\n",
      "  262/5000: episode: 261, duration: 0.143s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [0.101, 0.246], loss: 3169533.250000, mean_absolute_error: 320.873596, mean_q: -13.563986\n",
      "  263/5000: episode: 262, duration: 0.110s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [0.068, 0.196], loss: 3183305.000000, mean_absolute_error: 330.223328, mean_q: -13.582064\n",
      "  264/5000: episode: 263, duration: 0.127s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.170, 0.337], loss: 71276.031250, mean_absolute_error: 97.712738, mean_q: -13.594409\n",
      "  265/5000: episode: 264, duration: 0.102s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.996], loss: 4689740.000000, mean_absolute_error: 414.238647, mean_q: -13.603915\n",
      "  266/5000: episode: 265, duration: 0.132s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.695, 0.856], loss: 127388.023438, mean_absolute_error: 135.515747, mean_q: -13.626486\n",
      "  267/5000: episode: 266, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 43173.007812, mean_absolute_error: 79.487297, mean_q: -13.631191\n",
      "  268/5000: episode: 267, duration: 0.146s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.058 [0.019, 0.107], loss: 113190.250000, mean_absolute_error: 126.343643, mean_q: -13.639601\n",
      "  269/5000: episode: 268, duration: 0.121s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.133, 0.289], loss: 84715.242188, mean_absolute_error: 107.640930, mean_q: -13.643143\n",
      "  270/5000: episode: 269, duration: 0.112s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.532, 0.719], loss: 4702139.000000, mean_absolute_error: 424.209412, mean_q: -13.653887\n",
      "  271/5000: episode: 270, duration: 0.120s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.950, 0.999], loss: 1633005.750000, mean_absolute_error: 219.633575, mean_q: -13.666292\n",
      "  272/5000: episode: 271, duration: 0.128s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [0.080, 0.214], loss: 1618911.250000, mean_absolute_error: 210.337570, mean_q: -13.675188\n",
      "  273/5000: episode: 272, duration: 0.118s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 1647200.375000, mean_absolute_error: 229.253860, mean_q: -13.680168\n",
      "  274/5000: episode: 273, duration: 0.115s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.951, 0.999], loss: 1660575.500000, mean_absolute_error: 238.630737, mean_q: -13.692846\n",
      "  275/5000: episode: 274, duration: 0.118s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.075 [0.030, 0.131], loss: 56827.246094, mean_absolute_error: 89.823784, mean_q: -13.699186\n",
      "  276/5000: episode: 275, duration: 0.143s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.890 [0.825, 0.945], loss: 3123990.750000, mean_absolute_error: 294.418579, mean_q: -13.704815\n",
      "  277/5000: episode: 276, duration: 0.124s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.347 [0.257, 0.441], loss: 126774.359375, mean_absolute_error: 136.881897, mean_q: -13.727694\n",
      "  278/5000: episode: 277, duration: 0.126s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.095 [0.044, 0.156], loss: 28332.878906, mean_absolute_error: 71.656677, mean_q: -13.735681\n",
      "  279/5000: episode: 278, duration: 0.113s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 4699022.000000, mean_absolute_error: 425.124329, mean_q: -13.742178\n",
      "  280/5000: episode: 279, duration: 0.129s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.682, 0.845], loss: 4670637.500000, mean_absolute_error: 406.579407, mean_q: -13.751057\n",
      "  281/5000: episode: 280, duration: 0.119s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.927, 0.994], loss: 1589935.750000, mean_absolute_error: 193.006027, mean_q: -13.772955\n",
      "  282/5000: episode: 281, duration: 0.121s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.995], loss: 1631434.250000, mean_absolute_error: 220.976807, mean_q: -13.787977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  283/5000: episode: 282, duration: 0.141s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.789, 0.923], loss: 3164717.000000, mean_absolute_error: 323.358887, mean_q: -13.802805\n",
      "  284/5000: episode: 283, duration: 0.121s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.155, 0.318], loss: 3122598.000000, mean_absolute_error: 295.711639, mean_q: -13.820202\n",
      "  285/5000: episode: 284, duration: 0.130s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 70597.828125, mean_absolute_error: 100.705910, mean_q: -13.836855\n",
      "  286/5000: episode: 285, duration: 0.163s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.929 [0.875, 0.972], loss: 3206173.000000, mean_absolute_error: 351.787323, mean_q: -13.852802\n",
      "  287/5000: episode: 286, duration: 0.113s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.696 [0.605, 0.783], loss: 112699.351562, mean_absolute_error: 128.952454, mean_q: -13.866987\n",
      "  288/5000: episode: 287, duration: 0.102s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 1630822.375000, mean_absolute_error: 221.847198, mean_q: -13.874237\n",
      "  289/5000: episode: 288, duration: 0.128s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.732, 0.883], loss: 4654212.500000, mean_absolute_error: 398.577942, mean_q: -13.881567\n",
      "  290/5000: episode: 289, duration: 0.119s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.905 [0.844, 0.956], loss: 1588424.000000, mean_absolute_error: 194.175827, mean_q: -13.907549\n",
      "  291/5000: episode: 290, duration: 0.127s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 42520.472656, mean_absolute_error: 83.040710, mean_q: -13.925913\n",
      "  292/5000: episode: 291, duration: 0.129s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.579 [0.483, 0.674], loss: 28676.132812, mean_absolute_error: 73.857933, mean_q: -13.935476\n",
      "  293/5000: episode: 292, duration: 0.119s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.133, 0.290], loss: 1644349.125000, mean_absolute_error: 231.913803, mean_q: -13.944599\n",
      "  294/5000: episode: 293, duration: 0.135s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.414], loss: 1629898.750000, mean_absolute_error: 222.552750, mean_q: -13.956461\n",
      "  295/5000: episode: 294, duration: 0.136s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.570 [0.473, 0.665], loss: 6225797.000000, mean_absolute_error: 529.067383, mean_q: -13.957653\n",
      "  296/5000: episode: 295, duration: 0.126s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.052], loss: 1601927.875000, mean_absolute_error: 204.331390, mean_q: -13.979742\n",
      "  297/5000: episode: 296, duration: 0.108s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.555, 0.740], loss: 98276.578125, mean_absolute_error: 120.949913, mean_q: -13.995813\n",
      "  298/5000: episode: 297, duration: 0.130s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.080], loss: 1546176.750000, mean_absolute_error: 167.503540, mean_q: -13.996426\n",
      "  299/5000: episode: 298, duration: 0.119s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.071], loss: 3175372.000000, mean_absolute_error: 334.729126, mean_q: -13.999233\n",
      "  300/5000: episode: 299, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.014, 0.094], loss: 6182012.000000, mean_absolute_error: 501.802765, mean_q: -14.004179\n",
      "  301/5000: episode: 300, duration: 0.101s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 111964.562500, mean_absolute_error: 130.710236, mean_q: -14.024760\n",
      "  302/5000: episode: 301, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.340 [0.250, 0.434], loss: 1587882.125000, mean_absolute_error: 196.121475, mean_q: -14.031320\n",
      "  303/5000: episode: 302, duration: 0.103s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.015], loss: 14769.387695, mean_absolute_error: 66.395760, mean_q: -14.035001\n",
      "  304/5000: episode: 303, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.265, 0.450], loss: 1587293.250000, mean_absolute_error: 196.052765, mean_q: -14.036871\n",
      "  305/5000: episode: 304, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.294 [0.208, 0.384], loss: 4718805.500000, mean_absolute_error: 446.466064, mean_q: -14.037991\n",
      "  306/5000: episode: 305, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.048], loss: 83791.476562, mean_absolute_error: 112.734909, mean_q: -14.044333\n",
      "  307/5000: episode: 306, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.485 [0.389, 0.582], loss: 3159669.750000, mean_absolute_error: 326.214966, mean_q: -14.049331\n",
      "  308/5000: episode: 307, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.501, 0.691], loss: 3117546.000000, mean_absolute_error: 298.509308, mean_q: -14.059050\n",
      "  309/5000: episode: 308, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.328, 0.519], loss: 1559171.250000, mean_absolute_error: 178.304459, mean_q: -14.073236\n",
      "  310/5000: episode: 309, duration: 0.100s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.705, 0.863], loss: 4647734.000000, mean_absolute_error: 400.849762, mean_q: -14.089905\n",
      "  311/5000: episode: 310, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [0.081, 0.217], loss: 42151.382812, mean_absolute_error: 85.765068, mean_q: -14.119789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  312/5000: episode: 311, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.456 [0.360, 0.553], loss: 1586056.500000, mean_absolute_error: 197.052750, mean_q: -14.144098\n",
      "  313/5000: episode: 312, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.151], loss: 1572585.375000, mean_absolute_error: 188.131805, mean_q: -14.169995\n",
      "  314/5000: episode: 313, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.619, 0.794], loss: 1586006.500000, mean_absolute_error: 197.410217, mean_q: -14.191534\n",
      "  315/5000: episode: 314, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.039, 0.147], loss: 3102262.000000, mean_absolute_error: 290.336029, mean_q: -14.208822\n",
      "  316/5000: episode: 315, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.859 [0.788, 0.921], loss: 1613530.125000, mean_absolute_error: 216.042328, mean_q: -14.220570\n",
      "  317/5000: episode: 316, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 1599554.000000, mean_absolute_error: 206.963257, mean_q: -14.228206\n",
      "  318/5000: episode: 317, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.061], loss: 97204.250000, mean_absolute_error: 123.519035, mean_q: -14.240186\n",
      "  319/5000: episode: 318, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.088 [0.039, 0.147], loss: 3198408.000000, mean_absolute_error: 355.295776, mean_q: -14.249886\n",
      "  320/5000: episode: 319, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.452, 0.645], loss: 1599405.750000, mean_absolute_error: 207.450272, mean_q: -14.267690\n",
      "  321/5000: episode: 320, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.024], loss: 1571789.875000, mean_absolute_error: 189.366333, mean_q: -14.290460\n",
      "  322/5000: episode: 321, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.181 [0.111, 0.259], loss: 1612970.875000, mean_absolute_error: 216.899323, mean_q: -14.314329\n",
      "  323/5000: episode: 322, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.112 [0.057, 0.178], loss: 83395.750000, mean_absolute_error: 115.131416, mean_q: -14.338905\n",
      "  324/5000: episode: 323, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.148, 0.309], loss: 1612331.000000, mean_absolute_error: 217.166901, mean_q: -14.359420\n",
      "  325/5000: episode: 324, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.862 [0.791, 0.924], loss: 3169343.500000, mean_absolute_error: 337.720215, mean_q: -14.371704\n",
      "  326/5000: episode: 325, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.132, 0.288], loss: 4670397.000000, mean_absolute_error: 421.352203, mean_q: -14.383833\n",
      "  327/5000: episode: 326, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.261 [0.179, 0.349], loss: 3127412.000000, mean_absolute_error: 310.473297, mean_q: -14.403492\n",
      "  328/5000: episode: 327, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 1571173.000000, mean_absolute_error: 190.514008, mean_q: -14.424744\n",
      "  329/5000: episode: 328, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.119, 0.270], loss: 3154390.250000, mean_absolute_error: 329.023621, mean_q: -14.435629\n",
      "  330/5000: episode: 329, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.815 [0.736, 0.886], loss: 1529081.125000, mean_absolute_error: 163.066772, mean_q: -14.445955\n",
      "  331/5000: episode: 330, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.868, 0.969], loss: 3071431.000000, mean_absolute_error: 274.125793, mean_q: -14.448003\n",
      "  332/5000: episode: 331, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [0.081, 0.215], loss: 1584126.375000, mean_absolute_error: 199.967224, mean_q: -14.463076\n",
      "  333/5000: episode: 332, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.777 [0.693, 0.854], loss: 1611641.250000, mean_absolute_error: 218.467773, mean_q: -14.478031\n",
      "  334/5000: episode: 333, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.135, 0.292], loss: 14745.203125, mean_absolute_error: 71.053749, mean_q: -14.494064\n",
      "  335/5000: episode: 334, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.078], loss: 3098540.750000, mean_absolute_error: 293.217102, mean_q: -14.508797\n",
      "  336/5000: episode: 335, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.733, 0.883], loss: 1583268.500000, mean_absolute_error: 200.434891, mean_q: -14.530270\n",
      "  337/5000: episode: 336, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.597], loss: 1597127.500000, mean_absolute_error: 209.777542, mean_q: -14.558687\n",
      "  338/5000: episode: 337, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.822 [0.744, 0.891], loss: 3124548.500000, mean_absolute_error: 311.735840, mean_q: -14.578974\n",
      "  339/5000: episode: 338, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 1583119.875000, mean_absolute_error: 200.996796, mean_q: -14.599287\n",
      "  340/5000: episode: 339, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 1569226.000000, mean_absolute_error: 191.968262, mean_q: -14.614573\n",
      "  341/5000: episode: 340, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: 1569314.500000, mean_absolute_error: 192.127350, mean_q: -14.632784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  342/5000: episode: 341, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [0.075, 0.207], loss: 3096131.500000, mean_absolute_error: 293.967407, mean_q: -14.651155\n",
      "  343/5000: episode: 342, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.057], loss: 3123676.250000, mean_absolute_error: 312.385376, mean_q: -14.665894\n",
      "  344/5000: episode: 343, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 1596537.250000, mean_absolute_error: 210.802490, mean_q: -14.682914\n",
      "  345/5000: episode: 344, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.640, 0.811], loss: 1610412.000000, mean_absolute_error: 220.177368, mean_q: -14.707548\n",
      "  346/5000: episode: 345, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.127], loss: 3095521.500000, mean_absolute_error: 294.560547, mean_q: -14.724094\n",
      "  347/5000: episode: 346, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.710, 0.867], loss: 69543.679688, mean_absolute_error: 109.440353, mean_q: -14.741068\n",
      "  348/5000: episode: 347, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.996], loss: 69555.687500, mean_absolute_error: 109.640648, mean_q: -14.747807\n",
      "  349/5000: episode: 348, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.731 [0.642, 0.813], loss: 1582496.125000, mean_absolute_error: 202.431030, mean_q: -14.753965\n",
      "  350/5000: episode: 349, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.880 [0.813, 0.937], loss: 1664835.625000, mean_absolute_error: 257.185913, mean_q: -14.761293\n",
      "  351/5000: episode: 350, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 4621382.000000, mean_absolute_error: 396.929108, mean_q: -14.765100\n",
      "  352/5000: episode: 351, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.870 [0.800, 0.930], loss: 96686.968750, mean_absolute_error: 128.187119, mean_q: -14.774878\n",
      "  353/5000: episode: 352, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [0.063, 0.188], loss: 82750.109375, mean_absolute_error: 119.271812, mean_q: -14.783644\n",
      "  354/5000: episode: 353, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.826 [0.749, 0.895], loss: 55486.695312, mean_absolute_error: 101.187637, mean_q: -14.786263\n",
      "  355/5000: episode: 354, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 1568395.125000, mean_absolute_error: 194.000610, mean_q: -14.785518\n",
      "  356/5000: episode: 355, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.108 [0.053, 0.172], loss: 69145.023438, mean_absolute_error: 110.428543, mean_q: -14.780078\n",
      "  357/5000: episode: 356, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.132], loss: 1581040.875000, mean_absolute_error: 203.116394, mean_q: -14.777164\n",
      "  358/5000: episode: 357, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.965, 1.000], loss: 123753.421875, mean_absolute_error: 147.105545, mean_q: -14.783838\n",
      "  359/5000: episode: 358, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.212, 0.389], loss: 82780.382812, mean_absolute_error: 119.865334, mean_q: -14.782133\n",
      "  360/5000: episode: 359, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.855 [0.782, 0.918], loss: 1581253.375000, mean_absolute_error: 203.582275, mean_q: -14.774548\n",
      "  361/5000: episode: 360, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.952, 0.999], loss: 27972.275391, mean_absolute_error: 83.782211, mean_q: -14.772165\n",
      "  362/5000: episode: 361, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.078 [0.032, 0.135], loss: 95954.875000, mean_absolute_error: 129.230988, mean_q: -14.763840\n",
      "  363/5000: episode: 362, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.896, 0.982], loss: 41902.800781, mean_absolute_error: 93.202911, mean_q: -14.747593\n",
      "  364/5000: episode: 363, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.066], loss: 1594402.375000, mean_absolute_error: 212.920471, mean_q: -14.723619\n",
      "  365/5000: episode: 364, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 13789.504883, mean_absolute_error: 74.983757, mean_q: -14.706858\n",
      "  366/5000: episode: 365, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 1635528.500000, mean_absolute_error: 240.400330, mean_q: -14.690802\n",
      "  367/5000: episode: 366, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.081 [0.034, 0.138], loss: 3093083.750000, mean_absolute_error: 296.820435, mean_q: -14.671662\n",
      "  368/5000: episode: 367, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 54756.496094, mean_absolute_error: 102.475418, mean_q: -14.664989\n",
      "  369/5000: episode: 368, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.600 [0.504, 0.694], loss: 3119401.500000, mean_absolute_error: 315.041656, mean_q: -14.667606\n",
      "  370/5000: episode: 369, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.387, 0.581], loss: 109815.632812, mean_absolute_error: 139.075150, mean_q: -14.685708\n",
      "  371/5000: episode: 370, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.894 [0.830, 0.948], loss: 54934.007812, mean_absolute_error: 102.891853, mean_q: -14.694788\n",
      "  372/5000: episode: 371, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.430, 0.623], loss: 1566622.625000, mean_absolute_error: 195.643158, mean_q: -14.699647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  373/5000: episode: 372, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.916 [0.858, 0.964], loss: 1566332.375000, mean_absolute_error: 195.833374, mean_q: -14.705189\n",
      "  374/5000: episode: 373, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 55134.398438, mean_absolute_error: 103.323586, mean_q: -14.715356\n",
      "  375/5000: episode: 374, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.842 [0.767, 0.908], loss: 3091482.250000, mean_absolute_error: 297.781006, mean_q: -14.724226\n",
      "  376/5000: episode: 375, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.933 [0.880, 0.975], loss: 41476.308594, mean_absolute_error: 94.535255, mean_q: -14.732493\n",
      "  377/5000: episode: 376, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.227, 0.407], loss: 1566235.000000, mean_absolute_error: 196.421890, mean_q: -14.730947\n",
      "  378/5000: episode: 377, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 81844.289062, mean_absolute_error: 121.848969, mean_q: -14.742439\n",
      "  379/5000: episode: 378, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.881 [0.814, 0.938], loss: 1579853.875000, mean_absolute_error: 205.642151, mean_q: -14.745974\n",
      "  380/5000: episode: 379, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 1579524.875000, mean_absolute_error: 205.726959, mean_q: -14.747734\n",
      "  381/5000: episode: 380, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 82010.539062, mean_absolute_error: 122.237442, mean_q: -14.761305\n",
      "  382/5000: episode: 381, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.933 [0.880, 0.975], loss: 54770.593750, mean_absolute_error: 104.238678, mean_q: -14.758404\n",
      "  383/5000: episode: 382, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.729, 0.881], loss: 1565145.500000, mean_absolute_error: 196.942200, mean_q: -14.761496\n",
      "  384/5000: episode: 383, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.172 [0.103, 0.249], loss: 1565556.625000, mean_absolute_error: 197.083862, mean_q: -14.758891\n",
      "  385/5000: episode: 384, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.893, 0.981], loss: 68628.984375, mean_absolute_error: 113.771568, mean_q: -14.759022\n",
      "  386/5000: episode: 385, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.941 [0.891, 0.980], loss: 15029.574219, mean_absolute_error: 78.011002, mean_q: -14.760582\n",
      "  387/5000: episode: 386, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 3197786.000000, mean_absolute_error: 371.116394, mean_q: -14.762429\n",
      "  388/5000: episode: 387, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 54688.972656, mean_absolute_error: 104.915649, mean_q: -14.764036\n",
      "  389/5000: episode: 388, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.150, 0.311], loss: 40844.343750, mean_absolute_error: 95.990860, mean_q: -14.759130\n",
      "  390/5000: episode: 389, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.125, 0.279], loss: 41382.652344, mean_absolute_error: 96.277512, mean_q: -14.755766\n",
      "  391/5000: episode: 390, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.919 [0.862, 0.966], loss: 41582.750000, mean_absolute_error: 96.303917, mean_q: -14.759394\n",
      "  392/5000: episode: 391, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [0.099, 0.242], loss: 1537297.750000, mean_absolute_error: 179.876343, mean_q: -14.762938\n",
      "  393/5000: episode: 392, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 3115304.500000, mean_absolute_error: 317.616119, mean_q: -14.768709\n",
      "  394/5000: episode: 393, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.955, 1.000], loss: 3101551.000000, mean_absolute_error: 308.651855, mean_q: -14.775378\n",
      "  395/5000: episode: 394, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.702, 0.860], loss: 6121279.000000, mean_absolute_error: 493.866028, mean_q: -14.787043\n",
      "  396/5000: episode: 395, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.995], loss: 3114465.500000, mean_absolute_error: 317.842163, mean_q: -14.803875\n",
      "  397/5000: episode: 396, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.414, 0.608], loss: 1537146.500000, mean_absolute_error: 180.619507, mean_q: -14.835960\n",
      "  398/5000: episode: 397, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.414], loss: 3100983.500000, mean_absolute_error: 309.287018, mean_q: -14.869400\n",
      "  399/5000: episode: 398, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.294 [0.208, 0.385], loss: 81681.984375, mean_absolute_error: 124.322914, mean_q: -14.901854\n",
      "  400/5000: episode: 399, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.845], loss: 1551181.250000, mean_absolute_error: 190.309601, mean_q: -14.925837\n",
      "  401/5000: episode: 400, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.136, 0.294], loss: 3059632.500000, mean_absolute_error: 282.637726, mean_q: -14.948162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  402/5000: episode: 401, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 1603969.375000, mean_absolute_error: 226.158920, mean_q: -14.978363\n",
      "  403/5000: episode: 402, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.723, 0.876], loss: 1603853.875000, mean_absolute_error: 226.288330, mean_q: -14.998947\n",
      "  404/5000: episode: 403, duration: 0.043s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [0.090, 0.230], loss: 3099377.750000, mean_absolute_error: 309.995392, mean_q: -15.014854\n",
      "  405/5000: episode: 404, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.184, 0.355], loss: 55065.234375, mean_absolute_error: 107.249817, mean_q: -15.031130\n",
      "  406/5000: episode: 405, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.132, 0.288], loss: 4621286.000000, mean_absolute_error: 411.643677, mean_q: -15.036934\n",
      "  407/5000: episode: 406, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 108307.507812, mean_absolute_error: 143.243225, mean_q: -15.053274\n",
      "  408/5000: episode: 407, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.570 [0.474, 0.665], loss: 1563606.750000, mean_absolute_error: 200.196381, mean_q: -15.063224\n",
      "  409/5000: episode: 408, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.501, 0.691], loss: 3125465.500000, mean_absolute_error: 328.564941, mean_q: -15.073202\n",
      "  410/5000: episode: 409, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.328, 0.519], loss: 27138.013672, mean_absolute_error: 89.856598, mean_q: -15.085597\n",
      "  411/5000: episode: 410, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 40939.421875, mean_absolute_error: 99.021584, mean_q: -15.094667\n",
      "  412/5000: episode: 411, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.918 [0.860, 0.965], loss: 81459.437500, mean_absolute_error: 126.012848, mean_q: -15.102844\n",
      "  413/5000: episode: 412, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 1589864.375000, mean_absolute_error: 218.616608, mean_q: -15.111937\n",
      "  414/5000: episode: 413, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.147, 0.307], loss: 1576223.500000, mean_absolute_error: 209.656677, mean_q: -15.116898\n",
      "  415/5000: episode: 414, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.888 [0.823, 0.944], loss: 3111165.500000, mean_absolute_error: 320.232239, mean_q: -15.125364\n",
      "  416/5000: episode: 415, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.958, 1.000], loss: 1548798.375000, mean_absolute_error: 192.099884, mean_q: -15.146185\n",
      "  417/5000: episode: 416, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.894, 0.981], loss: 1603183.250000, mean_absolute_error: 227.957245, mean_q: -15.161324\n",
      "  418/5000: episode: 417, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.934, 0.996], loss: 3109977.000000, mean_absolute_error: 320.476929, mean_q: -15.167278\n",
      "  419/5000: episode: 418, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [0.102, 0.247], loss: 1548823.750000, mean_absolute_error: 192.557159, mean_q: -15.178653\n",
      "  420/5000: episode: 419, duration: 0.108s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.868 [0.799, 0.929], loss: 1575355.000000, mean_absolute_error: 210.377502, mean_q: -15.183859\n",
      "  421/5000: episode: 420, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.212, 0.388], loss: 3096533.500000, mean_absolute_error: 311.965271, mean_q: -15.193096\n",
      "  422/5000: episode: 421, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.281, 0.468], loss: 68076.757812, mean_absolute_error: 118.321716, mean_q: -15.211906\n",
      "  423/5000: episode: 422, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.212, 0.389], loss: 1615813.500000, mean_absolute_error: 237.714218, mean_q: -15.222530\n",
      "  424/5000: episode: 423, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.858 [0.786, 0.920], loss: 1575882.375000, mean_absolute_error: 211.182938, mean_q: -15.234455\n",
      "  425/5000: episode: 424, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.569 [0.473, 0.664], loss: 1561385.750000, mean_absolute_error: 202.208984, mean_q: -15.250417\n",
      "  426/5000: episode: 425, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.281, 0.469], loss: 53939.070312, mean_absolute_error: 109.832291, mean_q: -15.261973\n",
      "  427/5000: episode: 426, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.897 [0.834, 0.950], loss: 27019.794922, mean_absolute_error: 92.106621, mean_q: -15.261825\n",
      "  428/5000: episode: 427, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 1588286.000000, mean_absolute_error: 220.311005, mean_q: -15.256392\n",
      "  429/5000: episode: 428, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.870, 0.970], loss: 27002.486328, mean_absolute_error: 92.321800, mean_q: -15.249317\n",
      "  430/5000: episode: 429, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 1587872.250000, mean_absolute_error: 220.552399, mean_q: -15.239137\n",
      "  431/5000: episode: 430, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.015], loss: 1560928.625000, mean_absolute_error: 202.853104, mean_q: -15.230700\n",
      "  432/5000: episode: 431, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.656, 0.825], loss: 1575233.500000, mean_absolute_error: 212.076202, mean_q: -15.229542\n",
      "  433/5000: episode: 432, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.561, 0.745], loss: 1614447.750000, mean_absolute_error: 238.569412, mean_q: -15.221692\n",
      "  434/5000: episode: 433, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 1560665.500000, mean_absolute_error: 203.068741, mean_q: -15.214598\n",
      "  435/5000: episode: 434, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 13866.308594, mean_absolute_error: 84.174034, mean_q: -15.207667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  436/5000: episode: 435, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.460 [0.364, 0.557], loss: 3080448.500000, mean_absolute_error: 304.622314, mean_q: -15.194572\n",
      "  437/5000: episode: 436, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.515, 0.704], loss: 67453.078125, mean_absolute_error: 119.820221, mean_q: -15.197148\n",
      "  438/5000: episode: 437, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.496 [0.399, 0.593], loss: 4573439.500000, mean_absolute_error: 388.416504, mean_q: -15.193417\n",
      "  439/5000: episode: 438, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 1587408.250000, mean_absolute_error: 221.353577, mean_q: -15.202833\n",
      "  440/5000: episode: 439, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.092 [0.042, 0.152], loss: 3093194.000000, mean_absolute_error: 313.842163, mean_q: -15.212535\n",
      "  441/5000: episode: 440, duration: 0.069s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.995], loss: 40937.804688, mean_absolute_error: 102.621429, mean_q: -15.222655\n",
      "  442/5000: episode: 441, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 1600550.375000, mean_absolute_error: 230.628632, mean_q: -15.222807\n",
      "  443/5000: episode: 442, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.956 [0.911, 0.989], loss: 1587698.375000, mean_absolute_error: 221.940536, mean_q: -15.222612\n",
      "  444/5000: episode: 443, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.179 [0.109, 0.256], loss: 54636.335938, mean_absolute_error: 111.929413, mean_q: -15.220499\n",
      "  445/5000: episode: 444, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.062], loss: 3079073.000000, mean_absolute_error: 305.501984, mean_q: -15.212252\n",
      "  446/5000: episode: 445, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 107755.750000, mean_absolute_error: 147.454620, mean_q: -15.210908\n",
      "  447/5000: episode: 446, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 1573047.000000, mean_absolute_error: 213.357269, mean_q: -15.205072\n",
      "  448/5000: episode: 447, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 1559570.125000, mean_absolute_error: 204.561157, mean_q: -15.206014\n",
      "  449/5000: episode: 448, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.932 [0.878, 0.974], loss: 66929.976562, mean_absolute_error: 121.105362, mean_q: -15.206594\n",
      "  450/5000: episode: 449, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.895 [0.831, 0.948], loss: 1573077.500000, mean_absolute_error: 213.620407, mean_q: -15.203541\n",
      "  451/5000: episode: 450, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.105], loss: 41158.574219, mean_absolute_error: 103.773552, mean_q: -15.206985\n",
      "  452/5000: episode: 451, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.173, 0.341], loss: 3078044.500000, mean_absolute_error: 306.297852, mean_q: -15.206514\n",
      "  453/5000: episode: 452, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.097 [0.045, 0.158], loss: 1572509.750000, mean_absolute_error: 213.963608, mean_q: -15.217032\n",
      "  454/5000: episode: 453, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.061 [0.021, 0.112], loss: 53494.722656, mean_absolute_error: 112.832596, mean_q: -15.229601\n",
      "  455/5000: episode: 454, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.129, 0.284], loss: 67177.015625, mean_absolute_error: 121.884338, mean_q: -15.239494\n",
      "  456/5000: episode: 455, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.385, 0.579], loss: 67153.148438, mean_absolute_error: 122.007385, mean_q: -15.251143\n",
      "  457/5000: episode: 456, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.790, 0.923], loss: 54130.199219, mean_absolute_error: 113.335922, mean_q: -15.257088\n",
      "  458/5000: episode: 457, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.079], loss: 54394.910156, mean_absolute_error: 113.365433, mean_q: -15.262585\n",
      "  459/5000: episode: 458, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.540, 0.726], loss: 80758.515625, mean_absolute_error: 131.214920, mean_q: -15.269883\n",
      "  460/5000: episode: 459, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.181, 0.351], loss: 1586212.375000, mean_absolute_error: 223.849457, mean_q: -15.271707\n",
      "  461/5000: episode: 460, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.616, 0.792], loss: 3102748.250000, mean_absolute_error: 324.876373, mean_q: -15.278124\n",
      "  462/5000: episode: 461, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.063 [0.022, 0.114], loss: 1544670.375000, mean_absolute_error: 197.367264, mean_q: -15.290729\n",
      "  463/5000: episode: 462, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.179], loss: 1544571.125000, mean_absolute_error: 197.495407, mean_q: -15.305033\n",
      "  464/5000: episode: 463, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.032], loss: 53686.527344, mean_absolute_error: 114.109924, mean_q: -15.320272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  465/5000: episode: 464, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.215, 0.393], loss: 40404.359375, mean_absolute_error: 105.497269, mean_q: -15.327045\n",
      "  466/5000: episode: 465, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.928, 0.994], loss: 3088886.750000, mean_absolute_error: 316.681702, mean_q: -15.326324\n",
      "  467/5000: episode: 466, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.158, 0.322], loss: 1571167.750000, mean_absolute_error: 215.558945, mean_q: -15.330346\n",
      "  468/5000: episode: 467, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.612 [0.516, 0.705], loss: 54029.890625, mean_absolute_error: 114.702538, mean_q: -15.341323\n",
      "  469/5000: episode: 468, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.893, 0.981], loss: 3075755.250000, mean_absolute_error: 308.154663, mean_q: -15.341752\n",
      "  470/5000: episode: 469, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.037], loss: 1584532.500000, mean_absolute_error: 224.710114, mean_q: -15.352848\n",
      "  471/5000: episode: 470, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 14105.484375, mean_absolute_error: 88.564651, mean_q: -15.363947\n",
      "  472/5000: episode: 471, duration: 0.108s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.062], loss: 66918.875000, mean_absolute_error: 123.873909, mean_q: -15.369682\n",
      "  473/5000: episode: 472, duration: 0.161s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.600, 0.779], loss: 93842.921875, mean_absolute_error: 141.673340, mean_q: -15.375925\n",
      "  474/5000: episode: 473, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [0.090, 0.229], loss: 40327.664062, mean_absolute_error: 106.502953, mean_q: -15.376776\n",
      "  475/5000: episode: 474, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.151, 0.313], loss: 1570462.000000, mean_absolute_error: 216.431763, mean_q: -15.384430\n",
      "  476/5000: episode: 475, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.405, 0.598], loss: 66853.750000, mean_absolute_error: 124.324463, mean_q: -15.393971\n",
      "  477/5000: episode: 476, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 1556663.625000, mean_absolute_error: 207.875122, mean_q: -15.399092\n",
      "  478/5000: episode: 477, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.285 [0.201, 0.375], loss: 106559.265625, mean_absolute_error: 150.777954, mean_q: -15.395608\n",
      "  479/5000: episode: 478, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.910, 0.988], loss: 1556880.875000, mean_absolute_error: 208.161743, mean_q: -15.392077\n",
      "  480/5000: episode: 479, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.949, 0.999], loss: 28408.083984, mean_absolute_error: 98.517189, mean_q: -15.387930\n",
      "  481/5000: episode: 480, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.279 [0.195, 0.369], loss: 1556721.500000, mean_absolute_error: 208.278427, mean_q: -15.377691\n",
      "  482/5000: episode: 481, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 26612.775391, mean_absolute_error: 98.492950, mean_q: -15.367789\n",
      "  483/5000: episode: 482, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.116, 0.266], loss: 53435.000000, mean_absolute_error: 116.103874, mean_q: -15.356637\n",
      "  484/5000: episode: 483, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.063], loss: 1543705.000000, mean_absolute_error: 199.827271, mean_q: -15.342565\n",
      "  485/5000: episode: 484, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 3059244.250000, mean_absolute_error: 300.850739, mean_q: -15.329746\n",
      "  486/5000: episode: 485, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.950, 0.999], loss: 3085564.500000, mean_absolute_error: 318.473846, mean_q: -15.333433\n",
      "  487/5000: episode: 486, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.790, 0.923], loss: 1555979.000000, mean_absolute_error: 208.786041, mean_q: -15.344959\n",
      "  488/5000: episode: 487, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.931, 0.995], loss: 1543015.125000, mean_absolute_error: 200.154022, mean_q: -15.348463\n",
      "  489/5000: episode: 488, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 26911.773438, mean_absolute_error: 99.226898, mean_q: -15.355837\n",
      "  490/5000: episode: 489, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.138, 0.296], loss: 66218.187500, mean_absolute_error: 125.593277, mean_q: -15.362534\n",
      "  491/5000: episode: 490, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 3085060.500000, mean_absolute_error: 319.021423, mean_q: -15.360868\n",
      "  492/5000: episode: 491, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.281, 0.468], loss: 27711.699219, mean_absolute_error: 99.807526, mean_q: -15.361835\n",
      "  493/5000: episode: 492, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.165, 0.331], loss: 1568681.250000, mean_absolute_error: 218.184128, mean_q: -15.360673\n",
      "  494/5000: episode: 493, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.979, 1.000], loss: 27258.529297, mean_absolute_error: 99.829292, mean_q: -15.354960\n",
      "  495/5000: episode: 494, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 1515745.500000, mean_absolute_error: 183.392944, mean_q: -15.341478\n",
      "  496/5000: episode: 495, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.278, 0.464], loss: 1542468.250000, mean_absolute_error: 200.950836, mean_q: -15.333719\n",
      "  497/5000: episode: 496, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.094, 0.235], loss: 27288.849609, mean_absolute_error: 100.176796, mean_q: -15.331570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  498/5000: episode: 497, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 1541965.625000, mean_absolute_error: 201.128815, mean_q: -15.327841\n",
      "  499/5000: episode: 498, duration: 0.095s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.623, 0.798], loss: 1581861.375000, mean_absolute_error: 227.482773, mean_q: -15.327514\n",
      "  500/5000: episode: 499, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [0.075, 0.207], loss: 92420.539062, mean_absolute_error: 143.953491, mean_q: -15.334781\n",
      "  501/5000: episode: 500, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.479, 0.671], loss: 14078.678711, mean_absolute_error: 91.838058, mean_q: -15.333608\n",
      "  502/5000: episode: 501, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.181, 0.351], loss: 1542051.875000, mean_absolute_error: 201.527557, mean_q: -15.333051\n",
      "  503/5000: episode: 502, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.434, 0.627], loss: 4559452.000000, mean_absolute_error: 394.919617, mean_q: -15.340866\n",
      "  504/5000: episode: 503, duration: 0.110s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.295 [0.210, 0.386], loss: 40414.328125, mean_absolute_error: 109.545578, mean_q: -15.355869\n",
      "  505/5000: episode: 504, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.082 [0.035, 0.140], loss: 14116.368164, mean_absolute_error: 92.334732, mean_q: -15.365561\n",
      "  506/5000: episode: 505, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.927, 0.994], loss: 65975.671875, mean_absolute_error: 127.078026, mean_q: -15.368923\n",
      "  507/5000: episode: 506, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.415, 0.609], loss: 3056173.750000, mean_absolute_error: 302.955017, mean_q: -15.365645\n",
      "  508/5000: episode: 507, duration: 0.118s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.592, 0.771], loss: 1554397.000000, mean_absolute_error: 210.800507, mean_q: -15.376051\n",
      "  509/5000: episode: 508, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.115, 0.265], loss: 3043078.250000, mean_absolute_error: 294.564423, mean_q: -15.393015\n",
      "  510/5000: episode: 509, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.015], loss: 27605.408203, mean_absolute_error: 101.515717, mean_q: -15.415421\n",
      "  511/5000: episode: 510, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 53131.402344, mean_absolute_error: 118.932983, mean_q: -15.431616\n",
      "  512/5000: episode: 511, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.600 [0.504, 0.693], loss: 4570497.000000, mean_absolute_error: 404.654205, mean_q: -15.446102\n",
      "  513/5000: episode: 512, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.940, 0.997], loss: 52732.718750, mean_absolute_error: 119.201912, mean_q: -15.463012\n",
      "  514/5000: episode: 513, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.031], loss: 1568179.750000, mean_absolute_error: 220.477997, mean_q: -15.472738\n",
      "  515/5000: episode: 514, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.914 [0.855, 0.962], loss: 1553701.250000, mean_absolute_error: 211.678268, mean_q: -15.483771\n",
      "  516/5000: episode: 515, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.153, 0.316], loss: 1566755.375000, mean_absolute_error: 220.479828, mean_q: -15.491488\n",
      "  517/5000: episode: 516, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 1528047.875000, mean_absolute_error: 194.631897, mean_q: -15.501663\n",
      "  518/5000: episode: 517, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.970], loss: 1541123.125000, mean_absolute_error: 203.514893, mean_q: -15.517627\n",
      "  519/5000: episode: 518, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.080], loss: 67042.710938, mean_absolute_error: 128.853333, mean_q: -15.536240\n",
      "  520/5000: episode: 519, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.874 [0.805, 0.933], loss: 52648.761719, mean_absolute_error: 120.079544, mean_q: -15.544865\n",
      "  521/5000: episode: 520, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.249, 0.433], loss: 53039.281250, mean_absolute_error: 120.202377, mean_q: -15.543486\n",
      "  522/5000: episode: 521, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.911, 0.988], loss: 3041239.250000, mean_absolute_error: 296.102600, mean_q: -15.540791\n",
      "  523/5000: episode: 522, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.326, 0.517], loss: 544.334900, mean_absolute_error: 85.758942, mean_q: -15.549320\n",
      "  524/5000: episode: 523, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.575 [0.479, 0.670], loss: 66119.195312, mean_absolute_error: 129.219940, mean_q: -15.561512\n",
      "  525/5000: episode: 524, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.889, 0.979], loss: 1527450.500000, mean_absolute_error: 195.654572, mean_q: -15.568409\n",
      "  526/5000: episode: 525, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 39894.531250, mean_absolute_error: 112.158035, mean_q: -15.583273\n",
      "  527/5000: episode: 526, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.750, 0.896], loss: 26344.427734, mean_absolute_error: 103.522934, mean_q: -15.596418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  528/5000: episode: 527, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.907 [0.847, 0.958], loss: 1552955.000000, mean_absolute_error: 213.155426, mean_q: -15.602951\n",
      "  529/5000: episode: 528, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [0.089, 0.228], loss: 52981.195312, mean_absolute_error: 121.170685, mean_q: -15.614174\n",
      "  530/5000: episode: 529, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.476, 0.668], loss: 79187.734375, mean_absolute_error: 138.636353, mean_q: -15.624062\n",
      "  531/5000: episode: 530, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.974, 1.000], loss: 3052014.500000, mean_absolute_error: 305.651855, mean_q: -15.628443\n",
      "  532/5000: episode: 531, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.971, 1.000], loss: 52946.671875, mean_absolute_error: 121.510101, mean_q: -15.635910\n",
      "  533/5000: episode: 532, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.647 [0.553, 0.738], loss: 1578685.250000, mean_absolute_error: 231.019333, mean_q: -15.638360\n",
      "  534/5000: episode: 533, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.454, 0.647], loss: 14460.805664, mean_absolute_error: 95.794838, mean_q: -15.648129\n",
      "  535/5000: episode: 534, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 52470.671875, mean_absolute_error: 121.774780, mean_q: -15.655400\n",
      "  536/5000: episode: 535, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.874, 0.972], loss: 1565361.375000, mean_absolute_error: 222.743561, mean_q: -15.655142\n",
      "  537/5000: episode: 536, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.225, 0.405], loss: 14066.342773, mean_absolute_error: 96.157433, mean_q: -15.657656\n",
      "  538/5000: episode: 537, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.496 [0.400, 0.593], loss: 26690.017578, mean_absolute_error: 104.818008, mean_q: -15.656975\n",
      "  539/5000: episode: 538, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.713, 0.869], loss: 3064267.000000, mean_absolute_error: 315.139160, mean_q: -15.656873\n",
      "  540/5000: episode: 539, duration: 0.092s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.426, 0.619], loss: 1526246.500000, mean_absolute_error: 197.271530, mean_q: -15.660357\n",
      "  541/5000: episode: 540, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.106], loss: 53305.605469, mean_absolute_error: 122.592453, mean_q: -15.662627\n",
      "  542/5000: episode: 541, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.912 [0.853, 0.961], loss: 66315.585938, mean_absolute_error: 131.242981, mean_q: -15.655857\n",
      "  543/5000: episode: 542, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.094 [0.044, 0.155], loss: 3037623.750000, mean_absolute_error: 298.229950, mean_q: -15.644033\n",
      "  544/5000: episode: 543, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.353, 0.545], loss: 52350.984375, mean_absolute_error: 122.681679, mean_q: -15.649662\n",
      "  545/5000: episode: 544, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.139, 0.298], loss: 26695.345703, mean_absolute_error: 105.632011, mean_q: -15.652108\n",
      "  546/5000: episode: 545, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.678 [0.586, 0.766], loss: 13173.258789, mean_absolute_error: 97.007812, mean_q: -15.653996\n",
      "  547/5000: episode: 546, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.137, 0.295], loss: 1551473.875000, mean_absolute_error: 215.208817, mean_q: -15.654565\n",
      "  548/5000: episode: 547, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.529, 0.716], loss: 27557.103516, mean_absolute_error: 106.010010, mean_q: -15.655830\n",
      "  549/5000: episode: 548, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.021], loss: 1538302.125000, mean_absolute_error: 206.748520, mean_q: -15.652689\n",
      "  550/5000: episode: 549, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.255, 0.439], loss: 39246.039062, mean_absolute_error: 114.613182, mean_q: -15.657410\n",
      "  551/5000: episode: 550, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [0.075, 0.208], loss: 39667.574219, mean_absolute_error: 114.720604, mean_q: -15.661917\n",
      "  552/5000: episode: 551, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.347 [0.257, 0.441], loss: 1602830.125000, mean_absolute_error: 250.006592, mean_q: -15.668745\n",
      "  553/5000: episode: 552, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.495, 0.686], loss: 52729.406250, mean_absolute_error: 123.627304, mean_q: -15.671314\n",
      "  554/5000: episode: 553, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.838 [0.763, 0.905], loss: 1538788.500000, mean_absolute_error: 207.228012, mean_q: -15.670211\n",
      "  555/5000: episode: 554, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.657, 0.825], loss: 13150.059570, mean_absolute_error: 97.914093, mean_q: -15.676981\n",
      "  556/5000: episode: 555, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.940, 0.997], loss: 27069.554688, mean_absolute_error: 106.714828, mean_q: -15.678570\n",
      "  557/5000: episode: 556, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.049], loss: 1525623.125000, mean_absolute_error: 199.078293, mean_q: -15.681105\n",
      "  558/5000: episode: 557, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 13573.140625, mean_absolute_error: 98.241394, mean_q: -15.687469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  559/5000: episode: 558, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.521, 0.709], loss: 13606.863281, mean_absolute_error: 98.402634, mean_q: -15.688999\n",
      "  560/5000: episode: 559, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.883 [0.816, 0.940], loss: 65632.906250, mean_absolute_error: 132.831421, mean_q: -15.690887\n",
      "  561/5000: episode: 560, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.750, 0.896], loss: 65180.882812, mean_absolute_error: 132.896439, mean_q: -15.687454\n",
      "  562/5000: episode: 561, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.593 [0.496, 0.687], loss: 1537352.750000, mean_absolute_error: 207.880127, mean_q: -15.678461\n",
      "  563/5000: episode: 562, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 65581.203125, mean_absolute_error: 133.064087, mean_q: -15.677903\n",
      "  564/5000: episode: 563, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.931 [0.878, 0.974], loss: 65129.964844, mean_absolute_error: 133.147522, mean_q: -15.667592\n",
      "  565/5000: episode: 564, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 1523745.125000, mean_absolute_error: 199.543213, mean_q: -15.649351\n",
      "  566/5000: episode: 565, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.632, 0.805], loss: 1524134.500000, mean_absolute_error: 199.637970, mean_q: -15.637962\n",
      "  567/5000: episode: 566, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 39538.398438, mean_absolute_error: 116.222534, mean_q: -15.630522\n",
      "  568/5000: episode: 567, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.196, 0.370], loss: 1536575.250000, mean_absolute_error: 208.378830, mean_q: -15.620758\n",
      "  569/5000: episode: 568, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.656 [0.562, 0.746], loss: 28397.257812, mean_absolute_error: 108.030838, mean_q: -15.615643\n",
      "  570/5000: episode: 569, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.840 [0.765, 0.906], loss: 52524.289062, mean_absolute_error: 125.112846, mean_q: -15.607552\n",
      "  571/5000: episode: 570, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.413, 0.607], loss: 1536896.000000, mean_absolute_error: 208.728745, mean_q: -15.592747\n",
      "  572/5000: episode: 571, duration: 0.065s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.137, 0.295], loss: 3034628.250000, mean_absolute_error: 300.939209, mean_q: -15.578566\n",
      "  573/5000: episode: 572, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.601, 0.779], loss: 1523813.125000, mean_absolute_error: 200.329071, mean_q: -15.576011\n",
      "  574/5000: episode: 573, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 1524641.500000, mean_absolute_error: 200.445419, mean_q: -15.574101\n",
      "  575/5000: episode: 574, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 1549110.625000, mean_absolute_error: 217.545624, mean_q: -15.576008\n",
      "  576/5000: episode: 575, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [0.065, 0.191], loss: 51967.109375, mean_absolute_error: 125.568115, mean_q: -15.579926\n",
      "  577/5000: episode: 576, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.620, 0.795], loss: 1575338.875000, mean_absolute_error: 234.868744, mean_q: -15.575254\n",
      "  578/5000: episode: 577, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.958, 1.000], loss: 4543830.500000, mean_absolute_error: 402.168182, mean_q: -15.565744\n",
      "  579/5000: episode: 578, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.028], loss: 77831.648438, mean_absolute_error: 142.967926, mean_q: -15.576529\n",
      "  580/5000: episode: 579, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.159, 0.323], loss: 1575063.625000, mean_absolute_error: 235.174530, mean_q: -15.586346\n",
      "  581/5000: episode: 580, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.058, 0.181], loss: 1522719.250000, mean_absolute_error: 201.088959, mean_q: -15.598537\n",
      "  582/5000: episode: 581, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.917, 0.990], loss: 78281.234375, mean_absolute_error: 143.464005, mean_q: -15.616879\n",
      "  583/5000: episode: 582, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 1548417.750000, mean_absolute_error: 218.434723, mean_q: -15.619858\n",
      "  584/5000: episode: 583, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.072, 0.202], loss: 39383.351562, mean_absolute_error: 117.962769, mean_q: -15.629725\n",
      "  585/5000: episode: 584, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.294, 0.482], loss: 26952.746094, mean_absolute_error: 109.629257, mean_q: -15.634958\n",
      "  586/5000: episode: 585, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [0.090, 0.229], loss: 25983.589844, mean_absolute_error: 109.634407, mean_q: -15.637208\n",
      "  587/5000: episode: 586, duration: 0.136s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.249, 0.432], loss: 3070681.000000, mean_absolute_error: 327.963074, mean_q: -15.634871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  588/5000: episode: 587, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.956, 1.000], loss: 1573864.750000, mean_absolute_error: 236.014923, mean_q: -15.636710\n",
      "  589/5000: episode: 588, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.914 [0.856, 0.962], loss: 26477.523438, mean_absolute_error: 110.044678, mean_q: -15.633554\n",
      "  590/5000: episode: 589, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.701, 0.860], loss: 1561819.875000, mean_absolute_error: 227.865768, mean_q: -15.625784\n",
      "  591/5000: episode: 590, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.621, 0.797], loss: 39338.578125, mean_absolute_error: 118.713936, mean_q: -15.627443\n",
      "  592/5000: episode: 591, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [0.101, 0.246], loss: 1574479.875000, mean_absolute_error: 236.451050, mean_q: -15.629543\n",
      "  593/5000: episode: 592, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.154, 0.316], loss: 38839.546875, mean_absolute_error: 118.914627, mean_q: -15.635921\n",
      "  594/5000: episode: 593, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.210, 0.386], loss: 26911.197266, mean_absolute_error: 110.603760, mean_q: -15.637668\n",
      "  595/5000: episode: 594, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.632, 0.806], loss: 39819.613281, mean_absolute_error: 119.221222, mean_q: -15.630131\n",
      "  596/5000: episode: 595, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.529, 0.717], loss: 26384.326172, mean_absolute_error: 110.679840, mean_q: -15.620728\n",
      "  597/5000: episode: 596, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.969, 1.000], loss: 3069309.000000, mean_absolute_error: 328.850281, mean_q: -15.604634\n",
      "  598/5000: episode: 597, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.897 [0.834, 0.950], loss: 3043408.250000, mean_absolute_error: 311.908722, mean_q: -15.599437\n",
      "  599/5000: episode: 598, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [0.083, 0.219], loss: 65084.710938, mean_absolute_error: 136.550812, mean_q: -15.604498\n",
      "  600/5000: episode: 599, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 39284.710938, mean_absolute_error: 119.600632, mean_q: -15.599493\n",
      "  601/5000: episode: 600, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.251, 0.434], loss: 1547580.375000, mean_absolute_error: 220.158585, mean_q: -15.593397\n",
      "  602/5000: episode: 601, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 1535111.875000, mean_absolute_error: 211.769302, mean_q: -15.593481\n",
      "  603/5000: episode: 602, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.873, 0.972], loss: 13488.881836, mean_absolute_error: 102.849159, mean_q: -15.592909\n",
      "  604/5000: episode: 603, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.489, 0.679], loss: 1546885.875000, mean_absolute_error: 220.404022, mean_q: -15.588268\n",
      "  605/5000: episode: 604, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.117, 0.268], loss: 1534917.500000, mean_absolute_error: 212.068573, mean_q: -15.594648\n",
      "  606/5000: episode: 605, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.895 [0.832, 0.949], loss: 3028985.000000, mean_absolute_error: 304.144287, mean_q: -15.601922\n",
      "  607/5000: episode: 606, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.939, 0.997], loss: 25849.976562, mean_absolute_error: 111.764603, mean_q: -15.617817\n",
      "  608/5000: episode: 607, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.608, 0.785], loss: 3042454.000000, mean_absolute_error: 312.951416, mean_q: -15.635410\n",
      "  609/5000: episode: 608, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.176, 0.345], loss: 1520682.125000, mean_absolute_error: 204.044144, mean_q: -15.658324\n",
      "  610/5000: episode: 609, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 27372.332031, mean_absolute_error: 112.315262, mean_q: -15.676064\n",
      "  611/5000: episode: 610, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [0.068, 0.196], loss: 1546211.250000, mean_absolute_error: 221.246658, mean_q: -15.685527\n",
      "  612/5000: episode: 611, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 3040687.000000, mean_absolute_error: 313.354065, mean_q: -15.701622\n",
      "  613/5000: episode: 612, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.756 [0.670, 0.836], loss: 3040958.500000, mean_absolute_error: 313.516663, mean_q: -15.729779\n",
      "  614/5000: episode: 613, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.458, 0.650], loss: 26369.548828, mean_absolute_error: 112.850723, mean_q: -15.764566\n",
      "  615/5000: episode: 614, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [0.074, 0.205], loss: 64891.660156, mean_absolute_error: 138.428467, mean_q: -15.799917\n",
      "  616/5000: episode: 615, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.397, 0.591], loss: 38655.562500, mean_absolute_error: 121.545197, mean_q: -15.824203\n",
      "  617/5000: episode: 616, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.151], loss: 125.503540, mean_absolute_error: 96.265022, mean_q: -15.842203\n",
      "  618/5000: episode: 617, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.857, 0.963], loss: 1572108.875000, mean_absolute_error: 239.212433, mean_q: -15.860427\n",
      "  619/5000: episode: 618, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.317, 0.508], loss: 1545806.125000, mean_absolute_error: 222.377823, mean_q: -15.881424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  620/5000: episode: 619, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.560, 0.744], loss: 12959.960938, mean_absolute_error: 105.170212, mean_q: -15.904016\n",
      "  621/5000: episode: 620, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [0.081, 0.217], loss: 39747.242188, mean_absolute_error: 122.432076, mean_q: -15.918148\n",
      "  622/5000: episode: 621, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.094, 0.235], loss: 1519352.750000, mean_absolute_error: 205.796051, mean_q: -15.920113\n",
      "  623/5000: episode: 622, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.933 [0.880, 0.975], loss: 53605.023438, mean_absolute_error: 131.143433, mean_q: -15.922879\n",
      "  624/5000: episode: 623, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.643, 0.814], loss: 13506.325195, mean_absolute_error: 105.690674, mean_q: -15.915121\n",
      "  625/5000: episode: 624, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.659, 0.827], loss: 26277.160156, mean_absolute_error: 114.152618, mean_q: -15.907995\n",
      "  626/5000: episode: 625, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.893, 0.981], loss: 13487.701172, mean_absolute_error: 105.836472, mean_q: -15.901372\n",
      "  627/5000: episode: 626, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 1531880.250000, mean_absolute_error: 214.686813, mean_q: -15.890349\n",
      "  628/5000: episode: 627, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.845], loss: 78129.171875, mean_absolute_error: 148.391159, mean_q: -15.879507\n",
      "  629/5000: episode: 628, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.096 [0.045, 0.157], loss: 3038309.000000, mean_absolute_error: 315.227722, mean_q: -15.874348\n",
      "  630/5000: episode: 629, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.414, 0.608], loss: 3037603.250000, mean_absolute_error: 315.333374, mean_q: -15.886680\n",
      "  631/5000: episode: 630, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.128, 0.283], loss: 26247.835938, mean_absolute_error: 114.749054, mean_q: -15.904087\n",
      "  632/5000: episode: 631, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.845 [0.771, 0.910], loss: 1557610.625000, mean_absolute_error: 232.116623, mean_q: -15.914420\n",
      "  633/5000: episode: 632, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.873, 0.971], loss: 1544184.125000, mean_absolute_error: 223.777557, mean_q: -15.927886\n",
      "  634/5000: episode: 633, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.959, 1.000], loss: 1518502.500000, mean_absolute_error: 207.007278, mean_q: -15.938719\n",
      "  635/5000: episode: 634, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.751, 0.896], loss: 76874.890625, mean_absolute_error: 148.980652, mean_q: -15.952118\n",
      "  636/5000: episode: 635, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [0.078, 0.211], loss: 38492.460938, mean_absolute_error: 123.766731, mean_q: -15.951200\n",
      "  637/5000: episode: 636, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.848 [0.774, 0.912], loss: 1531652.875000, mean_absolute_error: 215.820129, mean_q: -15.944366\n",
      "  638/5000: episode: 637, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.131, 0.286], loss: 1557126.625000, mean_absolute_error: 232.725754, mean_q: -15.944017\n",
      "  639/5000: episode: 638, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.118, 0.269], loss: 1518155.625000, mean_absolute_error: 207.512558, mean_q: -15.948910\n",
      "  640/5000: episode: 639, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 25682.855469, mean_absolute_error: 115.733345, mean_q: -15.956964\n",
      "  641/5000: episode: 640, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.969, 1.000], loss: 1518010.000000, mean_absolute_error: 207.707291, mean_q: -15.959593\n",
      "  642/5000: episode: 641, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.900 [0.838, 0.952], loss: 1556851.250000, mean_absolute_error: 233.173309, mean_q: -15.965981\n",
      "  643/5000: episode: 642, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.114, 0.264], loss: 3048358.250000, mean_absolute_error: 325.046936, mean_q: -15.975553\n",
      "  644/5000: episode: 643, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.650, 0.820], loss: 1517763.000000, mean_absolute_error: 208.034714, mean_q: -15.993129\n",
      "  645/5000: episode: 644, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.078], loss: 1556547.875000, mean_absolute_error: 233.430954, mean_q: -16.006844\n",
      "  646/5000: episode: 645, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 26200.539062, mean_absolute_error: 116.403000, mean_q: -16.024479\n",
      "  647/5000: episode: 646, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 1518054.875000, mean_absolute_error: 208.381561, mean_q: -16.038555\n",
      "  648/5000: episode: 647, duration: 0.148s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.933 [0.881, 0.975], loss: 1568453.000000, mean_absolute_error: 242.152618, mean_q: -16.056467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  649/5000: episode: 648, duration: 0.120s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 25648.962891, mean_absolute_error: 116.790359, mean_q: -16.080334\n",
      "  650/5000: episode: 649, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 39007.546875, mean_absolute_error: 125.436111, mean_q: -16.098686\n",
      "  651/5000: episode: 650, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.627, 0.801], loss: 3021339.250000, mean_absolute_error: 309.124084, mean_q: -16.108555\n",
      "  652/5000: episode: 651, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.387, 0.581], loss: 1529761.375000, mean_absolute_error: 217.421478, mean_q: -16.129610\n",
      "  653/5000: episode: 652, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.018], loss: 39505.820312, mean_absolute_error: 125.757660, mean_q: -16.150146\n",
      "  654/5000: episode: 653, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 14033.153320, mean_absolute_error: 109.141159, mean_q: -16.170174\n",
      "  655/5000: episode: 654, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.217, 0.395], loss: 1516699.625000, mean_absolute_error: 209.424927, mean_q: -16.189831\n",
      "  656/5000: episode: 655, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.355, 0.547], loss: 131.406067, mean_absolute_error: 100.943909, mean_q: -16.210482\n",
      "  657/5000: episode: 656, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.972, 1.000], loss: 25615.697266, mean_absolute_error: 117.864502, mean_q: -16.225744\n",
      "  658/5000: episode: 657, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 13439.476562, mean_absolute_error: 109.617889, mean_q: -16.244438\n",
      "  659/5000: episode: 658, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.879 [0.812, 0.937], loss: 4548660.000000, mean_absolute_error: 427.110474, mean_q: -16.263580\n",
      "  660/5000: episode: 659, duration: 0.146s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.868, 0.969], loss: 3071012.500000, mean_absolute_error: 343.862915, mean_q: -16.294832\n",
      "  661/5000: episode: 660, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.644, 0.815], loss: 26211.046875, mean_absolute_error: 118.544418, mean_q: -16.322346\n",
      "  662/5000: episode: 661, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.852 [0.780, 0.916], loss: 1541402.375000, mean_absolute_error: 227.189972, mean_q: -16.348419\n",
      "  663/5000: episode: 662, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.155, 0.318], loss: 1516407.500000, mean_absolute_error: 210.608780, mean_q: -16.381401\n",
      "  664/5000: episode: 663, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.683, 0.846], loss: 3056694.500000, mean_absolute_error: 336.052856, mean_q: -16.415525\n",
      "  665/5000: episode: 664, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.243, 0.425], loss: 39480.054688, mean_absolute_error: 127.565384, mean_q: -16.454035\n",
      "  666/5000: episode: 665, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [0.066, 0.193], loss: 25578.207031, mean_absolute_error: 119.314323, mean_q: -16.487141\n",
      "  667/5000: episode: 666, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.679 [0.586, 0.767], loss: 89171.343750, mean_absolute_error: 161.375488, mean_q: -16.513021\n",
      "  668/5000: episode: 667, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.270], loss: 38286.320312, mean_absolute_error: 127.978020, mean_q: -16.530912\n",
      "  669/5000: episode: 668, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 50992.546875, mean_absolute_error: 136.497147, mean_q: -16.544901\n",
      "  670/5000: episode: 669, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.659, 0.827], loss: 1528362.625000, mean_absolute_error: 219.991577, mean_q: -16.547413\n",
      "  671/5000: episode: 670, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.938 [0.886, 0.978], loss: 14131.670898, mean_absolute_error: 111.790604, mean_q: -16.558605\n",
      "  672/5000: episode: 671, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.055 [0.018, 0.104], loss: 26139.828125, mean_absolute_error: 120.118423, mean_q: -16.569340\n",
      "  673/5000: episode: 672, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 1540192.875000, mean_absolute_error: 228.687469, mean_q: -16.571781\n",
      "  674/5000: episode: 673, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.666, 0.832], loss: 1514714.500000, mean_absolute_error: 212.046829, mean_q: -16.574018\n",
      "  675/5000: episode: 674, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 1527991.500000, mean_absolute_error: 220.632263, mean_q: -16.581816\n",
      "  676/5000: episode: 675, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.863 [0.793, 0.925], loss: 753.928406, mean_absolute_error: 103.862106, mean_q: -16.590172\n",
      "  677/5000: episode: 676, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.051], loss: 1502449.375000, mean_absolute_error: 204.107285, mean_q: -16.597153\n",
      "  678/5000: episode: 677, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.935, 0.996], loss: 38207.937500, mean_absolute_error: 129.144241, mean_q: -16.613989\n",
      "  679/5000: episode: 678, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 52176.230469, mean_absolute_error: 137.758728, mean_q: -16.628006\n",
      "  680/5000: episode: 679, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.094, 0.235], loss: 26131.560547, mean_absolute_error: 121.047508, mean_q: -16.633615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  681/5000: episode: 680, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.068 [0.026, 0.121], loss: 1540186.625000, mean_absolute_error: 229.624466, mean_q: -16.633669\n",
      "  682/5000: episode: 681, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.055 [0.018, 0.104], loss: 38785.980469, mean_absolute_error: 129.584473, mean_q: -16.637621\n",
      "  683/5000: episode: 682, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 3015185.750000, mean_absolute_error: 313.060303, mean_q: -16.638210\n",
      "  684/5000: episode: 683, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.480, 0.671], loss: 1514540.375000, mean_absolute_error: 213.181702, mean_q: -16.649475\n",
      "  685/5000: episode: 684, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.846 [0.772, 0.911], loss: 1527175.250000, mean_absolute_error: 221.711243, mean_q: -16.656406\n",
      "  686/5000: episode: 685, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 25475.568359, mean_absolute_error: 121.706383, mean_q: -16.668713\n",
      "  687/5000: episode: 686, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [0.066, 0.193], loss: 1564310.125000, mean_absolute_error: 246.881683, mean_q: -16.681025\n",
      "  688/5000: episode: 687, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 1538881.625000, mean_absolute_error: 230.320709, mean_q: -16.691025\n",
      "  689/5000: episode: 688, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.052], loss: 25458.207031, mean_absolute_error: 122.092438, mean_q: -16.696846\n",
      "  690/5000: episode: 689, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.168, 0.335], loss: 51404.429688, mean_absolute_error: 138.915314, mean_q: -16.701817\n",
      "  691/5000: episode: 690, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [0.063, 0.188], loss: 13434.059570, mean_absolute_error: 114.030716, mean_q: -16.709652\n",
      "  692/5000: episode: 691, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.540 [0.443, 0.636], loss: 3016362.250000, mean_absolute_error: 314.440216, mean_q: -16.716545\n",
      "  693/5000: episode: 692, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 1526412.625000, mean_absolute_error: 222.566528, mean_q: -16.729294\n",
      "  694/5000: episode: 693, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.155, 0.318], loss: 38713.156250, mean_absolute_error: 131.034637, mean_q: -16.742466\n",
      "  695/5000: episode: 694, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.272, 0.459], loss: 4527269.000000, mean_absolute_error: 422.799957, mean_q: -16.750042\n",
      "  696/5000: episode: 695, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.952, 0.999], loss: 783.314453, mean_absolute_error: 106.336205, mean_q: -16.776384\n",
      "  697/5000: episode: 696, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.149], loss: 4500754.500000, mean_absolute_error: 406.422546, mean_q: -16.791985\n",
      "  698/5000: episode: 697, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.934, 0.996], loss: 12778.344727, mean_absolute_error: 114.939117, mean_q: -16.822645\n",
      "  699/5000: episode: 698, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 26115.054688, mean_absolute_error: 123.507996, mean_q: -16.847218\n",
      "  700/5000: episode: 699, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.159, 0.324], loss: 14139.540039, mean_absolute_error: 115.334625, mean_q: -16.861828\n",
      "  701/5000: episode: 700, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.463, 0.655], loss: 142.276184, mean_absolute_error: 106.985168, mean_q: -16.867682\n",
      "  702/5000: episode: 701, duration: 0.115s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.405 [0.311, 0.501], loss: 1526254.750000, mean_absolute_error: 223.722122, mean_q: -16.871080\n",
      "  703/5000: episode: 702, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 76567.265625, mean_absolute_error: 157.101654, mean_q: -16.874290\n",
      "  704/5000: episode: 703, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.114, 0.263], loss: 25391.080078, mean_absolute_error: 123.896454, mean_q: -16.869045\n",
      "  705/5000: episode: 704, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.684, 0.847], loss: 1512119.375000, mean_absolute_error: 215.574005, mean_q: -16.858356\n",
      "  706/5000: episode: 705, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [0.060, 0.183], loss: 1512064.875000, mean_absolute_error: 215.642212, mean_q: -16.850710\n",
      "  707/5000: episode: 706, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 1512003.000000, mean_absolute_error: 215.736862, mean_q: -16.849257\n",
      "  708/5000: episode: 707, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.942, 0.998], loss: 37985.566406, mean_absolute_error: 132.527176, mean_q: -16.845722\n",
      "  709/5000: episode: 708, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 14066.654297, mean_absolute_error: 116.015526, mean_q: -16.836700\n",
      "  710/5000: episode: 709, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [0.055, 0.174], loss: 12750.963867, mean_absolute_error: 116.060349, mean_q: -16.822212\n",
      "  711/5000: episode: 710, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 3036109.000000, mean_absolute_error: 332.501770, mean_q: -16.803246\n",
      "  712/5000: episode: 711, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [0.007, 0.075], loss: 13465.244141, mean_absolute_error: 116.288368, mean_q: -16.792295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  713/5000: episode: 712, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 12742.390625, mean_absolute_error: 116.254974, mean_q: -16.777916\n",
      "  714/5000: episode: 713, duration: 0.069s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.136, 0.293], loss: 1524295.500000, mean_absolute_error: 224.494995, mean_q: -16.761766\n",
      "  715/5000: episode: 714, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.912 [0.853, 0.961], loss: 64537.828125, mean_absolute_error: 149.722412, mean_q: -16.756096\n",
      "  716/5000: episode: 715, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.037, 0.144], loss: 1499700.625000, mean_absolute_error: 208.140320, mean_q: -16.745472\n",
      "  717/5000: episode: 716, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.575 [0.478, 0.669], loss: 1575203.250000, mean_absolute_error: 257.946564, mean_q: -16.735794\n",
      "  718/5000: episode: 717, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.510, 0.699], loss: 1524766.875000, mean_absolute_error: 224.855194, mean_q: -16.726677\n",
      "  719/5000: episode: 718, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.006, 0.074], loss: 25310.785156, mean_absolute_error: 125.007286, mean_q: -16.717939\n",
      "  720/5000: episode: 719, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.923 [0.867, 0.969], loss: 12722.303711, mean_absolute_error: 116.797592, mean_q: -16.704742\n",
      "  721/5000: episode: 720, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.606, 0.783], loss: 3009998.250000, mean_absolute_error: 316.577057, mean_q: -16.685364\n",
      "  722/5000: episode: 721, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.582, 0.763], loss: 25294.365234, mean_absolute_error: 125.212791, mean_q: -16.679285\n",
      "  723/5000: episode: 722, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 3048183.500000, mean_absolute_error: 341.574127, mean_q: -16.669651\n",
      "  724/5000: episode: 723, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.024], loss: 37857.417969, mean_absolute_error: 133.677032, mean_q: -16.677608\n",
      "  725/5000: episode: 724, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.105], loss: 1512502.000000, mean_absolute_error: 217.219788, mean_q: -16.685318\n",
      "  726/5000: episode: 725, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.190, 0.362], loss: 139.445679, mean_absolute_error: 109.099556, mean_q: -16.699039\n",
      "  727/5000: episode: 726, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.098], loss: 25272.001953, mean_absolute_error: 125.751572, mean_q: -16.709995\n",
      "  728/5000: episode: 727, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.956, 1.000], loss: 37832.148438, mean_absolute_error: 134.126373, mean_q: -16.716358\n",
      "  729/5000: episode: 728, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 1523330.125000, mean_absolute_error: 225.775391, mean_q: -16.715296\n",
      "  730/5000: episode: 729, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.438 [0.343, 0.535], loss: 13438.879883, mean_absolute_error: 117.889557, mean_q: -16.716854\n",
      "  731/5000: episode: 730, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.421, 0.614], loss: 1535752.375000, mean_absolute_error: 234.220734, mean_q: -16.711704\n",
      "  732/5000: episode: 731, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.271 [0.188, 0.360], loss: 13371.199219, mean_absolute_error: 117.964386, mean_q: -16.705555\n",
      "  733/5000: episode: 732, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.842 [0.768, 0.908], loss: 817.777588, mean_absolute_error: 109.775375, mean_q: -16.693098\n",
      "  734/5000: episode: 733, duration: 0.101s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.731, 0.882], loss: 12687.864258, mean_absolute_error: 118.108047, mean_q: -16.681477\n",
      "  735/5000: episode: 734, duration: 0.126s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 1522989.000000, mean_absolute_error: 226.239471, mean_q: -16.670177\n",
      "  736/5000: episode: 735, duration: 0.158s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [0.097, 0.240], loss: 1522929.875000, mean_absolute_error: 226.319260, mean_q: -16.663797\n",
      "  737/5000: episode: 736, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.957, 1.000], loss: 3033744.000000, mean_absolute_error: 334.453003, mean_q: -16.658354\n",
      "  738/5000: episode: 737, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [0.063, 0.188], loss: 62831.027344, mean_absolute_error: 151.450104, mean_q: -16.658524\n",
      "  739/5000: episode: 738, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.531, 0.718], loss: 62815.242188, mean_absolute_error: 151.534103, mean_q: -16.650160\n",
      "  740/5000: episode: 739, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.199, 0.374], loss: 4505859.500000, mean_absolute_error: 418.096497, mean_q: -16.638020\n",
      "  741/5000: episode: 740, duration: 0.102s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [0.087, 0.225], loss: 1547655.125000, mean_absolute_error: 243.259583, mean_q: -16.644852\n",
      "  742/5000: episode: 741, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.419, 0.612], loss: 27340.511719, mean_absolute_error: 127.231461, mean_q: -16.649284\n",
      "  743/5000: episode: 742, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.905, 0.986], loss: 38468.394531, mean_absolute_error: 135.552750, mean_q: -16.649769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  744/5000: episode: 743, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.166, 0.332], loss: 39117.906250, mean_absolute_error: 135.597900, mean_q: -16.648987\n",
      "  745/5000: episode: 744, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.213, 0.390], loss: 12656.491211, mean_absolute_error: 119.170876, mean_q: -16.643293\n",
      "  746/5000: episode: 745, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.058 [0.020, 0.108], loss: 26692.238281, mean_absolute_error: 127.689590, mean_q: -16.634514\n",
      "  747/5000: episode: 746, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.697, 0.857], loss: 50186.785156, mean_absolute_error: 144.023651, mean_q: -16.623312\n",
      "  748/5000: episode: 747, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.961 [0.919, 0.991], loss: 1522147.875000, mean_absolute_error: 227.389984, mean_q: -16.605865\n",
      "  749/5000: episode: 748, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.098, 0.241], loss: 1511861.000000, mean_absolute_error: 219.486206, mean_q: -16.589622\n",
      "  750/5000: episode: 749, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.254, 0.438], loss: 1509534.875000, mean_absolute_error: 219.326660, mean_q: -16.583416\n",
      "  751/5000: episode: 750, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.317, 0.507], loss: 1522667.375000, mean_absolute_error: 227.650391, mean_q: -16.584265\n",
      "  752/5000: episode: 751, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.572, 0.755], loss: 13342.024414, mean_absolute_error: 119.804337, mean_q: -16.590557\n",
      "  753/5000: episode: 752, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.054 [0.017, 0.102], loss: 25130.941406, mean_absolute_error: 128.112778, mean_q: -16.593681\n",
      "  754/5000: episode: 753, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.868, 0.969], loss: 13346.789062, mean_absolute_error: 120.004745, mean_q: -16.592373\n",
      "  755/5000: episode: 754, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 1547383.125000, mean_absolute_error: 244.456757, mean_q: -16.586067\n",
      "  756/5000: episode: 755, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 25113.019531, mean_absolute_error: 128.364014, mean_q: -16.577858\n",
      "  757/5000: episode: 756, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.101 [0.048, 0.163], loss: 26597.396484, mean_absolute_error: 128.545700, mean_q: -16.564919\n",
      "  758/5000: episode: 757, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [0.087, 0.226], loss: 12619.421875, mean_absolute_error: 120.295021, mean_q: -16.550522\n",
      "  759/5000: episode: 758, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.622, 0.797], loss: 3005415.250000, mean_absolute_error: 319.784790, mean_q: -16.534588\n",
      "  760/5000: episode: 759, duration: 0.043s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.863 [0.792, 0.924], loss: 1497917.125000, mean_absolute_error: 211.967621, mean_q: -16.528345\n",
      "  761/5000: episode: 760, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.594, 0.773], loss: 2992719.000000, mean_absolute_error: 311.730713, mean_q: -16.523535\n",
      "  762/5000: episode: 761, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.148, 0.309], loss: 1547766.500000, mean_absolute_error: 245.054794, mean_q: -16.528355\n",
      "  763/5000: episode: 762, duration: 0.109s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.063], loss: 37550.457031, mean_absolute_error: 137.107758, mean_q: -16.529694\n",
      "  764/5000: episode: 763, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.889, 0.979], loss: 25858.244141, mean_absolute_error: 129.096680, mean_q: -16.524567\n",
      "  765/5000: episode: 764, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 25069.158203, mean_absolute_error: 129.065475, mean_q: -16.512802\n",
      "  766/5000: episode: 765, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.921, 0.992], loss: 49991.539062, mean_absolute_error: 145.509293, mean_q: -16.495720\n",
      "  767/5000: episode: 766, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 135.758087, mean_absolute_error: 112.795776, mean_q: -16.476748\n",
      "  768/5000: episode: 767, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.093], loss: 1496886.000000, mean_absolute_error: 212.640869, mean_q: -16.459511\n",
      "  769/5000: episode: 768, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.292, 0.480], loss: 2992710.500000, mean_absolute_error: 312.279236, mean_q: -16.446129\n",
      "  770/5000: episode: 769, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.120, 0.272], loss: 1508462.875000, mean_absolute_error: 220.860321, mean_q: -16.443336\n",
      "  771/5000: episode: 770, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.785 [0.702, 0.861], loss: 38221.531250, mean_absolute_error: 137.645615, mean_q: -16.443527\n",
      "  772/5000: episode: 771, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 12585.670898, mean_absolute_error: 121.357407, mean_q: -16.441212\n",
      "  773/5000: episode: 772, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 25031.529297, mean_absolute_error: 129.628052, mean_q: -16.437325\n",
      "  774/5000: episode: 773, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 50650.757812, mean_absolute_error: 146.073334, mean_q: -16.431429\n",
      "  775/5000: episode: 774, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [0.085, 0.222], loss: 25022.427734, mean_absolute_error: 129.775482, mean_q: -16.421547\n",
      "  776/5000: episode: 775, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.079], loss: 12576.251953, mean_absolute_error: 121.663216, mean_q: -16.410488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  777/5000: episode: 776, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.296, 0.485], loss: 38980.101562, mean_absolute_error: 138.176971, mean_q: -16.401274\n",
      "  778/5000: episode: 777, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.406, 0.600], loss: 1508035.875000, mean_absolute_error: 221.451462, mean_q: -16.393200\n",
      "  779/5000: episode: 778, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.156, 0.319], loss: 3003370.250000, mean_absolute_error: 321.191467, mean_q: -16.394804\n",
      "  780/5000: episode: 779, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [0.087, 0.226], loss: 13303.198242, mean_absolute_error: 122.027405, mean_q: -16.409239\n",
      "  781/5000: episode: 780, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.424, 0.617], loss: 13367.921875, mean_absolute_error: 122.231552, mean_q: -16.418514\n",
      "  782/5000: episode: 781, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 1496020.500000, mean_absolute_error: 213.717651, mean_q: -16.428497\n",
      "  783/5000: episode: 782, duration: 0.101s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 49845.164062, mean_absolute_error: 146.891968, mean_q: -16.449289\n",
      "  784/5000: episode: 783, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.903, 0.985], loss: 25762.705078, mean_absolute_error: 130.752457, mean_q: -16.467617\n",
      "  785/5000: episode: 784, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 1533032.750000, mean_absolute_error: 238.617126, mean_q: -16.475246\n",
      "  786/5000: episode: 785, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.939 [0.888, 0.979], loss: 37394.496094, mean_absolute_error: 139.088287, mean_q: -16.480225\n",
      "  787/5000: episode: 786, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.867 [0.797, 0.927], loss: 24969.189453, mean_absolute_error: 131.020081, mean_q: -16.475721\n",
      "  788/5000: episode: 787, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.890 [0.825, 0.945], loss: 13365.204102, mean_absolute_error: 123.049004, mean_q: -16.464653\n",
      "  789/5000: episode: 788, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 38184.207031, mean_absolute_error: 139.424957, mean_q: -16.447517\n",
      "  790/5000: episode: 789, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.679, 0.843], loss: 1508796.125000, mean_absolute_error: 222.875900, mean_q: -16.424847\n",
      "  791/5000: episode: 790, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.961, 1.000], loss: 24943.197266, mean_absolute_error: 131.292770, mean_q: -16.406799\n",
      "  792/5000: episode: 791, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.837, 0.952], loss: 12536.377930, mean_absolute_error: 123.200623, mean_q: -16.385263\n",
      "  793/5000: episode: 792, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.455, 0.648], loss: 1545842.375000, mean_absolute_error: 247.401764, mean_q: -16.363743\n",
      "  794/5000: episode: 793, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 24925.503906, mean_absolute_error: 131.473068, mean_q: -16.347927\n",
      "  795/5000: episode: 794, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.130, 0.285], loss: 14136.116211, mean_absolute_error: 123.550339, mean_q: -16.327454\n",
      "  796/5000: episode: 795, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.869, 0.969], loss: 38074.148438, mean_absolute_error: 139.750061, mean_q: -16.302698\n",
      "  797/5000: episode: 796, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 14166.467773, mean_absolute_error: 123.703552, mean_q: -16.278522\n",
      "  798/5000: episode: 797, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.112, 0.261], loss: 1520032.500000, mean_absolute_error: 231.260208, mean_q: -16.256132\n",
      "  799/5000: episode: 798, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.477, 0.668], loss: 49652.910156, mean_absolute_error: 148.015564, mean_q: -16.240856\n",
      "  800/5000: episode: 799, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.346, 0.538], loss: 62018.000000, mean_absolute_error: 156.215790, mean_q: -16.229464\n",
      "  801/5000: episode: 800, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.371 [0.280, 0.467], loss: 37255.140625, mean_absolute_error: 140.048584, mean_q: -16.218750\n",
      "  802/5000: episode: 801, duration: 0.103s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.561 [0.464, 0.656], loss: 1519071.375000, mean_absolute_error: 231.561829, mean_q: -16.201372\n",
      "  803/5000: episode: 802, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.290, 0.478], loss: 1507457.500000, mean_absolute_error: 223.590240, mean_q: -16.186008\n",
      "  804/5000: episode: 803, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 12494.046875, mean_absolute_error: 124.047073, mean_q: -16.172905\n",
      "  805/5000: episode: 804, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.629, 0.803], loss: 1531278.750000, mean_absolute_error: 239.901367, mean_q: -16.155651\n",
      "  806/5000: episode: 805, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.346, 0.538], loss: 1506508.500000, mean_absolute_error: 223.755920, mean_q: -16.145817\n",
      "  807/5000: episode: 806, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.940, 0.997], loss: 3013527.000000, mean_absolute_error: 331.510437, mean_q: -16.140423\n",
      "  808/5000: episode: 807, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.706, 0.863], loss: 4518838.000000, mean_absolute_error: 439.250916, mean_q: -16.143764\n",
      "  809/5000: episode: 808, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.890 [0.825, 0.945], loss: 25648.652344, mean_absolute_error: 132.734604, mean_q: -16.159801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  810/5000: episode: 809, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.545, 0.731], loss: 37170.265625, mean_absolute_error: 140.877075, mean_q: -16.164967\n",
      "  811/5000: episode: 810, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.309, 0.498], loss: 1530803.375000, mean_absolute_error: 240.499390, mean_q: -16.164806\n",
      "  812/5000: episode: 811, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.810 [0.731, 0.882], loss: 37979.093750, mean_absolute_error: 141.159775, mean_q: -16.168396\n",
      "  813/5000: episode: 812, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.269 [0.186, 0.358], loss: 37145.644531, mean_absolute_error: 141.180756, mean_q: -16.164383\n",
      "  814/5000: episode: 813, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.538, 0.724], loss: 24801.251953, mean_absolute_error: 133.154175, mean_q: -16.151623\n",
      "  815/5000: episode: 814, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.888 [0.823, 0.944], loss: 25576.453125, mean_absolute_error: 133.234222, mean_q: -16.134794\n",
      "  816/5000: episode: 815, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 37931.562500, mean_absolute_error: 141.429749, mean_q: -16.112804\n",
      "  817/5000: episode: 816, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.650, 0.820], loss: 1505824.250000, mean_absolute_error: 224.748108, mean_q: -16.087423\n",
      "  818/5000: episode: 817, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 1506569.375000, mean_absolute_error: 224.804138, mean_q: -16.067812\n",
      "  819/5000: episode: 818, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.911, 0.988], loss: 1494214.750000, mean_absolute_error: 216.786179, mean_q: -16.050694\n",
      "  820/5000: episode: 819, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.440 [0.345, 0.537], loss: 1542659.500000, mean_absolute_error: 249.187531, mean_q: -16.036568\n",
      "  821/5000: episode: 820, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.658 [0.564, 0.748], loss: 38641.230469, mean_absolute_error: 141.687622, mean_q: -16.022190\n",
      "  822/5000: episode: 821, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.900, 0.984], loss: 49372.222656, mean_absolute_error: 149.827225, mean_q: -16.002918\n",
      "  823/5000: episode: 822, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 13275.360352, mean_absolute_error: 125.736923, mean_q: -15.976863\n",
      "  824/5000: episode: 823, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.078, 0.212], loss: 12430.585938, mean_absolute_error: 125.704407, mean_q: -15.947412\n",
      "  825/5000: episode: 824, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.685, 0.847], loss: 24729.789062, mean_absolute_error: 133.825211, mean_q: -15.917142\n",
      "  826/5000: episode: 825, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 14079.531250, mean_absolute_error: 125.929146, mean_q: -15.885724\n",
      "  827/5000: episode: 826, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.705, 0.863], loss: 14095.846680, mean_absolute_error: 125.994873, mean_q: -15.852411\n",
      "  828/5000: episode: 827, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.271, 0.457], loss: 24704.798828, mean_absolute_error: 133.929626, mean_q: -15.818140\n",
      "  829/5000: episode: 828, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.114 [0.058, 0.180], loss: 1543160.750000, mean_absolute_error: 249.591034, mean_q: -15.782926\n",
      "  830/5000: episode: 829, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 1518596.000000, mean_absolute_error: 233.572693, mean_q: -15.757665\n",
      "  831/5000: episode: 830, duration: 0.115s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.468 [0.372, 0.565], loss: 12404.794922, mean_absolute_error: 126.020599, mean_q: -15.744793\n",
      "  832/5000: episode: 831, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [0.007, 0.075], loss: 1542209.125000, mean_absolute_error: 249.740204, mean_q: -15.734051\n",
      "  833/5000: episode: 832, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [0.060, 0.183], loss: 1506170.375000, mean_absolute_error: 225.773239, mean_q: -15.728067\n",
      "  834/5000: episode: 833, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.244, 0.426], loss: 37731.000000, mean_absolute_error: 142.388428, mean_q: -15.723972\n",
      "  835/5000: episode: 834, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.632, 0.805], loss: 50031.039062, mean_absolute_error: 150.573456, mean_q: -15.717066\n",
      "  836/5000: episode: 835, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.678, 0.842], loss: 123.460869, mean_absolute_error: 118.423569, mean_q: -15.712743\n",
      "  837/5000: episode: 836, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.463, 0.655], loss: 1811.867432, mean_absolute_error: 118.675949, mean_q: -15.711388\n",
      "  838/5000: episode: 837, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 1517280.375000, mean_absolute_error: 234.150360, mean_q: -15.703440\n",
      "  839/5000: episode: 838, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.530, 0.717], loss: 931.571960, mean_absolute_error: 118.711830, mean_q: -15.696106\n",
      "  840/5000: episode: 839, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.835 [0.760, 0.903], loss: 38558.500000, mean_absolute_error: 142.978424, mean_q: -15.682180\n",
      "  841/5000: episode: 840, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.249, 0.432], loss: 36883.820312, mean_absolute_error: 142.937012, mean_q: -15.662485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  842/5000: episode: 841, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.790, 0.923], loss: 3011269.000000, mean_absolute_error: 334.023193, mean_q: -15.642364\n",
      "  843/5000: episode: 842, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.470, 0.662], loss: 1505629.000000, mean_absolute_error: 226.544495, mean_q: -15.638052\n",
      "  844/5000: episode: 843, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [0.065, 0.190], loss: 1492468.875000, mean_absolute_error: 218.524689, mean_q: -15.633444\n",
      "  845/5000: episode: 844, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [0.072, 0.203], loss: 25419.021484, mean_absolute_error: 135.272705, mean_q: -15.643175\n",
      "  846/5000: episode: 845, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.145, 0.306], loss: 36845.148438, mean_absolute_error: 143.417328, mean_q: -15.658061\n",
      "  847/5000: episode: 846, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.652, 0.821], loss: 1516685.750000, mean_absolute_error: 234.934235, mean_q: -15.669810\n",
      "  848/5000: episode: 847, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.249, 0.432], loss: 37641.425781, mean_absolute_error: 143.662094, mean_q: -15.682637\n",
      "  849/5000: episode: 848, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 36824.718750, mean_absolute_error: 143.771149, mean_q: -15.689195\n",
      "  850/5000: episode: 849, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.126, 0.281], loss: 1491970.625000, mean_absolute_error: 219.238159, mean_q: -15.699821\n",
      "  851/5000: episode: 850, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 36812.917969, mean_absolute_error: 144.011108, mean_q: -15.715679\n",
      "  852/5000: episode: 851, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.209 [0.134, 0.291], loss: 1516245.875000, mean_absolute_error: 235.496857, mean_q: -15.721964\n",
      "  853/5000: episode: 852, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.479 [0.383, 0.576], loss: 13205.577148, mean_absolute_error: 128.249359, mean_q: -15.725301\n",
      "  854/5000: episode: 853, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.389, 0.583], loss: 36791.605469, mean_absolute_error: 144.318054, mean_q: -15.724483\n",
      "  855/5000: episode: 854, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.953 [0.907, 0.987], loss: 1503814.750000, mean_absolute_error: 227.764984, mean_q: -15.719044\n",
      "  856/5000: episode: 855, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 12340.867188, mean_absolute_error: 128.489456, mean_q: -15.719184\n",
      "  857/5000: episode: 856, duration: 0.104s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [0.096, 0.238], loss: 51626.902344, mean_absolute_error: 152.840698, mean_q: -15.716866\n",
      "  858/5000: episode: 857, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.495, 0.686], loss: 15727.352539, mean_absolute_error: 128.783722, mean_q: -15.705691\n",
      "  859/5000: episode: 858, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 13159.477539, mean_absolute_error: 128.728424, mean_q: -15.689154\n",
      "  860/5000: episode: 859, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.515, 0.703], loss: 13169.005859, mean_absolute_error: 128.802246, mean_q: -15.666874\n",
      "  861/5000: episode: 860, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.919 [0.862, 0.966], loss: 12325.586914, mean_absolute_error: 128.837280, mean_q: -15.641520\n",
      "  862/5000: episode: 861, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.248, 0.431], loss: 121.913391, mean_absolute_error: 120.883514, mean_q: -15.613954\n",
      "  863/5000: episode: 862, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.902 [0.840, 0.954], loss: 24517.253906, mean_absolute_error: 136.902817, mean_q: -15.585878\n",
      "  864/5000: episode: 863, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.122, 0.275], loss: 1527880.875000, mean_absolute_error: 244.278519, mean_q: -15.556810\n",
      "  865/5000: episode: 864, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.543, 0.729], loss: 1505218.750000, mean_absolute_error: 228.465805, mean_q: -15.531920\n",
      "  866/5000: episode: 865, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 14025.652344, mean_absolute_error: 129.108719, mean_q: -15.512348\n",
      "  867/5000: episode: 866, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.931, 0.995], loss: 4510895.000000, mean_absolute_error: 443.210022, mean_q: -15.490641\n",
      "  868/5000: episode: 867, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.100], loss: 36677.519531, mean_absolute_error: 145.132446, mean_q: -15.487112\n",
      "  869/5000: episode: 868, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.204, 0.380], loss: 1505053.000000, mean_absolute_error: 228.705017, mean_q: -15.476501\n",
      "  870/5000: episode: 869, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.081 [0.035, 0.139], loss: 1515424.500000, mean_absolute_error: 236.651642, mean_q: -15.471781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  871/5000: episode: 870, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.947, 0.999], loss: 37564.519531, mean_absolute_error: 145.507355, mean_q: -15.479587\n",
      "  872/5000: episode: 871, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.360 [0.269, 0.454], loss: 12295.750977, mean_absolute_error: 129.566132, mean_q: -15.487530\n",
      "  873/5000: episode: 872, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.629, 0.803], loss: 1528257.875000, mean_absolute_error: 245.059952, mean_q: -15.490598\n",
      "  874/5000: episode: 873, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.239, 0.421], loss: 12289.970703, mean_absolute_error: 129.808228, mean_q: -15.495430\n",
      "  875/5000: episode: 874, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 1527168.875000, mean_absolute_error: 245.173309, mean_q: -15.499073\n",
      "  876/5000: episode: 875, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [0.101, 0.246], loss: 1528914.000000, mean_absolute_error: 245.488968, mean_q: -15.506430\n",
      "  877/5000: episode: 876, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 12281.935547, mean_absolute_error: 130.167999, mean_q: -15.512595\n",
      "  878/5000: episode: 877, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.623 [0.528, 0.715], loss: 26261.472656, mean_absolute_error: 138.398972, mean_q: -15.510071\n",
      "  879/5000: episode: 878, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.406 [0.312, 0.502], loss: 48745.523438, mean_absolute_error: 154.232910, mean_q: -15.501135\n",
      "  880/5000: episode: 879, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.310, 0.500], loss: 13163.076172, mean_absolute_error: 130.470673, mean_q: -15.484625\n",
      "  881/5000: episode: 880, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 1503348.250000, mean_absolute_error: 229.797287, mean_q: -15.464589\n",
      "  882/5000: episode: 881, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 48710.992188, mean_absolute_error: 154.405731, mean_q: -15.448963\n",
      "  883/5000: episode: 882, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.135, 0.293], loss: 1919.341553, mean_absolute_error: 122.772598, mean_q: -15.427430\n",
      "  884/5000: episode: 883, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 1514511.000000, mean_absolute_error: 237.890686, mean_q: -15.407527\n",
      "  885/5000: episode: 884, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.068 [0.026, 0.122], loss: 1514462.125000, mean_absolute_error: 237.959442, mean_q: -15.395475\n",
      "  886/5000: episode: 885, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 27105.304688, mean_absolute_error: 138.953064, mean_q: -15.385275\n",
      "  887/5000: episode: 886, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.024], loss: 1515237.250000, mean_absolute_error: 238.134308, mean_q: -15.373631\n",
      "  888/5000: episode: 887, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.951, 0.999], loss: 1037.925781, mean_absolute_error: 123.122147, mean_q: -15.371338\n",
      "  889/5000: episode: 888, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.474, 0.666], loss: 1502091.250000, mean_absolute_error: 230.347977, mean_q: -15.370499\n",
      "  890/5000: episode: 889, duration: 0.127s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.213, 0.390], loss: 25247.000000, mean_absolute_error: 139.139832, mean_q: -15.373471\n",
      "  891/5000: episode: 890, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.916 [0.857, 0.963], loss: 24367.277344, mean_absolute_error: 139.210327, mean_q: -15.369980\n",
      "  892/5000: episode: 891, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.413], loss: 1514032.500000, mean_absolute_error: 238.533844, mean_q: -15.359214\n",
      "  893/5000: episode: 892, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.392, 0.586], loss: 2991463.000000, mean_absolute_error: 329.934265, mean_q: -15.355865\n",
      "  894/5000: episode: 893, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.593, 0.773], loss: 36472.367188, mean_absolute_error: 147.412628, mean_q: -15.365605\n",
      "  895/5000: episode: 894, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.289, 0.477], loss: 37336.097656, mean_absolute_error: 147.503540, mean_q: -15.365793\n",
      "  896/5000: episode: 895, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.556, 0.740], loss: 1537989.750000, mean_absolute_error: 254.746262, mean_q: -15.357824\n",
      "  897/5000: episode: 896, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.387, 0.580], loss: 48562.476562, mean_absolute_error: 155.598770, mean_q: -15.353777\n",
      "  898/5000: episode: 897, duration: 0.102s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.173, 0.341], loss: 13160.375977, mean_absolute_error: 131.993088, mean_q: -15.345146\n",
      "  899/5000: episode: 898, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.887 [0.822, 0.943], loss: 13104.141602, mean_absolute_error: 132.002228, mean_q: -15.330770\n",
      "  900/5000: episode: 899, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.692, 0.853], loss: 24319.880859, mean_absolute_error: 139.988281, mean_q: -15.315233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  901/5000: episode: 900, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.940, 0.997], loss: 13167.080078, mean_absolute_error: 132.242477, mean_q: -15.298957\n",
      "  902/5000: episode: 901, duration: 0.115s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.707, 0.864], loss: 1489258.750000, mean_absolute_error: 223.507065, mean_q: -15.282608\n",
      "  903/5000: episode: 902, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 25194.578125, mean_absolute_error: 140.219177, mean_q: -15.274382\n",
      "  904/5000: episode: 903, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.006, 0.074], loss: 12206.783203, mean_absolute_error: 132.360611, mean_q: -15.260695\n",
      "  905/5000: episode: 904, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [0.055, 0.175], loss: 26168.099609, mean_absolute_error: 140.486511, mean_q: -15.243050\n",
      "  906/5000: episode: 905, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 25207.542969, mean_absolute_error: 140.439087, mean_q: -15.224437\n",
      "  907/5000: episode: 906, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.127], loss: 14036.311523, mean_absolute_error: 132.632355, mean_q: -15.205441\n",
      "  908/5000: episode: 907, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.039], loss: 14114.254883, mean_absolute_error: 132.789795, mean_q: -15.183730\n",
      "  909/5000: episode: 908, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 14060.598633, mean_absolute_error: 132.752304, mean_q: -15.157682\n",
      "  910/5000: episode: 909, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 38154.339844, mean_absolute_error: 148.515518, mean_q: -15.130592\n",
      "  911/5000: episode: 910, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 4467722.000000, mean_absolute_error: 422.488464, mean_q: -15.100223\n",
      "  912/5000: episode: 911, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.095 [0.044, 0.156], loss: 1501019.250000, mean_absolute_error: 231.955414, mean_q: -15.092192\n",
      "  913/5000: episode: 912, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.959], loss: 3014843.000000, mean_absolute_error: 347.111786, mean_q: -15.087000\n",
      "  914/5000: episode: 913, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.092], loss: 25185.632812, mean_absolute_error: 140.904633, mean_q: -15.090455\n",
      "  915/5000: episode: 914, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.120, 0.273], loss: 36304.710938, mean_absolute_error: 148.811020, mean_q: -15.084199\n",
      "  916/5000: episode: 915, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.208, 0.383], loss: 24236.205078, mean_absolute_error: 141.002686, mean_q: -15.074981\n",
      "  917/5000: episode: 916, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [0.077, 0.209], loss: 1537803.500000, mean_absolute_error: 256.038788, mean_q: -15.063612\n",
      "  918/5000: episode: 917, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.901 [0.839, 0.953], loss: 26096.494141, mean_absolute_error: 141.282852, mean_q: -15.053330\n",
      "  919/5000: episode: 918, duration: 0.028s, episode steps: 1, steps per second: 36, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.879 [0.811, 0.937], loss: 3003932.250000, mean_absolute_error: 339.733459, mean_q: -15.041847\n",
      "  920/5000: episode: 919, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 1488503.875000, mean_absolute_error: 224.753738, mean_q: -15.043077\n",
      "  921/5000: episode: 920, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.062], loss: 27083.605469, mean_absolute_error: 141.711777, mean_q: -15.045777\n",
      "  922/5000: episode: 921, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 1512455.125000, mean_absolute_error: 240.717560, mean_q: -15.043478\n",
      "  923/5000: episode: 922, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.735, 0.885], loss: 1028.562378, mean_absolute_error: 125.943741, mean_q: -15.049487\n",
      "  924/5000: episode: 923, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 12151.650391, mean_absolute_error: 133.907349, mean_q: -15.051800\n",
      "  925/5000: episode: 924, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.261 [0.179, 0.349], loss: 25128.792969, mean_absolute_error: 141.924850, mean_q: -15.045719\n",
      "  926/5000: episode: 925, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.384, 0.577], loss: 1513165.500000, mean_absolute_error: 241.180405, mean_q: -15.031427\n",
      "  927/5000: episode: 926, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.333 [0.244, 0.426], loss: 24172.785156, mean_absolute_error: 142.025330, mean_q: -15.022754\n",
      "  928/5000: episode: 927, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.679 [0.586, 0.767], loss: 1525094.000000, mean_absolute_error: 249.184555, mean_q: -15.005708\n",
      "  929/5000: episode: 928, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.580, 0.762], loss: 12136.076172, mean_absolute_error: 134.308136, mean_q: -14.991308\n",
      "  930/5000: episode: 929, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.843 [0.768, 0.908], loss: 1512906.500000, mean_absolute_error: 241.375687, mean_q: -14.971259\n",
      "  931/5000: episode: 930, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.610, 0.787], loss: 12128.584961, mean_absolute_error: 134.456390, mean_q: -14.953789\n",
      "  932/5000: episode: 931, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.498, 0.688], loss: 1949.307983, mean_absolute_error: 126.693451, mean_q: -14.933029\n",
      "  933/5000: episode: 932, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.161, 0.326], loss: 3011622.500000, mean_absolute_error: 348.523651, mean_q: -14.907588\n",
      "  934/5000: episode: 933, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.987], loss: 25114.619141, mean_absolute_error: 142.629944, mean_q: -14.899195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  935/5000: episode: 934, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.667, 0.833], loss: 110.841827, mean_absolute_error: 126.920059, mean_q: -14.888044\n",
      "  936/5000: episode: 935, duration: 0.069s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.947, 0.999], loss: 36114.570312, mean_absolute_error: 150.525803, mean_q: -14.877190\n",
      "  937/5000: episode: 936, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.206 [0.131, 0.287], loss: 48104.082031, mean_absolute_error: 158.446747, mean_q: -14.866552\n",
      "  938/5000: episode: 937, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 1499603.875000, mean_absolute_error: 234.101685, mean_q: -14.853363\n",
      "  939/5000: episode: 938, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.971, 1.000], loss: 110.236053, mean_absolute_error: 127.242355, mean_q: -14.847302\n",
      "  940/5000: episode: 939, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [0.086, 0.223], loss: 1512404.750000, mean_absolute_error: 242.096909, mean_q: -14.840509\n",
      "  941/5000: episode: 940, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.368 [0.276, 0.463], loss: 25088.671875, mean_absolute_error: 143.210068, mean_q: -14.847062\n",
      "  942/5000: episode: 941, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.053], loss: 1512317.000000, mean_absolute_error: 242.414154, mean_q: -14.857442\n",
      "  943/5000: episode: 942, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.517, 0.706], loss: 1499232.500000, mean_absolute_error: 234.597992, mean_q: -14.873972\n",
      "  944/5000: episode: 943, duration: 0.137s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.030], loss: 48042.394531, mean_absolute_error: 159.145294, mean_q: -14.893591\n",
      "  945/5000: episode: 944, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 24072.173828, mean_absolute_error: 143.608063, mean_q: -14.904416\n",
      "  946/5000: episode: 945, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.309, 0.499], loss: 1536841.250000, mean_absolute_error: 258.563538, mean_q: -14.917107\n",
      "  947/5000: episode: 946, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.097 [0.046, 0.159], loss: 36037.906250, mean_absolute_error: 151.714600, mean_q: -14.938320\n",
      "  948/5000: episode: 947, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.821 [0.743, 0.891], loss: 2987372.750000, mean_absolute_error: 334.410522, mean_q: -14.951284\n",
      "  949/5000: episode: 948, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 12082.350586, mean_absolute_error: 136.346283, mean_q: -14.970509\n",
      "  950/5000: episode: 949, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.925, 0.993], loss: 24048.382812, mean_absolute_error: 144.287598, mean_q: -14.978979\n",
      "  951/5000: episode: 950, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.061], loss: 1500510.375000, mean_absolute_error: 235.766144, mean_q: -14.977601\n",
      "  952/5000: episode: 951, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.092 [0.042, 0.153], loss: 1502388.625000, mean_absolute_error: 235.934708, mean_q: -14.979771\n",
      "  953/5000: episode: 952, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.175, 0.343], loss: 1523292.750000, mean_absolute_error: 251.528641, mean_q: -14.985012\n",
      "  954/5000: episode: 953, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 12070.693359, mean_absolute_error: 136.907135, mean_q: -14.996874\n",
      "  955/5000: episode: 954, duration: 0.101s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.041], loss: 37009.640625, mean_absolute_error: 152.747070, mean_q: -15.002592\n",
      "  956/5000: episode: 955, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 25036.482422, mean_absolute_error: 145.015076, mean_q: -14.997652\n",
      "  957/5000: episode: 956, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 24011.873047, mean_absolute_error: 145.010468, mean_q: -14.986740\n",
      "  958/5000: episode: 957, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.099 [0.047, 0.161], loss: 12059.142578, mean_absolute_error: 137.274094, mean_q: -14.971054\n",
      "  959/5000: episode: 958, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.046 [0.012, 0.091], loss: 24959.074219, mean_absolute_error: 145.129486, mean_q: -14.951109\n",
      "  960/5000: episode: 959, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.832 [0.755, 0.900], loss: 1510878.625000, mean_absolute_error: 244.203979, mean_q: -14.926046\n",
      "  961/5000: episode: 960, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.517 [0.420, 0.613], loss: 1497920.125000, mean_absolute_error: 236.442505, mean_q: -14.905555\n",
      "  962/5000: episode: 961, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.895 [0.831, 0.948], loss: 110.854103, mean_absolute_error: 129.722824, mean_q: -14.888868\n",
      "  963/5000: episode: 962, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.305, 0.495], loss: 110.758842, mean_absolute_error: 129.813889, mean_q: -14.882469\n",
      "  964/5000: episode: 963, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.441, 0.634], loss: 23971.812500, mean_absolute_error: 145.499512, mean_q: -14.886251\n",
      "  965/5000: episode: 964, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.061], loss: 23967.593750, mean_absolute_error: 145.601135, mean_q: -14.890160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  966/5000: episode: 965, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 13004.509766, mean_absolute_error: 137.906616, mean_q: -14.890355\n",
      "  967/5000: episode: 966, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.878 [0.810, 0.936], loss: 14107.073242, mean_absolute_error: 138.164520, mean_q: -14.883096\n",
      "  968/5000: episode: 967, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.715, 0.871], loss: 1497508.375000, mean_absolute_error: 237.023529, mean_q: -14.869637\n",
      "  969/5000: episode: 968, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.710, 0.866], loss: 14049.084961, mean_absolute_error: 138.227905, mean_q: -14.859760\n",
      "  970/5000: episode: 969, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.354, 0.546], loss: 1122.987305, mean_absolute_error: 130.464935, mean_q: -14.845920\n",
      "  971/5000: episode: 970, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.611, 0.788], loss: 4482110.500000, mean_absolute_error: 443.108643, mean_q: -14.832628\n",
      "  972/5000: episode: 971, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.957, 1.000], loss: 1499378.625000, mean_absolute_error: 237.479080, mean_q: -14.842262\n",
      "  973/5000: episode: 972, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.216, 0.393], loss: 1497222.375000, mean_absolute_error: 237.410233, mean_q: -14.851556\n",
      "  974/5000: episode: 973, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.850 [0.776, 0.914], loss: 12999.831055, mean_absolute_error: 138.567169, mean_q: -14.860786\n",
      "  975/5000: episode: 974, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.443 [0.348, 0.540], loss: 14038.311523, mean_absolute_error: 138.728394, mean_q: -14.860701\n",
      "  976/5000: episode: 975, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.929 [0.875, 0.973], loss: 1545722.875000, mean_absolute_error: 268.851501, mean_q: -14.853020\n",
      "  977/5000: episode: 976, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.663, 0.831], loss: 12014.850586, mean_absolute_error: 138.815613, mean_q: -14.850096\n",
      "  978/5000: episode: 977, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 12012.345703, mean_absolute_error: 138.900620, mean_q: -14.843960\n",
      "  979/5000: episode: 978, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.840 [0.765, 0.906], loss: 1154.055420, mean_absolute_error: 131.292313, mean_q: -14.836279\n",
      "  980/5000: episode: 979, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.619, 0.795], loss: 12007.493164, mean_absolute_error: 139.056610, mean_q: -14.827288\n",
      "  981/5000: episode: 980, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.121, 0.273], loss: 1162.411377, mean_absolute_error: 131.438080, mean_q: -14.813202\n",
      "  982/5000: episode: 981, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.190, 0.362], loss: 23896.187500, mean_absolute_error: 146.931824, mean_q: -14.795685\n",
      "  983/5000: episode: 982, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.582, 0.764], loss: 1520491.625000, mean_absolute_error: 253.657516, mean_q: -14.777877\n",
      "  984/5000: episode: 983, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.880 [0.812, 0.937], loss: 11997.884766, mean_absolute_error: 139.293335, mean_q: -14.766918\n",
      "  985/5000: episode: 984, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.551, 0.736], loss: 2994180.500000, mean_absolute_error: 345.041992, mean_q: -14.756144\n",
      "  986/5000: episode: 985, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.006, 0.073], loss: 37852.105469, mean_absolute_error: 155.110428, mean_q: -14.761074\n",
      "  987/5000: episode: 986, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 2183.453125, mean_absolute_error: 131.905182, mean_q: -14.756346\n",
      "  988/5000: episode: 987, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.665, 0.832], loss: 47632.015625, mean_absolute_error: 162.887100, mean_q: -14.750867\n",
      "  989/5000: episode: 988, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [0.072, 0.203], loss: 24945.056641, mean_absolute_error: 147.581848, mean_q: -14.750370\n",
      "  990/5000: episode: 989, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.505, 0.694], loss: 3017276.500000, mean_absolute_error: 360.915527, mean_q: -14.749733\n",
      "  991/5000: episode: 990, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.699, 0.858], loss: 23855.646484, mean_absolute_error: 147.692291, mean_q: -14.757638\n",
      "  992/5000: episode: 991, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.852 [0.779, 0.916], loss: 1113.005493, mean_absolute_error: 132.309326, mean_q: -14.759977\n",
      "  993/5000: episode: 992, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.143, 0.302], loss: 36733.148438, mean_absolute_error: 155.659790, mean_q: -14.758266\n",
      "  994/5000: episode: 993, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.426, 0.620], loss: 27947.308594, mean_absolute_error: 148.086578, mean_q: -14.746937\n",
      "  995/5000: episode: 994, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.036, 0.141], loss: 11970.249023, mean_absolute_error: 140.325607, mean_q: -14.733856\n",
      "  996/5000: episode: 995, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.098, 0.241], loss: 11966.816406, mean_absolute_error: 140.405273, mean_q: -14.719138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  997/5000: episode: 996, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.375, 0.568], loss: 1508750.625000, mean_absolute_error: 247.097153, mean_q: -14.705167\n",
      "  998/5000: episode: 997, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.979, 1.000], loss: 1520557.250000, mean_absolute_error: 254.956848, mean_q: -14.702619\n",
      "  999/5000: episode: 998, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 2979487.250000, mean_absolute_error: 338.398071, mean_q: -14.704319\n",
      " 1000/5000: episode: 999, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.064], loss: 1139.026367, mean_absolute_error: 133.114410, mean_q: -14.715609\n",
      " 1001/5000: episode: 1000, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.117, 0.268], loss: 1496592.500000, mean_absolute_error: 239.783279, mean_q: -14.718533\n",
      " 1002/5000: episode: 1001, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 108.434883, mean_absolute_error: 133.314240, mean_q: -14.725498\n",
      " 1003/5000: episode: 1002, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 23790.515625, mean_absolute_error: 148.864716, mean_q: -14.728992\n",
      " 1004/5000: episode: 1003, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [0.079, 0.213], loss: 2992650.250000, mean_absolute_error: 346.616974, mean_q: -14.724444\n",
      " 1005/5000: episode: 1004, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.487, 0.678], loss: 1520019.125000, mean_absolute_error: 255.671249, mean_q: -14.734730\n",
      " 1006/5000: episode: 1005, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.105], loss: 1202.972534, mean_absolute_error: 133.842194, mean_q: -14.750412\n",
      " 1007/5000: episode: 1006, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.120, 0.272], loss: 23774.210938, mean_absolute_error: 149.312271, mean_q: -14.761065\n",
      " 1008/5000: episode: 1007, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.343, 0.535], loss: 1495013.000000, mean_absolute_error: 240.501221, mean_q: -14.762041\n",
      " 1009/5000: episode: 1008, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.287, 0.475], loss: 11937.117188, mean_absolute_error: 141.811111, mean_q: -14.767782\n",
      " 1010/5000: episode: 1009, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.102 [0.049, 0.165], loss: 28083.175781, mean_absolute_error: 149.853119, mean_q: -14.773319\n",
      " 1011/5000: episode: 1010, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 1229.209106, mean_absolute_error: 134.415955, mean_q: -14.774115\n",
      " 1012/5000: episode: 1011, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.367, 0.560], loss: 2990408.000000, mean_absolute_error: 347.391663, mean_q: -14.768129\n",
      " 1013/5000: episode: 1012, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.948, 0.999], loss: 23748.298828, mean_absolute_error: 149.917236, mean_q: -14.777187\n",
      " 1014/5000: episode: 1013, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.196, 0.370], loss: 1486009.375000, mean_absolute_error: 233.557190, mean_q: -14.785238\n",
      " 1015/5000: episode: 1014, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 1482659.375000, mean_absolute_error: 233.515900, mean_q: -14.801146\n",
      " 1016/5000: episode: 1015, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.557 [0.460, 0.652], loss: 1508357.375000, mean_absolute_error: 249.132355, mean_q: -14.819880\n",
      " 1017/5000: episode: 1016, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 24809.658203, mean_absolute_error: 150.435211, mean_q: -14.833706\n",
      " 1018/5000: episode: 1017, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.979, 1.000], loss: 1494210.750000, mean_absolute_error: 241.568314, mean_q: -14.837906\n",
      " 1019/5000: episode: 1018, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.491, 0.682], loss: 1518880.750000, mean_absolute_error: 257.172485, mean_q: -14.846441\n",
      " 1020/5000: episode: 1019, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 1484432.500000, mean_absolute_error: 234.206024, mean_q: -14.858128\n",
      " 1021/5000: episode: 1020, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.877 [0.809, 0.935], loss: 11914.137695, mean_absolute_error: 143.185501, mean_q: -14.874274\n",
      " 1022/5000: episode: 1021, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.667, 0.833], loss: 35516.726562, mean_absolute_error: 158.700317, mean_q: -14.893628\n",
      " 1023/5000: episode: 1022, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 11911.163086, mean_absolute_error: 143.438049, mean_q: -14.908253\n",
      " 1024/5000: episode: 1023, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.198, 0.372], loss: 1494818.000000, mean_absolute_error: 242.368195, mean_q: -14.918087\n",
      " 1025/5000: episode: 1024, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.896, 0.982], loss: 1505376.125000, mean_absolute_error: 250.076965, mean_q: -14.931838\n",
      " 1026/5000: episode: 1025, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 111.794708, mean_absolute_error: 136.133514, mean_q: -14.951906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1027/5000: episode: 1026, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.529, 0.716], loss: 13023.083008, mean_absolute_error: 144.004791, mean_q: -14.974296\n",
      " 1028/5000: episode: 1027, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.207, 0.382], loss: 11903.921875, mean_absolute_error: 144.071381, mean_q: -14.990475\n",
      " 1029/5000: episode: 1028, duration: 0.104s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.881 [0.814, 0.938], loss: 11902.565430, mean_absolute_error: 144.170654, mean_q: -14.996893\n",
      " 1030/5000: episode: 1029, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.439, 0.632], loss: 24831.691406, mean_absolute_error: 152.011993, mean_q: -14.995912\n",
      " 1031/5000: episode: 1030, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 23686.460938, mean_absolute_error: 151.993149, mean_q: -14.986620\n",
      " 1032/5000: episode: 1031, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.893, 0.981], loss: 1238.748535, mean_absolute_error: 136.758163, mean_q: -14.977385\n",
      " 1033/5000: episode: 1032, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [0.088, 0.227], loss: 12988.018555, mean_absolute_error: 144.456589, mean_q: -14.966970\n",
      " 1034/5000: episode: 1033, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.733 [0.645, 0.816], loss: 1482285.500000, mean_absolute_error: 235.495331, mean_q: -14.950292\n",
      " 1035/5000: episode: 1034, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.486, 0.677], loss: 23671.287109, mean_absolute_error: 152.233673, mean_q: -14.937870\n",
      " 1036/5000: episode: 1035, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.133, 0.290], loss: 23666.773438, mean_absolute_error: 152.289764, mean_q: -14.920556\n",
      " 1037/5000: episode: 1036, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.971, 1.000], loss: 11886.589844, mean_absolute_error: 144.660583, mean_q: -14.898866\n",
      " 1038/5000: episode: 1037, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.306], loss: 110.632492, mean_absolute_error: 137.030853, mean_q: -14.873978\n",
      " 1039/5000: episode: 1038, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.077], loss: 11881.230469, mean_absolute_error: 144.736633, mean_q: -14.846432\n",
      " 1040/5000: episode: 1039, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.578 [0.481, 0.672], loss: 35415.914062, mean_absolute_error: 160.099274, mean_q: -14.816883\n",
      " 1041/5000: episode: 1040, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.169, 0.337], loss: 12989.333008, mean_absolute_error: 144.821732, mean_q: -14.784559\n",
      " 1042/5000: episode: 1041, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.357 [0.266, 0.451], loss: 1504558.750000, mean_absolute_error: 251.135849, mean_q: -14.751547\n",
      " 1043/5000: episode: 1042, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 35391.746094, mean_absolute_error: 160.182953, mean_q: -14.725219\n",
      " 1044/5000: episode: 1043, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.094, 0.235], loss: 24739.179688, mean_absolute_error: 152.585724, mean_q: -14.694107\n",
      " 1045/5000: episode: 1044, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.738, 0.887], loss: 2973600.500000, mean_absolute_error: 342.252258, mean_q: -14.666549\n",
      " 1046/5000: episode: 1045, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [0.103, 0.248], loss: 2332.203369, mean_absolute_error: 137.428680, mean_q: -14.657567\n",
      " 1047/5000: episode: 1046, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.539, 0.725], loss: 35360.421875, mean_absolute_error: 160.409607, mean_q: -14.646559\n",
      " 1048/5000: episode: 1047, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.959], loss: 1494876.125000, mean_absolute_error: 243.960754, mean_q: -14.635363\n",
      " 1049/5000: episode: 1048, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.301, 0.490], loss: 35343.570312, mean_absolute_error: 160.562271, mean_q: -14.627103\n",
      " 1050/5000: episode: 1049, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.286, 0.474], loss: 12961.247070, mean_absolute_error: 145.355576, mean_q: -14.612488\n",
      " 1051/5000: episode: 1050, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.928, 0.994], loss: 1515924.250000, mean_absolute_error: 259.318237, mean_q: -14.593550\n",
      " 1052/5000: episode: 1051, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.885 [0.819, 0.942], loss: 24765.769531, mean_absolute_error: 153.218964, mean_q: -14.582865\n",
      " 1053/5000: episode: 1052, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.200, 0.374], loss: 14127.858398, mean_absolute_error: 145.656036, mean_q: -14.570958\n",
      " 1054/5000: episode: 1053, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.889 [0.824, 0.944], loss: 1492305.375000, mean_absolute_error: 244.237747, mean_q: -14.553780\n",
      " 1055/5000: episode: 1054, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.982, 1.000], loss: 13007.799805, mean_absolute_error: 145.766876, mean_q: -14.542515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1056/5000: episode: 1055, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [0.062, 0.186], loss: 23562.191406, mean_absolute_error: 153.369049, mean_q: -14.526632\n",
      " 1057/5000: episode: 1056, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.870 [0.801, 0.930], loss: 23558.734375, mean_absolute_error: 153.417450, mean_q: -14.506863\n",
      " 1058/5000: episode: 1057, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.731, 0.882], loss: 12959.203125, mean_absolute_error: 145.854324, mean_q: -14.483339\n",
      " 1059/5000: episode: 1058, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.129, 0.284], loss: 1505059.125000, mean_absolute_error: 252.202972, mean_q: -14.456960\n",
      " 1060/5000: episode: 1059, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.039], loss: 1505029.000000, mean_absolute_error: 252.253647, mean_q: -14.436210\n",
      " 1061/5000: episode: 1060, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 23538.548828, mean_absolute_error: 153.596558, mean_q: -14.420332\n",
      " 1062/5000: episode: 1061, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.531, 0.718], loss: 1493162.750000, mean_absolute_error: 244.634308, mean_q: -14.400715\n",
      " 1063/5000: episode: 1062, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.460 [0.364, 0.557], loss: 14118.974609, mean_absolute_error: 146.188171, mean_q: -14.385271\n",
      " 1064/5000: episode: 1063, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.515, 0.703], loss: 1480256.000000, mean_absolute_error: 237.133347, mean_q: -14.369534\n",
      " 1065/5000: episode: 1064, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.342, 0.534], loss: 103.161255, mean_absolute_error: 138.614975, mean_q: -14.362930\n",
      " 1066/5000: episode: 1065, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.207, 0.383], loss: 1503571.375000, mean_absolute_error: 252.517792, mean_q: -14.351223\n",
      " 1067/5000: episode: 1066, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.038, 0.146], loss: 36331.371094, mean_absolute_error: 161.621063, mean_q: -14.346891\n",
      " 1068/5000: episode: 1067, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.124, 0.278], loss: 12983.515625, mean_absolute_error: 146.565079, mean_q: -14.348699\n",
      " 1069/5000: episode: 1068, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.314, 0.504], loss: 12923.187500, mean_absolute_error: 146.591949, mean_q: -14.350327\n",
      " 1070/5000: episode: 1069, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [0.064, 0.189], loss: 102.958572, mean_absolute_error: 139.074112, mean_q: -14.348813\n",
      " 1071/5000: episode: 1070, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.667, 0.834], loss: 24629.984375, mean_absolute_error: 154.404739, mean_q: -14.345366\n",
      " 1072/5000: episode: 1071, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.617, 0.792], loss: 12980.789062, mean_absolute_error: 146.924500, mean_q: -14.332831\n",
      " 1073/5000: episode: 1072, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.709, 0.866], loss: 35170.785156, mean_absolute_error: 162.138290, mean_q: -14.319794\n",
      " 1074/5000: episode: 1073, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.386, 0.580], loss: 11788.803711, mean_absolute_error: 146.999741, mean_q: -14.304355\n",
      " 1075/5000: episode: 1074, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.161, 0.326], loss: 1492485.750000, mean_absolute_error: 245.628906, mean_q: -14.285373\n",
      " 1076/5000: episode: 1075, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.714, 0.870], loss: 11782.980469, mean_absolute_error: 147.131195, mean_q: -14.271025\n",
      " 1077/5000: episode: 1076, duration: 0.087s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.836, 0.952], loss: 3005812.000000, mean_absolute_error: 367.106384, mean_q: -14.254910\n",
      " 1078/5000: episode: 1077, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.099], loss: 11777.864258, mean_absolute_error: 147.285950, mean_q: -14.249194\n",
      " 1079/5000: episode: 1078, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 25862.128906, mean_absolute_error: 155.146759, mean_q: -14.243025\n",
      " 1080/5000: episode: 1079, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [0.055, 0.174], loss: 1503980.250000, mean_absolute_error: 253.673325, mean_q: -14.234317\n",
      " 1081/5000: episode: 1080, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [0.007, 0.076], loss: 1502707.625000, mean_absolute_error: 253.681915, mean_q: -14.233162\n",
      " 1082/5000: episode: 1081, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.120, 0.272], loss: 11768.992188, mean_absolute_error: 147.658249, mean_q: -14.236712\n",
      " 1083/5000: episode: 1082, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.911 [0.851, 0.960], loss: 1514229.125000, mean_absolute_error: 261.460388, mean_q: -14.231918\n",
      " 1084/5000: episode: 1083, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.929 [0.875, 0.972], loss: 14075.226562, mean_absolute_error: 147.891937, mean_q: -14.226738\n",
      " 1085/5000: episode: 1084, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.374 [0.282, 0.469], loss: 23417.902344, mean_absolute_error: 155.520920, mean_q: -14.215961\n",
      " 1086/5000: episode: 1085, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.921, 0.992], loss: 100.885864, mean_absolute_error: 140.442657, mean_q: -14.203637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1087/5000: episode: 1086, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.664, 0.831], loss: 1516276.750000, mean_absolute_error: 261.789581, mean_q: -14.195307\n",
      " 1088/5000: episode: 1087, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.900, 0.984], loss: 1504551.250000, mean_absolute_error: 254.307404, mean_q: -14.193132\n",
      " 1089/5000: episode: 1088, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.277 [0.194, 0.367], loss: 1504474.875000, mean_absolute_error: 254.410782, mean_q: -14.196223\n",
      " 1090/5000: episode: 1089, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.202, 0.377], loss: 23389.923828, mean_absolute_error: 156.037781, mean_q: -14.209074\n",
      " 1091/5000: episode: 1090, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.283, 0.470], loss: 23384.722656, mean_absolute_error: 156.169708, mean_q: -14.223158\n",
      " 1092/5000: episode: 1091, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.089], loss: 47821.882812, mean_absolute_error: 171.468948, mean_q: -14.240271\n",
      " 1093/5000: episode: 1092, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.969, 1.000], loss: 1490130.750000, mean_absolute_error: 247.392990, mean_q: -14.251184\n",
      " 1094/5000: episode: 1093, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.864 [0.793, 0.925], loss: 1491189.375000, mean_absolute_error: 247.541809, mean_q: -14.265442\n",
      " 1095/5000: episode: 1094, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.113, 0.262], loss: 11732.462891, mean_absolute_error: 149.218552, mean_q: -14.283374\n",
      " 1096/5000: episode: 1095, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.239, 0.421], loss: 102.260971, mean_absolute_error: 141.814194, mean_q: -14.300117\n",
      " 1097/5000: episode: 1096, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.850 [0.777, 0.914], loss: 25846.980469, mean_absolute_error: 157.242645, mean_q: -14.310240\n",
      " 1098/5000: episode: 1097, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 1501265.500000, mean_absolute_error: 255.615204, mean_q: -14.314389\n",
      " 1099/5000: episode: 1098, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.845], loss: 23338.761719, mean_absolute_error: 157.316437, mean_q: -14.333203\n",
      " 1100/5000: episode: 1099, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.537 [0.440, 0.633], loss: 3013625.000000, mean_absolute_error: 376.972382, mean_q: -14.348768\n",
      " 1101/5000: episode: 1100, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.388, 0.581], loss: 1512546.125000, mean_absolute_error: 263.594788, mean_q: -14.374712\n",
      " 1102/5000: episode: 1101, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.327, 0.518], loss: 103.774742, mean_absolute_error: 142.712799, mean_q: -14.405578\n",
      " 1103/5000: episode: 1102, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.723 [0.634, 0.807], loss: 24516.193359, mean_absolute_error: 157.966949, mean_q: -14.427866\n",
      " 1104/5000: episode: 1103, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 11709.071289, mean_absolute_error: 150.542786, mean_q: -14.437160\n",
      " 1105/5000: episode: 1104, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.535 [0.438, 0.631], loss: 1501735.500000, mean_absolute_error: 256.624817, mean_q: -14.437417\n",
      " 1106/5000: episode: 1105, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.957 [0.913, 0.989], loss: 34901.648438, mean_absolute_error: 165.840637, mean_q: -14.437937\n",
      " 1107/5000: episode: 1106, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.711, 0.867], loss: 1514438.375000, mean_absolute_error: 264.425293, mean_q: -14.434744\n",
      " 1108/5000: episode: 1107, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.035], loss: 23291.191406, mean_absolute_error: 158.528687, mean_q: -14.441735\n",
      " 1109/5000: episode: 1108, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 12955.656250, mean_absolute_error: 151.185440, mean_q: -14.444525\n",
      " 1110/5000: episode: 1109, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [0.069, 0.197], loss: 11692.125000, mean_absolute_error: 151.212189, mean_q: -14.438365\n",
      " 1111/5000: episode: 1110, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 104.045624, mean_absolute_error: 143.766663, mean_q: -14.424368\n",
      " 1112/5000: episode: 1111, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.140, 0.299], loss: 2978097.000000, mean_absolute_error: 355.731323, mean_q: -14.406399\n",
      " 1113/5000: episode: 1112, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.889 [0.824, 0.944], loss: 1397.780518, mean_absolute_error: 144.055359, mean_q: -14.406288\n",
      " 1114/5000: episode: 1113, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.415 [0.321, 0.512], loss: 34833.730469, mean_absolute_error: 166.607559, mean_q: -14.403774\n",
      " 1115/5000: episode: 1114, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 11677.718750, mean_absolute_error: 151.652649, mean_q: -14.393784\n",
      " 1116/5000: episode: 1115, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.112, 0.260], loss: 12973.827148, mean_absolute_error: 151.816574, mean_q: -14.378929\n",
      " 1117/5000: episode: 1116, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [0.062, 0.187], loss: 2626.977539, mean_absolute_error: 144.380768, mean_q: -14.365028\n",
      " 1118/5000: episode: 1117, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.031], loss: 57942.632812, mean_absolute_error: 181.895950, mean_q: -14.355597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1119/5000: episode: 1118, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.020], loss: 102.921585, mean_absolute_error: 144.422424, mean_q: -14.346235\n",
      " 1120/5000: episode: 1119, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 15536.967773, mean_absolute_error: 152.269272, mean_q: -14.336012\n",
      " 1121/5000: episode: 1120, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.902 [0.840, 0.954], loss: 2975856.500000, mean_absolute_error: 356.267090, mean_q: -14.320179\n",
      " 1122/5000: episode: 1121, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.014, 0.094], loss: 11660.362305, mean_absolute_error: 152.177841, mean_q: -14.317546\n",
      " 1123/5000: episode: 1122, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.667, 0.833], loss: 1490347.000000, mean_absolute_error: 250.634705, mean_q: -14.307278\n",
      " 1124/5000: episode: 1123, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.728, 0.880], loss: 12966.304688, mean_absolute_error: 152.466003, mean_q: -14.310168\n",
      " 1125/5000: episode: 1124, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.370 [0.278, 0.465], loss: 34754.832031, mean_absolute_error: 167.481323, mean_q: -14.318257\n",
      " 1126/5000: episode: 1125, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.012, 0.089], loss: 1346.154175, mean_absolute_error: 145.114120, mean_q: -14.320997\n",
      " 1127/5000: episode: 1126, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.030], loss: 1488887.000000, mean_absolute_error: 251.094482, mean_q: -14.317428\n",
      " 1128/5000: episode: 1127, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.612 [0.516, 0.705], loss: 23191.939453, mean_absolute_error: 160.270081, mean_q: -14.321433\n",
      " 1129/5000: episode: 1128, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.050, 0.167], loss: 1487427.750000, mean_absolute_error: 251.176880, mean_q: -14.320928\n",
      " 1130/5000: episode: 1129, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 1428.216309, mean_absolute_error: 145.586243, mean_q: -14.322178\n",
      " 1131/5000: episode: 1130, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.042 [0.010, 0.085], loss: 34718.906250, mean_absolute_error: 168.026337, mean_q: -14.316737\n",
      " 1132/5000: episode: 1131, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.868, 0.969], loss: 46249.238281, mean_absolute_error: 175.590546, mean_q: -14.309067\n",
      " 1133/5000: episode: 1132, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.858 [0.786, 0.921], loss: 102.225197, mean_absolute_error: 145.718735, mean_q: -14.297615\n",
      " 1134/5000: episode: 1133, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.046 [0.013, 0.091], loss: 1432.278931, mean_absolute_error: 145.885223, mean_q: -14.282585\n",
      " 1135/5000: episode: 1134, duration: 0.139s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.781, 0.917], loss: 23161.988281, mean_absolute_error: 160.811096, mean_q: -14.268416\n",
      " 1136/5000: episode: 1135, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.902 [0.841, 0.954], loss: 12964.171875, mean_absolute_error: 153.496490, mean_q: -14.252665\n",
      " 1137/5000: episode: 1136, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.565, 0.749], loss: 23152.582031, mean_absolute_error: 160.921814, mean_q: -14.232572\n",
      " 1138/5000: episode: 1137, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.003, 0.059], loss: 1487018.625000, mean_absolute_error: 251.761078, mean_q: -14.208113\n",
      " 1139/5000: episode: 1138, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.460 [0.364, 0.557], loss: 34664.648438, mean_absolute_error: 168.512085, mean_q: -14.199928\n",
      " 1140/5000: episode: 1139, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.733, 0.884], loss: 11619.803711, mean_absolute_error: 153.646820, mean_q: -14.194041\n",
      " 1141/5000: episode: 1140, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 14208.547852, mean_absolute_error: 153.831909, mean_q: -14.187465\n",
      " 1142/5000: episode: 1141, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.882 [0.816, 0.939], loss: 14292.423828, mean_absolute_error: 154.003784, mean_q: -14.178720\n",
      " 1143/5000: episode: 1142, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.240, 0.422], loss: 1498274.000000, mean_absolute_error: 259.590515, mean_q: -14.163515\n",
      " 1144/5000: episode: 1143, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.208, 0.384], loss: 1475201.875000, mean_absolute_error: 244.721512, mean_q: -14.153109\n",
      " 1145/5000: episode: 1144, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.240, 0.421], loss: 1486656.125000, mean_absolute_error: 252.267822, mean_q: -14.148531\n",
      " 1146/5000: episode: 1145, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.431 [0.336, 0.527], loss: 1487912.875000, mean_absolute_error: 252.415527, mean_q: -14.145935\n",
      " 1147/5000: episode: 1146, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 47383.250000, mean_absolute_error: 176.582703, mean_q: -14.146065\n",
      " 1148/5000: episode: 1147, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.354, 0.547], loss: 12910.575195, mean_absolute_error: 154.321594, mean_q: -14.136861\n",
      " 1149/5000: episode: 1148, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.504 [0.408, 0.601], loss: 14193.764648, mean_absolute_error: 154.410370, mean_q: -14.123204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1150/5000: episode: 1149, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.417 [0.323, 0.514], loss: 23097.738281, mean_absolute_error: 161.841431, mean_q: -14.104357\n",
      " 1151/5000: episode: 1150, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [0.064, 0.190], loss: 1486376.375000, mean_absolute_error: 252.655350, mean_q: -14.080120\n",
      " 1152/5000: episode: 1151, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.932, 0.995], loss: 2962410.000000, mean_absolute_error: 351.017883, mean_q: -14.063587\n",
      " 1153/5000: episode: 1152, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [0.096, 0.238], loss: 1497770.000000, mean_absolute_error: 260.254822, mean_q: -14.060364\n",
      " 1154/5000: episode: 1153, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 12859.293945, mean_absolute_error: 154.672211, mean_q: -14.058414\n",
      " 1155/5000: episode: 1154, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 2974852.000000, mean_absolute_error: 358.749695, mean_q: -14.049358\n",
      " 1156/5000: episode: 1155, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.167, 0.333], loss: 34555.503906, mean_absolute_error: 169.755188, mean_q: -14.051146\n",
      " 1157/5000: episode: 1156, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.305, 0.494], loss: 11581.118164, mean_absolute_error: 154.957016, mean_q: -14.044977\n",
      " 1158/5000: episode: 1157, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 1497452.500000, mean_absolute_error: 260.682159, mean_q: -14.032717\n",
      " 1159/5000: episode: 1158, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.219 [0.143, 0.303], loss: 2678.447021, mean_absolute_error: 147.715744, mean_q: -14.029323\n",
      " 1160/5000: episode: 1159, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 23048.845703, mean_absolute_error: 162.674011, mean_q: -14.032637\n",
      " 1161/5000: episode: 1160, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 1487127.500000, mean_absolute_error: 253.649719, mean_q: -14.035528\n",
      " 1162/5000: episode: 1161, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.469, 0.661], loss: 11568.413086, mean_absolute_error: 155.465759, mean_q: -14.038807\n",
      " 1163/5000: episode: 1162, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.179 [0.109, 0.257], loss: 11565.775391, mean_absolute_error: 155.574005, mean_q: -14.038797\n",
      " 1164/5000: episode: 1163, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.060 [0.021, 0.110], loss: 1485539.250000, mean_absolute_error: 253.852356, mean_q: -14.034418\n",
      " 1165/5000: episode: 1164, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [0.074, 0.205], loss: 1486846.125000, mean_absolute_error: 254.053345, mean_q: -14.031878\n",
      " 1166/5000: episode: 1165, duration: 0.092s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.557 [0.460, 0.652], loss: 12874.965820, mean_absolute_error: 155.917694, mean_q: -14.031622\n",
      " 1167/5000: episode: 1166, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.360 [0.269, 0.455], loss: 23010.820312, mean_absolute_error: 163.399521, mean_q: -14.024155\n",
      " 1168/5000: episode: 1167, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.179], loss: 1486587.125000, mean_absolute_error: 254.212585, mean_q: -14.010010\n",
      " 1169/5000: episode: 1168, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.831 [0.754, 0.899], loss: 23000.167969, mean_absolute_error: 163.548248, mean_q: -14.000250\n",
      " 1170/5000: episode: 1169, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 1508091.750000, mean_absolute_error: 269.198181, mean_q: -13.985653\n",
      " 1171/5000: episode: 1170, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.293, 0.482], loss: 12868.147461, mean_absolute_error: 156.316803, mean_q: -13.973448\n",
      " 1172/5000: episode: 1171, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.006, 0.074], loss: 1488.954224, mean_absolute_error: 149.051422, mean_q: -13.960464\n",
      " 1173/5000: episode: 1172, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.971], loss: 11537.746094, mean_absolute_error: 156.438507, mean_q: -13.947908\n",
      " 1174/5000: episode: 1173, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.953, 0.999], loss: 22973.990234, mean_absolute_error: 163.909164, mean_q: -13.930359\n",
      " 1175/5000: episode: 1174, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 1486280.875000, mean_absolute_error: 254.690552, mean_q: -13.908056\n",
      " 1176/5000: episode: 1175, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.938, 0.997], loss: 11529.818359, mean_absolute_error: 156.611069, mean_q: -13.891861\n",
      " 1177/5000: episode: 1176, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.121, 0.273], loss: 22958.103516, mean_absolute_error: 164.071472, mean_q: -13.871445\n",
      " 1178/5000: episode: 1177, duration: 0.133s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 1428.198242, mean_absolute_error: 149.336548, mean_q: -13.846183\n",
      " 1179/5000: episode: 1178, duration: 0.131s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.958, 1.000], loss: 14209.470703, mean_absolute_error: 156.857819, mean_q: -13.829348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1180/5000: episode: 1179, duration: 0.087s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.536, 0.723], loss: 11519.489258, mean_absolute_error: 156.850586, mean_q: -13.821115\n",
      " 1181/5000: episode: 1180, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.082 [0.035, 0.139], loss: 11517.270508, mean_absolute_error: 156.925323, mean_q: -13.812025\n",
      " 1182/5000: episode: 1181, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.896 [0.833, 0.950], loss: 1499.282471, mean_absolute_error: 149.716782, mean_q: -13.807970\n",
      " 1183/5000: episode: 1182, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.898 [0.835, 0.951], loss: 1484617.000000, mean_absolute_error: 255.216675, mean_q: -13.807186\n",
      " 1184/5000: episode: 1183, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.892, 0.980], loss: 1484549.750000, mean_absolute_error: 255.299194, mean_q: -13.807861\n",
      " 1185/5000: episode: 1184, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.147, 0.308], loss: 95.395691, mean_absolute_error: 149.894638, mean_q: -13.811726\n",
      " 1186/5000: episode: 1185, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.639, 0.811], loss: 1508649.625000, mean_absolute_error: 270.366180, mean_q: -13.812560\n",
      " 1187/5000: episode: 1186, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [0.077, 0.211], loss: 1495737.625000, mean_absolute_error: 262.986084, mean_q: -13.823790\n",
      " 1188/5000: episode: 1187, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.180 [0.110, 0.258], loss: 24266.484375, mean_absolute_error: 165.054764, mean_q: -13.844116\n",
      " 1189/5000: episode: 1188, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 2857.479004, mean_absolute_error: 150.504105, mean_q: -13.862439\n",
      " 1190/5000: episode: 1189, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.912 [0.852, 0.961], loss: 11502.303711, mean_absolute_error: 157.889374, mean_q: -13.875206\n",
      " 1191/5000: episode: 1190, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.607 [0.511, 0.700], loss: 22905.085938, mean_absolute_error: 165.365173, mean_q: -13.878193\n",
      " 1192/5000: episode: 1191, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.608, 0.786], loss: 12924.895508, mean_absolute_error: 158.180466, mean_q: -13.878698\n",
      " 1193/5000: episode: 1192, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.683 [0.591, 0.771], loss: 96.355377, mean_absolute_error: 150.789825, mean_q: -13.881029\n",
      " 1194/5000: episode: 1193, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [0.085, 0.222], loss: 18388.316406, mean_absolute_error: 158.485519, mean_q: -13.881784\n",
      " 1195/5000: episode: 1194, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: 1475157.625000, mean_absolute_error: 249.190979, mean_q: -13.873457\n",
      " 1196/5000: episode: 1195, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 11492.206055, mean_absolute_error: 158.419189, mean_q: -13.873583\n",
      " 1197/5000: episode: 1196, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 22884.898438, mean_absolute_error: 165.888123, mean_q: -13.875912\n",
      " 1198/5000: episode: 1197, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.895, 0.982], loss: 96.256058, mean_absolute_error: 151.226883, mean_q: -13.873873\n",
      " 1199/5000: episode: 1198, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.261, 0.445], loss: 1443.571411, mean_absolute_error: 151.297882, mean_q: -13.864799\n",
      " 1200/5000: episode: 1199, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 22872.916016, mean_absolute_error: 166.110229, mean_q: -13.852097\n",
      " 1201/5000: episode: 1200, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.938, 0.997], loss: 11482.371094, mean_absolute_error: 158.799698, mean_q: -13.838718\n",
      " 1202/5000: episode: 1201, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 95.578789, mean_absolute_error: 151.493729, mean_q: -13.824974\n",
      " 1203/5000: episode: 1202, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.860 [0.789, 0.922], loss: 11478.026367, mean_absolute_error: 158.915894, mean_q: -13.806953\n",
      " 1204/5000: episode: 1203, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.668, 0.834], loss: 1537.568237, mean_absolute_error: 151.715637, mean_q: -13.794735\n",
      " 1205/5000: episode: 1204, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.345, 0.538], loss: 22854.001953, mean_absolute_error: 166.414688, mean_q: -13.786686\n",
      " 1206/5000: episode: 1205, duration: 0.069s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.150, 0.312], loss: 94.871613, mean_absolute_error: 151.750366, mean_q: -13.773731\n",
      " 1207/5000: episode: 1206, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 12875.193359, mean_absolute_error: 159.212799, mean_q: -13.755379\n",
      " 1208/5000: episode: 1207, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.179], loss: 94.309738, mean_absolute_error: 151.845718, mean_q: -13.732880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1209/5000: episode: 1208, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 45581.250000, mean_absolute_error: 181.313751, mean_q: -13.707689\n",
      " 1210/5000: episode: 1209, duration: 0.120s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.476 [0.379, 0.573], loss: 93.564316, mean_absolute_error: 151.920624, mean_q: -13.678495\n",
      " 1211/5000: episode: 1210, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 22827.421875, mean_absolute_error: 166.659622, mean_q: -13.647507\n",
      " 1212/5000: episode: 1211, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.037], loss: 4440389.500000, mean_absolute_error: 460.905457, mean_q: -13.624923\n",
      " 1213/5000: episode: 1212, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.401, 0.594], loss: 12834.049805, mean_absolute_error: 159.496033, mean_q: -13.635264\n",
      " 1214/5000: episode: 1213, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.874, 0.972], loss: 22812.582031, mean_absolute_error: 166.929703, mean_q: -13.636256\n",
      " 1215/5000: episode: 1214, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.337, 0.529], loss: 34166.699219, mean_absolute_error: 174.370346, mean_q: -13.634537\n",
      " 1216/5000: episode: 1215, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.496, 0.686], loss: 12906.347656, mean_absolute_error: 159.882858, mean_q: -13.635223\n",
      " 1217/5000: episode: 1216, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.015], loss: 24197.007812, mean_absolute_error: 167.249298, mean_q: -13.631775\n",
      " 1218/5000: episode: 1217, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.493, 0.684], loss: 1493963.125000, mean_absolute_error: 265.286774, mean_q: -13.620583\n",
      " 1219/5000: episode: 1218, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [0.101, 0.246], loss: 1484020.500000, mean_absolute_error: 258.138153, mean_q: -13.612645\n",
      " 1220/5000: episode: 1219, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 22781.937500, mean_absolute_error: 167.490067, mean_q: -13.612938\n",
      " 1221/5000: episode: 1220, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 1482406.875000, mean_absolute_error: 258.238342, mean_q: -13.616470\n",
      " 1222/5000: episode: 1221, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.641, 0.812], loss: 12830.618164, mean_absolute_error: 160.415695, mean_q: -13.630958\n",
      " 1223/5000: episode: 1222, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.190, 0.362], loss: 1482218.500000, mean_absolute_error: 258.489258, mean_q: -13.644852\n",
      " 1224/5000: episode: 1223, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.543 [0.446, 0.639], loss: 12910.585938, mean_absolute_error: 160.769562, mean_q: -13.663187\n",
      " 1225/5000: episode: 1224, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.078], loss: 1504706.750000, mean_absolute_error: 273.380798, mean_q: -13.670437\n",
      " 1226/5000: episode: 1225, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.122, 0.275], loss: 1481956.250000, mean_absolute_error: 258.834778, mean_q: -13.677479\n",
      " 1227/5000: episode: 1226, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.374, 0.567], loss: 1481863.375000, mean_absolute_error: 258.960205, mean_q: -13.691401\n",
      " 1228/5000: episode: 1227, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.206, 0.382], loss: 93.989586, mean_absolute_error: 153.833710, mean_q: -13.709549\n",
      " 1229/5000: episode: 1228, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.046 [0.012, 0.091], loss: 11420.267578, mean_absolute_error: 161.266159, mean_q: -13.717360\n",
      " 1230/5000: episode: 1229, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.077], loss: 22742.234375, mean_absolute_error: 168.683487, mean_q: -13.715969\n",
      " 1231/5000: episode: 1230, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.531, 0.718], loss: 11415.627930, mean_absolute_error: 161.442383, mean_q: -13.706165\n",
      " 1232/5000: episode: 1231, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 1498.338257, mean_absolute_error: 154.180206, mean_q: -13.689631\n",
      " 1233/5000: episode: 1232, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.896, 0.982], loss: 1481488.000000, mean_absolute_error: 259.475220, mean_q: -13.671295\n",
      " 1234/5000: episode: 1233, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.609, 0.786], loss: 22723.367188, mean_absolute_error: 168.944763, mean_q: -13.658232\n",
      " 1235/5000: episode: 1234, duration: 0.124s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.976, 1.000], loss: 12841.269531, mean_absolute_error: 161.712250, mean_q: -13.639513\n",
      " 1236/5000: episode: 1235, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.106 [0.052, 0.170], loss: 1595.302490, mean_absolute_error: 154.514053, mean_q: -13.617279\n",
      " 1237/5000: episode: 1236, duration: 0.098s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.584 [0.488, 0.678], loss: 22710.886719, mean_absolute_error: 169.070160, mean_q: -13.593419\n",
      " 1238/5000: episode: 1237, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.685, 0.848], loss: 12809.323242, mean_absolute_error: 161.788513, mean_q: -13.566057\n",
      " 1239/5000: episode: 1238, duration: 0.107s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.557 [0.461, 0.653], loss: 12834.402344, mean_absolute_error: 161.841644, mean_q: -13.535898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1240/5000: episode: 1239, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.285, 0.473], loss: 11394.573242, mean_absolute_error: 161.832230, mean_q: -13.503844\n",
      " 1241/5000: episode: 1240, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.446 [0.350, 0.542], loss: 90.809479, mean_absolute_error: 154.556137, mean_q: -13.475608\n",
      " 1242/5000: episode: 1241, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.941 [0.891, 0.980], loss: 1498412.875000, mean_absolute_error: 267.309601, mean_q: -13.455441\n",
      " 1243/5000: episode: 1242, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.268 [0.185, 0.357], loss: 11387.731445, mean_absolute_error: 161.978195, mean_q: -13.444646\n",
      " 1244/5000: episode: 1243, duration: 0.097s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [0.085, 0.223], loss: 14311.964844, mean_absolute_error: 162.144028, mean_q: -13.428625\n",
      " 1245/5000: episode: 1244, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.489, 0.680], loss: 1481162.000000, mean_absolute_error: 259.976532, mean_q: -13.408218\n",
      " 1246/5000: episode: 1245, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.963, 1.000], loss: 24138.458984, mean_absolute_error: 169.498505, mean_q: -13.393409\n",
      " 1247/5000: episode: 1246, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.950, 0.999], loss: 1481089.000000, mean_absolute_error: 260.089478, mean_q: -13.372723\n",
      " 1248/5000: episode: 1247, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.614 [0.519, 0.707], loss: 12888.123047, mean_absolute_error: 162.375946, mean_q: -13.358944\n",
      " 1249/5000: episode: 1248, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.065], loss: 1493805.250000, mean_absolute_error: 267.603455, mean_q: -13.340513\n",
      " 1250/5000: episode: 1249, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.979, 1.000], loss: 88.812576, mean_absolute_error: 155.110764, mean_q: -13.326609\n",
      " 1251/5000: episode: 1250, duration: 0.087s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.701, 0.859], loss: 88.585060, mean_absolute_error: 155.171799, mean_q: -13.309526\n",
      " 1252/5000: episode: 1251, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.967, 1.000], loss: 46671.402344, mean_absolute_error: 184.410736, mean_q: -13.289383\n",
      " 1253/5000: episode: 1252, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.477, 0.669], loss: 22638.566406, mean_absolute_error: 169.830673, mean_q: -13.264631\n",
      " 1254/5000: episode: 1253, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.172, 0.340], loss: 1480853.500000, mean_absolute_error: 260.454193, mean_q: -13.236759\n",
      " 1255/5000: episode: 1254, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 46623.234375, mean_absolute_error: 184.503036, mean_q: -13.217196\n",
      " 1256/5000: episode: 1255, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.470, 0.662], loss: 11355.346680, mean_absolute_error: 162.693161, mean_q: -13.196943\n",
      " 1257/5000: episode: 1256, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.610, 0.787], loss: 22618.289062, mean_absolute_error: 170.026093, mean_q: -13.177353\n",
      " 1258/5000: episode: 1257, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.909 [0.849, 0.959], loss: 11349.642578, mean_absolute_error: 162.803925, mean_q: -13.154057\n",
      " 1259/5000: episode: 1258, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.969, 1.000], loss: 22607.250000, mean_absolute_error: 170.121780, mean_q: -13.127909\n",
      " 1260/5000: episode: 1259, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.582, 0.764], loss: 56375.328125, mean_absolute_error: 191.967117, mean_q: -13.099007\n",
      " 1261/5000: episode: 1260, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 24026.646484, mean_absolute_error: 170.209137, mean_q: -13.071144\n",
      " 1262/5000: episode: 1261, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.421, 0.614], loss: 22589.113281, mean_absolute_error: 170.258560, mean_q: -13.045174\n",
      " 1263/5000: episode: 1262, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.955, 1.000], loss: 22582.037109, mean_absolute_error: 170.304306, mean_q: -13.016154\n",
      " 1264/5000: episode: 1263, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.893, 0.981], loss: 11329.824219, mean_absolute_error: 163.080658, mean_q: -12.984158\n",
      " 1265/5000: episode: 1264, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.151], loss: 1480561.875000, mean_absolute_error: 260.987518, mean_q: -12.956392\n",
      " 1266/5000: episode: 1265, duration: 0.092s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [0.068, 0.196], loss: 1491728.375000, mean_absolute_error: 268.344147, mean_q: -12.955710\n",
      " 1267/5000: episode: 1266, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.869 [0.800, 0.929], loss: 1522.079834, mean_absolute_error: 156.152008, mean_q: -12.977388\n",
      " 1268/5000: episode: 1267, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.868 [0.798, 0.928], loss: 22553.560547, mean_absolute_error: 170.791336, mean_q: -12.992817\n",
      " 1269/5000: episode: 1268, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.178, 0.347], loss: 22549.210938, mean_absolute_error: 170.898819, mean_q: -12.997875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1270/5000: episode: 1269, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.102 [0.049, 0.165], loss: 84.514816, mean_absolute_error: 156.505737, mean_q: -13.000139\n",
      " 1271/5000: episode: 1270, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 24006.103516, mean_absolute_error: 171.117432, mean_q: -13.005697\n",
      " 1272/5000: episode: 1271, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.833 [0.757, 0.900], loss: 1543.058960, mean_absolute_error: 156.712463, mean_q: -13.006987\n",
      " 1273/5000: episode: 1272, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.934, 0.996], loss: 11309.391602, mean_absolute_error: 164.038986, mean_q: -13.006535\n",
      " 1274/5000: episode: 1273, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.520, 0.709], loss: 11307.446289, mean_absolute_error: 164.121490, mean_q: -13.002872\n",
      " 1275/5000: episode: 1274, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.094, 0.235], loss: 1556.161865, mean_absolute_error: 156.967896, mean_q: -12.991875\n",
      " 1276/5000: episode: 1275, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.610, 0.787], loss: 84.209633, mean_absolute_error: 157.013550, mean_q: -12.976645\n",
      " 1277/5000: episode: 1276, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.941, 0.997], loss: 22517.980469, mean_absolute_error: 171.532227, mean_q: -12.957363\n",
      " 1278/5000: episode: 1277, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.868 [0.798, 0.928], loss: 11298.557617, mean_absolute_error: 164.338272, mean_q: -12.934137\n",
      " 1279/5000: episode: 1278, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.353, 0.545], loss: 83.320251, mean_absolute_error: 157.140686, mean_q: -12.907930\n",
      " 1280/5000: episode: 1279, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 11293.750977, mean_absolute_error: 164.400864, mean_q: -12.880096\n",
      " 1281/5000: episode: 1280, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 1469993.000000, mean_absolute_error: 255.064102, mean_q: -12.851051\n",
      " 1282/5000: episode: 1281, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.457 [0.362, 0.554], loss: 23956.226562, mean_absolute_error: 171.696320, mean_q: -12.830614\n",
      " 1283/5000: episode: 1282, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.548 [0.451, 0.644], loss: 33697.593750, mean_absolute_error: 178.962402, mean_q: -12.806323\n",
      " 1284/5000: episode: 1283, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.612 [0.517, 0.705], loss: 81.704727, mean_absolute_error: 157.328125, mean_q: -12.782169\n",
      " 1285/5000: episode: 1284, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.605, 0.783], loss: 1505045.250000, mean_absolute_error: 277.004211, mean_q: -12.761545\n",
      " 1286/5000: episode: 1285, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.764 [0.678, 0.842], loss: 1479530.750000, mean_absolute_error: 262.452850, mean_q: -12.746000\n",
      " 1287/5000: episode: 1286, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.976, 1.000], loss: 81.130127, mean_absolute_error: 157.528107, mean_q: -12.737142\n",
      " 1288/5000: episode: 1287, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [0.101, 0.245], loss: 22468.421875, mean_absolute_error: 172.031799, mean_q: -12.723124\n",
      " 1289/5000: episode: 1288, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.007, 0.074], loss: 11272.203125, mean_absolute_error: 164.867035, mean_q: -12.704062\n",
      " 1290/5000: episode: 1289, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.069 [0.026, 0.122], loss: 1618.728027, mean_absolute_error: 157.770569, mean_q: -12.681736\n",
      " 1291/5000: episode: 1290, duration: 0.089s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.889, 0.979], loss: 11267.592773, mean_absolute_error: 164.946304, mean_q: -12.657110\n",
      " 1292/5000: episode: 1291, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.568, 0.751], loss: 23921.232422, mean_absolute_error: 172.189240, mean_q: -12.629019\n",
      " 1293/5000: episode: 1292, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.222, 0.400], loss: 1479319.250000, mean_absolute_error: 262.789124, mean_q: -12.604038\n",
      " 1294/5000: episode: 1293, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 1471184.500000, mean_absolute_error: 255.800873, mean_q: -12.591612\n",
      " 1295/5000: episode: 1294, duration: 0.109s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.824 [0.746, 0.893], loss: 11258.147461, mean_absolute_error: 165.158203, mean_q: -12.583671\n",
      " 1296/5000: episode: 1295, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.216, 0.394], loss: 1480671.875000, mean_absolute_error: 263.036499, mean_q: -12.573743\n",
      " 1297/5000: episode: 1296, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.668, 0.834], loss: 79.044846, mean_absolute_error: 158.139099, mean_q: -12.572372\n",
      " 1298/5000: episode: 1297, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.963, 1.000], loss: 22423.068359, mean_absolute_error: 172.619690, mean_q: -12.565136\n",
      " 1299/5000: episode: 1298, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.273, 0.459], loss: 1467838.250000, mean_absolute_error: 256.039368, mean_q: -12.551568\n",
      " 1300/5000: episode: 1299, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 11246.141602, mean_absolute_error: 165.561798, mean_q: -12.543463\n",
      " 1301/5000: episode: 1300, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.548, 0.734], loss: 1467750.250000, mean_absolute_error: 256.172699, mean_q: -12.529419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1302/5000: episode: 1301, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.132], loss: 78.418083, mean_absolute_error: 158.506012, mean_q: -12.522425\n",
      " 1303/5000: episode: 1302, duration: 0.101s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.147, 0.308], loss: 14283.765625, mean_absolute_error: 165.843521, mean_q: -12.514769\n",
      " 1304/5000: episode: 1303, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.288 [0.203, 0.378], loss: 33558.074219, mean_absolute_error: 180.225067, mean_q: -12.506697\n",
      " 1305/5000: episode: 1304, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 1480301.125000, mean_absolute_error: 263.751770, mean_q: -12.497225\n",
      " 1306/5000: episode: 1305, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [0.067, 0.195], loss: 1467494.500000, mean_absolute_error: 256.547058, mean_q: -12.496965\n",
      " 1307/5000: episode: 1306, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.269], loss: 1665.369629, mean_absolute_error: 159.020432, mean_q: -12.499889\n",
      " 1308/5000: episode: 1307, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 1467370.625000, mean_absolute_error: 256.724426, mean_q: -12.495208\n",
      " 1309/5000: episode: 1308, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.079, 0.213], loss: 1489590.250000, mean_absolute_error: 271.201904, mean_q: -12.504855\n",
      " 1310/5000: episode: 1309, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.904 [0.843, 0.956], loss: 78.439774, mean_absolute_error: 159.264191, mean_q: -12.524157\n",
      " 1311/5000: episode: 1310, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.724, 0.877], loss: 11224.302734, mean_absolute_error: 166.562805, mean_q: -12.532490\n",
      " 1312/5000: episode: 1311, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.161, 0.326], loss: 33509.539062, mean_absolute_error: 181.013275, mean_q: -12.531864\n",
      " 1313/5000: episode: 1312, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.661 [0.567, 0.750], loss: 22361.230469, mean_absolute_error: 173.914444, mean_q: -12.523863\n",
      " 1314/5000: episode: 1313, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 78.250168, mean_absolute_error: 159.624344, mean_q: -12.509008\n",
      " 1315/5000: episode: 1314, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.822 [0.744, 0.892], loss: 33488.734375, mean_absolute_error: 181.217590, mean_q: -12.496632\n",
      " 1316/5000: episode: 1315, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.290 [0.205, 0.380], loss: 14326.386719, mean_absolute_error: 167.051102, mean_q: -12.488408\n",
      " 1317/5000: episode: 1316, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.367, 0.560], loss: 12737.990234, mean_absolute_error: 167.042435, mean_q: -12.478567\n",
      " 1318/5000: episode: 1317, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [0.085, 0.222], loss: 12818.949219, mean_absolute_error: 167.223480, mean_q: -12.479628\n",
      " 1319/5000: episode: 1318, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.069 [0.026, 0.123], loss: 1477804.000000, mean_absolute_error: 264.907867, mean_q: -12.488014\n",
      " 1320/5000: episode: 1319, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.522 [0.425, 0.618], loss: 1603.653320, mean_absolute_error: 160.192627, mean_q: -12.496381\n",
      " 1321/5000: episode: 1320, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.338, 0.530], loss: 2956848.250000, mean_absolute_error: 370.036194, mean_q: -12.496242\n",
      " 1322/5000: episode: 1321, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.868, 0.969], loss: 12741.540039, mean_absolute_error: 167.598175, mean_q: -12.514031\n",
      " 1323/5000: episode: 1322, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.297, 0.486], loss: 2943737.250000, mean_absolute_error: 363.004761, mean_q: -12.526476\n",
      " 1324/5000: episode: 1323, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.286, 0.474], loss: 11196.943359, mean_absolute_error: 167.849884, mean_q: -12.549784\n",
      " 1325/5000: episode: 1324, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.199 [0.126, 0.280], loss: 1467758.125000, mean_absolute_error: 258.516663, mean_q: -12.561584\n",
      " 1326/5000: episode: 1325, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.802 [0.722, 0.875], loss: 11193.643555, mean_absolute_error: 168.085815, mean_q: -12.574434\n",
      " 1327/5000: episode: 1326, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.598], loss: 22304.767578, mean_absolute_error: 175.354340, mean_q: -12.583607\n",
      " 1328/5000: episode: 1327, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.397 [0.303, 0.493], loss: 36671.714844, mean_absolute_error: 182.798462, mean_q: -12.587040\n",
      " 1329/5000: episode: 1328, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.090], loss: 12729.248047, mean_absolute_error: 168.397202, mean_q: -12.580063\n",
      " 1330/5000: episode: 1329, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.204, 0.379], loss: 78.971214, mean_absolute_error: 161.318451, mean_q: -12.566514\n",
      " 1331/5000: episode: 1330, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.058 [0.019, 0.107], loss: 11181.673828, mean_absolute_error: 168.528107, mean_q: -12.549265\n",
      " 1332/5000: episode: 1331, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.062], loss: 46122.718750, mean_absolute_error: 190.125031, mean_q: -12.533221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1333/5000: episode: 1332, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 1478388.625000, mean_absolute_error: 266.301392, mean_q: -12.515247\n",
      " 1334/5000: episode: 1333, duration: 0.097s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.097 [0.045, 0.158], loss: 22266.818359, mean_absolute_error: 175.890091, mean_q: -12.502056\n",
      " 1335/5000: episode: 1334, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.222, 0.400], loss: 1487801.875000, mean_absolute_error: 273.574951, mean_q: -12.488514\n",
      " 1336/5000: episode: 1335, duration: 0.103s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.608, 0.786], loss: 22253.613281, mean_absolute_error: 176.091476, mean_q: -12.489631\n",
      " 1337/5000: episode: 1336, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.890, 0.980], loss: 11162.521484, mean_absolute_error: 169.072418, mean_q: -12.487789\n",
      " 1338/5000: episode: 1337, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [0.080, 0.215], loss: 22240.732422, mean_absolute_error: 176.297058, mean_q: -12.478916\n",
      " 1339/5000: episode: 1338, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.450, 0.642], loss: 77.687729, mean_absolute_error: 162.117706, mean_q: -12.463968\n",
      " 1340/5000: episode: 1339, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.899, 0.983], loss: 12729.992188, mean_absolute_error: 169.332016, mean_q: -12.445992\n",
      " 1341/5000: episode: 1340, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.092], loss: 1487459.250000, mean_absolute_error: 274.060272, mean_q: -12.423685\n",
      " 1342/5000: episode: 1341, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [0.082, 0.217], loss: 11147.264648, mean_absolute_error: 169.438507, mean_q: -12.407669\n",
      " 1343/5000: episode: 1342, duration: 0.169s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.441, 0.634], loss: 25515.181641, mean_absolute_error: 176.814758, mean_q: -12.391059\n",
      " 1344/5000: episode: 1343, duration: 0.121s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 33269.769531, mean_absolute_error: 183.816315, mean_q: -12.375020\n",
      " 1345/5000: episode: 1344, duration: 0.171s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.540, 0.727], loss: 11137.647461, mean_absolute_error: 169.652039, mean_q: -12.353968\n",
      " 1346/5000: episode: 1345, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [0.076, 0.208], loss: 12798.087891, mean_absolute_error: 169.804962, mean_q: -12.329558\n",
      " 1347/5000: episode: 1346, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 11131.584961, mean_absolute_error: 169.748734, mean_q: -12.302985\n",
      " 1348/5000: episode: 1347, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 1487207.125000, mean_absolute_error: 274.449951, mean_q: -12.274105\n",
      " 1349/5000: episode: 1348, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.879 [0.812, 0.937], loss: 75.091949, mean_absolute_error: 162.730286, mean_q: -12.253954\n",
      " 1350/5000: episode: 1349, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 22172.437500, mean_absolute_error: 176.985413, mean_q: -12.231312\n",
      " 1351/5000: episode: 1350, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [0.098, 0.241], loss: 34835.710938, mean_absolute_error: 184.176712, mean_q: -12.205959\n",
      " 1352/5000: episode: 1351, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.890 [0.825, 0.945], loss: 1487100.250000, mean_absolute_error: 274.616669, mean_q: -12.177258\n",
      " 1353/5000: episode: 1352, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.903 [0.841, 0.955], loss: 22156.880859, mean_absolute_error: 177.110504, mean_q: -12.155817\n",
      " 1354/5000: episode: 1353, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.582, 0.763], loss: 34834.941406, mean_absolute_error: 184.329742, mean_q: -12.131212\n",
      " 1355/5000: episode: 1354, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.950, 0.999], loss: 12780.658203, mean_absolute_error: 170.205734, mean_q: -12.102703\n",
      " 1356/5000: episode: 1355, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.955, 1.000], loss: 1477627.375000, mean_absolute_error: 267.806152, mean_q: -12.077902\n",
      " 1357/5000: episode: 1356, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.553 [0.456, 0.648], loss: 33166.167969, mean_absolute_error: 184.405060, mean_q: -12.066322\n",
      " 1358/5000: episode: 1357, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [0.080, 0.215], loss: 22129.527344, mean_absolute_error: 177.374496, mean_q: -12.048187\n",
      " 1359/5000: episode: 1358, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.509, 0.698], loss: 1486858.625000, mean_absolute_error: 274.988708, mean_q: -12.030581\n",
      " 1360/5000: episode: 1359, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.850 [0.777, 0.915], loss: 1464746.125000, mean_absolute_error: 260.914734, mean_q: -12.022579\n",
      " 1361/5000: episode: 1360, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.309, 0.499], loss: 1726.495361, mean_absolute_error: 163.562561, mean_q: -12.019501\n",
      " 1362/5000: episode: 1361, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.872, 0.971], loss: 12772.090820, mean_absolute_error: 170.757141, mean_q: -12.010099\n",
      " 1363/5000: episode: 1362, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.141, 0.300], loss: 11085.590820, mean_absolute_error: 170.721161, mean_q: -11.995104\n",
      " 1364/5000: episode: 1363, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 45782.437500, mean_absolute_error: 192.089508, mean_q: -11.976309\n",
      " 1365/5000: episode: 1364, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 11079.618164, mean_absolute_error: 170.851578, mean_q: -11.957256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1366/5000: episode: 1365, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.020], loss: 23721.486328, mean_absolute_error: 178.040253, mean_q: -11.940090\n",
      " 1367/5000: episode: 1366, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.889, 0.979], loss: 11073.929688, mean_absolute_error: 170.982376, mean_q: -11.924109\n",
      " 1368/5000: episode: 1367, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.318, 0.509], loss: 22071.773438, mean_absolute_error: 178.117325, mean_q: -11.909832\n",
      " 1369/5000: episode: 1368, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.424 [0.329, 0.521], loss: 3407.666504, mean_absolute_error: 164.201752, mean_q: -11.890896\n",
      " 1370/5000: episode: 1369, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.519, 0.707], loss: 1465951.125000, mean_absolute_error: 261.629333, mean_q: -11.875273\n",
      " 1371/5000: episode: 1370, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.281, 0.468], loss: 1465983.375000, mean_absolute_error: 261.790833, mean_q: -11.870893\n",
      " 1372/5000: episode: 1371, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.921, 0.992], loss: 11063.127930, mean_absolute_error: 171.328842, mean_q: -11.870900\n",
      " 1373/5000: episode: 1372, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.280, 0.467], loss: 1686.080688, mean_absolute_error: 164.379059, mean_q: -11.869822\n",
      " 1374/5000: episode: 1373, duration: 0.097s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.348 [0.258, 0.442], loss: 70.420074, mean_absolute_error: 164.440735, mean_q: -11.866610\n",
      " 1375/5000: episode: 1374, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.414], loss: 70.373428, mean_absolute_error: 164.522186, mean_q: -11.862679\n",
      " 1376/5000: episode: 1375, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.934 [0.882, 0.976], loss: 11055.995117, mean_absolute_error: 171.651367, mean_q: -11.858046\n",
      " 1377/5000: episode: 1376, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.692, 0.853], loss: 70.201996, mean_absolute_error: 164.662323, mean_q: -11.848219\n",
      " 1378/5000: episode: 1377, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.419, 0.613], loss: 1474916.750000, mean_absolute_error: 269.239624, mean_q: -11.833908\n",
      " 1379/5000: episode: 1378, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.467, 0.659], loss: 2938685.500000, mean_absolute_error: 366.783112, mean_q: -11.827351\n",
      " 1380/5000: episode: 1379, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.112 [0.057, 0.178], loss: 11050.806641, mean_absolute_error: 171.932007, mean_q: -11.841976\n",
      " 1381/5000: episode: 1380, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.049], loss: 1476317.250000, mean_absolute_error: 269.503296, mean_q: -11.851784\n",
      " 1382/5000: episode: 1381, duration: 0.125s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.519 [0.422, 0.615], loss: 70.378021, mean_absolute_error: 165.091766, mean_q: -11.863066\n",
      " 1383/5000: episode: 1382, duration: 0.104s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.720, 0.874], loss: 14357.778320, mean_absolute_error: 172.307922, mean_q: -11.866337\n",
      " 1384/5000: episode: 1383, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.873 [0.804, 0.932], loss: 1463530.750000, mean_absolute_error: 262.713013, mean_q: -11.860760\n",
      " 1385/5000: episode: 1384, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.943, 0.998], loss: 11043.620117, mean_absolute_error: 172.404205, mean_q: -11.864475\n",
      " 1386/5000: episode: 1385, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.235 [0.156, 0.320], loss: 38125.742188, mean_absolute_error: 186.885132, mean_q: -11.875580\n",
      " 1387/5000: episode: 1386, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.122, 0.275], loss: 70.646919, mean_absolute_error: 165.612396, mean_q: -11.885708\n",
      " 1388/5000: episode: 1387, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.079], loss: 22003.242188, mean_absolute_error: 179.794647, mean_q: -11.887407\n",
      " 1389/5000: episode: 1388, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 70.592766, mean_absolute_error: 165.805573, mean_q: -11.881151\n",
      " 1390/5000: episode: 1389, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 32956.277344, mean_absolute_error: 186.980286, mean_q: -11.870201\n",
      " 1391/5000: episode: 1390, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.637, 0.810], loss: 11029.670898, mean_absolute_error: 172.976349, mean_q: -11.853931\n",
      " 1392/5000: episode: 1391, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.939, 0.997], loss: 21984.246094, mean_absolute_error: 180.058258, mean_q: -11.833512\n",
      " 1393/5000: episode: 1392, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.865 [0.795, 0.926], loss: 1464761.125000, mean_absolute_error: 263.577789, mean_q: -11.815577\n",
      " 1394/5000: episode: 1393, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.826 [0.749, 0.895], loss: 1473926.500000, mean_absolute_error: 270.583984, mean_q: -11.810730\n",
      " 1395/5000: episode: 1394, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.021], loss: 12677.976562, mean_absolute_error: 173.281738, mean_q: -11.816036\n",
      " 1396/5000: episode: 1395, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.287, 0.475], loss: 21969.001953, mean_absolute_error: 180.387909, mean_q: -11.817860\n",
      " 1397/5000: episode: 1396, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.829 [0.753, 0.898], loss: 12729.596680, mean_absolute_error: 173.521454, mean_q: -11.811020\n",
      " 1398/5000: episode: 1397, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.078, 0.212], loss: 32902.250000, mean_absolute_error: 187.560791, mean_q: -11.797164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1399/5000: episode: 1398, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.501, 0.691], loss: 1473645.375000, mean_absolute_error: 270.986755, mean_q: -11.778344\n",
      " 1400/5000: episode: 1399, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.851 [0.779, 0.915], loss: 69.312981, mean_absolute_error: 166.661011, mean_q: -11.772952\n",
      " 1401/5000: episode: 1400, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.982, 1.000], loss: 12739.904297, mean_absolute_error: 173.857941, mean_q: -11.773588\n",
      " 1402/5000: episode: 1401, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.869, 0.970], loss: 21939.484375, mean_absolute_error: 180.874695, mean_q: -11.772038\n",
      " 1403/5000: episode: 1402, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.370 [0.278, 0.465], loss: 26996.378906, mean_absolute_error: 181.047318, mean_q: -11.763239\n",
      " 1404/5000: episode: 1403, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 23593.796875, mean_absolute_error: 181.030060, mean_q: -11.754014\n",
      " 1405/5000: episode: 1404, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 21926.480469, mean_absolute_error: 181.105103, mean_q: -11.749750\n",
      " 1406/5000: episode: 1405, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.441, 0.634], loss: 68.981522, mean_absolute_error: 167.175385, mean_q: -11.744767\n",
      " 1407/5000: episode: 1406, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.976, 1.000], loss: 1484129.125000, mean_absolute_error: 278.620789, mean_q: -11.734900\n",
      " 1408/5000: episode: 1407, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 10992.017578, mean_absolute_error: 174.327301, mean_q: -11.731029\n",
      " 1409/5000: episode: 1408, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.159, 0.324], loss: 23629.466797, mean_absolute_error: 181.448608, mean_q: -11.721544\n",
      " 1410/5000: episode: 1409, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.905, 0.986], loss: 1473049.000000, mean_absolute_error: 271.820160, mean_q: -11.711667\n",
      " 1411/5000: episode: 1410, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.354 [0.264, 0.449], loss: 10986.441406, mean_absolute_error: 174.551514, mean_q: -11.712280\n",
      " 1412/5000: episode: 1411, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.507, 0.696], loss: 12729.663086, mean_absolute_error: 174.702728, mean_q: -11.706575\n",
      " 1413/5000: episode: 1412, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.888 [0.822, 0.943], loss: 21897.296875, mean_absolute_error: 181.684067, mean_q: -11.695057\n",
      " 1414/5000: episode: 1413, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 43718.359375, mean_absolute_error: 195.739685, mean_q: -11.683861\n",
      " 1415/5000: episode: 1414, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 21889.742188, mean_absolute_error: 181.819839, mean_q: -11.671341\n",
      " 1416/5000: episode: 1415, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.210, 0.387], loss: 1461852.500000, mean_absolute_error: 265.240234, mean_q: -11.653794\n",
      " 1417/5000: episode: 1416, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.641 [0.546, 0.732], loss: 43688.871094, mean_absolute_error: 195.932343, mean_q: -11.643059\n",
      " 1418/5000: episode: 1417, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.597], loss: 1496227.375000, mean_absolute_error: 286.414825, mean_q: -11.625788\n",
      " 1419/5000: episode: 1418, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.166, 0.332], loss: 12750.609375, mean_absolute_error: 175.224274, mean_q: -11.613438\n",
      " 1420/5000: episode: 1419, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.666, 0.833], loss: 23616.511719, mean_absolute_error: 182.250854, mean_q: -11.596514\n",
      " 1421/5000: episode: 1420, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.958, 1.000], loss: 67.137909, mean_absolute_error: 168.296234, mean_q: -11.586744\n",
      " 1422/5000: episode: 1421, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.515, 0.704], loss: 1475969.000000, mean_absolute_error: 272.821106, mean_q: -11.582325\n",
      " 1423/5000: episode: 1422, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.895, 0.982], loss: 1474085.500000, mean_absolute_error: 272.807251, mean_q: -11.586870\n",
      " 1424/5000: episode: 1423, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.036], loss: 67.332489, mean_absolute_error: 168.655075, mean_q: -11.603523\n",
      " 1425/5000: episode: 1424, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.060], loss: 10950.732422, mean_absolute_error: 175.762253, mean_q: -11.615259\n",
      " 1426/5000: episode: 1425, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 23542.488281, mean_absolute_error: 182.858078, mean_q: -11.618240\n",
      " 1427/5000: episode: 1426, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 10945.935547, mean_absolute_error: 175.956573, mean_q: -11.613221\n",
      " 1428/5000: episode: 1427, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.176, 0.344], loss: 10943.697266, mean_absolute_error: 176.031555, mean_q: -11.603225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1429/5000: episode: 1428, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.520, 0.708], loss: 36250.539062, mean_absolute_error: 190.176239, mean_q: -11.588581\n",
      " 1430/5000: episode: 1429, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 2922085.500000, mean_absolute_error: 363.753510, mean_q: -11.568130\n",
      " 1431/5000: episode: 1430, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.085 [0.037, 0.143], loss: 1870.389282, mean_absolute_error: 169.365692, mean_q: -11.565767\n",
      " 1432/5000: episode: 1431, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.869 [0.799, 0.929], loss: 66.802979, mean_absolute_error: 169.345581, mean_q: -11.557804\n",
      " 1433/5000: episode: 1432, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.958, 1.000], loss: 21799.332031, mean_absolute_error: 183.334915, mean_q: -11.550320\n",
      " 1434/5000: episode: 1433, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.925, 0.993], loss: 1495274.875000, mean_absolute_error: 287.737976, mean_q: -11.542027\n",
      " 1435/5000: episode: 1434, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [0.055, 0.175], loss: 10928.501953, mean_absolute_error: 176.552887, mean_q: -11.542611\n",
      " 1436/5000: episode: 1435, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.176, 0.345], loss: 21785.464844, mean_absolute_error: 183.605118, mean_q: -11.541077\n",
      " 1437/5000: episode: 1436, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 34437.519531, mean_absolute_error: 190.734436, mean_q: -11.537868\n",
      " 1438/5000: episode: 1437, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.062 [0.022, 0.113], loss: 1482332.625000, mean_absolute_error: 281.051086, mean_q: -11.531811\n",
      " 1439/5000: episode: 1438, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.857 [0.785, 0.920], loss: 1460558.875000, mean_absolute_error: 267.258545, mean_q: -11.530096\n",
      " 1440/5000: episode: 1439, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.278, 0.464], loss: 10915.023438, mean_absolute_error: 177.065125, mean_q: -11.533333\n",
      " 1441/5000: episode: 1440, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.834 [0.758, 0.902], loss: 1460429.875000, mean_absolute_error: 267.455078, mean_q: -11.528795\n",
      " 1442/5000: episode: 1441, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.426 [0.331, 0.523], loss: 66.486694, mean_absolute_error: 170.310974, mean_q: -11.530409\n",
      " 1443/5000: episode: 1442, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.058 [0.019, 0.107], loss: 1471150.750000, mean_absolute_error: 274.565216, mean_q: -11.525944\n",
      " 1444/5000: episode: 1443, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 1483687.750000, mean_absolute_error: 281.620667, mean_q: -11.526195\n",
      " 1445/5000: episode: 1444, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.820, 0.942], loss: 1471020.125000, mean_absolute_error: 274.746338, mean_q: -11.529373\n",
      " 1446/5000: episode: 1445, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.039], loss: 66.553139, mean_absolute_error: 170.692291, mean_q: -11.536169\n",
      " 1447/5000: episode: 1446, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.277 [0.194, 0.367], loss: 10901.994141, mean_absolute_error: 177.722595, mean_q: -11.541328\n",
      " 1448/5000: episode: 1447, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 21734.312500, mean_absolute_error: 184.746552, mean_q: -11.544193\n",
      " 1449/5000: episode: 1448, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.628, 0.802], loss: 1914.556763, mean_absolute_error: 171.092255, mean_q: -11.550759\n",
      " 1450/5000: episode: 1449, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.037], loss: 45241.171875, mean_absolute_error: 198.921097, mean_q: -11.559980\n",
      " 1451/5000: episode: 1450, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.509, 0.698], loss: 10895.372070, mean_absolute_error: 178.116730, mean_q: -11.558407\n",
      " 1452/5000: episode: 1451, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.937, 0.997], loss: 12697.030273, mean_absolute_error: 178.260498, mean_q: -11.556301\n",
      " 1453/5000: episode: 1452, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.430, 0.623], loss: 66.744675, mean_absolute_error: 171.363846, mean_q: -11.552758\n",
      " 1454/5000: episode: 1453, duration: 0.167s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.939 [0.888, 0.979], loss: 10889.722656, mean_absolute_error: 178.356506, mean_q: -11.544048\n",
      " 1455/5000: episode: 1454, duration: 0.135s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 12696.907227, mean_absolute_error: 178.479080, mean_q: -11.536156\n",
      " 1456/5000: episode: 1455, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.741 [0.653, 0.822], loss: 34360.203125, mean_absolute_error: 192.422607, mean_q: -11.533798\n",
      " 1457/5000: episode: 1456, duration: 0.097s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.707, 0.864], loss: 1832.345093, mean_absolute_error: 171.683655, mean_q: -11.533939\n",
      " 1458/5000: episode: 1457, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.607, 0.785], loss: 66.581673, mean_absolute_error: 171.779373, mean_q: -11.538642\n",
      " 1459/5000: episode: 1458, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.952, 0.999], loss: 10881.333008, mean_absolute_error: 178.787231, mean_q: -11.541393\n",
      " 1460/5000: episode: 1459, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.293, 0.482], loss: 10879.646484, mean_absolute_error: 178.865097, mean_q: -11.536709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1461/5000: episode: 1460, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.476 [0.380, 0.573], loss: 1840.272705, mean_absolute_error: 172.020782, mean_q: -11.526037\n",
      " 1462/5000: episode: 1461, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.910, 0.988], loss: 66.317078, mean_absolute_error: 172.078156, mean_q: -11.515690\n",
      " 1463/5000: episode: 1462, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.628, 0.802], loss: 10874.165039, mean_absolute_error: 179.059372, mean_q: -11.505875\n",
      " 1464/5000: episode: 1463, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.147, 0.308], loss: 10872.374023, mean_absolute_error: 179.111328, mean_q: -11.491409\n",
      " 1465/5000: episode: 1464, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.097], loss: 10870.320312, mean_absolute_error: 179.160309, mean_q: -11.472639\n",
      " 1466/5000: episode: 1465, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 23475.308594, mean_absolute_error: 186.166016, mean_q: -11.461093\n",
      " 1467/5000: episode: 1466, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: 1480551.875000, mean_absolute_error: 283.357635, mean_q: -11.454330\n",
      " 1468/5000: episode: 1467, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.726 [0.637, 0.809], loss: 1480455.000000, mean_absolute_error: 283.480255, mean_q: -11.469456\n",
      " 1469/5000: episode: 1468, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.573, 0.755], loss: 43248.835938, mean_absolute_error: 200.324921, mean_q: -11.500885\n",
      " 1470/5000: episode: 1469, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [0.065, 0.192], loss: 2931682.500000, mean_absolute_error: 374.103424, mean_q: -11.516664\n",
      " 1471/5000: episode: 1470, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.222, 0.400], loss: 10856.708008, mean_absolute_error: 179.938766, mean_q: -11.544456\n",
      " 1472/5000: episode: 1471, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.359 [0.268, 0.453], loss: 10854.593750, mean_absolute_error: 180.101898, mean_q: -11.571482\n",
      " 1473/5000: episode: 1472, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.572 [0.476, 0.667], loss: 36204.152344, mean_absolute_error: 194.232697, mean_q: -11.596777\n",
      " 1474/5000: episode: 1473, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.873, 0.971], loss: 23433.445312, mean_absolute_error: 187.292694, mean_q: -11.607487\n",
      " 1475/5000: episode: 1474, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.545, 0.730], loss: 1479696.000000, mean_absolute_error: 284.486267, mean_q: -11.607691\n",
      " 1476/5000: episode: 1475, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.054 [0.017, 0.103], loss: 21619.503906, mean_absolute_error: 187.523697, mean_q: -11.611191\n",
      " 1477/5000: episode: 1476, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.275, 0.462], loss: 3746.900146, mean_absolute_error: 173.933044, mean_q: -11.612249\n",
      " 1478/5000: episode: 1477, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.014, 0.095], loss: 10837.723633, mean_absolute_error: 180.858917, mean_q: -11.611357\n",
      " 1479/5000: episode: 1478, duration: 0.103s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.106], loss: 10835.371094, mean_absolute_error: 180.945404, mean_q: -11.603539\n",
      " 1480/5000: episode: 1479, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.051], loss: 1457828.000000, mean_absolute_error: 271.202240, mean_q: -11.589699\n",
      " 1481/5000: episode: 1480, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.540, 0.726], loss: 21592.902344, mean_absolute_error: 187.978546, mean_q: -11.582990\n",
      " 1482/5000: episode: 1481, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.410 [0.316, 0.507], loss: 21587.988281, mean_absolute_error: 188.048676, mean_q: -11.569872\n",
      " 1483/5000: episode: 1482, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.954, 1.000], loss: 10824.937500, mean_absolute_error: 181.233093, mean_q: -11.551826\n",
      " 1484/5000: episode: 1483, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.252 [0.171, 0.339], loss: 66.491638, mean_absolute_error: 174.409424, mean_q: -11.530836\n",
      " 1485/5000: episode: 1484, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.887 [0.821, 0.943], loss: 2936748.000000, mean_absolute_error: 382.288635, mean_q: -11.507401\n",
      " 1486/5000: episode: 1485, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.156, 0.319], loss: 10818.295898, mean_absolute_error: 181.403625, mean_q: -11.502770\n",
      " 1487/5000: episode: 1486, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.343 [0.253, 0.437], loss: 21566.191406, mean_absolute_error: 188.353149, mean_q: -11.497616\n",
      " 1488/5000: episode: 1487, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.885, 0.978], loss: 14605.026367, mean_absolute_error: 181.719498, mean_q: -11.496099\n",
      " 1489/5000: episode: 1488, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.654, 0.823], loss: 43046.171875, mean_absolute_error: 202.266251, mean_q: -11.491898\n",
      " 1490/5000: episode: 1489, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.413, 0.606], loss: 10807.806641, mean_absolute_error: 181.767166, mean_q: -11.479269\n",
      " 1491/5000: episode: 1490, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.898 [0.835, 0.951], loss: 1457336.375000, mean_absolute_error: 272.012146, mean_q: -11.462034\n",
      " 1492/5000: episode: 1491, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 12708.660156, mean_absolute_error: 181.990051, mean_q: -11.452372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1493/5000: episode: 1492, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.432 [0.337, 0.529], loss: 10799.154297, mean_absolute_error: 181.990326, mean_q: -11.436642\n",
      " 1494/5000: episode: 1493, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.051], loss: 10796.175781, mean_absolute_error: 182.053375, mean_q: -11.416354\n",
      " 1495/5000: episode: 1494, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.556, 0.740], loss: 23467.839844, mean_absolute_error: 189.055710, mean_q: -11.393103\n",
      " 1496/5000: episode: 1495, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.820 [0.742, 0.890], loss: 64.671982, mean_absolute_error: 175.313538, mean_q: -11.371948\n",
      " 1497/5000: episode: 1496, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.018], loss: 12629.974609, mean_absolute_error: 182.229919, mean_q: -11.359073\n",
      " 1498/5000: episode: 1497, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [0.076, 0.208], loss: 21507.113281, mean_absolute_error: 189.140488, mean_q: -11.346699\n",
      " 1499/5000: episode: 1498, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [0.059, 0.182], loss: 1458990.875000, mean_absolute_error: 272.624756, mean_q: -11.328911\n",
      " 1500/5000: episode: 1499, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.181, 0.351], loss: 64.142380, mean_absolute_error: 175.599335, mean_q: -11.325286\n",
      " 1501/5000: episode: 1500, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.150, 0.312], loss: 1456920.500000, mean_absolute_error: 272.694458, mean_q: -11.327543\n",
      " 1502/5000: episode: 1501, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 64.301331, mean_absolute_error: 175.802826, mean_q: -11.339312\n",
      " 1503/5000: episode: 1502, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 1926.225220, mean_absolute_error: 175.908646, mean_q: -11.342187\n",
      " 1504/5000: episode: 1503, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.667, 0.834], loss: 10775.237305, mean_absolute_error: 182.823700, mean_q: -11.343029\n",
      " 1505/5000: episode: 1504, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 21482.654297, mean_absolute_error: 189.743347, mean_q: -11.341980\n",
      " 1506/5000: episode: 1505, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.269], loss: 1467300.250000, mean_absolute_error: 279.973114, mean_q: -11.339413\n",
      " 1507/5000: episode: 1506, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [0.088, 0.226], loss: 64.457413, mean_absolute_error: 176.287842, mean_q: -11.353066\n",
      " 1508/5000: episode: 1507, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.938, 0.997], loss: 64.639999, mean_absolute_error: 176.416794, mean_q: -11.369136\n",
      " 1509/5000: episode: 1508, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.615, 0.791], loss: 32171.835938, mean_absolute_error: 197.012421, mean_q: -11.381014\n",
      " 1510/5000: episode: 1509, duration: 0.089s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.269], loss: 64.791656, mean_absolute_error: 176.632019, mean_q: -11.382466\n",
      " 1511/5000: episode: 1510, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.169, 0.337], loss: 32160.140625, mean_absolute_error: 197.183044, mean_q: -11.377619\n",
      " 1512/5000: episode: 1511, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.619 [0.523, 0.711], loss: 21457.312500, mean_absolute_error: 190.427704, mean_q: -11.365886\n",
      " 1513/5000: episode: 1512, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.941 [0.890, 0.980], loss: 21452.531250, mean_absolute_error: 190.484924, mean_q: -11.348648\n",
      " 1514/5000: episode: 1513, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.820 [0.742, 0.890], loss: 2013.484863, mean_absolute_error: 176.982269, mean_q: -11.333279\n",
      " 1515/5000: episode: 1514, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.396, 0.590], loss: 12634.525391, mean_absolute_error: 183.823334, mean_q: -11.331765\n",
      " 1516/5000: episode: 1515, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 10752.091797, mean_absolute_error: 183.931519, mean_q: -11.339153\n",
      " 1517/5000: episode: 1516, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 1498629.125000, mean_absolute_error: 301.410522, mean_q: -11.343534\n",
      " 1518/5000: episode: 1517, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.548, 0.734], loss: 21430.523438, mean_absolute_error: 190.971436, mean_q: -11.349384\n",
      " 1519/5000: episode: 1518, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [0.103, 0.248], loss: 64.380783, mean_absolute_error: 177.450058, mean_q: -11.346315\n",
      " 1520/5000: episode: 1519, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.507 [0.410, 0.604], loss: 10742.166016, mean_absolute_error: 184.342133, mean_q: -11.337276\n",
      " 1521/5000: episode: 1520, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.840 [0.765, 0.906], loss: 1468302.125000, mean_absolute_error: 281.400665, mean_q: -11.322631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1522/5000: episode: 1521, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.167, 0.334], loss: 64.022636, mean_absolute_error: 177.691635, mean_q: -11.314709\n",
      " 1523/5000: episode: 1522, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 63.876240, mean_absolute_error: 177.764069, mean_q: -11.301764\n",
      " 1524/5000: episode: 1523, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 12647.168945, mean_absolute_error: 184.640244, mean_q: -11.284468\n",
      " 1525/5000: episode: 1524, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.598], loss: 14532.610352, mean_absolute_error: 184.677887, mean_q: -11.262644\n",
      " 1526/5000: episode: 1525, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.426, 0.619], loss: 1455481.375000, mean_absolute_error: 274.816925, mean_q: -11.237167\n",
      " 1527/5000: episode: 1526, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.390, 0.583], loss: 62.969784, mean_absolute_error: 177.977325, mean_q: -11.221279\n",
      " 1528/5000: episode: 1527, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 14652.032227, mean_absolute_error: 184.938751, mean_q: -11.201647\n",
      " 1529/5000: episode: 1528, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.181], loss: 1455391.875000, mean_absolute_error: 274.969116, mean_q: -11.183586\n",
      " 1530/5000: episode: 1529, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.916 [0.857, 0.963], loss: 62.500687, mean_absolute_error: 178.157959, mean_q: -11.179401\n",
      " 1531/5000: episode: 1530, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.938 [0.887, 0.978], loss: 12679.320312, mean_absolute_error: 185.060699, mean_q: -11.169481\n",
      " 1532/5000: episode: 1531, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.130, 0.285], loss: 1465912.250000, mean_absolute_error: 281.944305, mean_q: -11.155014\n",
      " 1533/5000: episode: 1532, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.719, 0.874], loss: 1455200.750000, mean_absolute_error: 275.233154, mean_q: -11.154098\n",
      " 1534/5000: episode: 1533, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.923, 0.993], loss: 62.396027, mean_absolute_error: 178.469299, mean_q: -11.170036\n",
      " 1535/5000: episode: 1534, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.174, 0.342], loss: 62.534515, mean_absolute_error: 178.577057, mean_q: -11.182426\n",
      " 1536/5000: episode: 1535, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.845 [0.771, 0.910], loss: 10711.485352, mean_absolute_error: 185.444824, mean_q: -11.186625\n",
      " 1537/5000: episode: 1536, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.913 [0.855, 0.962], loss: 14742.869141, mean_absolute_error: 185.697571, mean_q: -11.182898\n",
      " 1538/5000: episode: 1537, duration: 0.098s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.030], loss: 12718.349609, mean_absolute_error: 185.655945, mean_q: -11.171956\n",
      " 1539/5000: episode: 1538, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.582 [0.486, 0.677], loss: 21351.861328, mean_absolute_error: 192.400665, mean_q: -11.155851\n",
      " 1540/5000: episode: 1539, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.291, 0.479], loss: 1465463.750000, mean_absolute_error: 282.519073, mean_q: -11.135193\n",
      " 1541/5000: episode: 1540, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.036], loss: 61.865078, mean_absolute_error: 178.950867, mean_q: -11.122406\n",
      " 1542/5000: episode: 1541, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.029], loss: 61.673191, mean_absolute_error: 179.003876, mean_q: -11.105141\n",
      " 1543/5000: episode: 1542, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.127, 0.281], loss: 25277.375000, mean_absolute_error: 192.679993, mean_q: -11.085157\n",
      " 1544/5000: episode: 1543, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.830 [0.754, 0.898], loss: 1477890.250000, mean_absolute_error: 289.515167, mean_q: -11.072179\n",
      " 1545/5000: episode: 1544, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 2909160.250000, mean_absolute_error: 372.910583, mean_q: -11.075081\n",
      " 1546/5000: episode: 1545, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.293, 0.481], loss: 21322.380859, mean_absolute_error: 192.923828, mean_q: -11.098804\n",
      " 1547/5000: episode: 1546, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.077 [0.032, 0.133], loss: 1465033.750000, mean_absolute_error: 283.126099, mean_q: -11.115633\n",
      " 1548/5000: episode: 1547, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [0.090, 0.229], loss: 12628.512695, mean_absolute_error: 186.441772, mean_q: -11.133791\n",
      " 1549/5000: episode: 1548, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 33877.527344, mean_absolute_error: 200.087097, mean_q: -11.141273\n",
      " 1550/5000: episode: 1549, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 21302.992188, mean_absolute_error: 193.418716, mean_q: -11.138551\n",
      " 1551/5000: episode: 1550, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.415 [0.321, 0.512], loss: 61.938202, mean_absolute_error: 179.985092, mean_q: -11.128977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1552/5000: episode: 1551, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 61.849152, mean_absolute_error: 180.067230, mean_q: -11.120974\n",
      " 1553/5000: episode: 1552, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 4075.879883, mean_absolute_error: 180.250824, mean_q: -11.113628\n",
      " 1554/5000: episode: 1553, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.659, 0.827], loss: 2939708.500000, mean_absolute_error: 394.077637, mean_q: -11.106354\n",
      " 1555/5000: episode: 1554, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.857, 0.963], loss: 61.847626, mean_absolute_error: 180.357040, mean_q: -11.120836\n",
      " 1556/5000: episode: 1555, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.954, 1.000], loss: 21276.703125, mean_absolute_error: 193.979248, mean_q: -11.131554\n",
      " 1557/5000: episode: 1556, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 10667.033203, mean_absolute_error: 187.342316, mean_q: -11.137365\n",
      " 1558/5000: episode: 1557, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.367, 0.560], loss: 12741.619141, mean_absolute_error: 187.538589, mean_q: -11.135706\n",
      " 1559/5000: episode: 1558, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.115, 0.266], loss: 1474804.000000, mean_absolute_error: 291.040161, mean_q: -11.133739\n",
      " 1560/5000: episode: 1559, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.718 [0.628, 0.802], loss: 1466180.500000, mean_absolute_error: 284.488434, mean_q: -11.141691\n",
      " 1561/5000: episode: 1560, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.739 [0.651, 0.821], loss: 62.193909, mean_absolute_error: 181.018829, mean_q: -11.151928\n",
      " 1562/5000: episode: 1561, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 33933.964844, mean_absolute_error: 201.444641, mean_q: -11.153199\n",
      " 1563/5000: episode: 1562, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.048], loss: 10654.222656, mean_absolute_error: 187.959381, mean_q: -11.151092\n",
      " 1564/5000: episode: 1563, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.078 [0.032, 0.134], loss: 62.210686, mean_absolute_error: 181.332397, mean_q: -11.153433\n",
      " 1565/5000: episode: 1564, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.006, 0.071], loss: 10649.489258, mean_absolute_error: 188.168701, mean_q: -11.153870\n",
      " 1566/5000: episode: 1565, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.084 [0.036, 0.142], loss: 62.204929, mean_absolute_error: 181.533432, mean_q: -11.152916\n",
      " 1567/5000: episode: 1566, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.192, 0.365], loss: 62.251434, mean_absolute_error: 181.641556, mean_q: -11.157085\n",
      " 1568/5000: episode: 1567, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.253], loss: 62.409554, mean_absolute_error: 181.766785, mean_q: -11.171247\n",
      " 1569/5000: episode: 1568, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.583, 0.764], loss: 62.592598, mean_absolute_error: 181.893478, mean_q: -11.187618\n",
      " 1570/5000: episode: 1569, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [0.095, 0.237], loss: 10640.770508, mean_absolute_error: 188.721252, mean_q: -11.194786\n",
      " 1571/5000: episode: 1570, duration: 0.098s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 62.662888, mean_absolute_error: 182.079926, mean_q: -11.193899\n",
      " 1572/5000: episode: 1571, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.222, 0.400], loss: 1452679.250000, mean_absolute_error: 278.851807, mean_q: -11.186707\n",
      " 1573/5000: episode: 1572, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 21210.771484, mean_absolute_error: 195.673126, mean_q: -11.186233\n",
      " 1574/5000: episode: 1573, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 21207.660156, mean_absolute_error: 195.739136, mean_q: -11.178362\n",
      " 1575/5000: episode: 1574, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.036], loss: 62.467209, mean_absolute_error: 182.381989, mean_q: -11.176406\n",
      " 1576/5000: episode: 1575, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.701, 0.860], loss: 1494726.000000, mean_absolute_error: 306.047089, mean_q: -11.178883\n",
      " 1577/5000: episode: 1576, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.938 [0.886, 0.978], loss: 1464962.250000, mean_absolute_error: 286.001953, mean_q: -11.183258\n",
      " 1578/5000: episode: 1577, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.821 [0.743, 0.891], loss: 10627.186523, mean_absolute_error: 189.438385, mean_q: -11.195782\n",
      " 1579/5000: episode: 1578, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.569, 0.752], loss: 21186.267578, mean_absolute_error: 196.293198, mean_q: -11.209005\n",
      " 1580/5000: episode: 1579, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.093 [0.043, 0.154], loss: 2085.179932, mean_absolute_error: 183.002243, mean_q: -11.216660\n",
      " 1581/5000: episode: 1580, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.856 [0.783, 0.919], loss: 10618.610352, mean_absolute_error: 189.820786, mean_q: -11.214826\n",
      " 1582/5000: episode: 1581, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.638, 0.810], loss: 4168.622070, mean_absolute_error: 183.259750, mean_q: -11.204432\n",
      " 1583/5000: episode: 1582, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.419, 0.613], loss: 1483599.250000, mean_absolute_error: 300.055939, mean_q: -11.187834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1584/5000: episode: 1583, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 62.477947, mean_absolute_error: 183.387970, mean_q: -11.177366\n",
      " 1585/5000: episode: 1584, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.399, 0.592], loss: 62.305244, mean_absolute_error: 183.468445, mean_q: -11.161907\n",
      " 1586/5000: episode: 1585, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.882 [0.816, 0.939], loss: 31683.228516, mean_absolute_error: 203.623169, mean_q: -11.143322\n",
      " 1587/5000: episode: 1586, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.922 [0.866, 0.968], loss: 2114.732910, mean_absolute_error: 183.612000, mean_q: -11.119744\n",
      " 1588/5000: episode: 1587, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.353, 0.546], loss: 1453806.625000, mean_absolute_error: 280.289062, mean_q: -11.093636\n",
      " 1589/5000: episode: 1588, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.845], loss: 10594.381836, mean_absolute_error: 190.395645, mean_q: -11.077497\n",
      " 1590/5000: episode: 1589, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.862 [0.791, 0.924], loss: 10591.611328, mean_absolute_error: 190.450027, mean_q: -11.057827\n",
      " 1591/5000: episode: 1590, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.924, 0.993], loss: 1472738.875000, mean_absolute_error: 293.817078, mean_q: -11.034840\n",
      " 1592/5000: episode: 1591, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.069 [0.026, 0.122], loss: 60.736374, mean_absolute_error: 183.871521, mean_q: -11.020468\n",
      " 1593/5000: episode: 1592, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.314, 0.504], loss: 60.538216, mean_absolute_error: 183.925735, mean_q: -11.002474\n",
      " 1594/5000: episode: 1593, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.953 [0.908, 0.987], loss: 60.312485, mean_absolute_error: 183.970459, mean_q: -10.981939\n",
      " 1595/5000: episode: 1594, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.467, 0.659], loss: 60.059956, mean_absolute_error: 184.002838, mean_q: -10.958922\n",
      " 1596/5000: episode: 1595, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.911, 0.988], loss: 21099.240234, mean_absolute_error: 197.387772, mean_q: -10.933757\n",
      " 1597/5000: episode: 1596, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.247, 0.430], loss: 59.474995, mean_absolute_error: 184.058197, mean_q: -10.905419\n",
      " 1598/5000: episode: 1597, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.458, 0.651], loss: 59.217369, mean_absolute_error: 184.097168, mean_q: -10.881772\n",
      " 1599/5000: episode: 1598, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.342, 0.534], loss: 10573.485352, mean_absolute_error: 190.815613, mean_q: -10.861903\n",
      " 1600/5000: episode: 1599, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.149], loss: 23151.710938, mean_absolute_error: 197.570526, mean_q: -10.851198\n",
      " 1601/5000: episode: 1600, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 1472406.500000, mean_absolute_error: 294.262817, mean_q: -10.851751\n",
      " 1602/5000: episode: 1601, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 1463978.000000, mean_absolute_error: 287.805054, mean_q: -10.862061\n",
      " 1603/5000: episode: 1602, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.127], loss: 21073.542969, mean_absolute_error: 197.884644, mean_q: -10.874790\n",
      " 1604/5000: episode: 1603, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.138, 0.296], loss: 10563.688477, mean_absolute_error: 191.336823, mean_q: -10.882889\n",
      " 1605/5000: episode: 1604, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.346 [0.256, 0.440], loss: 1463632.500000, mean_absolute_error: 288.058533, mean_q: -10.887398\n",
      " 1606/5000: episode: 1605, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 59.362465, mean_absolute_error: 184.916473, mean_q: -10.895097\n",
      " 1607/5000: episode: 1606, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.867 [0.797, 0.928], loss: 1461410.000000, mean_absolute_error: 288.266602, mean_q: -10.894831\n",
      " 1608/5000: episode: 1607, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.477, 0.668], loss: 1450839.625000, mean_absolute_error: 281.711334, mean_q: -10.899538\n",
      " 1609/5000: episode: 1608, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.956 [0.912, 0.989], loss: 1461252.625000, mean_absolute_error: 288.479492, mean_q: -10.909472\n",
      " 1610/5000: episode: 1609, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.176, 0.345], loss: 10550.599609, mean_absolute_error: 192.026382, mean_q: -10.921749\n",
      " 1611/5000: episode: 1610, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.957 [0.913, 0.989], loss: 12646.070312, mean_absolute_error: 192.150482, mean_q: -10.923414\n",
      " 1612/5000: episode: 1611, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.938 [0.887, 0.978], loss: 31517.320312, mean_absolute_error: 205.527588, mean_q: -10.916449\n",
      " 1613/5000: episode: 1612, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.055 [0.018, 0.104], loss: 1471471.375000, mean_absolute_error: 295.511414, mean_q: -10.902193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1614/5000: episode: 1613, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.873, 0.971], loss: 12624.080078, mean_absolute_error: 192.396820, mean_q: -10.894911\n",
      " 1615/5000: episode: 1614, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.228, 0.408], loss: 10536.927734, mean_absolute_error: 192.484894, mean_q: -10.888063\n",
      " 1616/5000: episode: 1615, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 10534.476562, mean_absolute_error: 192.597916, mean_q: -10.893282\n",
      " 1617/5000: episode: 1616, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.857, 0.963], loss: 12709.222656, mean_absolute_error: 192.818375, mean_q: -10.907915\n",
      " 1618/5000: episode: 1617, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 10529.695312, mean_absolute_error: 192.881378, mean_q: -10.923166\n",
      " 1619/5000: episode: 1618, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.862 [0.791, 0.924], loss: 10527.766602, mean_absolute_error: 193.023727, mean_q: -10.940521\n",
      " 1620/5000: episode: 1619, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 12640.271484, mean_absolute_error: 193.161774, mean_q: -10.952782\n",
      " 1621/5000: episode: 1620, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.368, 0.561], loss: 60.024540, mean_absolute_error: 186.629181, mean_q: -10.955690\n",
      " 1622/5000: episode: 1621, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.700, 0.859], loss: 59.984409, mean_absolute_error: 186.714127, mean_q: -10.952027\n",
      " 1623/5000: episode: 1622, duration: 0.110s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.910, 0.988], loss: 59.882568, mean_absolute_error: 186.785065, mean_q: -10.942726\n",
      " 1624/5000: episode: 1623, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.341, 0.533], loss: 4336.625977, mean_absolute_error: 186.903778, mean_q: -10.934799\n",
      " 1625/5000: episode: 1624, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.605, 0.783], loss: 2290.148438, mean_absolute_error: 187.028580, mean_q: -10.926862\n",
      " 1626/5000: episode: 1625, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.887 [0.822, 0.943], loss: 1449644.625000, mean_absolute_error: 283.474213, mean_q: -10.913759\n",
      " 1627/5000: episode: 1626, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.844], loss: 17031.933594, mean_absolute_error: 193.832275, mean_q: -10.914576\n",
      " 1628/5000: episode: 1627, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.632, 0.805], loss: 2239.271973, mean_absolute_error: 187.195908, mean_q: -10.912457\n",
      " 1629/5000: episode: 1628, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.876 [0.808, 0.935], loss: 10510.471680, mean_absolute_error: 193.840546, mean_q: -10.909944\n",
      " 1630/5000: episode: 1629, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.319 [0.231, 0.411], loss: 20958.558594, mean_absolute_error: 200.540375, mean_q: -10.906618\n",
      " 1631/5000: episode: 1630, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.726, 0.879], loss: 59.368553, mean_absolute_error: 187.361237, mean_q: -10.895656\n",
      " 1632/5000: episode: 1631, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [0.011, 0.087], loss: 31397.707031, mean_absolute_error: 207.270294, mean_q: -10.879913\n",
      " 1633/5000: episode: 1632, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [0.086, 0.223], loss: 10503.111328, mean_absolute_error: 194.081131, mean_q: -10.859027\n",
      " 1634/5000: episode: 1633, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.159, 0.323], loss: 20942.894531, mean_absolute_error: 200.733231, mean_q: -10.834478\n",
      " 1635/5000: episode: 1634, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.082 [0.035, 0.140], loss: 1472320.875000, mean_absolute_error: 297.274689, mean_q: -10.806845\n",
      " 1636/5000: episode: 1635, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.733, 0.884], loss: 2208.097656, mean_absolute_error: 187.628174, mean_q: -10.787544\n",
      " 1637/5000: episode: 1636, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.863 [0.792, 0.925], loss: 57.946156, mean_absolute_error: 187.666245, mean_q: -10.764329\n",
      " 1638/5000: episode: 1637, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.957, 1.000], loss: 1459630.625000, mean_absolute_error: 290.785919, mean_q: -10.738933\n",
      " 1639/5000: episode: 1638, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.979, 1.000], loss: 20917.113281, mean_absolute_error: 200.987640, mean_q: -10.723406\n",
      " 1640/5000: episode: 1639, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.268 [0.185, 0.356], loss: 57.288513, mean_absolute_error: 187.828705, mean_q: -10.703066\n",
      " 1641/5000: episode: 1640, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.876 [0.808, 0.934], loss: 23044.074219, mean_absolute_error: 201.094666, mean_q: -10.686537\n",
      " 1642/5000: episode: 1641, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.926, 0.994], loss: 2194.533447, mean_absolute_error: 187.959854, mean_q: -10.668973\n",
      " 1643/5000: episode: 1642, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.317 [0.229, 0.409], loss: 1469872.625000, mean_absolute_error: 297.672058, mean_q: -10.647057\n",
      " 1644/5000: episode: 1643, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.996], loss: 56.549423, mean_absolute_error: 188.099701, mean_q: -10.633794\n",
      " 1645/5000: episode: 1644, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.081 [0.034, 0.138], loss: 20885.976562, mean_absolute_error: 201.351837, mean_q: -10.616306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1646/5000: episode: 1645, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.650, 0.820], loss: 1451152.250000, mean_absolute_error: 284.749939, mean_q: -10.594120\n",
      " 1647/5000: episode: 1646, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.289, 0.477], loss: 55.989971, mean_absolute_error: 188.298325, mean_q: -10.581057\n",
      " 1648/5000: episode: 1647, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 20870.691406, mean_absolute_error: 201.535889, mean_q: -10.564297\n",
      " 1649/5000: episode: 1648, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 2236.677490, mean_absolute_error: 188.459991, mean_q: -10.549323\n",
      " 1650/5000: episode: 1649, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.885, 0.977], loss: 1471755.750000, mean_absolute_error: 298.132843, mean_q: -10.536534\n",
      " 1651/5000: episode: 1650, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 10456.273438, mean_absolute_error: 195.166473, mean_q: -10.530575\n",
      " 1652/5000: episode: 1651, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.530, 0.718], loss: 55.323620, mean_absolute_error: 188.657318, mean_q: -10.517899\n",
      " 1653/5000: episode: 1652, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.024], loss: 10451.339844, mean_absolute_error: 195.298218, mean_q: -10.501161\n",
      " 1654/5000: episode: 1653, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 1473733.250000, mean_absolute_error: 298.377441, mean_q: -10.481375\n",
      " 1655/5000: episode: 1654, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: 54.814655, mean_absolute_error: 188.839798, mean_q: -10.469401\n",
      " 1656/5000: episode: 1655, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.866 [0.796, 0.927], loss: 20835.468750, mean_absolute_error: 202.045532, mean_q: -10.454401\n",
      " 1657/5000: episode: 1656, duration: 0.093s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.093], loss: 10442.865234, mean_absolute_error: 195.519180, mean_q: -10.434376\n",
      " 1658/5000: episode: 1657, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.210, 0.386], loss: 10440.447266, mean_absolute_error: 195.578979, mean_q: -10.417011\n",
      " 1659/5000: episode: 1658, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.943, 0.998], loss: 10438.197266, mean_absolute_error: 195.671661, mean_q: -10.414359\n",
      " 1660/5000: episode: 1659, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 12697.589844, mean_absolute_error: 195.872131, mean_q: -10.423716\n",
      " 1661/5000: episode: 1660, duration: 0.123s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 10435.088867, mean_absolute_error: 195.894806, mean_q: -10.429768\n",
      " 1662/5000: episode: 1661, duration: 0.098s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.098, 0.241], loss: 10432.122070, mean_absolute_error: 195.986023, mean_q: -10.426887\n",
      " 1663/5000: episode: 1662, duration: 0.092s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.493, 0.684], loss: 1481481.250000, mean_absolute_error: 305.612976, mean_q: -10.417429\n",
      " 1664/5000: episode: 1663, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.711, 0.868], loss: 31172.816406, mean_absolute_error: 209.279938, mean_q: -10.413316\n",
      " 1665/5000: episode: 1664, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [0.071, 0.200], loss: 1458412.000000, mean_absolute_error: 292.631012, mean_q: -10.401361\n",
      " 1666/5000: episode: 1665, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.846 [0.772, 0.911], loss: 1447982.250000, mean_absolute_error: 286.170715, mean_q: -10.396910\n",
      " 1667/5000: episode: 1666, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.941 [0.890, 0.980], loss: 20783.250000, mean_absolute_error: 203.005646, mean_q: -10.398727\n",
      " 1668/5000: episode: 1667, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.074 [0.030, 0.129], loss: 12608.138672, mean_absolute_error: 196.554214, mean_q: -10.391983\n",
      " 1669/5000: episode: 1668, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.136, 0.293], loss: 2318.535645, mean_absolute_error: 190.146774, mean_q: -10.379065\n",
      " 1670/5000: episode: 1669, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.834 [0.758, 0.902], loss: 12676.494141, mean_absolute_error: 196.753998, mean_q: -10.361820\n",
      " 1671/5000: episode: 1670, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 10408.115234, mean_absolute_error: 196.730774, mean_q: -10.340565\n",
      " 1672/5000: episode: 1671, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.517, 0.706], loss: 2360.191406, mean_absolute_error: 190.332596, mean_q: -10.316366\n",
      " 1673/5000: episode: 1672, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.664 [0.570, 0.753], loss: 12593.734375, mean_absolute_error: 196.818665, mean_q: -10.296959\n",
      " 1674/5000: episode: 1673, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.371 [0.279, 0.466], loss: 10401.704102, mean_absolute_error: 196.866440, mean_q: -10.279999\n",
      " 1675/5000: episode: 1674, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [0.054, 0.173], loss: 31094.636719, mean_absolute_error: 209.987061, mean_q: -10.259661\n",
      " 1676/5000: episode: 1675, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.308, 0.498], loss: 1447630.375000, mean_absolute_error: 286.775208, mean_q: -10.234919\n",
      " 1677/5000: episode: 1676, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.178 [0.108, 0.256], loss: 10395.218750, mean_absolute_error: 197.011948, mean_q: -10.221058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1678/5000: episode: 1677, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.750, 0.896], loss: 2258.469971, mean_absolute_error: 190.540039, mean_q: -10.203148\n",
      " 1679/5000: episode: 1678, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.879 [0.811, 0.937], loss: 20730.609375, mean_absolute_error: 203.649261, mean_q: -10.188164\n",
      " 1680/5000: episode: 1679, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.080], loss: 1449776.250000, mean_absolute_error: 287.072754, mean_q: -10.174191\n",
      " 1681/5000: episode: 1680, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 10387.328125, mean_absolute_error: 197.250885, mean_q: -10.167971\n",
      " 1682/5000: episode: 1681, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.556 [0.459, 0.652], loss: 10385.231445, mean_absolute_error: 197.317505, mean_q: -10.155914\n",
      " 1683/5000: episode: 1682, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.470, 0.662], loss: 41378.406250, mean_absolute_error: 216.945801, mean_q: -10.139397\n",
      " 1684/5000: episode: 1683, duration: 0.108s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.504 [0.407, 0.601], loss: 20709.945312, mean_absolute_error: 203.944199, mean_q: -10.117765\n",
      " 1685/5000: episode: 1684, duration: 0.043s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 12617.353516, mean_absolute_error: 197.507736, mean_q: -10.098799\n",
      " 1686/5000: episode: 1685, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.167, 0.334], loss: 20700.216797, mean_absolute_error: 204.056229, mean_q: -10.081186\n",
      " 1687/5000: episode: 1686, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.969, 1.000], loss: 1470126.125000, mean_absolute_error: 300.458954, mean_q: -10.058508\n",
      " 1688/5000: episode: 1687, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.220, 0.398], loss: 20688.833984, mean_absolute_error: 204.179810, mean_q: -10.044236\n",
      " 1689/5000: episode: 1688, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [0.093, 0.234], loss: 50.268204, mean_absolute_error: 191.213531, mean_q: -10.025784\n",
      " 1690/5000: episode: 1689, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.589, 0.770], loss: 1449376.000000, mean_absolute_error: 287.605347, mean_q: -10.005192\n",
      " 1691/5000: episode: 1690, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.265 [0.183, 0.354], loss: 10362.115234, mean_absolute_error: 197.848236, mean_q: -9.994427\n",
      " 1692/5000: episode: 1691, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.928, 0.994], loss: 1447076.000000, mean_absolute_error: 287.719971, mean_q: -9.978895\n",
      " 1693/5000: episode: 1692, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.044], loss: 10357.857422, mean_absolute_error: 197.984497, mean_q: -9.972121\n",
      " 1694/5000: episode: 1693, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.354, 0.547], loss: 30968.224609, mean_absolute_error: 211.057953, mean_q: -9.960178\n",
      " 1695/5000: episode: 1694, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.901, 0.984], loss: 49.432430, mean_absolute_error: 191.602066, mean_q: -9.942080\n",
      " 1696/5000: episode: 1695, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 12688.576172, mean_absolute_error: 198.259735, mean_q: -9.921679\n",
      " 1697/5000: episode: 1696, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.391, 0.585], loss: 10348.376953, mean_absolute_error: 198.198654, mean_q: -9.897301\n",
      " 1698/5000: episode: 1697, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 4553.793457, mean_absolute_error: 191.792603, mean_q: -9.870445\n",
      " 1699/5000: episode: 1698, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.071 [0.028, 0.126], loss: 1449112.875000, mean_absolute_error: 288.083832, mean_q: -9.842010\n",
      " 1700/5000: episode: 1699, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.683, 0.846], loss: 10341.417969, mean_absolute_error: 198.314514, mean_q: -9.824788\n",
      " 1701/5000: episode: 1700, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.894 [0.831, 0.948], loss: 10339.273438, mean_absolute_error: 198.357712, mean_q: -9.803761\n",
      " 1702/5000: episode: 1701, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.506 [0.409, 0.603], loss: 47.833233, mean_absolute_error: 191.904190, mean_q: -9.779923\n",
      " 1703/5000: episode: 1702, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.051, 0.168], loss: 10335.028320, mean_absolute_error: 198.439575, mean_q: -9.760771\n",
      " 1704/5000: episode: 1703, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.824 [0.747, 0.894], loss: 12560.173828, mean_absolute_error: 198.489929, mean_q: -9.743684\n",
      " 1705/5000: episode: 1704, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 2275.357666, mean_absolute_error: 192.042999, mean_q: -9.722416\n",
      " 1706/5000: episode: 1705, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.955, 1.000], loss: 30892.623047, mean_absolute_error: 211.531097, mean_q: -9.698420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1707/5000: episode: 1706, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.824 [0.747, 0.893], loss: 22922.339844, mean_absolute_error: 205.167725, mean_q: -9.676786\n",
      " 1708/5000: episode: 1707, duration: 0.086s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.513 [0.416, 0.610], loss: 2364.511719, mean_absolute_error: 192.266174, mean_q: -9.657597\n",
      " 1709/5000: episode: 1708, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.892, 0.981], loss: 1456898.000000, mean_absolute_error: 295.021362, mean_q: -9.641970\n",
      " 1710/5000: episode: 1709, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.356, 0.549], loss: 20592.019531, mean_absolute_error: 205.299530, mean_q: -9.640327\n",
      " 1711/5000: episode: 1710, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.042], loss: 46.396168, mean_absolute_error: 192.429901, mean_q: -9.631878\n",
      " 1712/5000: episode: 1711, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 46.267681, mean_absolute_error: 192.495117, mean_q: -9.618530\n",
      " 1713/5000: episode: 1712, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.494, 0.684], loss: 46.098953, mean_absolute_error: 192.548828, mean_q: -9.600973\n",
      " 1714/5000: episode: 1713, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 1471456.625000, mean_absolute_error: 301.851135, mean_q: -9.587372\n",
      " 1715/5000: episode: 1714, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.539, 0.726], loss: 12585.178711, mean_absolute_error: 199.195038, mean_q: -9.586859\n",
      " 1716/5000: episode: 1715, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 1446306.500000, mean_absolute_error: 289.039093, mean_q: -9.579664\n",
      " 1717/5000: episode: 1716, duration: 0.110s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [0.065, 0.191], loss: 2380.071289, mean_absolute_error: 192.945450, mean_q: -9.579855\n",
      " 1718/5000: episode: 1717, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.440 [0.345, 0.537], loss: 41070.835938, mean_absolute_error: 218.796738, mean_q: -9.571564\n",
      " 1719/5000: episode: 1718, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.822 [0.745, 0.892], loss: 10298.787109, mean_absolute_error: 199.477753, mean_q: -9.555796\n",
      " 1720/5000: episode: 1719, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.505, 0.694], loss: 1456384.000000, mean_absolute_error: 295.792969, mean_q: -9.536240\n",
      " 1721/5000: episode: 1720, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 10293.212891, mean_absolute_error: 199.616226, mean_q: -9.526221\n",
      " 1722/5000: episode: 1721, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.235, 0.416], loss: 10290.523438, mean_absolute_error: 199.686493, mean_q: -9.511544\n",
      " 1723/5000: episode: 1722, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.196, 0.369], loss: 22806.865234, mean_absolute_error: 206.210419, mean_q: -9.493046\n",
      " 1724/5000: episode: 1723, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.315, 0.505], loss: 12658.920898, mean_absolute_error: 199.893463, mean_q: -9.470442\n",
      " 1725/5000: episode: 1724, duration: 0.089s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.632 [0.537, 0.723], loss: 1445978.000000, mean_absolute_error: 289.627869, mean_q: -9.444199\n",
      " 1726/5000: episode: 1725, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.734, 0.884], loss: 44.534622, mean_absolute_error: 193.466583, mean_q: -9.436649\n",
      " 1727/5000: episode: 1726, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.062 [0.022, 0.113], loss: 44.544342, mean_absolute_error: 193.560455, mean_q: -9.437679\n",
      " 1728/5000: episode: 1727, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.611, 0.788], loss: 44.613579, mean_absolute_error: 193.666351, mean_q: -9.445013\n",
      " 1729/5000: episode: 1728, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.448 [0.353, 0.545], loss: 44.658558, mean_absolute_error: 193.768066, mean_q: -9.449773\n",
      " 1730/5000: episode: 1729, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.287, 0.475], loss: 44.637394, mean_absolute_error: 193.856216, mean_q: -9.447533\n",
      " 1731/5000: episode: 1730, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.890 [0.825, 0.945], loss: 10270.932617, mean_absolute_error: 200.363785, mean_q: -9.439899\n",
      " 1732/5000: episode: 1731, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.546, 0.731], loss: 44.444275, mean_absolute_error: 193.984985, mean_q: -9.427073\n",
      " 1733/5000: episode: 1732, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.635, 0.808], loss: 4833.770508, mean_absolute_error: 194.243866, mean_q: -9.409914\n",
      " 1734/5000: episode: 1733, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [0.085, 0.222], loss: 1455759.875000, mean_absolute_error: 296.728149, mean_q: -9.387733\n",
      " 1735/5000: episode: 1734, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.117, 0.267], loss: 12656.170898, mean_absolute_error: 200.683273, mean_q: -9.375549\n",
      " 1736/5000: episode: 1735, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.432 [0.337, 0.528], loss: 1455683.625000, mean_absolute_error: 296.848480, mean_q: -9.358862\n",
      " 1737/5000: episode: 1736, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.838 [0.763, 0.905], loss: 1445419.500000, mean_absolute_error: 290.497589, mean_q: -9.350492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1738/5000: episode: 1737, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.460 [0.364, 0.557], loss: 2335.430908, mean_absolute_error: 194.425079, mean_q: -9.362991\n",
      " 1739/5000: episode: 1738, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.068 [0.026, 0.121], loss: 22749.658203, mean_absolute_error: 207.398972, mean_q: -9.379542\n",
      " 1740/5000: episode: 1739, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 20458.880859, mean_absolute_error: 207.503815, mean_q: -9.385485\n",
      " 1741/5000: episode: 1740, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.535 [0.438, 0.631], loss: 1449884.750000, mean_absolute_error: 291.094788, mean_q: -9.382544\n",
      " 1742/5000: episode: 1741, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.830 [0.754, 0.898], loss: 1455241.125000, mean_absolute_error: 297.471680, mean_q: -9.392721\n",
      " 1743/5000: episode: 1742, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.524, 0.712], loss: 30646.519531, mean_absolute_error: 214.267715, mean_q: -9.413568\n",
      " 1744/5000: episode: 1743, duration: 0.108s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.640, 0.812], loss: 14936.072266, mean_absolute_error: 201.642853, mean_q: -9.429327\n",
      " 1745/5000: episode: 1744, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 44.574318, mean_absolute_error: 195.277710, mean_q: -9.440855\n",
      " 1746/5000: episode: 1745, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.711, 0.867], loss: 30627.531250, mean_absolute_error: 214.613525, mean_q: -9.444141\n",
      " 1747/5000: episode: 1746, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.136, 0.294], loss: 43242.996094, mean_absolute_error: 221.212784, mean_q: -9.438549\n",
      " 1748/5000: episode: 1747, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.597, 0.776], loss: 12580.465820, mean_absolute_error: 202.006638, mean_q: -9.431257\n",
      " 1749/5000: episode: 1748, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.931 [0.878, 0.974], loss: 10229.806641, mean_absolute_error: 202.070511, mean_q: -9.424565\n",
      " 1750/5000: episode: 1749, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.399, 0.592], loss: 10228.395508, mean_absolute_error: 202.144745, mean_q: -9.412938\n",
      " 1751/5000: episode: 1750, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.078 [0.032, 0.134], loss: 12631.027344, mean_absolute_error: 202.281372, mean_q: -9.397549\n",
      " 1752/5000: episode: 1751, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.874 [0.806, 0.933], loss: 22741.140625, mean_absolute_error: 208.678329, mean_q: -9.378082\n",
      " 1753/5000: episode: 1752, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.598], loss: 43.817055, mean_absolute_error: 195.925140, mean_q: -9.360309\n",
      " 1754/5000: episode: 1753, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.581, 0.763], loss: 10216.968750, mean_absolute_error: 202.385010, mean_q: -9.345865\n",
      " 1755/5000: episode: 1754, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.028], loss: 10214.647461, mean_absolute_error: 202.437180, mean_q: -9.327560\n",
      " 1756/5000: episode: 1755, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 12536.163086, mean_absolute_error: 202.483917, mean_q: -9.305551\n",
      " 1757/5000: episode: 1756, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.467, 0.659], loss: 2898701.000000, mean_absolute_error: 394.790985, mean_q: -9.287124\n",
      " 1758/5000: episode: 1757, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.314, 0.504], loss: 43.294479, mean_absolute_error: 196.285797, mean_q: -9.304319\n",
      " 1759/5000: episode: 1758, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.961 [0.920, 0.991], loss: 43.421455, mean_absolute_error: 196.411652, mean_q: -9.317953\n",
      " 1760/5000: episode: 1759, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 43.466133, mean_absolute_error: 196.508972, mean_q: -9.322747\n",
      " 1761/5000: episode: 1760, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.161, 0.326], loss: 10202.324219, mean_absolute_error: 202.991272, mean_q: -9.327328\n",
      " 1762/5000: episode: 1761, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.344, 0.536], loss: 10200.556641, mean_absolute_error: 203.086838, mean_q: -9.330385\n",
      " 1763/5000: episode: 1762, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.935 [0.882, 0.976], loss: 2506.113770, mean_absolute_error: 196.886276, mean_q: -9.326222\n",
      " 1764/5000: episode: 1763, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.638 [0.544, 0.730], loss: 10197.274414, mean_absolute_error: 203.219360, mean_q: -9.315706\n",
      " 1765/5000: episode: 1764, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.481, 0.672], loss: 43.261078, mean_absolute_error: 196.889938, mean_q: -9.300728\n",
      " 1766/5000: episode: 1765, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 43.092487, mean_absolute_error: 196.927765, mean_q: -9.282586\n",
      " 1767/5000: episode: 1766, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.851, 0.960], loss: 12602.822266, mean_absolute_error: 203.402863, mean_q: -9.268154\n",
      " 1768/5000: episode: 1767, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [0.065, 0.192], loss: 20339.962891, mean_absolute_error: 209.790558, mean_q: -9.261692\n",
      " 1769/5000: episode: 1768, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.472 [0.376, 0.569], loss: 2475.406982, mean_absolute_error: 197.187225, mean_q: -9.254856\n",
      " 1770/5000: episode: 1769, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.178, 0.348], loss: 10187.145508, mean_absolute_error: 203.546875, mean_q: -9.241947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1771/5000: episode: 1770, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.189, 0.361], loss: 2404.545410, mean_absolute_error: 197.235504, mean_q: -9.224733\n",
      " 1772/5000: episode: 1771, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.960], loss: 42.368034, mean_absolute_error: 197.267761, mean_q: -9.204219\n",
      " 1773/5000: episode: 1772, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.277, 0.464], loss: 20320.648438, mean_absolute_error: 210.032959, mean_q: -9.181175\n",
      " 1774/5000: episode: 1773, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.899, 0.984], loss: 41.914391, mean_absolute_error: 197.327728, mean_q: -9.154806\n",
      " 1775/5000: episode: 1774, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.477, 0.668], loss: 20313.017578, mean_absolute_error: 210.077606, mean_q: -9.127580\n",
      " 1776/5000: episode: 1775, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.974, 1.000], loss: 41.402561, mean_absolute_error: 197.376022, mean_q: -9.098732\n",
      " 1777/5000: episode: 1776, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.144, 0.304], loss: 1453640.375000, mean_absolute_error: 299.828918, mean_q: -9.069582\n",
      " 1778/5000: episode: 1777, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 40.976540, mean_absolute_error: 197.442139, mean_q: -9.051793\n",
      " 1779/5000: episode: 1778, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.820, 0.942], loss: 1448301.625000, mean_absolute_error: 293.660889, mean_q: -9.030361\n",
      " 1780/5000: episode: 1779, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.959], loss: 40.680504, mean_absolute_error: 197.544525, mean_q: -9.019033\n",
      " 1781/5000: episode: 1780, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 40.545811, mean_absolute_error: 197.593918, mean_q: -9.004088\n",
      " 1782/5000: episode: 1781, duration: 0.095s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [0.063, 0.188], loss: 25035.578125, mean_absolute_error: 210.362198, mean_q: -8.985405\n",
      " 1783/5000: episode: 1782, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.872 [0.804, 0.932], loss: 2466.172607, mean_absolute_error: 197.728348, mean_q: -8.961987\n",
      " 1784/5000: episode: 1783, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.648 [0.554, 0.739], loss: 35249.632812, mean_absolute_error: 216.861084, mean_q: -8.936056\n",
      " 1785/5000: episode: 1784, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 2502.322510, mean_absolute_error: 197.820557, mean_q: -8.906435\n",
      " 1786/5000: episode: 1785, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.718, 0.872], loss: 20269.431641, mean_absolute_error: 210.470886, mean_q: -8.883259\n",
      " 1787/5000: episode: 1786, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.018], loss: 10152.064453, mean_absolute_error: 204.188629, mean_q: -8.870487\n",
      " 1788/5000: episode: 1787, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.044], loss: 2442.069580, mean_absolute_error: 197.943985, mean_q: -8.860449\n",
      " 1789/5000: episode: 1788, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.396, 0.590], loss: 10147.958984, mean_absolute_error: 204.311798, mean_q: -8.846375\n",
      " 1790/5000: episode: 1789, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.208, 0.384], loss: 1453279.125000, mean_absolute_error: 300.411621, mean_q: -8.828499\n",
      " 1791/5000: episode: 1790, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.059 [0.020, 0.109], loss: 1445519.750000, mean_absolute_error: 294.156372, mean_q: -8.820688\n",
      " 1792/5000: episode: 1791, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.971], loss: 38.919056, mean_absolute_error: 198.177383, mean_q: -8.821590\n",
      " 1793/5000: episode: 1792, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.239, 0.420], loss: 1443023.375000, mean_absolute_error: 294.286255, mean_q: -8.816534\n",
      " 1794/5000: episode: 1793, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 1442962.500000, mean_absolute_error: 294.367615, mean_q: -8.819550\n",
      " 1795/5000: episode: 1794, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.007, 0.077], loss: 12642.730469, mean_absolute_error: 204.870697, mean_q: -8.829160\n",
      " 1796/5000: episode: 1795, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.543, 0.729], loss: 10137.079102, mean_absolute_error: 204.868210, mean_q: -8.837340\n",
      " 1797/5000: episode: 1796, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.119, 0.270], loss: 20231.451172, mean_absolute_error: 211.294128, mean_q: -8.843132\n",
      " 1798/5000: episode: 1797, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.529, 0.717], loss: 39.079185, mean_absolute_error: 198.717865, mean_q: -8.839722\n",
      " 1799/5000: episode: 1798, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 1445172.375000, mean_absolute_error: 294.905151, mean_q: -8.830976\n",
      " 1800/5000: episode: 1799, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.544, 0.730], loss: 39.065849, mean_absolute_error: 198.885635, mean_q: -8.838213\n",
      " 1801/5000: episode: 1800, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.574, 0.756], loss: 20217.949219, mean_absolute_error: 211.629486, mean_q: -8.844572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1802/5000: episode: 1801, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.588, 0.769], loss: 20214.640625, mean_absolute_error: 211.709229, mean_q: -8.842978\n",
      " 1803/5000: episode: 1802, duration: 0.108s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.717, 0.872], loss: 2897308.750000, mean_absolute_error: 397.459412, mean_q: -8.834381\n",
      " 1804/5000: episode: 1803, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.873 [0.805, 0.932], loss: 22693.574219, mean_absolute_error: 211.962799, mean_q: -8.846107\n",
      " 1805/5000: episode: 1804, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.987], loss: 12549.162109, mean_absolute_error: 205.708694, mean_q: -8.854851\n",
      " 1806/5000: episode: 1805, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.680, 0.844], loss: 39.269920, mean_absolute_error: 199.480316, mean_q: -8.861270\n",
      " 1807/5000: episode: 1806, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.121, 0.274], loss: 39.269650, mean_absolute_error: 199.575180, mean_q: -8.861239\n",
      " 1808/5000: episode: 1807, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.521, 0.709], loss: 5110.150391, mean_absolute_error: 199.847870, mean_q: -8.855663\n",
      " 1809/5000: episode: 1808, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.705, 0.863], loss: 1442055.250000, mean_absolute_error: 295.673096, mean_q: -8.845158\n",
      " 1810/5000: episode: 1809, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.851 [0.778, 0.915], loss: 10112.177734, mean_absolute_error: 206.100204, mean_q: -8.850878\n",
      " 1811/5000: episode: 1810, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.066], loss: 1462069.625000, mean_absolute_error: 308.468079, mean_q: -8.854731\n",
      " 1812/5000: episode: 1811, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.274, 0.460], loss: 10108.487305, mean_absolute_error: 206.322388, mean_q: -8.869308\n",
      " 1813/5000: episode: 1812, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.602, 0.780], loss: 20172.683594, mean_absolute_error: 212.754211, mean_q: -8.879929\n",
      " 1814/5000: episode: 1813, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.455, 0.648], loss: 10103.471680, mean_absolute_error: 206.585190, mean_q: -8.894374\n",
      " 1815/5000: episode: 1814, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.175, 0.344], loss: 39.718445, mean_absolute_error: 200.423340, mean_q: -8.911737\n",
      " 1816/5000: episode: 1815, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [0.091, 0.232], loss: 10099.070312, mean_absolute_error: 206.838074, mean_q: -8.920557\n",
      " 1817/5000: episode: 1816, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.286 [0.202, 0.377], loss: 10097.168945, mean_absolute_error: 206.930664, mean_q: -8.921783\n",
      " 1818/5000: episode: 1817, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.343, 0.536], loss: 30205.671875, mean_absolute_error: 219.602264, mean_q: -8.915289\n",
      " 1819/5000: episode: 1818, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.826 [0.749, 0.895], loss: 10091.936523, mean_absolute_error: 207.084030, mean_q: -8.901449\n",
      " 1820/5000: episode: 1819, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.900 [0.838, 0.953], loss: 39.472183, mean_absolute_error: 200.853119, mean_q: -8.884064\n",
      " 1821/5000: episode: 1820, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.712, 0.868], loss: 39.297165, mean_absolute_error: 200.900436, mean_q: -8.864344\n",
      " 1822/5000: episode: 1821, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.018], loss: 1453892.250000, mean_absolute_error: 303.246765, mean_q: -8.842859\n",
      " 1823/5000: episode: 1822, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 39.012745, mean_absolute_error: 200.997406, mean_q: -8.832203\n",
      " 1824/5000: episode: 1823, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [0.090, 0.229], loss: 12655.121094, mean_absolute_error: 207.467560, mean_q: -8.831013\n",
      " 1825/5000: episode: 1824, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.672 [0.579, 0.761], loss: 12569.654297, mean_absolute_error: 207.498825, mean_q: -8.835012\n",
      " 1826/5000: episode: 1825, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.239, 0.421], loss: 39.005951, mean_absolute_error: 201.266968, mean_q: -8.831434\n",
      " 1827/5000: episode: 1826, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.270], loss: 38.934471, mean_absolute_error: 201.333771, mean_q: -8.823338\n",
      " 1828/5000: episode: 1827, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.037], loss: 2581.821533, mean_absolute_error: 201.457306, mean_q: -8.811123\n",
      " 1829/5000: episode: 1828, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.085 [0.037, 0.143], loss: 38.682583, mean_absolute_error: 201.428864, mean_q: -8.794746\n",
      " 1830/5000: episode: 1829, duration: 0.109s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.021], loss: 10071.458008, mean_absolute_error: 207.743164, mean_q: -8.775599\n",
      " 1831/5000: episode: 1830, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.657, 0.826], loss: 38.378464, mean_absolute_error: 201.514542, mean_q: -8.760103\n",
      " 1832/5000: episode: 1831, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.909 [0.849, 0.959], loss: 38.272331, mean_absolute_error: 201.572159, mean_q: -8.747980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1833/5000: episode: 1832, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 2881651.000000, mean_absolute_error: 393.397400, mean_q: -8.732147\n",
      " 1834/5000: episode: 1833, duration: 0.095s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 10064.750977, mean_absolute_error: 207.993744, mean_q: -8.739553\n",
      " 1835/5000: episode: 1834, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.132], loss: 12617.881836, mean_absolute_error: 208.165390, mean_q: -8.745941\n",
      " 1836/5000: episode: 1835, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 10061.212891, mean_absolute_error: 208.209717, mean_q: -8.757240\n",
      " 1837/5000: episode: 1836, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.685, 0.847], loss: 10059.170898, mean_absolute_error: 208.322601, mean_q: -8.765791\n",
      " 1838/5000: episode: 1837, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.838 [0.763, 0.905], loss: 1440489.750000, mean_absolute_error: 298.020966, mean_q: -8.765205\n",
      " 1839/5000: episode: 1838, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [0.070, 0.199], loss: 38.480259, mean_absolute_error: 202.271042, mean_q: -8.771713\n",
      " 1840/5000: episode: 1839, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.084 [0.036, 0.142], loss: 10052.345703, mean_absolute_error: 208.623901, mean_q: -8.771282\n",
      " 1841/5000: episode: 1840, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.454, 0.647], loss: 2627.432617, mean_absolute_error: 202.525574, mean_q: -8.764269\n",
      " 1842/5000: episode: 1841, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.904, 0.986], loss: 20058.039062, mean_absolute_error: 215.020111, mean_q: -8.751370\n",
      " 1843/5000: episode: 1842, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.505, 0.694], loss: 38.146561, mean_absolute_error: 202.554932, mean_q: -8.733593\n",
      " 1844/5000: episode: 1843, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 37.972111, mean_absolute_error: 202.593979, mean_q: -8.713598\n",
      " 1845/5000: episode: 1844, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.059 [0.020, 0.109], loss: 37.778111, mean_absolute_error: 202.628006, mean_q: -8.691308\n",
      " 1846/5000: episode: 1845, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.032], loss: 10040.868164, mean_absolute_error: 208.907898, mean_q: -8.667244\n",
      " 1847/5000: episode: 1846, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 22658.667969, mean_absolute_error: 215.286530, mean_q: -8.641317\n",
      " 1848/5000: episode: 1847, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.249, 0.432], loss: 1462752.750000, mean_absolute_error: 311.113892, mean_q: -8.612664\n",
      " 1849/5000: episode: 1848, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.668 [0.574, 0.757], loss: 10034.475586, mean_absolute_error: 209.002930, mean_q: -8.595471\n",
      " 1850/5000: episode: 1849, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.871 [0.801, 0.930], loss: 2900203.750000, mean_absolute_error: 406.975128, mean_q: -8.575021\n",
      " 1851/5000: episode: 1850, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [0.083, 0.219], loss: 36.855400, mean_absolute_error: 202.926270, mean_q: -8.584499\n",
      " 1852/5000: episode: 1851, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.555, 0.740], loss: 2661.299805, mean_absolute_error: 203.145630, mean_q: -8.592333\n",
      " 1853/5000: episode: 1852, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 36.920265, mean_absolute_error: 203.138000, mean_q: -8.592051\n",
      " 1854/5000: episode: 1853, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.521, 0.709], loss: 36.864895, mean_absolute_error: 203.216766, mean_q: -8.585605\n",
      " 1855/5000: episode: 1854, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.556 [0.459, 0.652], loss: 36.827328, mean_absolute_error: 203.295181, mean_q: -8.581228\n",
      " 1856/5000: episode: 1855, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.037, 0.144], loss: 36.795715, mean_absolute_error: 203.375778, mean_q: -8.577544\n",
      " 1857/5000: episode: 1856, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.539, 0.726], loss: 36.717358, mean_absolute_error: 203.439087, mean_q: -8.568405\n",
      " 1858/5000: episode: 1857, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 12552.053711, mean_absolute_error: 209.763840, mean_q: -8.562699\n",
      " 1859/5000: episode: 1858, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.528 [0.431, 0.624], loss: 36.625381, mean_absolute_error: 203.584412, mean_q: -8.557666\n",
      " 1860/5000: episode: 1859, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.874 [0.805, 0.933], loss: 36.543564, mean_absolute_error: 203.640350, mean_q: -8.548101\n",
      " 1861/5000: episode: 1860, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 2563.931641, mean_absolute_error: 203.690750, mean_q: -8.534909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1862/5000: episode: 1861, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.625 [0.530, 0.717], loss: 36.291183, mean_absolute_error: 203.714615, mean_q: -8.518528\n",
      " 1863/5000: episode: 1862, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 2901567.500000, mean_absolute_error: 407.906006, mean_q: -8.506907\n",
      " 1864/5000: episode: 1863, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.625, 0.799], loss: 36.327061, mean_absolute_error: 203.867844, mean_q: -8.522738\n",
      " 1865/5000: episode: 1864, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.611, 0.788], loss: 10007.638672, mean_absolute_error: 210.182526, mean_q: -8.529745\n",
      " 1866/5000: episode: 1865, duration: 0.132s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.654, 0.824], loss: 15228.355469, mean_absolute_error: 210.394135, mean_q: -8.529831\n",
      " 1867/5000: episode: 1866, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.177, 0.347], loss: 1454445.875000, mean_absolute_error: 306.218933, mean_q: -8.522842\n",
      " 1868/5000: episode: 1867, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.848 [0.774, 0.912], loss: 10004.090820, mean_absolute_error: 210.392532, mean_q: -8.523597\n",
      " 1869/5000: episode: 1868, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.052], loss: 1439166.625000, mean_absolute_error: 300.007019, mean_q: -8.517813\n",
      " 1870/5000: episode: 1869, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.281, 0.469], loss: 36.302979, mean_absolute_error: 204.316833, mean_q: -8.519913\n",
      " 1871/5000: episode: 1870, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.712, 0.868], loss: 2890624.750000, mean_absolute_error: 402.171326, mean_q: -8.515356\n",
      " 1872/5000: episode: 1871, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.696, 0.856], loss: 36.401527, mean_absolute_error: 204.513519, mean_q: -8.531470\n",
      " 1873/5000: episode: 1872, duration: 0.109s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 36.462753, mean_absolute_error: 204.610596, mean_q: -8.538643\n",
      " 1874/5000: episode: 1873, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.869, 0.970], loss: 36.466015, mean_absolute_error: 204.687225, mean_q: -8.539024\n",
      " 1875/5000: episode: 1874, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.513 [0.417, 0.610], loss: 5363.760742, mean_absolute_error: 204.935593, mean_q: -8.533192\n",
      " 1876/5000: episode: 1875, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.918 [0.860, 0.965], loss: 1438775.625000, mean_absolute_error: 300.548035, mean_q: -8.521120\n",
      " 1877/5000: episode: 1876, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.128, 0.282], loss: 9991.487305, mean_absolute_error: 211.088348, mean_q: -8.519239\n",
      " 1878/5000: episode: 1877, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.658 [0.565, 0.748], loss: 12548.131836, mean_absolute_error: 211.150879, mean_q: -8.511479\n",
      " 1879/5000: episode: 1878, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.176, 0.345], loss: 9988.361328, mean_absolute_error: 211.215912, mean_q: -8.504840\n",
      " 1880/5000: episode: 1879, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.598 [0.502, 0.692], loss: 19936.673828, mean_absolute_error: 217.499725, mean_q: -8.498749\n",
      " 1881/5000: episode: 1880, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 36.021622, mean_absolute_error: 205.148132, mean_q: -8.486829\n",
      " 1882/5000: episode: 1881, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.901, 0.985], loss: 1448470.375000, mean_absolute_error: 307.132507, mean_q: -8.471780\n",
      " 1883/5000: episode: 1882, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 12638.144531, mean_absolute_error: 211.554199, mean_q: -8.467309\n",
      " 1884/5000: episode: 1883, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.374, 0.567], loss: 9978.903320, mean_absolute_error: 211.539673, mean_q: -8.457039\n",
      " 1885/5000: episode: 1884, duration: 0.100s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.820, 0.942], loss: 35.645748, mean_absolute_error: 205.387512, mean_q: -8.442429\n",
      " 1886/5000: episode: 1885, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.024], loss: 35.501774, mean_absolute_error: 205.427216, mean_q: -8.425360\n",
      " 1887/5000: episode: 1886, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.732, 0.883], loss: 1458238.875000, mean_absolute_error: 313.575867, mean_q: -8.406152\n",
      " 1888/5000: episode: 1887, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.295 [0.209, 0.386], loss: 35.258064, mean_absolute_error: 205.521210, mean_q: -8.396387\n",
      " 1889/5000: episode: 1888, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.838 [0.762, 0.904], loss: 9970.905273, mean_absolute_error: 211.768311, mean_q: -8.382399\n",
      " 1890/5000: episode: 1889, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.202 [0.128, 0.283], loss: 9968.823242, mean_absolute_error: 211.814819, mean_q: -8.363825\n",
      " 1891/5000: episode: 1890, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.030], loss: 19897.396484, mean_absolute_error: 218.072174, mean_q: -8.347771\n",
      " 1892/5000: episode: 1891, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.892 [0.827, 0.946], loss: 9963.046875, mean_absolute_error: 211.951569, mean_q: -8.333250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1893/5000: episode: 1892, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 9960.046875, mean_absolute_error: 212.016815, mean_q: -8.314884\n",
      " 1894/5000: episode: 1893, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.685, 0.848], loss: 34.400055, mean_absolute_error: 205.883347, mean_q: -8.293583\n",
      " 1895/5000: episode: 1894, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [0.059, 0.182], loss: 2634.852539, mean_absolute_error: 205.944092, mean_q: -8.271137\n",
      " 1896/5000: episode: 1895, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.039], loss: 12660.921875, mean_absolute_error: 212.244919, mean_q: -8.247272\n",
      " 1897/5000: episode: 1896, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [0.007, 0.075], loss: 19868.396484, mean_absolute_error: 218.346344, mean_q: -8.221670\n",
      " 1898/5000: episode: 1897, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.987, 1.000], loss: 19864.638672, mean_absolute_error: 218.363861, mean_q: -8.194246\n",
      " 1899/5000: episode: 1898, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.689, 0.850], loss: 29774.218750, mean_absolute_error: 224.583832, mean_q: -8.173096\n",
      " 1900/5000: episode: 1899, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.207 [0.132, 0.289], loss: 9944.636719, mean_absolute_error: 212.277969, mean_q: -8.155113\n",
      " 1901/5000: episode: 1900, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.877 [0.810, 0.936], loss: 33.093613, mean_absolute_error: 206.148880, mean_q: -8.134552\n",
      " 1902/5000: episode: 1901, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [0.093, 0.233], loss: 29754.652344, mean_absolute_error: 224.707336, mean_q: -8.112032\n",
      " 1903/5000: episode: 1902, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.426, 0.619], loss: 32.698349, mean_absolute_error: 206.223190, mean_q: -8.085822\n",
      " 1904/5000: episode: 1903, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.042 [0.010, 0.086], loss: 22527.414062, mean_absolute_error: 218.676758, mean_q: -8.059172\n",
      " 1905/5000: episode: 1904, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 19834.167969, mean_absolute_error: 218.612671, mean_q: -8.030531\n",
      " 1906/5000: episode: 1905, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.140, 0.299], loss: 1450551.500000, mean_absolute_error: 308.240173, mean_q: -8.000710\n",
      " 1907/5000: episode: 1906, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.938, 0.997], loss: 22435.451172, mean_absolute_error: 218.698914, mean_q: -7.983665\n",
      " 1908/5000: episode: 1907, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [0.077, 0.210], loss: 1460360.250000, mean_absolute_error: 314.459351, mean_q: -7.962661\n",
      " 1909/5000: episode: 1908, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.963, 1.000], loss: 29709.976562, mean_absolute_error: 224.942200, mean_q: -7.952155\n",
      " 1910/5000: episode: 1909, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.386, 0.580], loss: 1457613.250000, mean_absolute_error: 314.540009, mean_q: -7.943485\n",
      " 1911/5000: episode: 1910, duration: 0.087s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.729, 0.881], loss: 31.666784, mean_absolute_error: 206.677155, mean_q: -7.957239\n",
      " 1912/5000: episode: 1911, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.874, 0.972], loss: 9917.083008, mean_absolute_error: 212.955399, mean_q: -7.969649\n",
      " 1913/5000: episode: 1912, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.452, 0.644], loss: 31.792637, mean_absolute_error: 206.902145, mean_q: -7.973038\n",
      " 1914/5000: episode: 1913, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.312 [0.225, 0.404], loss: 31.761604, mean_absolute_error: 206.992294, mean_q: -7.969145\n",
      " 1915/5000: episode: 1914, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 31.685598, mean_absolute_error: 207.067169, mean_q: -7.959603\n",
      " 1916/5000: episode: 1915, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.124, 0.278], loss: 31.581551, mean_absolute_error: 207.127319, mean_q: -7.946522\n",
      " 1917/5000: episode: 1916, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [0.071, 0.200], loss: 9905.930664, mean_absolute_error: 213.322800, mean_q: -7.930577\n",
      " 1918/5000: episode: 1917, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.906 [0.845, 0.956], loss: 9904.036133, mean_absolute_error: 213.380157, mean_q: -7.918918\n",
      " 1919/5000: episode: 1918, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.318, 0.508], loss: 31.289337, mean_absolute_error: 207.304550, mean_q: -7.909668\n",
      " 1920/5000: episode: 1919, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.161, 0.326], loss: 9900.513672, mean_absolute_error: 213.521088, mean_q: -7.903461\n",
      " 1921/5000: episode: 1920, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.247, 0.430], loss: 31.202332, mean_absolute_error: 207.460236, mean_q: -7.898662\n",
      " 1922/5000: episode: 1921, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 19760.859375, mean_absolute_error: 219.817413, mean_q: -7.896309\n",
      " 1923/5000: episode: 1922, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.895 [0.832, 0.949], loss: 9893.906250, mean_absolute_error: 213.784882, mean_q: -7.901722\n",
      " 1924/5000: episode: 1923, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.105 [0.052, 0.169], loss: 19752.390625, mean_absolute_error: 220.021606, mean_q: -7.906578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1925/5000: episode: 1924, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.249 [0.169, 0.336], loss: 31.240314, mean_absolute_error: 207.843628, mean_q: -7.903469\n",
      " 1926/5000: episode: 1925, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.136, 0.293], loss: 1436979.750000, mean_absolute_error: 303.554810, mean_q: -7.902915\n",
      " 1927/5000: episode: 1926, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.049 [0.014, 0.096], loss: 1446739.625000, mean_absolute_error: 309.819275, mean_q: -7.924581\n",
      " 1928/5000: episode: 1927, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.931, 0.995], loss: 22372.203125, mean_absolute_error: 220.495834, mean_q: -7.955494\n",
      " 1929/5000: episode: 1928, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 1436680.125000, mean_absolute_error: 303.983154, mean_q: -7.972804\n",
      " 1930/5000: episode: 1929, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [0.099, 0.242], loss: 12590.865234, mean_absolute_error: 214.703247, mean_q: -7.995189\n",
      " 1931/5000: episode: 1930, duration: 0.043s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.542, 0.728], loss: 22407.531250, mean_absolute_error: 220.920685, mean_q: -8.006895\n",
      " 1932/5000: episode: 1931, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.452, 0.645], loss: 32.128799, mean_absolute_error: 208.770615, mean_q: -8.015083\n",
      " 1933/5000: episode: 1932, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.093 [0.042, 0.153], loss: 1446196.500000, mean_absolute_error: 310.579865, mean_q: -8.021471\n",
      " 1934/5000: episode: 1933, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.498, 0.688], loss: 19706.320312, mean_absolute_error: 221.248657, mean_q: -8.033686\n",
      " 1935/5000: episode: 1934, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.457 [0.361, 0.554], loss: 32.293404, mean_absolute_error: 209.134796, mean_q: -8.035591\n",
      " 1936/5000: episode: 1935, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 32.259895, mean_absolute_error: 209.226181, mean_q: -8.031421\n",
      " 1937/5000: episode: 1936, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.394, 0.587], loss: 32.191998, mean_absolute_error: 209.298569, mean_q: -8.022964\n",
      " 1938/5000: episode: 1937, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.093], loss: 32.154198, mean_absolute_error: 209.371246, mean_q: -8.018251\n",
      " 1939/5000: episode: 1938, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.243, 0.425], loss: 1436001.250000, mean_absolute_error: 305.020905, mean_q: -8.022247\n",
      " 1940/5000: episode: 1939, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 2728.963867, mean_absolute_error: 209.618988, mean_q: -8.039085\n",
      " 1941/5000: episode: 1940, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.368, 0.561], loss: 32.377975, mean_absolute_error: 209.705109, mean_q: -8.046108\n",
      " 1942/5000: episode: 1941, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.061 [0.021, 0.111], loss: 32.382683, mean_absolute_error: 209.788696, mean_q: -8.046693\n",
      " 1943/5000: episode: 1942, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.573 [0.476, 0.668], loss: 12533.254883, mean_absolute_error: 215.971558, mean_q: -8.049389\n",
      " 1944/5000: episode: 1943, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.394, 0.588], loss: 1448175.625000, mean_absolute_error: 311.588379, mean_q: -8.051559\n",
      " 1945/5000: episode: 1944, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.655, 0.824], loss: 32.544926, mean_absolute_error: 210.080505, mean_q: -8.066828\n",
      " 1946/5000: episode: 1945, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.955, 1.000], loss: 32.649433, mean_absolute_error: 210.199860, mean_q: -8.079771\n",
      " 1947/5000: episode: 1946, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 9845.062500, mean_absolute_error: 216.423447, mean_q: -8.099758\n",
      " 1948/5000: episode: 1947, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 12542.881836, mean_absolute_error: 216.561737, mean_q: -8.122742\n",
      " 1949/5000: episode: 1948, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.895, 0.982], loss: 2892814.000000, mean_absolute_error: 413.833130, mean_q: -8.141687\n",
      " 1950/5000: episode: 1949, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.921 [0.865, 0.967], loss: 33.494385, mean_absolute_error: 210.791321, mean_q: -8.183666\n",
      " 1951/5000: episode: 1950, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.277 [0.193, 0.366], loss: 12589.641602, mean_absolute_error: 217.067245, mean_q: -8.211514\n",
      " 1952/5000: episode: 1951, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.704, 0.862], loss: 1437628.250000, mean_absolute_error: 306.566254, mean_q: -8.226640\n",
      " 1953/5000: episode: 1952, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.001, 0.047], loss: 1444594.625000, mean_absolute_error: 312.794189, mean_q: -8.253007\n",
      " 1954/5000: episode: 1953, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.938, 0.997], loss: 19628.771484, mean_absolute_error: 223.603210, mean_q: -8.294979\n",
      " 1955/5000: episode: 1954, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.163, 0.328], loss: 34.749920, mean_absolute_error: 211.639191, mean_q: -8.335655\n",
      " 1956/5000: episode: 1955, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.876, 0.973], loss: 9827.141602, mean_absolute_error: 217.896622, mean_q: -8.369788\n",
      " 1957/5000: episode: 1956, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.958, 1.000], loss: 2801.270020, mean_absolute_error: 211.999435, mean_q: -8.397627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1958/5000: episode: 1957, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.181], loss: 9823.002930, mean_absolute_error: 218.202789, mean_q: -8.419867\n",
      " 1959/5000: episode: 1958, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.131], loss: 35.548958, mean_absolute_error: 212.253220, mean_q: -8.430958\n",
      " 1960/5000: episode: 1959, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.304 [0.217, 0.395], loss: 35.573364, mean_absolute_error: 212.351990, mean_q: -8.433851\n",
      " 1961/5000: episode: 1960, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.181], loss: 1443781.250000, mean_absolute_error: 313.916565, mean_q: -8.429561\n",
      " 1962/5000: episode: 1961, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.089], loss: 9814.676758, mean_absolute_error: 218.602402, mean_q: -8.432391\n",
      " 1963/5000: episode: 1962, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 1449271.250000, mean_absolute_error: 314.192596, mean_q: -8.435287\n",
      " 1964/5000: episode: 1963, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 29359.455078, mean_absolute_error: 230.963837, mean_q: -8.450672\n",
      " 1965/5000: episode: 1964, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.971, 1.000], loss: 2828.827393, mean_absolute_error: 212.911865, mean_q: -8.454735\n",
      " 1966/5000: episode: 1965, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.037, 0.144], loss: 35.718903, mean_absolute_error: 212.982788, mean_q: -8.451088\n",
      " 1967/5000: episode: 1966, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.878 [0.810, 0.936], loss: 35.636848, mean_absolute_error: 213.066254, mean_q: -8.441376\n",
      " 1968/5000: episode: 1967, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 9800.691406, mean_absolute_error: 219.220856, mean_q: -8.441122\n",
      " 1969/5000: episode: 1968, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.517, 0.706], loss: 9797.845703, mean_absolute_error: 219.336273, mean_q: -8.447844\n",
      " 1970/5000: episode: 1969, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 9795.601562, mean_absolute_error: 219.446793, mean_q: -8.454148\n",
      " 1971/5000: episode: 1970, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 2949.263184, mean_absolute_error: 213.608109, mean_q: -8.459980\n",
      " 1972/5000: episode: 1971, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.455, 0.648], loss: 9791.540039, mean_absolute_error: 219.643341, mean_q: -8.457991\n",
      " 1973/5000: episode: 1972, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.704, 0.862], loss: 19542.953125, mean_absolute_error: 225.761841, mean_q: -8.449329\n",
      " 1974/5000: episode: 1973, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.071, 0.201], loss: 9786.927734, mean_absolute_error: 219.781677, mean_q: -8.435246\n",
      " 1975/5000: episode: 1974, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.521, 0.709], loss: 35.562424, mean_absolute_error: 213.830811, mean_q: -8.432554\n",
      " 1976/5000: episode: 1975, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [0.010, 0.084], loss: 29277.703125, mean_absolute_error: 232.068390, mean_q: -8.445518\n",
      " 1977/5000: episode: 1976, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 15542.259766, mean_absolute_error: 220.231903, mean_q: -8.454681\n",
      " 1978/5000: episode: 1977, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.610, 0.787], loss: 35.747349, mean_absolute_error: 214.170044, mean_q: -8.454453\n",
      " 1979/5000: episode: 1978, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.061 [0.021, 0.112], loss: 35.693375, mean_absolute_error: 214.247513, mean_q: -8.448068\n",
      " 1980/5000: episode: 1979, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.842 [0.768, 0.908], loss: 9774.458984, mean_absolute_error: 220.346390, mean_q: -8.436584\n",
      " 1981/5000: episode: 1980, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 35.461002, mean_absolute_error: 214.372101, mean_q: -8.420520\n",
      " 1982/5000: episode: 1981, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.870, 0.970], loss: 1445339.750000, mean_absolute_error: 315.797974, mean_q: -8.401194\n",
      " 1983/5000: episode: 1982, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.558, 0.742], loss: 35.217762, mean_absolute_error: 214.500916, mean_q: -8.391587\n",
      " 1984/5000: episode: 1983, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.555, 0.739], loss: 19495.572266, mean_absolute_error: 226.616058, mean_q: -8.377478\n",
      " 1985/5000: episode: 1984, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.003, 0.060], loss: 2850.907227, mean_absolute_error: 214.624390, mean_q: -8.358459\n",
      " 1986/5000: episode: 1985, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.561 [0.464, 0.657], loss: 1464637.000000, mean_absolute_error: 328.050079, mean_q: -8.336199\n",
      " 1987/5000: episode: 1986, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [0.088, 0.226], loss: 12600.228516, mean_absolute_error: 220.781448, mean_q: -8.323818\n",
      " 1988/5000: episode: 1987, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.072, 0.201], loss: 19476.140625, mean_absolute_error: 226.838348, mean_q: -8.306753\n",
      " 1989/5000: episode: 1988, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.949, 0.999], loss: 22301.648438, mean_absolute_error: 226.915985, mean_q: -8.292368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1990/5000: episode: 1989, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.277 [0.193, 0.366], loss: 9749.243164, mean_absolute_error: 220.976608, mean_q: -8.279061\n",
      " 1991/5000: episode: 1990, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.580, 0.761], loss: 1432452.625000, mean_absolute_error: 310.340729, mean_q: -8.262266\n",
      " 1992/5000: episode: 1991, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 9744.083008, mean_absolute_error: 221.129593, mean_q: -8.257380\n",
      " 1993/5000: episode: 1992, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.609 [0.514, 0.702], loss: 1454711.375000, mean_absolute_error: 322.589417, mean_q: -8.247437\n",
      " 1994/5000: episode: 1993, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [0.075, 0.208], loss: 9739.619141, mean_absolute_error: 221.292816, mean_q: -8.245871\n",
      " 1995/5000: episode: 1994, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.136, 0.293], loss: 22364.404297, mean_absolute_error: 227.443787, mean_q: -8.237349\n",
      " 1996/5000: episode: 1995, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.967, 1.000], loss: 33.809517, mean_absolute_error: 215.444336, mean_q: -8.222079\n",
      " 1997/5000: episode: 1996, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.520, 0.709], loss: 33.659897, mean_absolute_error: 215.498657, mean_q: -8.203863\n",
      " 1998/5000: episode: 1997, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.149], loss: 5819.965820, mean_absolute_error: 215.635239, mean_q: -8.183304\n",
      " 1999/5000: episode: 1998, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 33.296555, mean_absolute_error: 215.585480, mean_q: -8.159459\n",
      " 2000/5000: episode: 1999, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.957, 1.000], loss: 33.096954, mean_absolute_error: 215.615250, mean_q: -8.134964\n",
      " 2001/5000: episode: 2000, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.009], loss: 32.888916, mean_absolute_error: 215.637756, mean_q: -8.109352\n",
      " 2002/5000: episode: 2001, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.934 [0.882, 0.976], loss: 12660.243164, mean_absolute_error: 221.723083, mean_q: -8.082264\n",
      " 2003/5000: episode: 2002, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 9720.097656, mean_absolute_error: 221.668228, mean_q: -8.052563\n",
      " 2004/5000: episode: 2003, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 9717.772461, mean_absolute_error: 221.688934, mean_q: -8.021557\n",
      " 2005/5000: episode: 2004, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.170, 0.337], loss: 31.927475, mean_absolute_error: 215.726349, mean_q: -7.989929\n",
      " 2006/5000: episode: 2005, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.969, 1.000], loss: 31.672764, mean_absolute_error: 215.743896, mean_q: -7.957991\n",
      " 2007/5000: episode: 2006, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.114, 0.263], loss: 9711.211914, mean_absolute_error: 221.737610, mean_q: -7.926294\n",
      " 2008/5000: episode: 2007, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.105 [0.052, 0.169], loss: 9709.343750, mean_absolute_error: 221.749924, mean_q: -7.894563\n",
      " 2009/5000: episode: 2008, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.910, 0.988], loss: 1434935.875000, mean_absolute_error: 311.089752, mean_q: -7.869871\n",
      " 2010/5000: episode: 2009, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.498 [0.401, 0.595], loss: 19380.742188, mean_absolute_error: 227.825424, mean_q: -7.864573\n",
      " 2011/5000: episode: 2010, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.921 [0.864, 0.967], loss: 9703.919922, mean_absolute_error: 221.927765, mean_q: -7.860711\n",
      " 2012/5000: episode: 2011, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.209 [0.134, 0.291], loss: 48386.507812, mean_absolute_error: 245.906479, mean_q: -7.857878\n",
      " 2013/5000: episode: 2012, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.070 [0.027, 0.123], loss: 30.853645, mean_absolute_error: 216.132141, mean_q: -7.854399\n",
      " 2014/5000: episode: 2013, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 9697.061523, mean_absolute_error: 222.193726, mean_q: -7.852838\n",
      " 2015/5000: episode: 2014, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.594, 0.774], loss: 1451103.375000, mean_absolute_error: 323.497131, mean_q: -7.845191\n",
      " 2016/5000: episode: 2015, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.657, 0.825], loss: 12681.836914, mean_absolute_error: 222.470688, mean_q: -7.846514\n",
      " 2017/5000: episode: 2016, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.517, 0.706], loss: 1431662.750000, mean_absolute_error: 311.738770, mean_q: -7.840848\n",
      " 2018/5000: episode: 2017, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.410 [0.315, 0.506], loss: 9687.961914, mean_absolute_error: 222.555695, mean_q: -7.844408\n",
      " 2019/5000: episode: 2018, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 9685.320312, mean_absolute_error: 222.642914, mean_q: -7.839641\n",
      " 2020/5000: episode: 2019, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.508, 0.698], loss: 30.656567, mean_absolute_error: 216.767029, mean_q: -7.829270\n",
      " 2021/5000: episode: 2020, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.426, 0.619], loss: 2899.906006, mean_absolute_error: 216.830750, mean_q: -7.815171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2022/5000: episode: 2021, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.454, 0.646], loss: 9678.105469, mean_absolute_error: 222.871521, mean_q: -7.812540\n",
      " 2023/5000: episode: 2022, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.557 [0.460, 0.653], loss: 9676.190430, mean_absolute_error: 222.973602, mean_q: -7.817956\n",
      " 2024/5000: episode: 2023, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.837 [0.762, 0.904], loss: 28961.988281, mean_absolute_error: 234.958832, mean_q: -7.816499\n",
      " 2025/5000: episode: 2024, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.444 [0.348, 0.540], loss: 28955.464844, mean_absolute_error: 235.044373, mean_q: -7.815658\n",
      " 2026/5000: episode: 2025, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.082 [0.035, 0.140], loss: 30.537745, mean_absolute_error: 217.299026, mean_q: -7.814081\n",
      " 2027/5000: episode: 2026, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.635, 0.808], loss: 9666.862305, mean_absolute_error: 223.331863, mean_q: -7.806719\n",
      " 2028/5000: episode: 2027, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 19298.367188, mean_absolute_error: 229.344345, mean_q: -7.794577\n",
      " 2029/5000: episode: 2028, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.834 [0.758, 0.901], loss: 30.258350, mean_absolute_error: 217.520004, mean_q: -7.778248\n",
      " 2030/5000: episode: 2029, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.532, 0.719], loss: 30.110359, mean_absolute_error: 217.568512, mean_q: -7.759201\n",
      " 2031/5000: episode: 2030, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 1431011.875000, mean_absolute_error: 312.814453, mean_q: -7.738108\n",
      " 2032/5000: episode: 2031, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.163, 0.329], loss: 1433993.625000, mean_absolute_error: 312.978912, mean_q: -7.729913\n",
      " 2033/5000: episode: 2032, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.064 [0.023, 0.115], loss: 1433806.250000, mean_absolute_error: 312.960114, mean_q: -7.731654\n",
      " 2034/5000: episode: 2033, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.681 [0.589, 0.769], loss: 9653.372070, mean_absolute_error: 223.798096, mean_q: -7.741004\n",
      " 2035/5000: episode: 2034, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.454, 0.646], loss: 9651.059570, mean_absolute_error: 223.886536, mean_q: -7.741219\n",
      " 2036/5000: episode: 2035, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.233 [0.155, 0.318], loss: 1433774.125000, mean_absolute_error: 313.322296, mean_q: -7.734726\n",
      " 2037/5000: episode: 2036, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.976, 1.000], loss: 2938.051514, mean_absolute_error: 218.144699, mean_q: -7.745676\n",
      " 2038/5000: episode: 2037, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.506, 0.696], loss: 22192.757812, mean_absolute_error: 230.120590, mean_q: -7.755378\n",
      " 2039/5000: episode: 2038, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.077], loss: 1436505.250000, mean_absolute_error: 313.624237, mean_q: -7.755896\n",
      " 2040/5000: episode: 2039, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.178], loss: 6130.678711, mean_absolute_error: 218.648651, mean_q: -7.764068\n",
      " 2041/5000: episode: 2040, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 9640.894531, mean_absolute_error: 224.444885, mean_q: -7.763805\n",
      " 2042/5000: episode: 2041, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.433, 0.627], loss: 9639.200195, mean_absolute_error: 224.528687, mean_q: -7.764076\n",
      " 2043/5000: episode: 2042, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.921, 0.992], loss: 1430294.875000, mean_absolute_error: 313.852783, mean_q: -7.764038\n",
      " 2044/5000: episode: 2043, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.820 [0.742, 0.890], loss: 30.215107, mean_absolute_error: 218.806976, mean_q: -7.772687\n",
      " 2045/5000: episode: 2044, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.098], loss: 19236.544922, mean_absolute_error: 230.730621, mean_q: -7.773077\n",
      " 2046/5000: episode: 2045, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.922 [0.866, 0.968], loss: 30.161634, mean_absolute_error: 218.981506, mean_q: -7.765806\n",
      " 2047/5000: episode: 2046, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.165, 0.331], loss: 28826.132812, mean_absolute_error: 236.784760, mean_q: -7.753728\n",
      " 2048/5000: episode: 2047, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.996], loss: 2962.967773, mean_absolute_error: 219.108658, mean_q: -7.735935\n",
      " 2049/5000: episode: 2048, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.543, 0.729], loss: 29.771217, mean_absolute_error: 219.155502, mean_q: -7.715374\n",
      " 2050/5000: episode: 2049, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.845 [0.771, 0.910], loss: 1432985.375000, mean_absolute_error: 314.359009, mean_q: -7.693279\n",
      " 2051/5000: episode: 2050, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.098, 0.241], loss: 29.526081, mean_absolute_error: 219.254700, mean_q: -7.683541\n",
      " 2052/5000: episode: 2051, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.919 [0.862, 0.966], loss: 12558.959961, mean_absolute_error: 225.209717, mean_q: -7.669847\n",
      " 2053/5000: episode: 2052, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.925, 0.993], loss: 19204.240234, mean_absolute_error: 231.151260, mean_q: -7.651831\n",
      " 2054/5000: episode: 2053, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.669, 0.835], loss: 9614.685547, mean_absolute_error: 225.283890, mean_q: -7.629732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2055/5000: episode: 2054, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.331 [0.242, 0.425], loss: 28780.007812, mean_absolute_error: 237.116241, mean_q: -7.605485\n",
      " 2056/5000: episode: 2055, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.957 [0.913, 0.989], loss: 12560.160156, mean_absolute_error: 225.358566, mean_q: -7.577694\n",
      " 2057/5000: episode: 2056, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.288, 0.476], loss: 19185.478516, mean_absolute_error: 231.278839, mean_q: -7.547613\n",
      " 2058/5000: episode: 2057, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.449, 0.641], loss: 6013.297852, mean_absolute_error: 219.593597, mean_q: -7.515918\n",
      " 2059/5000: episode: 2058, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.616, 0.792], loss: 1449014.750000, mean_absolute_error: 326.442535, mean_q: -7.483190\n",
      " 2060/5000: episode: 2059, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.003, 0.059], loss: 1448983.625000, mean_absolute_error: 326.490295, mean_q: -7.464239\n",
      " 2061/5000: episode: 2060, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 27.924105, mean_absolute_error: 219.708252, mean_q: -7.472166\n",
      " 2062/5000: episode: 2061, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.097 [0.045, 0.158], loss: 9594.666016, mean_absolute_error: 225.724243, mean_q: -7.487380\n",
      " 2063/5000: episode: 2062, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.075 [0.030, 0.130], loss: 3003.561035, mean_absolute_error: 219.959991, mean_q: -7.493597\n",
      " 2064/5000: episode: 2063, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.041], loss: 9590.908203, mean_absolute_error: 225.909515, mean_q: -7.492499\n",
      " 2065/5000: episode: 2064, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.961 [0.920, 0.991], loss: 28.014114, mean_absolute_error: 220.099609, mean_q: -7.484200\n",
      " 2066/5000: episode: 2065, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.655 [0.561, 0.745], loss: 1442100.875000, mean_absolute_error: 321.217041, mean_q: -7.470888\n",
      " 2067/5000: episode: 2066, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.619, 0.795], loss: 9584.154297, mean_absolute_error: 226.128815, mean_q: -7.467793\n",
      " 2068/5000: episode: 2067, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.921 [0.865, 0.967], loss: 12567.537109, mean_absolute_error: 226.220062, mean_q: -7.458899\n",
      " 2069/5000: episode: 2068, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.670 [0.577, 0.759], loss: 38234.683594, mean_absolute_error: 243.882065, mean_q: -7.445096\n",
      " 2070/5000: episode: 2069, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.594, 0.773], loss: 9576.630859, mean_absolute_error: 226.320648, mean_q: -7.426236\n",
      " 2071/5000: episode: 2070, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.491 [0.394, 0.588], loss: 12543.427734, mean_absolute_error: 226.369110, mean_q: -7.404384\n",
      " 2072/5000: episode: 2071, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [0.090, 0.229], loss: 9571.445312, mean_absolute_error: 226.405121, mean_q: -7.378885\n",
      " 2073/5000: episode: 2072, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [0.075, 0.207], loss: 27.032505, mean_absolute_error: 220.573914, mean_q: -7.351892\n",
      " 2074/5000: episode: 2073, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.941, 0.997], loss: 26.830528, mean_absolute_error: 220.605011, mean_q: -7.324371\n",
      " 2075/5000: episode: 2074, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.357 [0.266, 0.452], loss: 26.618237, mean_absolute_error: 220.629425, mean_q: -7.295333\n",
      " 2076/5000: episode: 2075, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.046 [0.012, 0.091], loss: 9175.662109, mean_absolute_error: 220.857162, mean_q: -7.273837\n",
      " 2077/5000: episode: 2076, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.542, 0.728], loss: 26.390045, mean_absolute_error: 220.735336, mean_q: -7.263991\n",
      " 2078/5000: episode: 2077, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.088 [0.039, 0.147], loss: 19089.972656, mean_absolute_error: 232.516617, mean_q: -7.257129\n",
      " 2079/5000: episode: 2078, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.038, 0.145], loss: 26.244478, mean_absolute_error: 220.865433, mean_q: -7.243927\n",
      " 2080/5000: episode: 2079, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.971, 1.000], loss: 26.125446, mean_absolute_error: 220.918427, mean_q: -7.227478\n",
      " 2081/5000: episode: 2080, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.438 [0.343, 0.535], loss: 9551.771484, mean_absolute_error: 226.812515, mean_q: -7.208513\n",
      " 2082/5000: episode: 2081, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.066], loss: 9549.535156, mean_absolute_error: 226.850235, mean_q: -7.186543\n",
      " 2083/5000: episode: 2082, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.617, 0.793], loss: 25.658218, mean_absolute_error: 221.038483, mean_q: -7.162549\n",
      " 2084/5000: episode: 2083, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.622, 0.797], loss: 25.483807, mean_absolute_error: 221.065018, mean_q: -7.138161\n",
      " 2085/5000: episode: 2084, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.952, 0.999], loss: 9543.477539, mean_absolute_error: 226.932343, mean_q: -7.112516\n",
      " 2086/5000: episode: 2085, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.922 [0.866, 0.968], loss: 9541.375000, mean_absolute_error: 226.974670, mean_q: -7.092491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2087/5000: episode: 2086, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.930, 0.995], loss: 3045.780762, mean_absolute_error: 221.215698, mean_q: -7.076738\n",
      " 2088/5000: episode: 2087, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.392 [0.299, 0.488], loss: 9537.265625, mean_absolute_error: 227.106308, mean_q: -7.073783\n",
      " 2089/5000: episode: 2088, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.662, 0.830], loss: 9535.457031, mean_absolute_error: 227.204926, mean_q: -7.079899\n",
      " 2090/5000: episode: 2089, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.689, 0.851], loss: 9533.602539, mean_absolute_error: 227.284149, mean_q: -7.078579\n",
      " 2091/5000: episode: 2090, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.421, 0.615], loss: 25.007929, mean_absolute_error: 221.516449, mean_q: -7.071189\n",
      " 2092/5000: episode: 2091, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.644, 0.815], loss: 1447760.000000, mean_absolute_error: 328.287292, mean_q: -7.060130\n",
      " 2093/5000: episode: 2092, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.452, 0.644], loss: 1438200.000000, mean_absolute_error: 322.531342, mean_q: -7.059706\n",
      " 2094/5000: episode: 2093, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.038], loss: 12660.401367, mean_absolute_error: 227.689301, mean_q: -7.068298\n",
      " 2095/5000: episode: 2094, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.484, 0.675], loss: 19025.357422, mean_absolute_error: 233.502090, mean_q: -7.068239\n",
      " 2096/5000: episode: 2095, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.499, 0.689], loss: 1428502.000000, mean_absolute_error: 316.992126, mean_q: -7.076261\n",
      " 2097/5000: episode: 2096, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.845 [0.771, 0.910], loss: 9520.295898, mean_absolute_error: 227.964798, mean_q: -7.113286\n",
      " 2098/5000: episode: 2097, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.630, 0.804], loss: 9518.176758, mean_absolute_error: 228.133026, mean_q: -7.144560\n",
      " 2099/5000: episode: 2098, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.466 [0.370, 0.563], loss: 25.661514, mean_absolute_error: 222.452148, mean_q: -7.163009\n",
      " 2100/5000: episode: 2099, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.058, 0.179], loss: 9513.556641, mean_absolute_error: 228.384964, mean_q: -7.171511\n",
      " 2101/5000: episode: 2100, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 3075.228516, mean_absolute_error: 222.697327, mean_q: -7.180274\n",
      " 2102/5000: episode: 2101, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.098, 0.241], loss: 25.844879, mean_absolute_error: 222.786102, mean_q: -7.188559\n",
      " 2103/5000: episode: 2102, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 1440432.375000, mean_absolute_error: 323.687500, mean_q: -7.189697\n",
      " 2104/5000: episode: 2103, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.941, 0.997], loss: 9505.968750, mean_absolute_error: 228.794067, mean_q: -7.198884\n",
      " 2105/5000: episode: 2104, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.859 [0.787, 0.921], loss: 18982.224609, mean_absolute_error: 234.711670, mean_q: -7.208169\n",
      " 2106/5000: episode: 2105, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.060 [0.021, 0.111], loss: 9501.868164, mean_absolute_error: 229.012009, mean_q: -7.215496\n",
      " 2107/5000: episode: 2106, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.659, 0.827], loss: 1449697.000000, mean_absolute_error: 329.983459, mean_q: -7.230342\n",
      " 2108/5000: episode: 2107, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.909 [0.849, 0.959], loss: 9497.338867, mean_absolute_error: 229.314682, mean_q: -7.263632\n",
      " 2109/5000: episode: 2108, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.137, 0.295], loss: 3182.163330, mean_absolute_error: 223.735504, mean_q: -7.284331\n",
      " 2110/5000: episode: 2109, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [0.005, 0.067], loss: 1449412.625000, mean_absolute_error: 330.407501, mean_q: -7.294209\n",
      " 2111/5000: episode: 2110, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.906 [0.845, 0.956], loss: 9489.617188, mean_absolute_error: 229.721893, mean_q: -7.309075\n",
      " 2112/5000: episode: 2111, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.889, 0.979], loss: 3204.507812, mean_absolute_error: 224.128128, mean_q: -7.314528\n",
      " 2113/5000: episode: 2112, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.821 [0.743, 0.890], loss: 12564.637695, mean_absolute_error: 229.942917, mean_q: -7.311763\n",
      " 2114/5000: episode: 2113, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.656 [0.562, 0.746], loss: 26.725311, mean_absolute_error: 224.237000, mean_q: -7.309993\n",
      " 2115/5000: episode: 2114, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.301 [0.215, 0.393], loss: 26.781822, mean_absolute_error: 224.353912, mean_q: -7.317719\n",
      " 2116/5000: episode: 2115, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.401 [0.308, 0.497], loss: 26.835516, mean_absolute_error: 224.469543, mean_q: -7.325052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2117/5000: episode: 2116, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.417, 0.611], loss: 26.835033, mean_absolute_error: 224.564056, mean_q: -7.324986\n",
      " 2118/5000: episode: 2117, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.530, 0.718], loss: 26.852404, mean_absolute_error: 224.655243, mean_q: -7.327356\n",
      " 2119/5000: episode: 2118, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 26.878597, mean_absolute_error: 224.743591, mean_q: -7.330930\n",
      " 2120/5000: episode: 2119, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.538, 0.725], loss: 1426687.000000, mean_absolute_error: 319.719330, mean_q: -7.327844\n",
      " 2121/5000: episode: 2120, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.924, 0.993], loss: 26.904812, mean_absolute_error: 224.905548, mean_q: -7.334505\n",
      " 2122/5000: episode: 2121, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.288, 0.476], loss: 3142.921875, mean_absolute_error: 224.997375, mean_q: -7.334910\n",
      " 2123/5000: episode: 2122, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.065], loss: 9466.721680, mean_absolute_error: 230.810776, mean_q: -7.329438\n",
      " 2124/5000: episode: 2123, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.430 [0.335, 0.527], loss: 3256.846680, mean_absolute_error: 225.181229, mean_q: -7.319441\n",
      " 2125/5000: episode: 2124, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.074 [0.030, 0.129], loss: 18902.738281, mean_absolute_error: 236.667572, mean_q: -7.304900\n",
      " 2126/5000: episode: 2125, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 1426424.625000, mean_absolute_error: 320.076355, mean_q: -7.301500\n",
      " 2127/5000: episode: 2126, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 26.812672, mean_absolute_error: 225.317780, mean_q: -7.321933\n",
      " 2128/5000: episode: 2127, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.943, 0.998], loss: 18894.183594, mean_absolute_error: 236.964539, mean_q: -7.332817\n",
      " 2129/5000: episode: 2128, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.274, 0.461], loss: 12662.982422, mean_absolute_error: 231.346924, mean_q: -7.334575\n",
      " 2130/5000: episode: 2129, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.683 [0.591, 0.771], loss: 26.868126, mean_absolute_error: 225.577835, mean_q: -7.329502\n",
      " 2131/5000: episode: 2130, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.972, 1.000], loss: 9455.804688, mean_absolute_error: 231.403000, mean_q: -7.319959\n",
      " 2132/5000: episode: 2131, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.972, 1.000], loss: 9454.108398, mean_absolute_error: 231.449921, mean_q: -7.305796\n",
      " 2133/5000: episode: 2132, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 21996.781250, mean_absolute_error: 237.263000, mean_q: -7.288096\n",
      " 2134/5000: episode: 2133, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.956, 1.000], loss: 26.404909, mean_absolute_error: 225.770813, mean_q: -7.266037\n",
      " 2135/5000: episode: 2134, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.662 [0.568, 0.751], loss: 18869.455078, mean_absolute_error: 237.330460, mean_q: -7.242557\n",
      " 2136/5000: episode: 2135, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.024], loss: 9445.395508, mean_absolute_error: 231.602890, mean_q: -7.216536\n",
      " 2137/5000: episode: 2136, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.879 [0.811, 0.937], loss: 3283.977539, mean_absolute_error: 225.996048, mean_q: -7.196907\n",
      " 2138/5000: episode: 2137, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.166, 0.333], loss: 25.797085, mean_absolute_error: 225.954224, mean_q: -7.181908\n",
      " 2139/5000: episode: 2138, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.553 [0.456, 0.649], loss: 25.727764, mean_absolute_error: 226.020782, mean_q: -7.172251\n",
      " 2140/5000: episode: 2139, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [0.104, 0.250], loss: 18847.753906, mean_absolute_error: 237.600159, mean_q: -7.165893\n",
      " 2141/5000: episode: 2140, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.129, 0.285], loss: 9434.330078, mean_absolute_error: 231.914062, mean_q: -7.153514\n",
      " 2142/5000: episode: 2141, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.900, 0.984], loss: 12653.672852, mean_absolute_error: 232.042328, mean_q: -7.136817\n",
      " 2143/5000: episode: 2142, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.624, 0.799], loss: 3167.118164, mean_absolute_error: 226.285187, mean_q: -7.116416\n",
      " 2144/5000: episode: 2143, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.673, 0.838], loss: 9426.934570, mean_absolute_error: 232.059753, mean_q: -7.093939\n",
      " 2145/5000: episode: 2144, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.054 [0.017, 0.103], loss: 9424.605469, mean_absolute_error: 232.094040, mean_q: -7.069476\n",
      " 2146/5000: episode: 2145, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 1435138.875000, mean_absolute_error: 326.983154, mean_q: -7.051367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2147/5000: episode: 2146, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.419, 0.612], loss: 1435077.375000, mean_absolute_error: 327.074402, mean_q: -7.052539\n",
      " 2148/5000: episode: 2147, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.639, 0.811], loss: 9417.799805, mean_absolute_error: 232.356079, mean_q: -7.061933\n",
      " 2149/5000: episode: 2148, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.082 [0.035, 0.140], loss: 24.950756, mean_absolute_error: 226.718475, mean_q: -7.063100\n",
      " 2150/5000: episode: 2149, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.692, 0.853], loss: 9413.553711, mean_absolute_error: 232.528152, mean_q: -7.058372\n",
      " 2151/5000: episode: 2150, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.902, 0.985], loss: 3272.119385, mean_absolute_error: 226.938416, mean_q: -7.048020\n",
      " 2152/5000: episode: 2151, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 3243.493652, mean_absolute_error: 226.967773, mean_q: -7.033407\n",
      " 2153/5000: episode: 2152, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.049], loss: 9407.878906, mean_absolute_error: 232.680115, mean_q: -7.015570\n",
      " 2154/5000: episode: 2153, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.413, 0.607], loss: 3275.240967, mean_absolute_error: 227.065811, mean_q: -6.995024\n",
      " 2155/5000: episode: 2154, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.452, 0.644], loss: 1425367.750000, mean_absolute_error: 321.851318, mean_q: -6.980030\n",
      " 2156/5000: episode: 2155, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.328, 0.519], loss: 3176.273926, mean_absolute_error: 227.131714, mean_q: -6.984884\n",
      " 2157/5000: episode: 2156, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.039, 0.146], loss: 1434633.000000, mean_absolute_error: 327.738129, mean_q: -6.982322\n",
      " 2158/5000: episode: 2157, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.455 [0.359, 0.552], loss: 37521.144531, mean_absolute_error: 250.238708, mean_q: -7.003902\n",
      " 2159/5000: episode: 2158, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.092], loss: 24.755556, mean_absolute_error: 227.543594, mean_q: -7.035413\n",
      " 2160/5000: episode: 2159, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.210, 0.387], loss: 24.950897, mean_absolute_error: 227.710663, mean_q: -7.063120\n",
      " 2161/5000: episode: 2160, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.725 [0.636, 0.809], loss: 12564.625977, mean_absolute_error: 233.557022, mean_q: -7.080603\n",
      " 2162/5000: episode: 2161, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.063], loss: 25.125725, mean_absolute_error: 227.950195, mean_q: -7.087826\n",
      " 2163/5000: episode: 2162, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.868 [0.798, 0.928], loss: 9387.239258, mean_absolute_error: 233.751221, mean_q: -7.087892\n",
      " 2164/5000: episode: 2163, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.167, 0.334], loss: 25.140123, mean_absolute_error: 228.134033, mean_q: -7.089857\n",
      " 2165/5000: episode: 2164, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.828 [0.751, 0.897], loss: 12568.500000, mean_absolute_error: 233.933014, mean_q: -7.092892\n",
      " 2166/5000: episode: 2165, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.976, 1.000], loss: 25.126017, mean_absolute_error: 228.302246, mean_q: -7.087867\n",
      " 2167/5000: episode: 2166, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.403, 0.596], loss: 18734.054688, mean_absolute_error: 239.774139, mean_q: -7.077153\n",
      " 2168/5000: episode: 2167, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.461, 0.653], loss: 9376.912109, mean_absolute_error: 234.131775, mean_q: -7.060958\n",
      " 2169/5000: episode: 2168, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.700 [0.609, 0.786], loss: 9374.263672, mean_absolute_error: 234.183746, mean_q: -7.041219\n",
      " 2170/5000: episode: 2169, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.632, 0.805], loss: 24.640549, mean_absolute_error: 228.531113, mean_q: -7.019050\n",
      " 2171/5000: episode: 2170, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.280, 0.467], loss: 3217.678711, mean_absolute_error: 228.569885, mean_q: -6.995386\n",
      " 2172/5000: episode: 2171, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 1437101.250000, mean_absolute_error: 329.147461, mean_q: -6.969292\n",
      " 2173/5000: episode: 2172, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.875 [0.806, 0.933], loss: 3334.361328, mean_absolute_error: 228.756226, mean_q: -6.955884\n",
      " 2174/5000: episode: 2173, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.927, 0.994], loss: 3220.483643, mean_absolute_error: 228.727112, mean_q: -6.938081\n",
      " 2175/5000: episode: 2174, duration: 0.080s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.715, 0.870], loss: 1452350.000000, mean_absolute_error: 340.575256, mean_q: -6.916811\n",
      " 2176/5000: episode: 2175, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.613, 0.790], loss: 6568.569336, mean_absolute_error: 228.978653, mean_q: -6.915258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2177/5000: episode: 2176, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.464 [0.367, 0.561], loss: 23.915779, mean_absolute_error: 228.968353, mean_q: -6.915036\n",
      " 2178/5000: episode: 2177, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 1433513.000000, mean_absolute_error: 329.453491, mean_q: -6.909895\n",
      " 2179/5000: episode: 2178, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [0.090, 0.230], loss: 23.911230, mean_absolute_error: 229.146179, mean_q: -6.914379\n",
      " 2180/5000: episode: 2179, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.014, 0.094], loss: 3244.247559, mean_absolute_error: 229.239883, mean_q: -6.912149\n",
      " 2181/5000: episode: 2180, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 3249.303223, mean_absolute_error: 229.309006, mean_q: -6.904784\n",
      " 2182/5000: episode: 2181, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.951, 0.999], loss: 9344.748047, mean_absolute_error: 235.047394, mean_q: -6.900295\n",
      " 2183/5000: episode: 2182, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.600, 0.778], loss: 2857158.500000, mean_absolute_error: 424.552673, mean_q: -6.897010\n",
      " 2184/5000: episode: 2183, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.052], loss: 1423837.625000, mean_absolute_error: 324.304504, mean_q: -6.918523\n",
      " 2185/5000: episode: 2184, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.100 [0.047, 0.162], loss: 3253.946777, mean_absolute_error: 229.773285, mean_q: -6.953152\n",
      " 2186/5000: episode: 2185, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.955, 1.000], loss: 9336.843750, mean_absolute_error: 235.594696, mean_q: -6.981116\n",
      " 2187/5000: episode: 2186, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.696, 0.856], loss: 3273.606201, mean_absolute_error: 230.064819, mean_q: -6.997309\n",
      " 2188/5000: episode: 2187, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.872, 0.971], loss: 18641.916016, mean_absolute_error: 241.503784, mean_q: -7.012839\n",
      " 2189/5000: episode: 2188, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.429 [0.334, 0.526], loss: 24.689304, mean_absolute_error: 230.293839, mean_q: -7.025991\n",
      " 2190/5000: episode: 2189, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 24.726757, mean_absolute_error: 230.386597, mean_q: -7.031319\n",
      " 2191/5000: episode: 2190, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.093 [0.042, 0.153], loss: 9328.380859, mean_absolute_error: 236.123474, mean_q: -7.029994\n",
      " 2192/5000: episode: 2191, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.429 [0.334, 0.526], loss: 24.666059, mean_absolute_error: 230.530304, mean_q: -7.022683\n",
      " 2193/5000: episode: 2192, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [0.011, 0.087], loss: 24.586704, mean_absolute_error: 230.581955, mean_q: -7.011375\n",
      " 2194/5000: episode: 2193, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.289, 0.477], loss: 24.485483, mean_absolute_error: 230.620758, mean_q: -6.996926\n",
      " 2195/5000: episode: 2194, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.950 [0.904, 0.986], loss: 3383.539795, mean_absolute_error: 230.730621, mean_q: -6.978877\n",
      " 2196/5000: episode: 2195, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.924, 0.993], loss: 3304.862305, mean_absolute_error: 230.700073, mean_q: -6.957325\n",
      " 2197/5000: episode: 2196, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.073 [0.029, 0.128], loss: 9320.194336, mean_absolute_error: 236.374313, mean_q: -6.941646\n",
      " 2198/5000: episode: 2197, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.050], loss: 24.014004, mean_absolute_error: 230.772705, mean_q: -6.929224\n",
      " 2199/5000: episode: 2198, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.458, 0.651], loss: 18608.800781, mean_absolute_error: 242.114700, mean_q: -6.912528\n",
      " 2200/5000: episode: 2199, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 9314.218750, mean_absolute_error: 236.505249, mean_q: -6.891560\n",
      " 2201/5000: episode: 2200, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.189, 0.362], loss: 9312.223633, mean_absolute_error: 236.536469, mean_q: -6.868288\n",
      " 2202/5000: episode: 2201, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.870 [0.801, 0.930], loss: 1423004.375000, mean_absolute_error: 325.565643, mean_q: -6.843180\n",
      " 2203/5000: episode: 2202, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.835 [0.759, 0.902], loss: 18593.119141, mean_absolute_error: 242.307495, mean_q: -6.848706\n",
      " 2204/5000: episode: 2203, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.343, 0.536], loss: 23.541943, mean_absolute_error: 231.142914, mean_q: -6.860770\n",
      " 2205/5000: episode: 2204, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.925, 0.993], loss: 23.570877, mean_absolute_error: 231.240601, mean_q: -6.864985\n",
      " 2206/5000: episode: 2205, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.053 [0.017, 0.101], loss: 23.554054, mean_absolute_error: 231.319122, mean_q: -6.862535\n",
      " 2207/5000: episode: 2206, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.692, 0.853], loss: 9301.045898, mean_absolute_error: 237.015808, mean_q: -6.854957\n",
      " 2208/5000: episode: 2207, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [0.005, 0.067], loss: 12599.988281, mean_absolute_error: 237.084579, mean_q: -6.842573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2209/5000: episode: 2208, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 12575.468750, mean_absolute_error: 237.111389, mean_q: -6.825758\n",
      " 2210/5000: episode: 2209, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.494 [0.398, 0.591], loss: 9295.608398, mean_absolute_error: 237.147522, mean_q: -6.805545\n",
      " 2211/5000: episode: 2210, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.373, 0.567], loss: 23.013008, mean_absolute_error: 231.546265, mean_q: -6.783247\n",
      " 2212/5000: episode: 2211, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 3399.186523, mean_absolute_error: 231.640869, mean_q: -6.760323\n",
      " 2213/5000: episode: 2212, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 22.693298, mean_absolute_error: 231.591827, mean_q: -6.735958\n",
      " 2214/5000: episode: 2213, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.212, 0.389], loss: 22.522888, mean_absolute_error: 231.609222, mean_q: -6.710615\n",
      " 2215/5000: episode: 2214, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.243, 0.425], loss: 9287.581055, mean_absolute_error: 237.248840, mean_q: -6.684145\n",
      " 2216/5000: episode: 2215, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 1431849.875000, mean_absolute_error: 331.887115, mean_q: -6.656370\n",
      " 2217/5000: episode: 2216, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.939, 0.997], loss: 9283.609375, mean_absolute_error: 237.326035, mean_q: -6.643219\n",
      " 2218/5000: episode: 2217, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [0.061, 0.185], loss: 18540.914062, mean_absolute_error: 242.996490, mean_q: -6.626316\n",
      " 2219/5000: episode: 2218, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 21.882111, mean_absolute_error: 231.822235, mean_q: -6.614453\n",
      " 2220/5000: episode: 2219, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.844], loss: 9276.933594, mean_absolute_error: 237.531357, mean_q: -6.615061\n",
      " 2221/5000: episode: 2220, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 18527.521484, mean_absolute_error: 243.243805, mean_q: -6.616835\n",
      " 2222/5000: episode: 2221, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.129, 0.284], loss: 3315.323975, mean_absolute_error: 232.099731, mean_q: -6.611536\n",
      " 2223/5000: episode: 2222, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.869 [0.799, 0.929], loss: 21.793938, mean_absolute_error: 232.170135, mean_q: -6.601111\n",
      " 2224/5000: episode: 2223, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.911 [0.851, 0.960], loss: 9267.770508, mean_absolute_error: 237.857056, mean_q: -6.595743\n",
      " 2225/5000: episode: 2224, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 3321.512451, mean_absolute_error: 232.327698, mean_q: -6.592884\n",
      " 2226/5000: episode: 2225, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.610, 0.787], loss: 15942.333984, mean_absolute_error: 238.048660, mean_q: -6.584770\n",
      " 2227/5000: episode: 2226, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 21.596626, mean_absolute_error: 232.437790, mean_q: -6.571157\n",
      " 2228/5000: episode: 2227, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 3328.402100, mean_absolute_error: 232.479385, mean_q: -6.554285\n",
      " 2229/5000: episode: 2228, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.275, 0.462], loss: 3351.671143, mean_absolute_error: 232.525970, mean_q: -6.534580\n",
      " 2230/5000: episode: 2229, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.507, 0.696], loss: 1425485.750000, mean_absolute_error: 327.180634, mean_q: -6.512783\n",
      " 2231/5000: episode: 2230, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.625, 0.800], loss: 21.214294, mean_absolute_error: 232.598114, mean_q: -6.512723\n",
      " 2232/5000: episode: 2231, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 3373.176758, mean_absolute_error: 232.703522, mean_q: -6.514647\n",
      " 2233/5000: episode: 2232, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.584, 0.765], loss: 21.192785, mean_absolute_error: 232.729782, mean_q: -6.509420\n",
      " 2234/5000: episode: 2233, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.174, 0.343], loss: 9253.712891, mean_absolute_error: 238.391357, mean_q: -6.507610\n",
      " 2235/5000: episode: 2234, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.954, 0.999], loss: 9252.920898, mean_absolute_error: 238.461029, mean_q: -6.508132\n",
      " 2236/5000: episode: 2235, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 3414.578857, mean_absolute_error: 232.978958, mean_q: -6.502510\n",
      " 2237/5000: episode: 2236, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.582 [0.486, 0.677], loss: 9249.839844, mean_absolute_error: 238.588470, mean_q: -6.499327\n",
      " 2238/5000: episode: 2237, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 27701.859375, mean_absolute_error: 249.853607, mean_q: -6.497753\n",
      " 2239/5000: episode: 2238, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.949, 0.999], loss: 1421673.625000, mean_absolute_error: 327.702881, mean_q: -6.489151\n",
      " 2240/5000: episode: 2239, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 21.084003, mean_absolute_error: 233.242981, mean_q: -6.492690\n",
      " 2241/5000: episode: 2240, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.088 [0.039, 0.148], loss: 21.069000, mean_absolute_error: 233.318558, mean_q: -6.490379\n",
      " 2242/5000: episode: 2241, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.889, 0.979], loss: 3353.342041, mean_absolute_error: 233.381317, mean_q: -6.483187\n",
      " 2243/5000: episode: 2242, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.453 [0.357, 0.550], loss: 1430716.375000, mean_absolute_error: 333.557831, mean_q: -6.471213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2244/5000: episode: 2243, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.111, 0.260], loss: 18454.046875, mean_absolute_error: 244.676300, mean_q: -6.471086\n",
      " 2245/5000: episode: 2244, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.691 [0.599, 0.778], loss: 18450.234375, mean_absolute_error: 244.762207, mean_q: -6.472429\n",
      " 2246/5000: episode: 2245, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.340, 0.532], loss: 9233.293945, mean_absolute_error: 239.276535, mean_q: -6.474278\n",
      " 2247/5000: episode: 2246, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.211, 0.387], loss: 18440.906250, mean_absolute_error: 244.937469, mean_q: -6.469703\n",
      " 2248/5000: episode: 2247, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.415 [0.320, 0.511], loss: 20.867191, mean_absolute_error: 233.853424, mean_q: -6.459215\n",
      " 2249/5000: episode: 2248, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.917, 0.991], loss: 20.779354, mean_absolute_error: 233.908325, mean_q: -6.445604\n",
      " 2250/5000: episode: 2249, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.696, 0.856], loss: 18428.373047, mean_absolute_error: 245.101929, mean_q: -6.429916\n",
      " 2251/5000: episode: 2250, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.445 [0.350, 0.542], loss: 20.610607, mean_absolute_error: 234.016373, mean_q: -6.419375\n",
      " 2252/5000: episode: 2251, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.175, 0.344], loss: 9220.973633, mean_absolute_error: 239.653473, mean_q: -6.413552\n",
      " 2253/5000: episode: 2252, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.896, 0.982], loss: 3399.972412, mean_absolute_error: 234.156219, mean_q: -6.403382\n",
      " 2254/5000: episode: 2253, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.621, 0.796], loss: 9218.059570, mean_absolute_error: 239.767029, mean_q: -6.398050\n",
      " 2255/5000: episode: 2254, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.844 [0.770, 0.909], loss: 20.512329, mean_absolute_error: 234.290100, mean_q: -6.404049\n",
      " 2256/5000: episode: 2255, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.600 [0.504, 0.694], loss: 3408.081299, mean_absolute_error: 234.403336, mean_q: -6.410588\n",
      " 2257/5000: episode: 2256, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.948, 0.999], loss: 1439253.000000, mean_absolute_error: 340.088318, mean_q: -6.409522\n",
      " 2258/5000: episode: 2257, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.540 [0.443, 0.636], loss: 20.601738, mean_absolute_error: 234.569870, mean_q: -6.417993\n",
      " 2259/5000: episode: 2258, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [0.010, 0.084], loss: 18398.779297, mean_absolute_error: 245.780258, mean_q: -6.419481\n",
      " 2260/5000: episode: 2259, duration: 0.069s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.388, 0.581], loss: 20.631004, mean_absolute_error: 234.757812, mean_q: -6.422551\n",
      " 2261/5000: episode: 2260, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.903 [0.841, 0.954], loss: 20.662655, mean_absolute_error: 234.854813, mean_q: -6.427476\n",
      " 2262/5000: episode: 2261, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.613, 0.790], loss: 27570.246094, mean_absolute_error: 251.620514, mean_q: -6.433558\n",
      " 2263/5000: episode: 2262, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 12723.497070, mean_absolute_error: 240.723526, mean_q: -6.437615\n",
      " 2264/5000: episode: 2263, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.080 [0.034, 0.137], loss: 20.703022, mean_absolute_error: 235.167053, mean_q: -6.433753\n",
      " 2265/5000: episode: 2264, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.037], loss: 18370.437500, mean_absolute_error: 246.343079, mean_q: -6.425622\n",
      " 2266/5000: episode: 2265, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 20.567108, mean_absolute_error: 235.316162, mean_q: -6.412596\n",
      " 2267/5000: episode: 2266, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 20.466621, mean_absolute_error: 235.368027, mean_q: -6.396909\n",
      " 2268/5000: episode: 2267, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.650 [0.556, 0.741], loss: 9188.969727, mean_absolute_error: 240.950684, mean_q: -6.378679\n",
      " 2269/5000: episode: 2268, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.093], loss: 9186.800781, mean_absolute_error: 241.010284, mean_q: -6.365583\n",
      " 2270/5000: episode: 2269, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.489, 0.680], loss: 1438572.250000, mean_absolute_error: 341.074646, mean_q: -6.356435\n",
      " 2271/5000: episode: 2270, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.219, 0.397], loss: 20.221132, mean_absolute_error: 235.644531, mean_q: -6.358423\n",
      " 2272/5000: episode: 2271, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.114, 0.264], loss: 9179.844727, mean_absolute_error: 241.263062, mean_q: -6.354965\n",
      " 2273/5000: episode: 2272, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.674, 0.839], loss: 6942.653809, mean_absolute_error: 235.882675, mean_q: -6.346437\n",
      " 2274/5000: episode: 2273, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.595 [0.498, 0.688], loss: 9175.935547, mean_absolute_error: 241.386932, mean_q: -6.332366\n",
      " 2275/5000: episode: 2274, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 12620.128906, mean_absolute_error: 241.462616, mean_q: -6.314885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2276/5000: episode: 2275, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.150, 0.312], loss: 19.815453, mean_absolute_error: 235.945145, mean_q: -6.294308\n",
      " 2277/5000: episode: 2276, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.118, 0.269], loss: 3425.897461, mean_absolute_error: 235.983154, mean_q: -6.272349\n",
      " 2278/5000: episode: 2277, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.247, 0.429], loss: 21861.455078, mean_absolute_error: 247.160248, mean_q: -6.248447\n",
      " 2279/5000: episode: 2278, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 15984.510742, mean_absolute_error: 241.567780, mean_q: -6.222059\n",
      " 2280/5000: episode: 2279, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.151, 0.313], loss: 19.239853, mean_absolute_error: 236.080307, mean_q: -6.202201\n",
      " 2281/5000: episode: 2280, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.049 [0.014, 0.095], loss: 19.203918, mean_absolute_error: 236.150208, mean_q: -6.196405\n",
      " 2282/5000: episode: 2281, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.971, 1.000], loss: 1419887.750000, mean_absolute_error: 330.656921, mean_q: -6.192887\n",
      " 2283/5000: episode: 2282, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.156, 0.319], loss: 1419822.125000, mean_absolute_error: 330.750732, mean_q: -6.199914\n",
      " 2284/5000: episode: 2283, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 19.325001, mean_absolute_error: 236.446075, mean_q: -6.215913\n",
      " 2285/5000: episode: 2284, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.572 [0.476, 0.667], loss: 2842771.500000, mean_absolute_error: 425.423340, mean_q: -6.232238\n",
      " 2286/5000: episode: 2285, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 3598.159912, mean_absolute_error: 236.877701, mean_q: -6.287167\n",
      " 2287/5000: episode: 2286, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 3500.269287, mean_absolute_error: 237.006500, mean_q: -6.341320\n",
      " 2288/5000: episode: 2287, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.180, 0.350], loss: 3611.989014, mean_absolute_error: 237.259094, mean_q: -6.386564\n",
      " 2289/5000: episode: 2288, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.834 [0.758, 0.902], loss: 3470.422119, mean_absolute_error: 237.321869, mean_q: -6.425451\n",
      " 2290/5000: episode: 2289, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.831 [0.755, 0.899], loss: 20.861206, mean_absolute_error: 237.474182, mean_q: -6.458289\n",
      " 2291/5000: episode: 2290, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.877 [0.809, 0.935], loss: 1437175.250000, mean_absolute_error: 342.971985, mean_q: -6.479794\n",
      " 2292/5000: episode: 2291, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.883 [0.817, 0.940], loss: 3488.571777, mean_absolute_error: 237.745697, mean_q: -6.506611\n",
      " 2293/5000: episode: 2292, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 9143.569336, mean_absolute_error: 243.361847, mean_q: -6.522077\n",
      " 2294/5000: episode: 2293, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.125, 0.279], loss: 9141.884766, mean_absolute_error: 243.463928, mean_q: -6.528152\n",
      " 2295/5000: episode: 2294, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.739, 0.888], loss: 3495.629883, mean_absolute_error: 238.054169, mean_q: -6.526250\n",
      " 2296/5000: episode: 2295, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.338, 0.529], loss: 21.249620, mean_absolute_error: 238.128143, mean_q: -6.518144\n",
      " 2297/5000: episode: 2296, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.080 [0.034, 0.137], loss: 2858832.250000, mean_absolute_error: 437.880798, mean_q: -6.506308\n",
      " 2298/5000: episode: 2297, duration: 0.069s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 18244.347656, mean_absolute_error: 249.304779, mean_q: -6.521274\n",
      " 2299/5000: episode: 2298, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 12763.899414, mean_absolute_error: 244.029800, mean_q: -6.526257\n",
      " 2300/5000: episode: 2299, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.697, 0.857], loss: 18233.468750, mean_absolute_error: 249.505707, mean_q: -6.523333\n",
      " 2301/5000: episode: 2300, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.461, 0.653], loss: 21.223534, mean_absolute_error: 238.611877, mean_q: -6.514142\n",
      " 2302/5000: episode: 2301, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.117, 0.267], loss: 21.133627, mean_absolute_error: 238.674896, mean_q: -6.500327\n",
      " 2303/5000: episode: 2302, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.103 [0.050, 0.166], loss: 21.025909, mean_absolute_error: 238.729919, mean_q: -6.483737\n",
      " 2304/5000: episode: 2303, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.851 [0.778, 0.915], loss: 9117.755859, mean_absolute_error: 244.258224, mean_q: -6.464758\n",
      " 2305/5000: episode: 2304, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.549, 0.734], loss: 20.759245, mean_absolute_error: 238.822189, mean_q: -6.442484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2306/5000: episode: 2305, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.064], loss: 1421714.250000, mean_absolute_error: 333.188904, mean_q: -6.418952\n",
      " 2307/5000: episode: 2306, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.939, 0.997], loss: 20.552967, mean_absolute_error: 238.926971, mean_q: -6.410391\n",
      " 2308/5000: episode: 2307, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.532, 0.719], loss: 3661.811279, mean_absolute_error: 239.079803, mean_q: -6.398211\n",
      " 2309/5000: episode: 2308, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.880 [0.813, 0.938], loss: 20.427248, mean_absolute_error: 239.038452, mean_q: -6.390752\n",
      " 2310/5000: episode: 2309, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 20.403034, mean_absolute_error: 239.110764, mean_q: -6.386963\n",
      " 2311/5000: episode: 2310, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 1427092.250000, mean_absolute_error: 338.933929, mean_q: -6.378341\n",
      " 2312/5000: episode: 2311, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [0.095, 0.237], loss: 18187.037109, mean_absolute_error: 250.185791, mean_q: -6.381494\n",
      " 2313/5000: episode: 2312, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 1417902.250000, mean_absolute_error: 333.613129, mean_q: -6.377600\n",
      " 2314/5000: episode: 2313, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 18180.500000, mean_absolute_error: 250.352051, mean_q: -6.384871\n",
      " 2315/5000: episode: 2314, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.415, 0.608], loss: 16281.613281, mean_absolute_error: 245.082840, mean_q: -6.383034\n",
      " 2316/5000: episode: 2315, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 20.314081, mean_absolute_error: 239.595840, mean_q: -6.373023\n",
      " 2317/5000: episode: 2316, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.922 [0.865, 0.967], loss: 20.227661, mean_absolute_error: 239.662964, mean_q: -6.359450\n",
      " 2318/5000: episode: 2317, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 20.127369, mean_absolute_error: 239.716522, mean_q: -6.343662\n",
      " 2319/5000: episode: 2318, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.176, 0.345], loss: 9088.987305, mean_absolute_error: 245.213074, mean_q: -6.325657\n",
      " 2320/5000: episode: 2319, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.672, 0.837], loss: 9086.551758, mean_absolute_error: 245.252472, mean_q: -6.305045\n",
      " 2321/5000: episode: 2320, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.868, 0.969], loss: 30858.425781, mean_absolute_error: 256.274109, mean_q: -6.282546\n",
      " 2322/5000: episode: 2321, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.405, 0.598], loss: 7226.144531, mean_absolute_error: 239.998306, mean_q: -6.265296\n",
      " 2323/5000: episode: 2322, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.790, 0.923], loss: 9079.942383, mean_absolute_error: 245.396744, mean_q: -6.252394\n",
      " 2324/5000: episode: 2323, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.123, 0.276], loss: 1444720.875000, mean_absolute_error: 350.606262, mean_q: -6.236232\n",
      " 2325/5000: episode: 2324, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 1426535.750000, mean_absolute_error: 339.817993, mean_q: -6.240952\n",
      " 2326/5000: episode: 2325, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.739, 0.888], loss: 18126.253906, mean_absolute_error: 251.144897, mean_q: -6.262284\n",
      " 2327/5000: episode: 2326, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 19.731754, mean_absolute_error: 240.411987, mean_q: -6.280999\n",
      " 2328/5000: episode: 2327, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.662, 0.830], loss: 1417204.250000, mean_absolute_error: 334.798645, mean_q: -6.298005\n",
      " 2329/5000: episode: 2328, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.780, 0.916], loss: 27154.695312, mean_absolute_error: 257.017303, mean_q: -6.322422\n",
      " 2330/5000: episode: 2329, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [0.011, 0.087], loss: 18104.210938, mean_absolute_error: 251.714188, mean_q: -6.335455\n",
      " 2331/5000: episode: 2330, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.772 [0.688, 0.850], loss: 3723.634521, mean_absolute_error: 241.063568, mean_q: -6.339868\n",
      " 2332/5000: episode: 2331, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.465, 0.657], loss: 20.142277, mean_absolute_error: 241.072739, mean_q: -6.346012\n",
      " 2333/5000: episode: 2332, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 20.189285, mean_absolute_error: 241.176559, mean_q: -6.353414\n",
      " 2334/5000: episode: 2333, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.249, 0.433], loss: 3590.044434, mean_absolute_error: 241.264252, mean_q: -6.354656\n",
      " 2335/5000: episode: 2334, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.649, 0.819], loss: 1429435.000000, mean_absolute_error: 341.034882, mean_q: -6.350035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2336/5000: episode: 2335, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.856, 0.963], loss: 1416662.250000, mean_absolute_error: 335.620941, mean_q: -6.356138\n",
      " 2337/5000: episode: 2336, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.466, 0.658], loss: 1416583.000000, mean_absolute_error: 335.728516, mean_q: -6.372416\n",
      " 2338/5000: episode: 2337, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.697, 0.857], loss: 9048.631836, mean_absolute_error: 247.081268, mean_q: -6.396502\n",
      " 2339/5000: episode: 2338, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [0.086, 0.223], loss: 20.548817, mean_absolute_error: 241.769409, mean_q: -6.409743\n",
      " 2340/5000: episode: 2339, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.714, 0.870], loss: 20.584303, mean_absolute_error: 241.853607, mean_q: -6.415277\n",
      " 2341/5000: episode: 2340, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.115, 0.265], loss: 1425331.750000, mean_absolute_error: 341.517120, mean_q: -6.414502\n",
      " 2342/5000: episode: 2341, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 12781.634766, mean_absolute_error: 247.529968, mean_q: -6.424106\n",
      " 2343/5000: episode: 2342, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.821, 0.942], loss: 20.702522, mean_absolute_error: 242.110855, mean_q: -6.433675\n",
      " 2344/5000: episode: 2343, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.475 [0.379, 0.572], loss: 20.822617, mean_absolute_error: 242.225830, mean_q: -6.452312\n",
      " 2345/5000: episode: 2344, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 9041.109375, mean_absolute_error: 247.770630, mean_q: -6.478625\n",
      " 2346/5000: episode: 2345, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.098], loss: 2835542.500000, mean_absolute_error: 430.891724, mean_q: -6.501633\n",
      " 2347/5000: episode: 2346, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 21.431158, mean_absolute_error: 242.670700, mean_q: -6.545932\n",
      " 2348/5000: episode: 2347, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.064 [0.023, 0.116], loss: 9037.138672, mean_absolute_error: 248.232101, mean_q: -6.575517\n",
      " 2349/5000: episode: 2348, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.259, 0.444], loss: 1415562.375000, mean_absolute_error: 337.103149, mean_q: -6.601068\n",
      " 2350/5000: episode: 2349, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.521 [0.424, 0.617], loss: 22.052200, mean_absolute_error: 243.159698, mean_q: -6.640114\n",
      " 2351/5000: episode: 2350, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.093], loss: 18041.011719, mean_absolute_error: 254.114777, mean_q: -6.666383\n",
      " 2352/5000: episode: 2351, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.151, 0.313], loss: 22.317715, mean_absolute_error: 243.437164, mean_q: -6.679976\n",
      " 2353/5000: episode: 2352, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.925, 0.993], loss: 22.343670, mean_absolute_error: 243.543076, mean_q: -6.683859\n",
      " 2354/5000: episode: 2353, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.080 [0.033, 0.137], loss: 2839256.000000, mean_absolute_error: 437.224396, mean_q: -6.681006\n",
      " 2355/5000: episode: 2354, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 18022.972656, mean_absolute_error: 254.575378, mean_q: -6.704659\n",
      " 2356/5000: episode: 2355, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.895, 0.982], loss: 22.566874, mean_absolute_error: 243.914307, mean_q: -6.717165\n",
      " 2357/5000: episode: 2356, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [0.082, 0.217], loss: 9018.055664, mean_absolute_error: 249.431885, mean_q: -6.729553\n",
      " 2358/5000: episode: 2357, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.738, 0.887], loss: 18008.691406, mean_absolute_error: 254.943130, mean_q: -6.740139\n",
      " 2359/5000: episode: 2358, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.888 [0.822, 0.943], loss: 9013.017578, mean_absolute_error: 249.681915, mean_q: -6.749768\n",
      " 2360/5000: episode: 2359, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.921, 0.992], loss: 17998.007812, mean_absolute_error: 255.190857, mean_q: -6.758746\n",
      " 2361/5000: episode: 2360, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 22.850876, mean_absolute_error: 244.530441, mean_q: -6.759307\n",
      " 2362/5000: episode: 2361, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.121, 0.273], loss: 22.816515, mean_absolute_error: 244.614563, mean_q: -6.754222\n",
      " 2363/5000: episode: 2362, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.597, 0.776], loss: 9003.291992, mean_absolute_error: 250.061981, mean_q: -6.744243\n",
      " 2364/5000: episode: 2363, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.621, 0.796], loss: 22.646816, mean_absolute_error: 244.744537, mean_q: -6.729054\n",
      " 2365/5000: episode: 2364, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.490 [0.393, 0.587], loss: 22.582287, mean_absolute_error: 244.815140, mean_q: -6.719460\n",
      " 2366/5000: episode: 2365, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.132], loss: 1423351.375000, mean_absolute_error: 344.306213, mean_q: -6.713826\n",
      " 2367/5000: episode: 2366, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.873 [0.804, 0.932], loss: 3740.781006, mean_absolute_error: 245.005112, mean_q: -6.719143\n",
      " 2368/5000: episode: 2367, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.392 [0.299, 0.488], loss: 22.563757, mean_absolute_error: 245.070557, mean_q: -6.716702\n",
      " 2369/5000: episode: 2368, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.286, 0.473], loss: 8991.558594, mean_absolute_error: 250.510300, mean_q: -6.707589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2370/5000: episode: 2369, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.914 [0.855, 0.962], loss: 7573.520996, mean_absolute_error: 245.308701, mean_q: -6.693081\n",
      " 2371/5000: episode: 2370, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.143, 0.303], loss: 8987.292969, mean_absolute_error: 250.612045, mean_q: -6.673631\n",
      " 2372/5000: episode: 2371, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.249 [0.169, 0.336], loss: 8984.987305, mean_absolute_error: 250.651855, mean_q: -6.650807\n",
      " 2373/5000: episode: 2372, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.101 [0.049, 0.164], loss: 21644.351562, mean_absolute_error: 256.066772, mean_q: -6.633695\n",
      " 2374/5000: episode: 2373, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.937, 0.996], loss: 21.963909, mean_absolute_error: 245.437836, mean_q: -6.626807\n",
      " 2375/5000: episode: 2374, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [0.004, 0.067], loss: 8976.965820, mean_absolute_error: 250.891571, mean_q: -6.622519\n",
      " 2376/5000: episode: 2375, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.059 [0.020, 0.109], loss: 8974.248047, mean_absolute_error: 250.972076, mean_q: -6.613054\n",
      " 2377/5000: episode: 2376, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.368 [0.276, 0.463], loss: 8971.657227, mean_absolute_error: 251.038208, mean_q: -6.598924\n",
      " 2378/5000: episode: 2377, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.347 [0.257, 0.441], loss: 8969.143555, mean_absolute_error: 251.095795, mean_q: -6.581138\n",
      " 2379/5000: episode: 2378, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.711, 0.867], loss: 21.525686, mean_absolute_error: 245.799225, mean_q: -6.560355\n",
      " 2380/5000: episode: 2379, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.872, 0.971], loss: 8964.117188, mean_absolute_error: 251.189499, mean_q: -6.537459\n",
      " 2381/5000: episode: 2380, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.451 [0.355, 0.548], loss: 21.213099, mean_absolute_error: 245.884338, mean_q: -6.512540\n",
      " 2382/5000: episode: 2381, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.030], loss: 8959.386719, mean_absolute_error: 251.281189, mean_q: -6.495644\n",
      " 2383/5000: episode: 2382, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.996], loss: 8957.052734, mean_absolute_error: 251.348022, mean_q: -6.483356\n",
      " 2384/5000: episode: 2383, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.038], loss: 8954.500000, mean_absolute_error: 251.427704, mean_q: -6.474867\n",
      " 2385/5000: episode: 2384, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.565 [0.468, 0.660], loss: 12828.231445, mean_absolute_error: 251.621368, mean_q: -6.468050\n",
      " 2386/5000: episode: 2385, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.876, 0.973], loss: 8948.859375, mean_absolute_error: 251.589127, mean_q: -6.455112\n",
      " 2387/5000: episode: 2386, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.491, 0.682], loss: 8946.176758, mean_absolute_error: 251.652115, mean_q: -6.438299\n",
      " 2388/5000: episode: 2387, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.039], loss: 20.659571, mean_absolute_error: 246.397705, mean_q: -6.426997\n",
      " 2389/5000: episode: 2388, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [0.100, 0.243], loss: 20.670593, mean_absolute_error: 246.497864, mean_q: -6.428711\n",
      " 2390/5000: episode: 2389, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.099], loss: 12785.743164, mean_absolute_error: 251.997452, mean_q: -6.432228\n",
      " 2391/5000: episode: 2390, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.645 [0.550, 0.735], loss: 8937.233398, mean_absolute_error: 252.000671, mean_q: -6.427981\n",
      " 2392/5000: episode: 2391, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.426 [0.331, 0.523], loss: 3759.540283, mean_absolute_error: 246.749695, mean_q: -6.418681\n",
      " 2393/5000: episode: 2392, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.657, 0.826], loss: 8933.486328, mean_absolute_error: 252.116638, mean_q: -6.404456\n",
      " 2394/5000: episode: 2393, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.455 [0.359, 0.552], loss: 8931.704102, mean_absolute_error: 252.157455, mean_q: -6.385971\n",
      " 2395/5000: episode: 2394, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.898 [0.835, 0.951], loss: 20.258717, mean_absolute_error: 246.877426, mean_q: -6.364331\n",
      " 2396/5000: episode: 2395, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.865 [0.795, 0.926], loss: 1429867.500000, mean_absolute_error: 346.303375, mean_q: -6.341491\n",
      " 2397/5000: episode: 2396, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.819 [0.741, 0.890], loss: 8926.036133, mean_absolute_error: 252.284012, mean_q: -6.331212\n",
      " 2398/5000: episode: 2397, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.663 [0.570, 0.753], loss: 8923.795898, mean_absolute_error: 252.339844, mean_q: -6.315939\n",
      " 2399/5000: episode: 2398, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.341, 0.533], loss: 1422116.500000, mean_absolute_error: 346.351807, mean_q: -6.296972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2400/5000: episode: 2399, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.620, 0.795], loss: 1422067.000000, mean_absolute_error: 346.429810, mean_q: -6.291760\n",
      " 2401/5000: episode: 2400, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.510, 0.699], loss: 12684.375977, mean_absolute_error: 252.586792, mean_q: -6.297593\n",
      " 2402/5000: episode: 2401, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.104], loss: 19.822460, mean_absolute_error: 247.364197, mean_q: -6.295421\n",
      " 2403/5000: episode: 2402, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.615, 0.791], loss: 12818.263672, mean_absolute_error: 252.834442, mean_q: -6.288287\n",
      " 2404/5000: episode: 2403, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.067 [0.025, 0.120], loss: 8910.894531, mean_absolute_error: 252.788696, mean_q: -6.275792\n",
      " 2405/5000: episode: 2404, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.903 [0.841, 0.954], loss: 19.596333, mean_absolute_error: 247.537750, mean_q: -6.259405\n",
      " 2406/5000: episode: 2405, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.051, 0.168], loss: 12713.708984, mean_absolute_error: 252.903366, mean_q: -6.240162\n",
      " 2407/5000: episode: 2406, duration: 0.044s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.100], loss: 19.326767, mean_absolute_error: 247.616150, mean_q: -6.216197\n",
      " 2408/5000: episode: 2407, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.902, 0.985], loss: 1412903.125000, mean_absolute_error: 341.599335, mean_q: -6.190084\n",
      " 2409/5000: episode: 2408, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.625, 0.800], loss: 17779.886719, mean_absolute_error: 258.314941, mean_q: -6.179959\n",
      " 2410/5000: episode: 2409, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.860 [0.788, 0.922], loss: 19.009455, mean_absolute_error: 247.805115, mean_q: -6.164948\n",
      " 2411/5000: episode: 2410, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [0.065, 0.192], loss: 18.894320, mean_absolute_error: 247.867767, mean_q: -6.146246\n",
      " 2412/5000: episode: 2411, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 18.764320, mean_absolute_error: 247.924408, mean_q: -6.125062\n",
      " 2413/5000: episode: 2412, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.265, 0.450], loss: 18.627659, mean_absolute_error: 247.969025, mean_q: -6.102714\n",
      " 2414/5000: episode: 2413, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.888 [0.822, 0.943], loss: 8886.185547, mean_absolute_error: 253.285034, mean_q: -6.079498\n",
      " 2415/5000: episode: 2414, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.438 [0.343, 0.535], loss: 3915.032959, mean_absolute_error: 248.121780, mean_q: -6.055215\n",
      " 2416/5000: episode: 2415, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.080], loss: 8882.201172, mean_absolute_error: 253.337463, mean_q: -6.028919\n",
      " 2417/5000: episode: 2416, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.037], loss: 17741.750000, mean_absolute_error: 258.631836, mean_q: -5.999996\n",
      " 2418/5000: episode: 2417, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.063], loss: 17.824209, mean_absolute_error: 248.115158, mean_q: -5.969630\n",
      " 2419/5000: episode: 2418, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 8874.936523, mean_absolute_error: 253.407501, mean_q: -5.939927\n",
      " 2420/5000: episode: 2419, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.137, 0.295], loss: 17.468853, mean_absolute_error: 248.160309, mean_q: -5.909812\n",
      " 2421/5000: episode: 2420, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.938, 0.997], loss: 8870.473633, mean_absolute_error: 253.445114, mean_q: -5.880398\n",
      " 2422/5000: episode: 2421, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 8868.383789, mean_absolute_error: 253.462204, mean_q: -5.850951\n",
      " 2423/5000: episode: 2422, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.069 [0.026, 0.122], loss: 8866.169922, mean_absolute_error: 253.480621, mean_q: -5.821090\n",
      " 2424/5000: episode: 2423, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.159, 0.324], loss: 3834.212402, mean_absolute_error: 248.266525, mean_q: -5.790865\n",
      " 2425/5000: episode: 2424, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.176, 0.345], loss: 3833.974365, mean_absolute_error: 248.275131, mean_q: -5.759371\n",
      " 2426/5000: episode: 2425, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.874, 0.972], loss: 16.400967, mean_absolute_error: 248.250519, mean_q: -5.726297\n",
      " 2427/5000: episode: 2426, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.035], loss: 16.215626, mean_absolute_error: 248.250671, mean_q: -5.693844\n",
      " 2428/5000: episode: 2427, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.959, 1.000], loss: 21515.001953, mean_absolute_error: 258.813202, mean_q: -5.671422\n",
      " 2429/5000: episode: 2428, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.028], loss: 12642.737305, mean_absolute_error: 253.581848, mean_q: -5.653569\n",
      " 2430/5000: episode: 2429, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 8852.933594, mean_absolute_error: 253.610825, mean_q: -5.632514\n",
      " 2431/5000: episode: 2430, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.434, 0.627], loss: 15.743366, mean_absolute_error: 248.393616, mean_q: -5.610304\n",
      " 2432/5000: episode: 2431, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 15.614903, mean_absolute_error: 248.418488, mean_q: -5.587364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2433/5000: episode: 2432, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.903, 0.985], loss: 17679.423828, mean_absolute_error: 258.953186, mean_q: -5.572780\n",
      " 2434/5000: episode: 2433, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.711, 0.868], loss: 15.479723, mean_absolute_error: 248.516266, mean_q: -5.563121\n",
      " 2435/5000: episode: 2434, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.542, 0.728], loss: 3947.635742, mean_absolute_error: 248.665497, mean_q: -5.549717\n",
      " 2436/5000: episode: 2435, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.441 [0.345, 0.538], loss: 15.359674, mean_absolute_error: 248.623703, mean_q: -5.541504\n",
      " 2437/5000: episode: 2436, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 15.341860, mean_absolute_error: 248.689224, mean_q: -5.538289\n",
      " 2438/5000: episode: 2437, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.287, 0.475], loss: 15.303116, mean_absolute_error: 248.742126, mean_q: -5.531290\n",
      " 2439/5000: episode: 2438, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.895, 0.982], loss: 1412432.625000, mean_absolute_error: 342.712524, mean_q: -5.529972\n",
      " 2440/5000: episode: 2439, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.942, 0.998], loss: 15.400482, mean_absolute_error: 248.915680, mean_q: -5.548862\n",
      " 2441/5000: episode: 2440, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.682, 0.845], loss: 1421108.125000, mean_absolute_error: 348.147156, mean_q: -5.559162\n",
      " 2442/5000: episode: 2441, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [0.073, 0.203], loss: 15.565172, mean_absolute_error: 249.133057, mean_q: -5.578457\n",
      " 2443/5000: episode: 2442, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.331 [0.242, 0.424], loss: 8833.756836, mean_absolute_error: 254.468979, mean_q: -5.589377\n",
      " 2444/5000: episode: 2443, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.780, 0.917], loss: 15.646623, mean_absolute_error: 249.316925, mean_q: -5.593037\n",
      " 2445/5000: episode: 2444, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.831 [0.754, 0.899], loss: 3827.026611, mean_absolute_error: 249.385345, mean_q: -5.591581\n",
      " 2446/5000: episode: 2445, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 15.599977, mean_absolute_error: 249.433914, mean_q: -5.584692\n",
      " 2447/5000: episode: 2446, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.294, 0.482], loss: 15.541960, mean_absolute_error: 249.475525, mean_q: -5.574296\n",
      " 2448/5000: episode: 2447, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.588 [0.492, 0.682], loss: 17640.958984, mean_absolute_error: 259.967834, mean_q: -5.561387\n",
      " 2449/5000: episode: 2448, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [0.075, 0.207], loss: 8826.859375, mean_absolute_error: 254.769180, mean_q: -5.544783\n",
      " 2450/5000: episode: 2449, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.772 [0.687, 0.849], loss: 3875.391113, mean_absolute_error: 249.625580, mean_q: -5.534437\n",
      " 2451/5000: episode: 2450, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.115, 0.265], loss: 8823.692383, mean_absolute_error: 254.882904, mean_q: -5.528454\n",
      " 2452/5000: episode: 2451, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.293, 0.481], loss: 1433295.125000, mean_absolute_error: 354.076721, mean_q: -5.527403\n",
      " 2453/5000: episode: 2452, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.592 [0.496, 0.686], loss: 17624.980469, mean_absolute_error: 260.304321, mean_q: -5.544505\n",
      " 2454/5000: episode: 2453, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.836 [0.761, 0.903], loss: 3869.664062, mean_absolute_error: 250.011017, mean_q: -5.560837\n",
      " 2455/5000: episode: 2454, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.834 [0.758, 0.902], loss: 8815.504883, mean_absolute_error: 255.360931, mean_q: -5.584949\n",
      " 2456/5000: episode: 2455, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.935, 0.996], loss: 15.723200, mean_absolute_error: 250.290726, mean_q: -5.606709\n",
      " 2457/5000: episode: 2456, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.211, 0.388], loss: 15.793026, mean_absolute_error: 250.411331, mean_q: -5.619147\n",
      " 2458/5000: episode: 2457, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.832 [0.756, 0.900], loss: 8808.816406, mean_absolute_error: 255.723969, mean_q: -5.623652\n",
      " 2459/5000: episode: 2458, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [0.099, 0.242], loss: 15.855635, mean_absolute_error: 250.616119, mean_q: -5.630276\n",
      " 2460/5000: episode: 2459, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.311 [0.224, 0.403], loss: 17593.734375, mean_absolute_error: 261.139038, mean_q: -5.638798\n",
      " 2461/5000: episode: 2460, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.342, 0.534], loss: 8802.515625, mean_absolute_error: 256.022339, mean_q: -5.639136\n",
      " 2462/5000: episode: 2461, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.178], loss: 4029.939209, mean_absolute_error: 251.000031, mean_q: -5.633930\n",
      " 2463/5000: episode: 2462, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.669, 0.835], loss: 15.819790, mean_absolute_error: 250.957443, mean_q: -5.623907\n",
      " 2464/5000: episode: 2463, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 17577.248047, mean_absolute_error: 261.411560, mean_q: -5.610953\n",
      " 2465/5000: episode: 2464, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.675 [0.582, 0.763], loss: 8794.513672, mean_absolute_error: 256.275818, mean_q: -5.603075\n",
      " 2466/5000: episode: 2465, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.724, 0.877], loss: 1423699.625000, mean_absolute_error: 350.225037, mean_q: -5.598351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2467/5000: episode: 2466, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.100 [0.048, 0.163], loss: 3997.712646, mean_absolute_error: 251.332001, mean_q: -5.604817\n",
      " 2468/5000: episode: 2467, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.595 [0.499, 0.689], loss: 15.702980, mean_absolute_error: 251.339066, mean_q: -5.603102\n",
      " 2469/5000: episode: 2468, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.031], loss: 8786.128906, mean_absolute_error: 256.601990, mean_q: -5.594717\n",
      " 2470/5000: episode: 2469, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.002, 0.059], loss: 15.581917, mean_absolute_error: 251.476120, mean_q: -5.581458\n",
      " 2471/5000: episode: 2470, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.052], loss: 15.490287, mean_absolute_error: 251.529175, mean_q: -5.565020\n",
      " 2472/5000: episode: 2471, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.049 [0.014, 0.095], loss: 3891.813721, mean_absolute_error: 251.573883, mean_q: -5.546527\n",
      " 2473/5000: episode: 2472, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.822 [0.745, 0.892], loss: 15.274639, mean_absolute_error: 251.604950, mean_q: -5.526140\n",
      " 2474/5000: episode: 2473, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.664, 0.831], loss: 15.156784, mean_absolute_error: 251.630310, mean_q: -5.504776\n",
      " 2475/5000: episode: 2474, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.598], loss: 15.035110, mean_absolute_error: 251.650589, mean_q: -5.482632\n",
      " 2476/5000: episode: 2475, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.032], loss: 7772.383789, mean_absolute_error: 251.665344, mean_q: -5.460146\n",
      " 2477/5000: episode: 2476, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.330, 0.521], loss: 3904.504395, mean_absolute_error: 251.682404, mean_q: -5.435536\n",
      " 2478/5000: episode: 2477, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.239, 0.420], loss: 12754.663086, mean_absolute_error: 256.927887, mean_q: -5.409885\n",
      " 2479/5000: episode: 2478, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [0.064, 0.190], loss: 14.494911, mean_absolute_error: 251.682251, mean_q: -5.383220\n",
      " 2480/5000: episode: 2479, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.078, 0.212], loss: 1419442.750000, mean_absolute_error: 350.647949, mean_q: -5.357124\n",
      " 2481/5000: episode: 2480, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [0.069, 0.197], loss: 17521.675781, mean_absolute_error: 262.082428, mean_q: -5.347697\n",
      " 2482/5000: episode: 2481, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.123, 0.276], loss: 8766.576172, mean_absolute_error: 256.943970, mean_q: -5.333900\n",
      " 2483/5000: episode: 2482, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.884 [0.818, 0.941], loss: 8765.026367, mean_absolute_error: 256.978333, mean_q: -5.317306\n",
      " 2484/5000: episode: 2483, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 12646.694336, mean_absolute_error: 257.011108, mean_q: -5.298552\n",
      " 2485/5000: episode: 2484, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [0.075, 0.207], loss: 1410591.250000, mean_absolute_error: 345.641815, mean_q: -5.276961\n",
      " 2486/5000: episode: 2485, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.656 [0.562, 0.746], loss: 13.898613, mean_absolute_error: 251.926575, mean_q: -5.271307\n",
      " 2487/5000: episode: 2486, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.092 [0.042, 0.152], loss: 8758.476562, mean_absolute_error: 257.151245, mean_q: -5.261776\n",
      " 2488/5000: episode: 2487, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.905, 0.986], loss: 7822.022461, mean_absolute_error: 252.050674, mean_q: -5.248656\n",
      " 2489/5000: episode: 2488, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.079, 0.213], loss: 13.688735, mean_absolute_error: 252.066574, mean_q: -5.231349\n",
      " 2490/5000: episode: 2489, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.492 [0.396, 0.589], loss: 34973.636719, mean_absolute_error: 272.756012, mean_q: -5.212654\n",
      " 2491/5000: episode: 2490, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.488, 0.679], loss: 17488.751953, mean_absolute_error: 262.461395, mean_q: -5.189850\n",
      " 2492/5000: episode: 2491, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.517 [0.420, 0.614], loss: 17483.304688, mean_absolute_error: 262.497742, mean_q: -5.165130\n",
      " 2493/5000: episode: 2492, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 1419165.500000, mean_absolute_error: 351.141022, mean_q: -5.139059\n",
      " 2494/5000: episode: 2493, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 1410394.750000, mean_absolute_error: 346.065613, mean_q: -5.129381\n",
      " 2495/5000: episode: 2494, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.995], loss: 1410316.000000, mean_absolute_error: 346.195740, mean_q: -5.142575\n",
      " 2496/5000: episode: 2495, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.305, 0.494], loss: 13.390347, mean_absolute_error: 252.605927, mean_q: -5.174007\n",
      " 2497/5000: episode: 2496, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.943, 0.998], loss: 8734.963867, mean_absolute_error: 257.890442, mean_q: -5.195179\n",
      " 2498/5000: episode: 2497, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.969, 1.000], loss: 26171.628906, mean_absolute_error: 268.295959, mean_q: -5.206659\n",
      " 2499/5000: episode: 2498, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.127], loss: 12671.832031, mean_absolute_error: 258.118652, mean_q: -5.208918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2500/5000: episode: 2499, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.929, 0.994], loss: 13.549103, mean_absolute_error: 253.041412, mean_q: -5.204594\n",
      " 2501/5000: episode: 2500, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.864 [0.794, 0.926], loss: 13.553914, mean_absolute_error: 253.130875, mean_q: -5.205518\n",
      " 2502/5000: episode: 2501, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.227 [0.150, 0.311], loss: 1409835.000000, mean_absolute_error: 346.955566, mean_q: -5.209520\n",
      " 2503/5000: episode: 2502, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.932 [0.878, 0.974], loss: 13.655759, mean_absolute_error: 253.351501, mean_q: -5.225042\n",
      " 2504/5000: episode: 2503, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.427 [0.332, 0.524], loss: 1418382.750000, mean_absolute_error: 352.324402, mean_q: -5.242429\n",
      " 2505/5000: episode: 2504, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [0.075, 0.206], loss: 8717.675781, mean_absolute_error: 258.776642, mean_q: -5.275878\n",
      " 2506/5000: episode: 2505, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 14.032643, mean_absolute_error: 253.789917, mean_q: -5.296668\n",
      " 2507/5000: episode: 2506, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.369 [0.277, 0.464], loss: 14.091602, mean_absolute_error: 253.913971, mean_q: -5.307786\n",
      " 2508/5000: episode: 2507, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.869, 0.969], loss: 4009.116211, mean_absolute_error: 254.048172, mean_q: -5.311049\n",
      " 2509/5000: episode: 2508, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.098], loss: 1417990.625000, mean_absolute_error: 352.908844, mean_q: -5.308005\n",
      " 2510/5000: episode: 2509, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 3997.590332, mean_absolute_error: 254.222275, mean_q: -5.317320\n",
      " 2511/5000: episode: 2510, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.847 [0.773, 0.912], loss: 17394.730469, mean_absolute_error: 264.528137, mean_q: -5.319190\n",
      " 2512/5000: episode: 2511, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 1409134.750000, mean_absolute_error: 348.040009, mean_q: -5.314322\n",
      " 2513/5000: episode: 2512, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.481, 0.672], loss: 14.170670, mean_absolute_error: 254.470276, mean_q: -5.322659\n",
      " 2514/5000: episode: 2513, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.716, 0.871], loss: 14.180470, mean_absolute_error: 254.553131, mean_q: -5.324499\n",
      " 2515/5000: episode: 2514, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.151], loss: 14.160559, mean_absolute_error: 254.619324, mean_q: -5.320759\n",
      " 2516/5000: episode: 2515, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 4066.811768, mean_absolute_error: 254.731125, mean_q: -5.312761\n",
      " 2517/5000: episode: 2516, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 8694.144531, mean_absolute_error: 259.833160, mean_q: -5.300630\n",
      " 2518/5000: episode: 2517, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.838 [0.763, 0.905], loss: 1421673.750000, mean_absolute_error: 353.676514, mean_q: -5.303876\n",
      " 2519/5000: episode: 2518, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.887 [0.821, 0.943], loss: 17367.492188, mean_absolute_error: 265.195984, mean_q: -5.342945\n",
      " 2520/5000: episode: 2519, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.090], loss: 14.458617, mean_absolute_error: 255.154053, mean_q: -5.376475\n",
      " 2521/5000: episode: 2520, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [0.092, 0.232], loss: 8686.393555, mean_absolute_error: 260.400330, mean_q: -5.398088\n",
      " 2522/5000: episode: 2521, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.267, 0.453], loss: 14.637383, mean_absolute_error: 255.420914, mean_q: -5.409616\n",
      " 2523/5000: episode: 2522, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.961, 1.000], loss: 17348.949219, mean_absolute_error: 265.720459, mean_q: -5.414120\n",
      " 2524/5000: episode: 2523, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.302, 0.492], loss: 17343.929688, mean_absolute_error: 265.806396, mean_q: -5.411301\n",
      " 2525/5000: episode: 2524, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.911, 0.988], loss: 17338.378906, mean_absolute_error: 265.883057, mean_q: -5.402610\n",
      " 2526/5000: episode: 2525, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.700, 0.859], loss: 14.524702, mean_absolute_error: 255.770111, mean_q: -5.388750\n",
      " 2527/5000: episode: 2526, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.124, 0.278], loss: 4050.003174, mean_absolute_error: 255.854492, mean_q: -5.372125\n",
      " 2528/5000: episode: 2527, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.979, 1.000], loss: 14.334124, mean_absolute_error: 255.885239, mean_q: -5.353274\n",
      " 2529/5000: episode: 2528, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.735, 0.885], loss: 14.228460, mean_absolute_error: 255.924011, mean_q: -5.333503\n",
      " 2530/5000: episode: 2529, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 21339.117188, mean_absolute_error: 266.132446, mean_q: -5.313014\n",
      " 2531/5000: episode: 2530, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.664, 0.831], loss: 8662.401367, mean_absolute_error: 261.063293, mean_q: -5.289819\n",
      " 2532/5000: episode: 2531, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.668, 0.834], loss: 13.913754, mean_absolute_error: 256.024445, mean_q: -5.274179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2533/5000: episode: 2532, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.392, 0.586], loss: 4133.928223, mean_absolute_error: 256.157928, mean_q: -5.264459\n",
      " 2534/5000: episode: 2533, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.079], loss: 8656.820312, mean_absolute_error: 261.237000, mean_q: -5.260066\n",
      " 2535/5000: episode: 2534, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 8654.932617, mean_absolute_error: 261.339905, mean_q: -5.268514\n",
      " 2536/5000: episode: 2535, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.173, 0.341], loss: 13.930904, mean_absolute_error: 256.372253, mean_q: -5.277429\n",
      " 2537/5000: episode: 2536, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 4067.853027, mean_absolute_error: 256.477844, mean_q: -5.279037\n",
      " 2538/5000: episode: 2537, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [0.089, 0.228], loss: 13.914125, mean_absolute_error: 256.525024, mean_q: -5.274249\n",
      " 2539/5000: episode: 2538, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.103 [0.050, 0.166], loss: 4071.606445, mean_absolute_error: 256.596741, mean_q: -5.264812\n",
      " 2540/5000: episode: 2539, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.286, 0.474], loss: 4055.655762, mean_absolute_error: 256.623169, mean_q: -5.250628\n",
      " 2541/5000: episode: 2540, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.648, 0.818], loss: 13.700112, mean_absolute_error: 256.643250, mean_q: -5.233522\n",
      " 2542/5000: episode: 2541, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.333, 0.524], loss: 8644.279297, mean_absolute_error: 261.732269, mean_q: -5.214827\n",
      " 2543/5000: episode: 2542, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.092], loss: 13.491199, mean_absolute_error: 256.683594, mean_q: -5.193459\n",
      " 2544/5000: episode: 2543, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.111 [0.056, 0.176], loss: 17269.703125, mean_absolute_error: 266.834106, mean_q: -5.171238\n",
      " 2545/5000: episode: 2544, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.331 [0.242, 0.424], loss: 12828.570312, mean_absolute_error: 261.889618, mean_q: -5.146638\n",
      " 2546/5000: episode: 2545, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.496 [0.399, 0.592], loss: 12787.821289, mean_absolute_error: 261.878845, mean_q: -5.119217\n",
      " 2547/5000: episode: 2546, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [0.069, 0.197], loss: 1425018.125000, mean_absolute_error: 360.452820, mean_q: -5.090293\n",
      " 2548/5000: episode: 2547, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.938, 0.997], loss: 12.902165, mean_absolute_error: 256.807739, mean_q: -5.078796\n",
      " 2549/5000: episode: 2548, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.240, 0.422], loss: 12821.457031, mean_absolute_error: 262.009949, mean_q: -5.064757\n",
      " 2550/5000: episode: 2549, duration: 0.105s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.974, 1.000], loss: 12.740871, mean_absolute_error: 256.896057, mean_q: -5.046945\n",
      " 2551/5000: episode: 2550, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.875 [0.806, 0.934], loss: 12782.943359, mean_absolute_error: 262.057251, mean_q: -5.027398\n",
      " 2552/5000: episode: 2551, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.327, 0.519], loss: 8626.824219, mean_absolute_error: 262.006927, mean_q: -5.005059\n",
      " 2553/5000: episode: 2552, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.720, 0.874], loss: 8089.483398, mean_absolute_error: 256.982971, mean_q: -4.980953\n",
      " 2554/5000: episode: 2553, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.068 [0.026, 0.121], loss: 12818.622070, mean_absolute_error: 262.156006, mean_q: -4.953219\n",
      " 2555/5000: episode: 2554, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.330 [0.241, 0.423], loss: 25836.300781, mean_absolute_error: 272.162170, mean_q: -4.923013\n",
      " 2556/5000: episode: 2555, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.969, 1.000], loss: 11.970510, mean_absolute_error: 257.043610, mean_q: -4.891956\n",
      " 2557/5000: episode: 2556, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.640, 0.812], loss: 11.825027, mean_absolute_error: 257.057922, mean_q: -4.862132\n",
      " 2558/5000: episode: 2557, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.177 [0.108, 0.255], loss: 12697.200195, mean_absolute_error: 262.139648, mean_q: -4.833382\n",
      " 2559/5000: episode: 2558, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.714, 0.869], loss: 17211.761719, mean_absolute_error: 267.157654, mean_q: -4.804065\n",
      " 2560/5000: episode: 2559, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.462 [0.366, 0.559], loss: 2819542.000000, mean_absolute_error: 444.327698, mean_q: -4.774487\n",
      " 2561/5000: episode: 2560, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.933 [0.880, 0.975], loss: 1407634.375000, mean_absolute_error: 350.762573, mean_q: -4.780180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2562/5000: episode: 2561, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.533, 0.720], loss: 8605.262695, mean_absolute_error: 262.351624, mean_q: -4.796892\n",
      " 2563/5000: episode: 2562, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.554 [0.457, 0.650], loss: 8602.852539, mean_absolute_error: 262.461182, mean_q: -4.804764\n",
      " 2564/5000: episode: 2563, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.907 [0.847, 0.958], loss: 17189.472656, mean_absolute_error: 267.580811, mean_q: -4.805589\n",
      " 2565/5000: episode: 2564, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.510, 0.699], loss: 1424571.500000, mean_absolute_error: 361.209991, mean_q: -4.799943\n",
      " 2566/5000: episode: 2565, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.657, 0.825], loss: 1407333.500000, mean_absolute_error: 351.266418, mean_q: -4.806499\n",
      " 2567/5000: episode: 2566, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [0.096, 0.238], loss: 17173.302734, mean_absolute_error: 267.901489, mean_q: -4.824288\n",
      " 2568/5000: episode: 2567, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.446 [0.350, 0.543], loss: 8589.257812, mean_absolute_error: 263.027344, mean_q: -4.841663\n",
      " 2569/5000: episode: 2568, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.097, 0.240], loss: 8585.925781, mean_absolute_error: 263.196198, mean_q: -4.867014\n",
      " 2570/5000: episode: 2569, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.419, 0.612], loss: 8582.661133, mean_absolute_error: 263.359192, mean_q: -4.890025\n",
      " 2571/5000: episode: 2570, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 12.023582, mean_absolute_error: 258.484222, mean_q: -4.902791\n",
      " 2572/5000: episode: 2571, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.423, 0.616], loss: 1423953.125000, mean_absolute_error: 362.125610, mean_q: -4.908158\n",
      " 2573/5000: episode: 2572, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.181], loss: 12.176454, mean_absolute_error: 258.757629, mean_q: -4.933867\n",
      " 2574/5000: episode: 2573, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.358, 0.551], loss: 17130.802734, mean_absolute_error: 268.914429, mean_q: -4.958363\n",
      " 2575/5000: episode: 2574, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.913 [0.854, 0.962], loss: 12.367105, mean_absolute_error: 259.037872, mean_q: -4.972350\n",
      " 2576/5000: episode: 2575, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 12.401876, mean_absolute_error: 259.138916, mean_q: -4.979337\n",
      " 2577/5000: episode: 2576, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.705, 0.863], loss: 12.409283, mean_absolute_error: 259.219879, mean_q: -4.980824\n",
      " 2578/5000: episode: 2577, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.268 [0.185, 0.356], loss: 17114.904297, mean_absolute_error: 269.279388, mean_q: -4.976247\n",
      " 2579/5000: episode: 2578, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.143, 0.303], loss: 12.329887, mean_absolute_error: 259.354095, mean_q: -4.964861\n",
      " 2580/5000: episode: 2579, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.067 [0.025, 0.120], loss: 12.301659, mean_absolute_error: 259.433319, mean_q: -4.959173\n",
      " 2581/5000: episode: 2580, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [0.095, 0.236], loss: 1406278.625000, mean_absolute_error: 352.997375, mean_q: -4.958116\n",
      " 2582/5000: episode: 2581, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.950, 0.999], loss: 1406208.625000, mean_absolute_error: 353.103851, mean_q: -4.969712\n",
      " 2583/5000: episode: 2582, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.489, 0.679], loss: 12.457399, mean_absolute_error: 259.766235, mean_q: -4.990473\n",
      " 2584/5000: episode: 2583, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.617, 0.793], loss: 4184.641602, mean_absolute_error: 259.907959, mean_q: -5.001660\n",
      " 2585/5000: episode: 2584, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.262, 0.446], loss: 8548.730469, mean_absolute_error: 264.948486, mean_q: -5.004908\n",
      " 2586/5000: episode: 2585, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.130, 0.286], loss: 12562.669922, mean_absolute_error: 260.145142, mean_q: -5.002313\n",
      " 2587/5000: episode: 2586, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.112, 0.260], loss: 12.465418, mean_absolute_error: 260.100067, mean_q: -4.992079\n",
      " 2588/5000: episode: 2587, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 21254.261719, mean_absolute_error: 270.129822, mean_q: -4.978034\n",
      " 2589/5000: episode: 2588, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.536, 0.723], loss: 12.397859, mean_absolute_error: 260.236206, mean_q: -4.978530\n",
      " 2590/5000: episode: 2589, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.130, 0.286], loss: 12.461932, mean_absolute_error: 260.351898, mean_q: -4.991381\n",
      " 2591/5000: episode: 2590, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.415, 0.609], loss: 12798.990234, mean_absolute_error: 265.492249, mean_q: -4.997547\n",
      " 2592/5000: episode: 2591, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.078], loss: 12.488471, mean_absolute_error: 260.516296, mean_q: -4.996694\n",
      " 2593/5000: episode: 2592, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.967, 1.000], loss: 1414157.250000, mean_absolute_error: 358.977234, mean_q: -4.991740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2594/5000: episode: 2593, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.635, 0.807], loss: 8532.902344, mean_absolute_error: 265.637939, mean_q: -5.000058\n",
      " 2595/5000: episode: 2594, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.192, 0.365], loss: 17049.976562, mean_absolute_error: 270.685547, mean_q: -5.001592\n",
      " 2596/5000: episode: 2595, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: 1413997.500000, mean_absolute_error: 359.212036, mean_q: -4.996915\n",
      " 2597/5000: episode: 2596, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 12.533217, mean_absolute_error: 260.935974, mean_q: -5.005639\n",
      " 2598/5000: episode: 2597, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 12.546532, mean_absolute_error: 261.021942, mean_q: -5.008298\n",
      " 2599/5000: episode: 2598, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.611, 0.788], loss: 12.534435, mean_absolute_error: 261.092468, mean_q: -5.005882\n",
      " 2600/5000: episode: 2599, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 12.501033, mean_absolute_error: 261.148560, mean_q: -4.999207\n",
      " 2601/5000: episode: 2600, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.847 [0.774, 0.912], loss: 8521.237305, mean_absolute_error: 266.151886, mean_q: -4.989626\n",
      " 2602/5000: episode: 2601, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.480 [0.384, 0.577], loss: 12690.206055, mean_absolute_error: 266.188324, mean_q: -4.976371\n",
      " 2603/5000: episode: 2602, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.005, 0.070], loss: 12.298479, mean_absolute_error: 261.260651, mean_q: -4.958532\n",
      " 2604/5000: episode: 2603, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 12848.816406, mean_absolute_error: 266.341919, mean_q: -4.937867\n",
      " 2605/5000: episode: 2604, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 17018.599609, mean_absolute_error: 271.212585, mean_q: -4.914250\n",
      " 2606/5000: episode: 2605, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 8513.865234, mean_absolute_error: 266.280273, mean_q: -4.889521\n",
      " 2607/5000: episode: 2606, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.057], loss: 11.837186, mean_absolute_error: 261.352051, mean_q: -4.864632\n",
      " 2608/5000: episode: 2607, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 17008.326172, mean_absolute_error: 271.262085, mean_q: -4.840218\n",
      " 2609/5000: episode: 2608, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 17004.925781, mean_absolute_error: 271.278931, mean_q: -4.814554\n",
      " 2610/5000: episode: 2609, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.611 [0.515, 0.704], loss: 8505.939453, mean_absolute_error: 266.354675, mean_q: -4.787992\n",
      " 2611/5000: episode: 2610, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.455 [0.359, 0.552], loss: 11.339756, mean_absolute_error: 261.433899, mean_q: -4.761301\n",
      " 2612/5000: episode: 2611, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.884 [0.818, 0.941], loss: 11.213554, mean_absolute_error: 261.451447, mean_q: -4.734727\n",
      " 2613/5000: episode: 2612, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.212 [0.137, 0.295], loss: 12831.192383, mean_absolute_error: 266.506592, mean_q: -4.708332\n",
      " 2614/5000: episode: 2613, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 8498.379883, mean_absolute_error: 266.416748, mean_q: -4.680858\n",
      " 2615/5000: episode: 2614, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [0.086, 0.224], loss: 8496.674805, mean_absolute_error: 266.425781, mean_q: -4.653362\n",
      " 2616/5000: episode: 2615, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.876, 0.973], loss: 4314.426270, mean_absolute_error: 261.583221, mean_q: -4.625949\n",
      " 2617/5000: episode: 2616, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.135, 0.292], loss: 8493.657227, mean_absolute_error: 266.436218, mean_q: -4.598713\n",
      " 2618/5000: episode: 2617, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.668 [0.575, 0.757], loss: 1405197.500000, mean_absolute_error: 354.911560, mean_q: -4.571651\n",
      " 2619/5000: episode: 2618, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 10.416792, mean_absolute_error: 261.562317, mean_q: -4.563382\n",
      " 2620/5000: episode: 2619, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.844], loss: 8489.448242, mean_absolute_error: 266.533752, mean_q: -4.552675\n",
      " 2621/5000: episode: 2620, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.124, 0.278], loss: 12782.827148, mean_absolute_error: 266.646271, mean_q: -4.539049\n",
      " 2622/5000: episode: 2621, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.052], loss: 10.229193, mean_absolute_error: 261.672485, mean_q: -4.522095\n",
      " 2623/5000: episode: 2622, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.407 [0.313, 0.503], loss: 4203.709473, mean_absolute_error: 261.730774, mean_q: -4.513881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2624/5000: episode: 2623, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.520, 0.708], loss: 1405042.250000, mean_absolute_error: 355.169037, mean_q: -4.510518\n",
      " 2625/5000: episode: 2624, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.105], loss: 16954.828125, mean_absolute_error: 271.725037, mean_q: -4.521235\n",
      " 2626/5000: episode: 2625, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 10.237360, mean_absolute_error: 261.962402, mean_q: -4.523900\n",
      " 2627/5000: episode: 2626, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [0.072, 0.202], loss: 1421803.375000, mean_absolute_error: 365.280396, mean_q: -4.530949\n",
      " 2628/5000: episode: 2627, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.559 [0.462, 0.655], loss: 8476.102539, mean_absolute_error: 267.137207, mean_q: -4.556910\n",
      " 2629/5000: episode: 2628, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.950 [0.904, 0.986], loss: 12720.340820, mean_absolute_error: 267.297729, mean_q: -4.572643\n",
      " 2630/5000: episode: 2629, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.005, 0.070], loss: 1413087.125000, mean_absolute_error: 360.736816, mean_q: -4.579487\n",
      " 2631/5000: episode: 2630, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.135], loss: 10.573214, mean_absolute_error: 262.599854, mean_q: -4.597525\n",
      " 2632/5000: episode: 2631, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.473 [0.377, 0.570], loss: 10.619940, mean_absolute_error: 262.707642, mean_q: -4.607675\n",
      " 2633/5000: episode: 2632, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.057], loss: 8464.464844, mean_absolute_error: 267.704163, mean_q: -4.611007\n",
      " 2634/5000: episode: 2633, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.461, 0.653], loss: 16914.533203, mean_absolute_error: 272.681183, mean_q: -4.607777\n",
      " 2635/5000: episode: 2634, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.608 [0.513, 0.702], loss: 8460.522461, mean_absolute_error: 267.836609, mean_q: -4.599325\n",
      " 2636/5000: episode: 2635, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.566 [0.469, 0.661], loss: 4233.621582, mean_absolute_error: 262.987366, mean_q: -4.587794\n",
      " 2637/5000: episode: 2636, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.624, 0.799], loss: 1421197.625000, mean_absolute_error: 366.164032, mean_q: -4.573424\n",
      " 2638/5000: episode: 2637, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.572 [0.476, 0.667], loss: 16899.394531, mean_absolute_error: 272.907104, mean_q: -4.573250\n",
      " 2639/5000: episode: 2638, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.140, 0.299], loss: 10.431162, mean_absolute_error: 263.180359, mean_q: -4.566529\n",
      " 2640/5000: episode: 2639, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.597 [0.501, 0.690], loss: 16890.523438, mean_absolute_error: 273.032898, mean_q: -4.556381\n",
      " 2641/5000: episode: 2640, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.873, 0.972], loss: 8447.875977, mean_absolute_error: 268.218567, mean_q: -4.551693\n",
      " 2642/5000: episode: 2641, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.892, 0.981], loss: 10.363928, mean_absolute_error: 263.415894, mean_q: -4.551785\n",
      " 2643/5000: episode: 2642, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.387, 0.581], loss: 8443.476562, mean_absolute_error: 268.378265, mean_q: -4.547821\n",
      " 2644/5000: episode: 2643, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.896 [0.833, 0.950], loss: 10.308617, mean_absolute_error: 263.553558, mean_q: -4.539620\n",
      " 2645/5000: episode: 2644, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.716, 0.871], loss: 10.258831, mean_absolute_error: 263.604126, mean_q: -4.528643\n",
      " 2646/5000: episode: 2645, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 12725.353516, mean_absolute_error: 268.584259, mean_q: -4.525304\n",
      " 2647/5000: episode: 2646, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.537 [0.440, 0.633], loss: 10.244987, mean_absolute_error: 263.744324, mean_q: -4.525585\n",
      " 2648/5000: episode: 2647, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 8435.486328, mean_absolute_error: 268.682190, mean_q: -4.521786\n",
      " 2649/5000: episode: 2648, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [0.080, 0.215], loss: 10.191122, mean_absolute_error: 263.852631, mean_q: -4.513670\n",
      " 2650/5000: episode: 2649, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.686, 0.848], loss: 12764.671875, mean_absolute_error: 268.830017, mean_q: -4.502839\n",
      " 2651/5000: episode: 2650, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.883 [0.816, 0.940], loss: 10.077362, mean_absolute_error: 263.930176, mean_q: -4.488401\n",
      " 2652/5000: episode: 2651, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 8430.058594, mean_absolute_error: 268.834412, mean_q: -4.472754\n",
      " 2653/5000: episode: 2652, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.416 [0.322, 0.512], loss: 12798.090820, mean_absolute_error: 268.931213, mean_q: -4.454868\n",
      " 2654/5000: episode: 2653, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.671 [0.578, 0.760], loss: 8427.664062, mean_absolute_error: 268.868286, mean_q: -4.434335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2655/5000: episode: 2654, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.421, 0.615], loss: 8426.255859, mean_absolute_error: 268.886902, mean_q: -4.412777\n",
      " 2656/5000: episode: 2655, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.058 [0.019, 0.108], loss: 9.640408, mean_absolute_error: 264.033203, mean_q: -4.389993\n",
      " 2657/5000: episode: 2656, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.885, 0.977], loss: 9.540618, mean_absolute_error: 264.049744, mean_q: -4.367208\n",
      " 2658/5000: episode: 2657, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.458 [0.362, 0.555], loss: 1403755.000000, mean_absolute_error: 357.364380, mean_q: -4.344172\n",
      " 2659/5000: episode: 2658, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.164, 0.330], loss: 8419.856445, mean_absolute_error: 268.995331, mean_q: -4.338997\n",
      " 2660/5000: episode: 2659, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.389, 0.582], loss: 8417.913086, mean_absolute_error: 269.053284, mean_q: -4.329615\n",
      " 2661/5000: episode: 2660, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.100], loss: 9.323153, mean_absolute_error: 264.237915, mean_q: -4.317137\n",
      " 2662/5000: episode: 2661, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.036, 0.141], loss: 9.262861, mean_absolute_error: 264.277191, mean_q: -4.303152\n",
      " 2663/5000: episode: 2662, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.548 [0.451, 0.643], loss: 9.197442, mean_absolute_error: 264.308228, mean_q: -4.287926\n",
      " 2664/5000: episode: 2663, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.976, 1.000], loss: 9.124609, mean_absolute_error: 264.332916, mean_q: -4.270910\n",
      " 2665/5000: episode: 2664, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.129, 0.284], loss: 9.046000, mean_absolute_error: 264.352051, mean_q: -4.252469\n",
      " 2666/5000: episode: 2665, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.142, 0.301], loss: 8.964984, mean_absolute_error: 264.368469, mean_q: -4.233379\n",
      " 2667/5000: episode: 2666, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.931 [0.878, 0.974], loss: 8408.372070, mean_absolute_error: 269.242065, mean_q: -4.213423\n",
      " 2668/5000: episode: 2667, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.100], loss: 16805.285156, mean_absolute_error: 274.114136, mean_q: -4.192391\n",
      " 2669/5000: episode: 2668, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 8.699635, mean_absolute_error: 264.419373, mean_q: -4.170243\n",
      " 2670/5000: episode: 2669, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.943, 0.998], loss: 8.607745, mean_absolute_error: 264.434387, mean_q: -4.148155\n",
      " 2671/5000: episode: 2670, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.267, 0.452], loss: 8.516813, mean_absolute_error: 264.448090, mean_q: -4.126182\n",
      " 2672/5000: episode: 2671, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.261 [0.179, 0.349], loss: 8.426758, mean_absolute_error: 264.461121, mean_q: -4.104303\n",
      " 2673/5000: episode: 2672, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.198, 0.372], loss: 8400.171875, mean_absolute_error: 269.347260, mean_q: -4.092307\n",
      " 2674/5000: episode: 2673, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.495, 0.685], loss: 1403528.000000, mean_absolute_error: 357.830688, mean_q: -4.086909\n",
      " 2675/5000: episode: 2674, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.125, 0.279], loss: 8397.648438, mean_absolute_error: 269.486694, mean_q: -4.097445\n",
      " 2676/5000: episode: 2675, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 8.415564, mean_absolute_error: 264.713135, mean_q: -4.101576\n",
      " 2677/5000: episode: 2676, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.391, 0.584], loss: 8.414124, mean_absolute_error: 264.773315, mean_q: -4.101225\n",
      " 2678/5000: episode: 2677, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.348 [0.258, 0.442], loss: 16780.283203, mean_absolute_error: 274.513947, mean_q: -4.096853\n",
      " 2679/5000: episode: 2678, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.189, 0.361], loss: 1403321.250000, mean_absolute_error: 358.139130, mean_q: -4.087955\n",
      " 2680/5000: episode: 2679, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [0.078, 0.211], loss: 8391.261719, mean_absolute_error: 269.801086, mean_q: -4.094878\n",
      " 2681/5000: episode: 2680, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.920 [0.863, 0.966], loss: 8389.436523, mean_absolute_error: 269.876556, mean_q: -4.094948\n",
      " 2682/5000: episode: 2681, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.919 [0.862, 0.966], loss: 8.408953, mean_absolute_error: 265.123291, mean_q: -4.099964\n",
      " 2683/5000: episode: 2682, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.178, 0.348], loss: 8385.927734, mean_absolute_error: 270.056732, mean_q: -4.108719\n",
      " 2684/5000: episode: 2683, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.877 [0.809, 0.935], loss: 1419814.125000, mean_absolute_error: 368.222656, mean_q: -4.110232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2685/5000: episode: 2684, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.260, 0.444], loss: 8.506311, mean_absolute_error: 265.418884, mean_q: -4.123636\n",
      " 2686/5000: episode: 2685, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [0.007, 0.076], loss: 8379.815430, mean_absolute_error: 270.345703, mean_q: -4.130343\n",
      " 2687/5000: episode: 2686, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 1419608.500000, mean_absolute_error: 368.514648, mean_q: -4.141056\n",
      " 2688/5000: episode: 2687, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.891 [0.827, 0.946], loss: 8375.708008, mean_absolute_error: 270.603333, mean_q: -4.171973\n",
      " 2689/5000: episode: 2688, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.696 [0.605, 0.783], loss: 8373.474609, mean_absolute_error: 270.736359, mean_q: -4.191767\n",
      " 2690/5000: episode: 2689, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 8371.279297, mean_absolute_error: 270.844604, mean_q: -4.202459\n",
      " 2691/5000: episode: 2690, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.238, 0.419], loss: 8369.216797, mean_absolute_error: 270.933990, mean_q: -4.206737\n",
      " 2692/5000: episode: 2691, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 8.846277, mean_absolute_error: 266.191315, mean_q: -4.205252\n",
      " 2693/5000: episode: 2692, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 8.824183, mean_absolute_error: 266.255432, mean_q: -4.199996\n",
      " 2694/5000: episode: 2693, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 8.788395, mean_absolute_error: 266.308167, mean_q: -4.191468\n",
      " 2695/5000: episode: 2694, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 8.738512, mean_absolute_error: 266.351562, mean_q: -4.179553\n",
      " 2696/5000: episode: 2695, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.969, 1.000], loss: 8.679180, mean_absolute_error: 266.386108, mean_q: -4.165337\n",
      " 2697/5000: episode: 2696, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.571, 0.754], loss: 8.613731, mean_absolute_error: 266.414917, mean_q: -4.149598\n",
      " 2698/5000: episode: 2697, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.262 [0.180, 0.350], loss: 8358.036133, mean_absolute_error: 271.252197, mean_q: -4.132750\n",
      " 2699/5000: episode: 2698, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.261, 0.445], loss: 8.468968, mean_absolute_error: 266.460083, mean_q: -4.114573\n",
      " 2700/5000: episode: 2699, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.274, 0.461], loss: 8.391648, mean_absolute_error: 266.479065, mean_q: -4.095742\n",
      " 2701/5000: episode: 2700, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.478 [0.382, 0.575], loss: 1410718.500000, mean_absolute_error: 364.504913, mean_q: -4.075859\n",
      " 2702/5000: episode: 2701, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.127], loss: 8353.183594, mean_absolute_error: 271.361450, mean_q: -4.073598\n",
      " 2703/5000: episode: 2702, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.874, 0.972], loss: 12803.912109, mean_absolute_error: 271.488251, mean_q: -4.067390\n",
      " 2704/5000: episode: 2703, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 8.228439, mean_absolute_error: 266.649017, mean_q: -4.055708\n",
      " 2705/5000: episode: 2704, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 8.165318, mean_absolute_error: 266.687317, mean_q: -4.040118\n",
      " 2706/5000: episode: 2705, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.468 [0.371, 0.565], loss: 8.093397, mean_absolute_error: 266.722595, mean_q: -4.022282\n",
      " 2707/5000: episode: 2706, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.876 [0.809, 0.935], loss: 16681.316406, mean_absolute_error: 276.355896, mean_q: -4.003505\n",
      " 2708/5000: episode: 2707, duration: 0.029s, episode steps: 1, steps per second: 35, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 12726.432617, mean_absolute_error: 271.640228, mean_q: -3.992725\n",
      " 2709/5000: episode: 2708, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.949, 0.999], loss: 1418843.625000, mean_absolute_error: 369.665497, mean_q: -3.987243\n",
      " 2710/5000: episode: 2709, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 7.989153, mean_absolute_error: 266.998474, mean_q: -3.996287\n",
      " 2711/5000: episode: 2710, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.358, 0.551], loss: 8.003664, mean_absolute_error: 267.090210, mean_q: -3.999916\n",
      " 2712/5000: episode: 2711, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.921, 0.992], loss: 8.000505, mean_absolute_error: 267.160400, mean_q: -3.999126\n",
      " 2713/5000: episode: 2712, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 1410291.375000, mean_absolute_error: 365.208679, mean_q: -4.004747\n",
      " 2714/5000: episode: 2713, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.886, 0.978], loss: 8330.490234, mean_absolute_error: 272.173859, mean_q: -4.032138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2715/5000: episode: 2714, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 8.243158, mean_absolute_error: 267.526611, mean_q: -4.059335\n",
      " 2716/5000: episode: 2715, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.474, 0.666], loss: 12778.430664, mean_absolute_error: 272.502014, mean_q: -4.086158\n",
      " 2717/5000: episode: 2716, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.672, 0.838], loss: 8325.341797, mean_absolute_error: 272.559570, mean_q: -4.101767\n",
      " 2718/5000: episode: 2717, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.863 [0.792, 0.924], loss: 1401541.000000, mean_absolute_error: 361.043701, mean_q: -4.119571\n",
      " 2719/5000: episode: 2718, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.971, 1.000], loss: 8.646404, mean_absolute_error: 268.070190, mean_q: -4.157462\n",
      " 2720/5000: episode: 2719, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.280 [0.196, 0.370], loss: 12869.172852, mean_absolute_error: 273.087463, mean_q: -4.183484\n",
      " 2721/5000: episode: 2720, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.781, 0.917], loss: 8.817053, mean_absolute_error: 268.320251, mean_q: -4.198298\n",
      " 2722/5000: episode: 2721, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.123, 0.275], loss: 8.849618, mean_absolute_error: 268.411835, mean_q: -4.206046\n",
      " 2723/5000: episode: 2722, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 8314.905273, mean_absolute_error: 273.260071, mean_q: -4.208147\n",
      " 2724/5000: episode: 2723, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.307], loss: 8.884905, mean_absolute_error: 268.576599, mean_q: -4.214425\n",
      " 2725/5000: episode: 2724, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.874 [0.806, 0.933], loss: 8311.339844, mean_absolute_error: 273.447571, mean_q: -4.223460\n",
      " 2726/5000: episode: 2725, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.591, 0.771], loss: 8.929527, mean_absolute_error: 268.761597, mean_q: -4.224998\n",
      " 2727/5000: episode: 2726, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.542, 0.728], loss: 16606.023438, mean_absolute_error: 278.365112, mean_q: -4.221656\n",
      " 2728/5000: episode: 2727, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 8.879044, mean_absolute_error: 268.901031, mean_q: -4.213035\n",
      " 2729/5000: episode: 2728, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.618, 0.794], loss: 4530.318848, mean_absolute_error: 269.024109, mean_q: -4.201289\n",
      " 2730/5000: episode: 2729, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.625, 0.799], loss: 8.766949, mean_absolute_error: 268.992493, mean_q: -4.186350\n",
      " 2731/5000: episode: 2730, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.897, 0.983], loss: 4583.973145, mean_absolute_error: 269.148926, mean_q: -4.180300\n",
      " 2732/5000: episode: 2731, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.532, 0.719], loss: 8.738324, mean_absolute_error: 269.111267, mean_q: -4.179508\n",
      " 2733/5000: episode: 2732, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.071 [0.028, 0.125], loss: 1400773.500000, mean_absolute_error: 362.254089, mean_q: -4.174617\n",
      " 2734/5000: episode: 2733, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.181, 0.351], loss: 1409798.500000, mean_absolute_error: 362.494354, mean_q: -4.185383\n",
      " 2735/5000: episode: 2734, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.324, 0.515], loss: 8295.267578, mean_absolute_error: 274.125732, mean_q: -4.207106\n",
      " 2736/5000: episode: 2735, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.371, 0.564], loss: 8.909038, mean_absolute_error: 269.468292, mean_q: -4.220146\n",
      " 2737/5000: episode: 2736, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.913 [0.854, 0.962], loss: 1400514.750000, mean_absolute_error: 362.613525, mean_q: -4.225712\n",
      " 2738/5000: episode: 2737, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [0.076, 0.209], loss: 9.011265, mean_absolute_error: 269.646088, mean_q: -4.244295\n",
      " 2739/5000: episode: 2738, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.954, 1.000], loss: 8291.555664, mean_absolute_error: 274.483643, mean_q: -4.255040\n",
      " 2740/5000: episode: 2739, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.703 [0.612, 0.789], loss: 1408593.375000, mean_absolute_error: 367.643036, mean_q: -4.268055\n",
      " 2741/5000: episode: 2740, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.328, 0.519], loss: 9.251755, mean_absolute_error: 269.993774, mean_q: -4.300570\n",
      " 2742/5000: episode: 2741, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 8286.230469, mean_absolute_error: 274.868439, mean_q: -4.322536\n",
      " 2743/5000: episode: 2742, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.671, 0.837], loss: 1404634.500000, mean_absolute_error: 363.342010, mean_q: -4.335649\n",
      " 2744/5000: episode: 2743, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.830 [0.753, 0.898], loss: 1399966.875000, mean_absolute_error: 363.392029, mean_q: -4.359782\n",
      " 2745/5000: episode: 2744, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.064], loss: 1408132.750000, mean_absolute_error: 368.278961, mean_q: -4.393753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2746/5000: episode: 2745, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.487, 0.677], loss: 1399739.875000, mean_absolute_error: 363.707306, mean_q: -4.435493\n",
      " 2747/5000: episode: 2746, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.144, 0.305], loss: 10.057438, mean_absolute_error: 270.881165, mean_q: -4.483961\n",
      " 2748/5000: episode: 2747, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.680 [0.588, 0.768], loss: 10.214136, mean_absolute_error: 271.036469, mean_q: -4.518764\n",
      " 2749/5000: episode: 2748, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.472 [0.376, 0.569], loss: 10.366546, mean_absolute_error: 271.188354, mean_q: -4.552361\n",
      " 2750/5000: episode: 2749, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.557 [0.461, 0.653], loss: 8274.580078, mean_absolute_error: 276.089966, mean_q: -4.593210\n",
      " 2751/5000: episode: 2750, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.580, 0.761], loss: 10.719711, mean_absolute_error: 271.512756, mean_q: -4.629272\n",
      " 2752/5000: episode: 2751, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.061 [0.022, 0.112], loss: 1403607.125000, mean_absolute_error: 364.620483, mean_q: -4.653367\n",
      " 2753/5000: episode: 2752, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.879 [0.811, 0.937], loss: 8270.645508, mean_absolute_error: 276.520477, mean_q: -4.685298\n",
      " 2754/5000: episode: 2753, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.496 [0.399, 0.593], loss: 2806033.000000, mean_absolute_error: 462.559692, mean_q: -4.704977\n",
      " 2755/5000: episode: 2754, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.222, 0.401], loss: 11.294004, mean_absolute_error: 272.118713, mean_q: -4.751684\n",
      " 2756/5000: episode: 2755, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.495, 0.685], loss: 11.448538, mean_absolute_error: 272.286743, mean_q: -4.784089\n",
      " 2757/5000: episode: 2756, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.961, 1.000], loss: 8262.756836, mean_absolute_error: 277.142487, mean_q: -4.804596\n",
      " 2758/5000: episode: 2757, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 11.596751, mean_absolute_error: 272.531342, mean_q: -4.814963\n",
      " 2759/5000: episode: 2758, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.869, 0.970], loss: 11.614206, mean_absolute_error: 272.622650, mean_q: -4.818586\n",
      " 2760/5000: episode: 2759, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.551 [0.454, 0.647], loss: 11.607028, mean_absolute_error: 272.694916, mean_q: -4.817097\n",
      " 2761/5000: episode: 2760, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.837, 0.952], loss: 16500.339844, mean_absolute_error: 282.211853, mean_q: -4.820951\n",
      " 2762/5000: episode: 2761, duration: 0.110s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.127, 0.281], loss: 12979.714844, mean_absolute_error: 277.695312, mean_q: -4.826954\n",
      " 2763/5000: episode: 2762, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.876, 0.973], loss: 11.648133, mean_absolute_error: 272.958313, mean_q: -4.825621\n",
      " 2764/5000: episode: 2763, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.875 [0.807, 0.934], loss: 8250.659180, mean_absolute_error: 277.734009, mean_q: -4.819771\n",
      " 2765/5000: episode: 2764, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.106 [0.052, 0.170], loss: 8248.928711, mean_absolute_error: 277.789917, mean_q: -4.809565\n",
      " 2766/5000: episode: 2765, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.034], loss: 11.506260, mean_absolute_error: 273.127075, mean_q: -4.796137\n",
      " 2767/5000: episode: 2766, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.178, 0.348], loss: 8245.846680, mean_absolute_error: 277.873047, mean_q: -4.780875\n",
      " 2768/5000: episode: 2767, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.934 [0.881, 0.976], loss: 8244.429688, mean_absolute_error: 277.904358, mean_q: -4.763337\n",
      " 2769/5000: episode: 2768, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 11.255930, mean_absolute_error: 273.228729, mean_q: -4.743667\n",
      " 2770/5000: episode: 2769, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.525 [0.428, 0.621], loss: 11.158117, mean_absolute_error: 273.253754, mean_q: -4.723006\n",
      " 2771/5000: episode: 2770, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.126, 0.281], loss: 11.056654, mean_absolute_error: 273.274292, mean_q: -4.701479\n",
      " 2772/5000: episode: 2771, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.114, 0.264], loss: 8238.768555, mean_absolute_error: 277.993652, mean_q: -4.678948\n",
      " 2773/5000: episode: 2772, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 10.837786, mean_absolute_error: 273.316711, mean_q: -4.654703\n",
      " 2774/5000: episode: 2773, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.218, 0.396], loss: 8235.137695, mean_absolute_error: 278.037231, mean_q: -4.629160\n",
      " 2775/5000: episode: 2774, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.533 [0.436, 0.629], loss: 10.596500, mean_absolute_error: 273.359100, mean_q: -4.602586\n",
      " 2776/5000: episode: 2775, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.338, 0.530], loss: 1402750.000000, mean_absolute_error: 366.353271, mean_q: -4.576263\n",
      " 2777/5000: episode: 2776, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.148, 0.309], loss: 4635.270020, mean_absolute_error: 273.507935, mean_q: -4.588015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2778/5000: episode: 2777, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.341, 0.533], loss: 10.630091, mean_absolute_error: 273.607666, mean_q: -4.609877\n",
      " 2779/5000: episode: 2778, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.699 [0.607, 0.785], loss: 1402430.625000, mean_absolute_error: 366.650757, mean_q: -4.632537\n",
      " 2780/5000: episode: 2779, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.236, 0.417], loss: 8225.324219, mean_absolute_error: 278.631500, mean_q: -4.682850\n",
      " 2781/5000: episode: 2780, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.649, 0.819], loss: 1405720.750000, mean_absolute_error: 371.706116, mean_q: -4.737005\n",
      " 2782/5000: episode: 2781, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.780, 0.916], loss: 11.539783, mean_absolute_error: 274.409912, mean_q: -4.803120\n",
      " 2783/5000: episode: 2782, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 8219.619141, mean_absolute_error: 279.295715, mean_q: -4.851906\n",
      " 2784/5000: episode: 2783, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 4802.800781, mean_absolute_error: 274.911011, mean_q: -4.895259\n",
      " 2785/5000: episode: 2784, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.906 [0.846, 0.957], loss: 12.165899, mean_absolute_error: 274.996033, mean_q: -4.931727\n",
      " 2786/5000: episode: 2785, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.392, 0.586], loss: 16412.644531, mean_absolute_error: 284.498199, mean_q: -4.955234\n",
      " 2787/5000: episode: 2786, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.284, 0.472], loss: 12.344440, mean_absolute_error: 275.271484, mean_q: -4.967791\n",
      " 2788/5000: episode: 2787, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 8208.191406, mean_absolute_error: 280.043518, mean_q: -4.973408\n",
      " 2789/5000: episode: 2788, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.089], loss: 12.366100, mean_absolute_error: 275.454041, mean_q: -4.972148\n",
      " 2790/5000: episode: 2789, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.127], loss: 8204.821289, mean_absolute_error: 280.185760, mean_q: -4.965994\n",
      " 2791/5000: episode: 2790, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.466 [0.370, 0.563], loss: 22558.367188, mean_absolute_error: 280.503052, mean_q: -4.965052\n",
      " 2792/5000: episode: 2791, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.976, 1.000], loss: 12.372314, mean_absolute_error: 275.717438, mean_q: -4.973397\n",
      " 2793/5000: episode: 2792, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.247, 0.430], loss: 8197.897461, mean_absolute_error: 280.503937, mean_q: -4.982754\n",
      " 2794/5000: episode: 2793, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.208, 0.384], loss: 4710.044922, mean_absolute_error: 275.969543, mean_q: -4.983630\n",
      " 2795/5000: episode: 2794, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.821 [0.744, 0.891], loss: 4807.616699, mean_absolute_error: 276.108490, mean_q: -4.978210\n",
      " 2796/5000: episode: 2795, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.763 [0.677, 0.842], loss: 8190.752930, mean_absolute_error: 280.751465, mean_q: -4.967287\n",
      " 2797/5000: episode: 2796, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.094, 0.235], loss: 4685.455566, mean_absolute_error: 276.146667, mean_q: -4.951742\n",
      " 2798/5000: episode: 2797, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.498 [0.401, 0.595], loss: 12.169264, mean_absolute_error: 276.186462, mean_q: -4.932409\n",
      " 2799/5000: episode: 2798, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.597], loss: 1396247.500000, mean_absolute_error: 368.989319, mean_q: -4.910851\n",
      " 2800/5000: episode: 2799, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.179 [0.109, 0.256], loss: 12.040649, mean_absolute_error: 276.289368, mean_q: -4.906270\n",
      " 2801/5000: episode: 2800, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.073 [0.029, 0.127], loss: 8181.958984, mean_absolute_error: 280.989868, mean_q: -4.897001\n",
      " 2802/5000: episode: 2801, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.178], loss: 11.931052, mean_absolute_error: 276.387939, mean_q: -4.883885\n",
      " 2803/5000: episode: 2802, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.849 [0.776, 0.914], loss: 1404293.000000, mean_absolute_error: 373.836914, mean_q: -4.867680\n",
      " 2804/5000: episode: 2803, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 1412403.250000, mean_absolute_error: 378.565887, mean_q: -4.867312\n",
      " 2805/5000: episode: 2804, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 11.909101, mean_absolute_error: 276.651794, mean_q: -4.879390\n",
      " 2806/5000: episode: 2805, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.859 [0.788, 0.922], loss: 13037.839844, mean_absolute_error: 281.529114, mean_q: -4.893905\n",
      " 2807/5000: episode: 2806, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.005, 0.070], loss: 13007.041992, mean_absolute_error: 281.641174, mean_q: -4.907703\n",
      " 2808/5000: episode: 2807, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.127, 0.281], loss: 8166.428223, mean_absolute_error: 281.665161, mean_q: -4.912299\n",
      " 2809/5000: episode: 2808, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.943, 0.998], loss: 12.062735, mean_absolute_error: 277.124329, mean_q: -4.910769\n",
      " 2810/5000: episode: 2809, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.079], loss: 16312.337891, mean_absolute_error: 286.457581, mean_q: -4.904926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2811/5000: episode: 2810, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.921, 0.992], loss: 4739.010254, mean_absolute_error: 277.300507, mean_q: -4.903914\n",
      " 2812/5000: episode: 2811, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 12.085459, mean_absolute_error: 277.416138, mean_q: -4.915393\n",
      " 2813/5000: episode: 2812, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.207, 0.382], loss: 12.149807, mean_absolute_error: 277.541779, mean_q: -4.928464\n",
      " 2814/5000: episode: 2813, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.741 [0.654, 0.823], loss: 12.225915, mean_absolute_error: 277.666504, mean_q: -4.943879\n",
      " 2815/5000: episode: 2814, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.929, 0.994], loss: 1403449.625000, mean_absolute_error: 375.113525, mean_q: -4.959334\n",
      " 2816/5000: episode: 2815, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.884 [0.817, 0.940], loss: 8149.576660, mean_absolute_error: 282.558289, mean_q: -4.984727\n",
      " 2817/5000: episode: 2816, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.285, 0.472], loss: 8147.570312, mean_absolute_error: 282.680969, mean_q: -4.999575\n",
      " 2818/5000: episode: 2817, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.955, 1.000], loss: 12.532415, mean_absolute_error: 278.171448, mean_q: -5.005479\n",
      " 2819/5000: episode: 2818, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.620, 0.796], loss: 12.530338, mean_absolute_error: 278.253235, mean_q: -5.005064\n",
      " 2820/5000: episode: 2819, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.820, 0.942], loss: 12.505316, mean_absolute_error: 278.317322, mean_q: -5.000063\n",
      " 2821/5000: episode: 2820, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.043], loss: 12.461657, mean_absolute_error: 278.368469, mean_q: -4.991325\n",
      " 2822/5000: episode: 2821, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.358 [0.267, 0.453], loss: 8140.354980, mean_absolute_error: 283.014832, mean_q: -4.979474\n",
      " 2823/5000: episode: 2822, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.271, 0.457], loss: 8139.275879, mean_absolute_error: 283.049988, mean_q: -4.964632\n",
      " 2824/5000: episode: 2823, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 8138.121582, mean_absolute_error: 283.077972, mean_q: -4.946651\n",
      " 2825/5000: episode: 2824, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.841 [0.766, 0.907], loss: 12.133777, mean_absolute_error: 278.499634, mean_q: -4.925211\n",
      " 2826/5000: episode: 2825, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.039, 0.147], loss: 12902.465820, mean_absolute_error: 283.137451, mean_q: -4.900764\n",
      " 2827/5000: episode: 2826, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.701, 0.860], loss: 11.877464, mean_absolute_error: 278.546997, mean_q: -4.872903\n",
      " 2828/5000: episode: 2827, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.199, 0.374], loss: 1394852.250000, mean_absolute_error: 371.239075, mean_q: -4.845520\n",
      " 2829/5000: episode: 2828, duration: 0.122s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.101 [0.048, 0.164], loss: 11.749592, mean_absolute_error: 278.656067, mean_q: -4.846596\n",
      " 2830/5000: episode: 2829, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 8126.813965, mean_absolute_error: 283.380188, mean_q: -4.860501\n",
      " 2831/5000: episode: 2830, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.296, 0.485], loss: 11.937738, mean_absolute_error: 278.937897, mean_q: -4.885254\n",
      " 2832/5000: episode: 2831, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.832 [0.755, 0.900], loss: 1410744.250000, mean_absolute_error: 380.920380, mean_q: -4.909380\n",
      " 2833/5000: episode: 2832, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.822 [0.744, 0.892], loss: 12.215183, mean_absolute_error: 279.264160, mean_q: -4.941709\n",
      " 2834/5000: episode: 2833, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.063 [0.022, 0.114], loss: 8117.875488, mean_absolute_error: 283.999390, mean_q: -4.962580\n",
      " 2835/5000: episode: 2834, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.819 [0.741, 0.890], loss: 16218.376953, mean_absolute_error: 288.709290, mean_q: -4.973135\n",
      " 2836/5000: episode: 2835, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 8112.681152, mean_absolute_error: 284.230072, mean_q: -4.975389\n",
      " 2837/5000: episode: 2836, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.719, 0.874], loss: 12.362761, mean_absolute_error: 279.739746, mean_q: -4.971477\n",
      " 2838/5000: episode: 2837, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.745 [0.657, 0.826], loss: 12.319380, mean_absolute_error: 279.813538, mean_q: -4.962745\n",
      " 2839/5000: episode: 2838, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.955, 1.000], loss: 8105.949707, mean_absolute_error: 284.449646, mean_q: -4.950500\n",
      " 2840/5000: episode: 2839, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.325, 0.516], loss: 8104.155273, mean_absolute_error: 284.495941, mean_q: -4.935230\n",
      " 2841/5000: episode: 2840, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 8102.409668, mean_absolute_error: 284.536530, mean_q: -4.917496\n",
      " 2842/5000: episode: 2841, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.876, 0.973], loss: 8100.648438, mean_absolute_error: 284.595123, mean_q: -4.907351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2843/5000: episode: 2842, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [0.073, 0.205], loss: 12.021615, mean_absolute_error: 280.094788, mean_q: -4.902390\n",
      " 2844/5000: episode: 2843, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.255, 0.439], loss: 4994.344238, mean_absolute_error: 280.279144, mean_q: -4.902883\n",
      " 2845/5000: episode: 2844, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 16179.320312, mean_absolute_error: 289.403229, mean_q: -4.905886\n",
      " 2846/5000: episode: 2845, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.426, 0.619], loss: 1393785.250000, mean_absolute_error: 372.939270, mean_q: -4.901787\n",
      " 2847/5000: episode: 2846, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.957, 1.000], loss: 12.069040, mean_absolute_error: 280.451355, mean_q: -4.912052\n",
      " 2848/5000: episode: 2847, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.607, 0.784], loss: 12.086102, mean_absolute_error: 280.540192, mean_q: -4.915524\n",
      " 2849/5000: episode: 2848, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.112 [0.056, 0.177], loss: 8089.097168, mean_absolute_error: 285.201569, mean_q: -4.923355\n",
      " 2850/5000: episode: 2849, duration: 0.103s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.018], loss: 16162.407227, mean_absolute_error: 289.867737, mean_q: -4.932794\n",
      " 2851/5000: episode: 2850, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 1393449.000000, mean_absolute_error: 373.445251, mean_q: -4.942911\n",
      " 2852/5000: episode: 2851, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.505, 0.694], loss: 12.373073, mean_absolute_error: 281.047607, mean_q: -4.973550\n",
      " 2853/5000: episode: 2852, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.600, 0.779], loss: 12.523245, mean_absolute_error: 281.212250, mean_q: -5.003647\n",
      " 2854/5000: episode: 2853, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.042], loss: 4920.678223, mean_absolute_error: 281.401398, mean_q: -5.032125\n",
      " 2855/5000: episode: 2854, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.053], loss: 8076.948242, mean_absolute_error: 286.045013, mean_q: -5.048477\n",
      " 2856/5000: episode: 2855, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.581, 0.762], loss: 21053.730469, mean_absolute_error: 290.723145, mean_q: -5.054861\n",
      " 2857/5000: episode: 2856, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 4893.976074, mean_absolute_error: 281.687805, mean_q: -5.051988\n",
      " 2858/5000: episode: 2857, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.872, 0.971], loss: 12.725452, mean_absolute_error: 281.746704, mean_q: -5.043889\n",
      " 2859/5000: episode: 2858, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 12.717217, mean_absolute_error: 281.824768, mean_q: -5.042256\n",
      " 2860/5000: episode: 2859, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.063], loss: 1392788.250000, mean_absolute_error: 374.439880, mean_q: -5.044030\n",
      " 2861/5000: episode: 2860, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.475, 0.666], loss: 12961.549805, mean_absolute_error: 286.606476, mean_q: -5.069133\n",
      " 2862/5000: episode: 2861, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.870 [0.800, 0.930], loss: 16115.891602, mean_absolute_error: 291.277100, mean_q: -5.092062\n",
      " 2863/5000: episode: 2862, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.406 [0.313, 0.503], loss: 13132.140625, mean_absolute_error: 286.959595, mean_q: -5.104443\n",
      " 2864/5000: episode: 2863, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.212, 0.388], loss: 16108.113281, mean_absolute_error: 291.510590, mean_q: -5.117971\n",
      " 2865/5000: episode: 2864, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.803 [0.723, 0.876], loss: 8058.463379, mean_absolute_error: 287.102295, mean_q: -5.131871\n",
      " 2866/5000: episode: 2865, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.891 [0.827, 0.946], loss: 13.202762, mean_absolute_error: 282.676453, mean_q: -5.137630\n",
      " 2867/5000: episode: 2866, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.347, 0.539], loss: 13.201836, mean_absolute_error: 282.760315, mean_q: -5.137450\n",
      " 2868/5000: episode: 2867, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [0.098, 0.242], loss: 13.175920, mean_absolute_error: 282.827789, mean_q: -5.132404\n",
      " 2869/5000: episode: 2868, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.218, 0.396], loss: 8052.069824, mean_absolute_error: 287.409180, mean_q: -5.123082\n",
      " 2870/5000: episode: 2869, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.873, 0.971], loss: 8050.853516, mean_absolute_error: 287.454285, mean_q: -5.109646\n",
      " 2871/5000: episode: 2870, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.093 [0.042, 0.153], loss: 16085.291992, mean_absolute_error: 292.020020, mean_q: -5.093280\n",
      " 2872/5000: episode: 2871, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.053], loss: 8047.395996, mean_absolute_error: 287.531494, mean_q: -5.073788\n",
      " 2873/5000: episode: 2872, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.215, 0.393], loss: 12.767252, mean_absolute_error: 283.045959, mean_q: -5.052168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2874/5000: episode: 2873, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 12.702302, mean_absolute_error: 283.101929, mean_q: -5.039298\n",
      " 2875/5000: episode: 2874, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.129, 0.285], loss: 21136.875000, mean_absolute_error: 292.297485, mean_q: -5.032232\n",
      " 2876/5000: episode: 2875, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 12.598099, mean_absolute_error: 283.244019, mean_q: -5.018581\n",
      " 2877/5000: episode: 2876, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.623, 0.798], loss: 1399974.000000, mean_absolute_error: 380.280396, mean_q: -5.002392\n",
      " 2878/5000: episode: 2877, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.900 [0.837, 0.952], loss: 8034.874512, mean_absolute_error: 287.909332, mean_q: -5.001960\n",
      " 2879/5000: episode: 2878, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.524 [0.427, 0.621], loss: 8032.411133, mean_absolute_error: 287.991333, mean_q: -4.994987\n",
      " 2880/5000: episode: 2879, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.389, 0.583], loss: 12.422884, mean_absolute_error: 283.554504, mean_q: -4.983553\n",
      " 2881/5000: episode: 2880, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.719, 0.873], loss: 1391782.250000, mean_absolute_error: 376.069153, mean_q: -4.969187\n",
      " 2882/5000: episode: 2881, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.159, 0.323], loss: 8025.620605, mean_absolute_error: 288.216919, mean_q: -4.970560\n",
      " 2883/5000: episode: 2882, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.914 [0.855, 0.962], loss: 12.329347, mean_absolute_error: 283.801880, mean_q: -4.964752\n",
      " 2884/5000: episode: 2883, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.438 [0.343, 0.535], loss: 16029.787109, mean_absolute_error: 292.870117, mean_q: -4.954956\n",
      " 2885/5000: episode: 2884, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.472 [0.376, 0.569], loss: 13084.298828, mean_absolute_error: 288.503571, mean_q: -4.940968\n",
      " 2886/5000: episode: 2885, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 8016.335938, mean_absolute_error: 288.484558, mean_q: -4.922741\n",
      " 2887/5000: episode: 2886, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.574, 0.756], loss: 8014.066406, mean_absolute_error: 288.529541, mean_q: -4.902308\n",
      " 2888/5000: episode: 2887, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 1407531.125000, mean_absolute_error: 385.522461, mean_q: -4.890554\n",
      " 2889/5000: episode: 2888, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.090 [0.040, 0.150], loss: 12.025782, mean_absolute_error: 284.238312, mean_q: -4.903239\n",
      " 2890/5000: episode: 2889, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.856, 0.963], loss: 12.048597, mean_absolute_error: 284.346619, mean_q: -4.907889\n",
      " 2891/5000: episode: 2890, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.217, 0.395], loss: 1399328.375000, mean_absolute_error: 381.343231, mean_q: -4.906162\n",
      " 2892/5000: episode: 2891, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.085 [0.037, 0.143], loss: 8002.331543, mean_absolute_error: 289.047455, mean_q: -4.917371\n",
      " 2893/5000: episode: 2892, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [0.081, 0.216], loss: 5000.567871, mean_absolute_error: 284.690125, mean_q: -4.919715\n",
      " 2894/5000: episode: 2893, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [0.112, 0.261], loss: 1404248.250000, mean_absolute_error: 381.745605, mean_q: -4.915172\n",
      " 2895/5000: episode: 2894, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 1399053.625000, mean_absolute_error: 381.780151, mean_q: -4.924097\n",
      " 2896/5000: episode: 2895, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.559, 0.743], loss: 7991.632324, mean_absolute_error: 289.525330, mean_q: -4.944706\n",
      " 2897/5000: episode: 2896, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.840 [0.765, 0.906], loss: 7989.287598, mean_absolute_error: 289.644928, mean_q: -4.954864\n",
      " 2898/5000: episode: 2897, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.042], loss: 7986.958496, mean_absolute_error: 289.746277, mean_q: -4.956902\n",
      " 2899/5000: episode: 2898, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.212, 0.389], loss: 12.268054, mean_absolute_error: 285.368835, mean_q: -4.952394\n",
      " 2900/5000: episode: 2899, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.219, 0.398], loss: 12.219238, mean_absolute_error: 285.437836, mean_q: -4.942529\n",
      " 2901/5000: episode: 2900, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.458 [0.362, 0.555], loss: 12.149464, mean_absolute_error: 285.491119, mean_q: -4.928394\n",
      " 2902/5000: episode: 2901, duration: 0.080s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.181, 0.351], loss: 12.067883, mean_absolute_error: 285.534851, mean_q: -4.911817\n",
      " 2903/5000: episode: 2902, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.874 [0.806, 0.933], loss: 1398655.750000, mean_absolute_error: 382.405090, mean_q: -4.893674\n",
      " 2904/5000: episode: 2903, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.578 [0.481, 0.672], loss: 11.973170, mean_absolute_error: 285.646851, mean_q: -4.892500\n",
      " 2905/5000: episode: 2904, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 11.993241, mean_absolute_error: 285.733765, mean_q: -4.896600\n",
      " 2906/5000: episode: 2905, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.706, 0.863], loss: 1403585.250000, mean_absolute_error: 382.700928, mean_q: -4.904391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2907/5000: episode: 2906, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.882 [0.815, 0.939], loss: 7972.019043, mean_absolute_error: 290.430420, mean_q: -4.933870\n",
      " 2908/5000: episode: 2907, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.798 [0.717, 0.872], loss: 7970.513184, mean_absolute_error: 290.577759, mean_q: -4.961618\n",
      " 2909/5000: episode: 2908, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.176, 0.344], loss: 12.398026, mean_absolute_error: 286.247498, mean_q: -4.978563\n",
      " 2910/5000: episode: 2909, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.156, 0.319], loss: 7967.555176, mean_absolute_error: 290.822937, mean_q: -4.997458\n",
      " 2911/5000: episode: 2910, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.596, 0.775], loss: 12.582999, mean_absolute_error: 286.502136, mean_q: -5.015573\n",
      " 2912/5000: episode: 2911, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.072, 0.202], loss: 12.630106, mean_absolute_error: 286.602966, mean_q: -5.024954\n",
      " 2913/5000: episode: 2912, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.854 [0.782, 0.917], loss: 12.643804, mean_absolute_error: 286.681610, mean_q: -5.027678\n",
      " 2914/5000: episode: 2913, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.071 [0.028, 0.126], loss: 1397845.125000, mean_absolute_error: 383.511047, mean_q: -5.025067\n",
      " 2915/5000: episode: 2914, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 5070.747559, mean_absolute_error: 286.852142, mean_q: -5.036942\n",
      " 2916/5000: episode: 2915, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 7960.305664, mean_absolute_error: 291.370361, mean_q: -5.040603\n",
      " 2917/5000: episode: 2916, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.655, 0.824], loss: 12.693996, mean_absolute_error: 286.996033, mean_q: -5.037650\n",
      " 2918/5000: episode: 2917, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.971, 1.000], loss: 5093.133789, mean_absolute_error: 287.090973, mean_q: -5.040046\n",
      " 2919/5000: episode: 2918, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.398, 0.592], loss: 12.728936, mean_absolute_error: 287.171936, mean_q: -5.044580\n",
      " 2920/5000: episode: 2919, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.108 [0.054, 0.173], loss: 5095.750000, mean_absolute_error: 287.254089, mean_q: -5.043057\n",
      " 2921/5000: episode: 2920, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.830 [0.753, 0.898], loss: 12.683164, mean_absolute_error: 287.303101, mean_q: -5.035500\n",
      " 2922/5000: episode: 2921, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.216, 0.393], loss: 1397411.750000, mean_absolute_error: 384.103699, mean_q: -5.034473\n",
      " 2923/5000: episode: 2922, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.720 [0.630, 0.804], loss: 12.789289, mean_absolute_error: 287.514648, mean_q: -5.056527\n",
      " 2924/5000: episode: 2923, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.164, 0.329], loss: 12.853568, mean_absolute_error: 287.625275, mean_q: -5.069221\n",
      " 2925/5000: episode: 2924, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.876 [0.808, 0.934], loss: 5114.880371, mean_absolute_error: 287.720947, mean_q: -5.074033\n",
      " 2926/5000: episode: 2925, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.939 [0.888, 0.979], loss: 12.857954, mean_absolute_error: 287.780670, mean_q: -5.070086\n",
      " 2927/5000: episode: 2926, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.480, 0.671], loss: 12.811163, mean_absolute_error: 287.835907, mean_q: -5.060850\n",
      " 2928/5000: episode: 2927, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 12.800510, mean_absolute_error: 287.904846, mean_q: -5.058745\n",
      " 2929/5000: episode: 2928, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.710, 0.866], loss: 12.812166, mean_absolute_error: 287.985474, mean_q: -5.061049\n",
      " 2930/5000: episode: 2929, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.106], loss: 7942.223633, mean_absolute_error: 292.477905, mean_q: -5.057354\n",
      " 2931/5000: episode: 2930, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 5147.951172, mean_absolute_error: 288.132416, mean_q: -5.047734\n",
      " 2932/5000: episode: 2931, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.432 [0.337, 0.529], loss: 7938.771973, mean_absolute_error: 292.588501, mean_q: -5.032954\n",
      " 2933/5000: episode: 2932, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [0.090, 0.229], loss: 21021.289062, mean_absolute_error: 297.082672, mean_q: -5.015110\n",
      " 2934/5000: episode: 2933, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.071], loss: 10462.458984, mean_absolute_error: 288.388763, mean_q: -4.992879\n",
      " 2935/5000: episode: 2934, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 7932.580078, mean_absolute_error: 292.754730, mean_q: -4.987758\n",
      " 2936/5000: episode: 2935, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.063], loss: 12.484848, mean_absolute_error: 288.459656, mean_q: -4.995969\n",
      " 2937/5000: episode: 2936, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.669, 0.835], loss: 7928.049805, mean_absolute_error: 292.969421, mean_q: -4.997506\n",
      " 2938/5000: episode: 2937, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.353 [0.262, 0.447], loss: 7925.909668, mean_absolute_error: 293.051910, mean_q: -4.992895\n",
      " 2939/5000: episode: 2938, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.021], loss: 7923.768066, mean_absolute_error: 293.121277, mean_q: -4.983265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2940/5000: episode: 2939, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.031], loss: 7921.740234, mean_absolute_error: 293.180237, mean_q: -4.969996\n",
      " 2941/5000: episode: 2940, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.018], loss: 12.274729, mean_absolute_error: 288.825836, mean_q: -4.953741\n",
      " 2942/5000: episode: 2941, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.714 [0.624, 0.799], loss: 12.182337, mean_absolute_error: 288.869202, mean_q: -4.935059\n",
      " 2943/5000: episode: 2942, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.058 [0.020, 0.108], loss: 1388558.000000, mean_absolute_error: 381.135132, mean_q: -4.914470\n",
      " 2944/5000: episode: 2943, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.105], loss: 7914.532715, mean_absolute_error: 293.386536, mean_q: -4.912670\n",
      " 2945/5000: episode: 2944, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [0.071, 0.201], loss: 7912.801270, mean_absolute_error: 293.453674, mean_q: -4.905334\n",
      " 2946/5000: episode: 2945, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.579 [0.482, 0.673], loss: 15810.000977, mean_absolute_error: 297.907501, mean_q: -4.893669\n",
      " 2947/5000: episode: 2946, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.931, 0.995], loss: 1393551.875000, mean_absolute_error: 381.388794, mean_q: -4.878122\n",
      " 2948/5000: episode: 2947, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.313 [0.225, 0.404], loss: 11.906839, mean_absolute_error: 289.262146, mean_q: -4.878926\n",
      " 2949/5000: episode: 2948, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.442 [0.346, 0.538], loss: 11.883121, mean_absolute_error: 289.336853, mean_q: -4.874063\n",
      " 2950/5000: episode: 2949, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.911, 0.988], loss: 11.840137, mean_absolute_error: 289.401611, mean_q: -4.865238\n",
      " 2951/5000: episode: 2950, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [0.081, 0.216], loss: 11.780151, mean_absolute_error: 289.452942, mean_q: -4.852896\n",
      " 2952/5000: episode: 2951, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 11.706923, mean_absolute_error: 289.493469, mean_q: -4.837786\n",
      " 2953/5000: episode: 2952, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.905 [0.844, 0.956], loss: 15787.606445, mean_absolute_error: 298.298462, mean_q: -4.820859\n",
      " 2954/5000: episode: 2953, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 7898.045410, mean_absolute_error: 293.942505, mean_q: -4.801085\n",
      " 2955/5000: episode: 2954, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.905 [0.844, 0.956], loss: 11.427064, mean_absolute_error: 289.585144, mean_q: -4.779599\n",
      " 2956/5000: episode: 2955, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.315, 0.505], loss: 13054.225586, mean_absolute_error: 293.994080, mean_q: -4.757195\n",
      " 2957/5000: episode: 2956, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.505 [0.408, 0.602], loss: 11.200027, mean_absolute_error: 289.630768, mean_q: -4.731870\n",
      " 2958/5000: episode: 2957, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.957, 1.000], loss: 11.078007, mean_absolute_error: 289.651642, mean_q: -4.706017\n",
      " 2959/5000: episode: 2958, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.648, 0.818], loss: 7890.453613, mean_absolute_error: 294.044617, mean_q: -4.679461\n",
      " 2960/5000: episode: 2959, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.635, 0.808], loss: 10535.830078, mean_absolute_error: 289.801178, mean_q: -4.652106\n",
      " 2961/5000: episode: 2960, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.897 [0.834, 0.950], loss: 7887.645996, mean_absolute_error: 294.058777, mean_q: -4.622727\n",
      " 2962/5000: episode: 2961, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.421, 0.614], loss: 5342.263672, mean_absolute_error: 289.798645, mean_q: -4.592228\n",
      " 2963/5000: episode: 2962, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.489, 0.680], loss: 7883.802246, mean_absolute_error: 294.081573, mean_q: -4.560898\n",
      " 2964/5000: episode: 2963, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 15753.439453, mean_absolute_error: 298.488464, mean_q: -4.540201\n",
      " 2965/5000: episode: 2964, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.851, 0.960], loss: 7879.568359, mean_absolute_error: 294.176758, mean_q: -4.526061\n",
      " 2966/5000: episode: 2965, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.900, 0.984], loss: 5174.995117, mean_absolute_error: 289.893372, mean_q: -4.519710\n",
      " 2967/5000: episode: 2966, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.059 [0.020, 0.110], loss: 7875.071777, mean_absolute_error: 294.343323, mean_q: -4.517973\n",
      " 2968/5000: episode: 2967, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.089], loss: 5352.671875, mean_absolute_error: 290.157532, mean_q: -4.510679\n",
      " 2969/5000: episode: 2968, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.665, 0.832], loss: 15731.496094, mean_absolute_error: 298.836975, mean_q: -4.498901\n",
      " 2970/5000: episode: 2969, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.031], loss: 10.054136, mean_absolute_error: 290.173279, mean_q: -4.483225\n",
      " 2971/5000: episode: 2970, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.169, 0.337], loss: 9.972971, mean_absolute_error: 290.216492, mean_q: -4.465088\n",
      " 2972/5000: episode: 2971, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.371, 0.564], loss: 9.883554, mean_absolute_error: 290.252808, mean_q: -4.445022\n",
      " 2973/5000: episode: 2972, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.179 [0.109, 0.257], loss: 7863.123535, mean_absolute_error: 294.635681, mean_q: -4.424118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2974/5000: episode: 2973, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.075 [0.030, 0.130], loss: 7861.270996, mean_absolute_error: 294.663330, mean_q: -4.401337\n",
      " 2975/5000: episode: 2974, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.271, 0.457], loss: 1387852.250000, mean_absolute_error: 382.520081, mean_q: -4.377117\n",
      " 2976/5000: episode: 2975, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.123, 0.275], loss: 9.563698, mean_absolute_error: 290.421387, mean_q: -4.372488\n",
      " 2977/5000: episode: 2976, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.147, 0.309], loss: 5218.495117, mean_absolute_error: 290.507538, mean_q: -4.363375\n",
      " 2978/5000: episode: 2977, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 20909.285156, mean_absolute_error: 299.250305, mean_q: -4.350217\n",
      " 2979/5000: episode: 2978, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.174, 0.343], loss: 5221.193359, mean_absolute_error: 290.606628, mean_q: -4.332683\n",
      " 2980/5000: episode: 2979, duration: 0.117s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.619 [0.524, 0.712], loss: 15689.587891, mean_absolute_error: 299.303375, mean_q: -4.312964\n",
      " 2981/5000: episode: 2980, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 7847.463867, mean_absolute_error: 295.020996, mean_q: -4.301891\n",
      " 2982/5000: episode: 2981, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.014, 0.094], loss: 1387639.625000, mean_absolute_error: 382.922302, mean_q: -4.297112\n",
      " 2983/5000: episode: 2982, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.292 [0.207, 0.383], loss: 9.286488, mean_absolute_error: 290.875153, mean_q: -4.308638\n",
      " 2984/5000: episode: 2983, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.589 [0.492, 0.683], loss: 9.308268, mean_absolute_error: 290.967285, mean_q: -4.313688\n",
      " 2985/5000: episode: 2984, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.139, 0.297], loss: 5239.075195, mean_absolute_error: 291.063202, mean_q: -4.312786\n",
      " 2986/5000: episode: 2985, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.972, 1.000], loss: 7838.480469, mean_absolute_error: 295.437561, mean_q: -4.305914\n",
      " 2987/5000: episode: 2986, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.725, 0.878], loss: 15664.586914, mean_absolute_error: 299.816956, mean_q: -4.295377\n",
      " 2988/5000: episode: 2987, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.386, 0.579], loss: 9.168595, mean_absolute_error: 291.208252, mean_q: -4.281195\n",
      " 2989/5000: episode: 2988, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.216, 0.394], loss: 9.101023, mean_absolute_error: 291.246368, mean_q: -4.265386\n",
      " 2990/5000: episode: 2989, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.182], loss: 9.025744, mean_absolute_error: 291.274078, mean_q: -4.247705\n",
      " 2991/5000: episode: 2990, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.446 [0.351, 0.543], loss: 8.941562, mean_absolute_error: 291.297729, mean_q: -4.227844\n",
      " 2992/5000: episode: 2991, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.275, 0.461], loss: 15650.357422, mean_absolute_error: 299.992065, mean_q: -4.217478\n",
      " 2993/5000: episode: 2992, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 5271.380371, mean_absolute_error: 291.447693, mean_q: -4.213003\n",
      " 2994/5000: episode: 2993, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.834 [0.758, 0.901], loss: 1387230.625000, mean_absolute_error: 383.608826, mean_q: -4.203303\n",
      " 2995/5000: episode: 2994, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.329, 0.520], loss: 7824.317871, mean_absolute_error: 295.893066, mean_q: -4.210712\n",
      " 2996/5000: episode: 2995, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.675, 0.840], loss: 5280.241211, mean_absolute_error: 291.690399, mean_q: -4.211801\n",
      " 2997/5000: episode: 2996, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.333, 0.524], loss: 8.854863, mean_absolute_error: 291.724213, mean_q: -4.207293\n",
      " 2998/5000: episode: 2997, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.602, 0.780], loss: 8.821604, mean_absolute_error: 291.774231, mean_q: -4.199382\n",
      " 2999/5000: episode: 2998, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 2795063.750000, mean_absolute_error: 484.783966, mean_q: -4.188457\n",
      " 3000/5000: episode: 2999, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.348 [0.258, 0.442], loss: 7816.895508, mean_absolute_error: 296.269104, mean_q: -4.212552\n",
      " 3001/5000: episode: 3000, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.881 [0.814, 0.938], loss: 7815.000977, mean_absolute_error: 296.412872, mean_q: -4.237803\n",
      " 3002/5000: episode: 3001, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.062 [0.022, 0.114], loss: 9.088317, mean_absolute_error: 292.250885, mean_q: -4.262406\n",
      " 3003/5000: episode: 3002, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.719, 0.873], loss: 18516.310547, mean_absolute_error: 296.796112, mean_q: -4.277775\n",
      " 3004/5000: episode: 3003, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.255, 0.439], loss: 9.175956, mean_absolute_error: 292.462860, mean_q: -4.282913\n",
      " 3005/5000: episode: 3004, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 7808.041504, mean_absolute_error: 296.839783, mean_q: -4.282866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3006/5000: episode: 3005, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.849 [0.776, 0.914], loss: 7806.607422, mean_absolute_error: 296.900269, mean_q: -4.277847\n",
      " 3007/5000: episode: 3006, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.935, 0.996], loss: 9.115530, mean_absolute_error: 292.653015, mean_q: -4.268785\n",
      " 3008/5000: episode: 3007, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.649, 0.819], loss: 7804.109863, mean_absolute_error: 296.991333, mean_q: -4.257140\n",
      " 3009/5000: episode: 3008, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.043], loss: 7802.698730, mean_absolute_error: 297.027618, mean_q: -4.242782\n",
      " 3010/5000: episode: 3009, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.216 [0.140, 0.299], loss: 8.934456, mean_absolute_error: 292.761902, mean_q: -4.226164\n",
      " 3011/5000: episode: 3010, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.290 [0.205, 0.380], loss: 8.856947, mean_absolute_error: 292.781799, mean_q: -4.207788\n",
      " 3012/5000: episode: 3011, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 15589.810547, mean_absolute_error: 301.388580, mean_q: -4.187178\n",
      " 3013/5000: episode: 3012, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.876 [0.808, 0.935], loss: 7797.792480, mean_absolute_error: 297.112488, mean_q: -4.163952\n",
      " 3014/5000: episode: 3013, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.829 [0.752, 0.897], loss: 7796.291504, mean_absolute_error: 297.129120, mean_q: -4.140233\n",
      " 3015/5000: episode: 3014, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.054 [0.017, 0.102], loss: 8.475426, mean_absolute_error: 292.854431, mean_q: -4.116141\n",
      " 3016/5000: episode: 3015, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.072, 0.202], loss: 5453.695312, mean_absolute_error: 292.972107, mean_q: -4.091083\n",
      " 3017/5000: episode: 3016, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 8.261705, mean_absolute_error: 292.881287, mean_q: -4.063900\n",
      " 3018/5000: episode: 3017, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.052], loss: 8.154346, mean_absolute_error: 292.891174, mean_q: -4.037402\n",
      " 3019/5000: episode: 3018, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.903 [0.841, 0.954], loss: 15569.311523, mean_absolute_error: 301.472382, mean_q: -4.010159\n",
      " 3020/5000: episode: 3019, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.363 [0.272, 0.458], loss: 13061.859375, mean_absolute_error: 297.212952, mean_q: -3.981052\n",
      " 3021/5000: episode: 3020, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.871 [0.801, 0.930], loss: 7784.124023, mean_absolute_error: 297.221130, mean_q: -3.951235\n",
      " 3022/5000: episode: 3021, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.970], loss: 7.692554, mean_absolute_error: 292.957031, mean_q: -3.921386\n",
      " 3023/5000: episode: 3022, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.678 [0.586, 0.766], loss: 7780.115723, mean_absolute_error: 297.244965, mean_q: -3.892617\n",
      " 3024/5000: episode: 3023, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.713, 0.869], loss: 1386420.625000, mean_absolute_error: 385.055603, mean_q: -3.864588\n",
      " 3025/5000: episode: 3024, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.077 [0.032, 0.133], loss: 7.446076, mean_absolute_error: 293.041809, mean_q: -3.858035\n",
      " 3026/5000: episode: 3025, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.953 [0.907, 0.987], loss: 7774.976074, mean_absolute_error: 297.394470, mean_q: -3.859183\n",
      " 3027/5000: episode: 3026, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.517 [0.420, 0.613], loss: 7772.911133, mean_absolute_error: 297.517334, mean_q: -3.875105\n",
      " 3028/5000: episode: 3027, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.760 [0.674, 0.839], loss: 7770.627930, mean_absolute_error: 297.646667, mean_q: -3.892315\n",
      " 3029/5000: episode: 3028, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.177 [0.108, 0.255], loss: 5470.620605, mean_absolute_error: 293.593262, mean_q: -3.901642\n",
      " 3030/5000: episode: 3029, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.043], loss: 13050.657227, mean_absolute_error: 297.838257, mean_q: -3.904034\n",
      " 3031/5000: episode: 3030, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.616, 0.792], loss: 7.609215, mean_absolute_error: 293.641602, mean_q: -3.900081\n",
      " 3032/5000: episode: 3031, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.020], loss: 1385994.750000, mean_absolute_error: 385.743408, mean_q: -3.892570\n",
      " 3033/5000: episode: 3032, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.483, 0.674], loss: 7762.241699, mean_absolute_error: 298.083191, mean_q: -3.913236\n",
      " 3034/5000: episode: 3033, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [0.009, 0.083], loss: 7.745881, mean_absolute_error: 293.954315, mean_q: -3.934958\n",
      " 3035/5000: episode: 3034, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.334, 0.525], loss: 7.798083, mean_absolute_error: 294.060822, mean_q: -3.948198\n",
      " 3036/5000: episode: 3035, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.689, 0.851], loss: 7.821976, mean_absolute_error: 294.149963, mean_q: -3.954244\n",
      " 3037/5000: episode: 3036, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.081 [0.034, 0.138], loss: 7.824089, mean_absolute_error: 294.221680, mean_q: -3.954778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3038/5000: episode: 3037, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 7754.601562, mean_absolute_error: 298.531158, mean_q: -3.950502\n",
      " 3039/5000: episode: 3038, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.836, 0.952], loss: 15499.037109, mean_absolute_error: 302.826050, mean_q: -3.941151\n",
      " 3040/5000: episode: 3039, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.045], loss: 15495.576172, mean_absolute_error: 302.891632, mean_q: -3.937346\n",
      " 3041/5000: episode: 3040, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.641, 0.812], loss: 7.752574, mean_absolute_error: 294.481293, mean_q: -3.936658\n",
      " 3042/5000: episode: 3041, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 13067.997070, mean_absolute_error: 298.800995, mean_q: -3.930861\n",
      " 3043/5000: episode: 3042, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [0.093, 0.234], loss: 7744.819824, mean_absolute_error: 298.862854, mean_q: -3.919137\n",
      " 3044/5000: episode: 3043, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.305, 0.495], loss: 7.621684, mean_absolute_error: 294.677429, mean_q: -3.903275\n",
      " 3045/5000: episode: 3044, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [0.005, 0.068], loss: 7.550225, mean_absolute_error: 294.723907, mean_q: -3.884930\n",
      " 3046/5000: episode: 3045, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 7.473372, mean_absolute_error: 294.759399, mean_q: -3.865102\n",
      " 3047/5000: episode: 3046, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [0.071, 0.201], loss: 7.393475, mean_absolute_error: 294.791595, mean_q: -3.844380\n",
      " 3048/5000: episode: 3047, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.701, 0.860], loss: 7734.228516, mean_absolute_error: 299.050232, mean_q: -3.822233\n",
      " 3049/5000: episode: 3048, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 7732.113281, mean_absolute_error: 299.104797, mean_q: -3.809296\n",
      " 3050/5000: episode: 3049, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.941, 0.997], loss: 5341.335938, mean_absolute_error: 294.948700, mean_q: -3.803589\n",
      " 3051/5000: episode: 3050, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.261, 0.446], loss: 7.240210, mean_absolute_error: 295.033691, mean_q: -3.804315\n",
      " 3052/5000: episode: 3051, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 7.259249, mean_absolute_error: 295.129761, mean_q: -3.809315\n",
      " 3053/5000: episode: 3052, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 7.255067, mean_absolute_error: 295.208740, mean_q: -3.808217\n",
      " 3054/5000: episode: 3053, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.500, 0.690], loss: 7.234056, mean_absolute_error: 295.275238, mean_q: -3.802697\n",
      " 3055/5000: episode: 3054, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.173, 0.341], loss: 7.195590, mean_absolute_error: 295.332428, mean_q: -3.792571\n",
      " 3056/5000: episode: 3055, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.270], loss: 1385073.625000, mean_absolute_error: 387.360199, mean_q: -3.779243\n",
      " 3057/5000: episode: 3056, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.728, 0.880], loss: 7.163063, mean_absolute_error: 295.475708, mean_q: -3.783987\n",
      " 3058/5000: episode: 3057, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 7.159335, mean_absolute_error: 295.558289, mean_q: -3.783002\n",
      " 3059/5000: episode: 3058, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.695 [0.604, 0.782], loss: 1390319.250000, mean_absolute_error: 387.614380, mean_q: -3.778336\n",
      " 3060/5000: episode: 3059, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.824 [0.747, 0.894], loss: 7.224037, mean_absolute_error: 295.754639, mean_q: -3.800062\n",
      " 3061/5000: episode: 3060, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.598], loss: 7.308793, mean_absolute_error: 295.879456, mean_q: -3.822295\n",
      " 3062/5000: episode: 3061, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.233, 0.414], loss: 7.359142, mean_absolute_error: 295.979553, mean_q: -3.835442\n",
      " 3063/5000: episode: 3062, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 5474.230469, mean_absolute_error: 296.115875, mean_q: -3.841813\n",
      " 3064/5000: episode: 3063, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 7.381979, mean_absolute_error: 296.126556, mean_q: -3.841390\n",
      " 3065/5000: episode: 3064, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.293, 0.481], loss: 7.364503, mean_absolute_error: 296.179199, mean_q: -3.836839\n",
      " 3066/5000: episode: 3065, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.850 [0.777, 0.914], loss: 7.335032, mean_absolute_error: 296.220398, mean_q: -3.829152\n",
      " 3067/5000: episode: 3066, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.704, 0.862], loss: 7703.156738, mean_absolute_error: 300.456787, mean_q: -3.818959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3068/5000: episode: 3067, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.684, 0.847], loss: 7.244125, mean_absolute_error: 296.278564, mean_q: -3.805344\n",
      " 3069/5000: episode: 3068, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 7.180241, mean_absolute_error: 296.294128, mean_q: -3.788523\n",
      " 3070/5000: episode: 3069, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.045], loss: 7.106752, mean_absolute_error: 296.305908, mean_q: -3.769080\n",
      " 3071/5000: episode: 3070, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.315, 0.505], loss: 7.030200, mean_absolute_error: 296.318176, mean_q: -3.748720\n",
      " 3072/5000: episode: 3071, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.835 [0.760, 0.902], loss: 6.952328, mean_absolute_error: 296.327850, mean_q: -3.727895\n",
      " 3073/5000: episode: 3072, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.585 [0.489, 0.679], loss: 6.871675, mean_absolute_error: 296.342407, mean_q: -3.706203\n",
      " 3074/5000: episode: 3073, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.590, 0.770], loss: 1390037.125000, mean_absolute_error: 388.400421, mean_q: -3.683264\n",
      " 3075/5000: episode: 3074, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [0.066, 0.193], loss: 15381.690430, mean_absolute_error: 304.822937, mean_q: -3.679380\n",
      " 3076/5000: episode: 3075, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.916 [0.858, 0.964], loss: 6.739182, mean_absolute_error: 296.497192, mean_q: -3.670289\n",
      " 3077/5000: episode: 3076, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.398, 0.591], loss: 7689.932617, mean_absolute_error: 300.746002, mean_q: -3.658780\n",
      " 3078/5000: episode: 3077, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [0.081, 0.216], loss: 6.645168, mean_absolute_error: 296.601990, mean_q: -3.644591\n",
      " 3079/5000: episode: 3078, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.633, 0.806], loss: 6.585870, mean_absolute_error: 296.641113, mean_q: -3.628289\n",
      " 3080/5000: episode: 3079, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 6.520833, mean_absolute_error: 296.673096, mean_q: -3.610325\n",
      " 3081/5000: episode: 3080, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 7682.869629, mean_absolute_error: 300.886810, mean_q: -3.591277\n",
      " 3082/5000: episode: 3081, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.670 [0.577, 0.759], loss: 6.374868, mean_absolute_error: 296.730530, mean_q: -3.569677\n",
      " 3083/5000: episode: 3082, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.065], loss: 6.290045, mean_absolute_error: 296.758240, mean_q: -3.545842\n",
      " 3084/5000: episode: 3083, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.957, 1.000], loss: 6.203971, mean_absolute_error: 296.781616, mean_q: -3.521491\n",
      " 3085/5000: episode: 3084, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.131], loss: 6.120529, mean_absolute_error: 296.801819, mean_q: -3.497723\n",
      " 3086/5000: episode: 3085, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.037], loss: 1384305.000000, mean_absolute_error: 388.744141, mean_q: -3.473726\n",
      " 3087/5000: episode: 3086, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.746 [0.658, 0.827], loss: 6.024961, mean_absolute_error: 296.885559, mean_q: -3.470300\n",
      " 3088/5000: episode: 3087, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.842 [0.768, 0.908], loss: 6.000326, mean_absolute_error: 296.943542, mean_q: -3.463196\n",
      " 3089/5000: episode: 3088, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.640, 0.812], loss: 7668.486816, mean_absolute_error: 301.159912, mean_q: -3.453140\n",
      " 3090/5000: episode: 3089, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.907, 0.987], loss: 7667.079102, mean_absolute_error: 301.195312, mean_q: -3.440033\n",
      " 3091/5000: episode: 3090, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.107 [0.053, 0.171], loss: 1384179.375000, mean_absolute_error: 388.974060, mean_q: -3.425132\n",
      " 3092/5000: episode: 3091, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.872 [0.803, 0.931], loss: 1384128.875000, mean_absolute_error: 389.050690, mean_q: -3.430355\n",
      " 3093/5000: episode: 3092, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.573, 0.755], loss: 15319.489258, mean_absolute_error: 305.595520, mean_q: -3.451217\n",
      " 3094/5000: episode: 3093, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 5.996200, mean_absolute_error: 297.373413, mean_q: -3.462005\n",
      " 3095/5000: episode: 3094, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.168, 0.334], loss: 6.012662, mean_absolute_error: 297.464508, mean_q: -3.466755\n",
      " 3096/5000: episode: 3095, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.521, 0.709], loss: 5609.831543, mean_absolute_error: 297.642090, mean_q: -3.467015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3097/5000: episode: 3096, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.055 [0.018, 0.104], loss: 15304.832031, mean_absolute_error: 305.915985, mean_q: -3.461928\n",
      " 3098/5000: episode: 3097, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.038], loss: 1383827.000000, mean_absolute_error: 389.547058, mean_q: -3.452051\n",
      " 3099/5000: episode: 3098, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.976, 1.000], loss: 5486.023438, mean_absolute_error: 297.785461, mean_q: -3.461027\n",
      " 3100/5000: episode: 3099, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.929, 0.994], loss: 5488.794922, mean_absolute_error: 297.866272, mean_q: -3.463067\n",
      " 3101/5000: episode: 3100, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.271, 0.457], loss: 5.986271, mean_absolute_error: 297.900757, mean_q: -3.459136\n",
      " 3102/5000: episode: 3101, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.539 [0.442, 0.635], loss: 30567.128906, mean_absolute_error: 314.564514, mean_q: -3.451151\n",
      " 3103/5000: episode: 3102, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 5.911162, mean_absolute_error: 298.025482, mean_q: -3.437361\n",
      " 3104/5000: episode: 3103, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.452, 0.645], loss: 7640.652344, mean_absolute_error: 302.228333, mean_q: -3.421135\n",
      " 3105/5000: episode: 3104, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.302, 0.492], loss: 13261.239258, mean_absolute_error: 302.378510, mean_q: -3.402734\n",
      " 3106/5000: episode: 3105, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.491, 0.681], loss: 5.722093, mean_absolute_error: 298.170807, mean_q: -3.381926\n",
      " 3107/5000: episode: 3106, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.511, 0.700], loss: 5.650804, mean_absolute_error: 298.207428, mean_q: -3.360786\n",
      " 3108/5000: episode: 3107, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.263 [0.180, 0.351], loss: 5.580083, mean_absolute_error: 298.236511, mean_q: -3.339684\n",
      " 3109/5000: episode: 3108, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 7629.630371, mean_absolute_error: 302.396545, mean_q: -3.319055\n",
      " 3110/5000: episode: 3109, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.274, 0.460], loss: 5568.717285, mean_absolute_error: 298.353882, mean_q: -3.297741\n",
      " 3111/5000: episode: 3110, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.090 [0.041, 0.150], loss: 5.367080, mean_absolute_error: 298.299103, mean_q: -3.275303\n",
      " 3112/5000: episode: 3111, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.408 [0.314, 0.505], loss: 22864.468750, mean_absolute_error: 310.708466, mean_q: -3.253679\n",
      " 3113/5000: episode: 3112, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.444, 0.637], loss: 10932.277344, mean_absolute_error: 298.360718, mean_q: -3.230572\n",
      " 3114/5000: episode: 3113, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.166, 0.333], loss: 5.247787, mean_absolute_error: 298.429077, mean_q: -3.238688\n",
      " 3115/5000: episode: 3114, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.501 [0.404, 0.598], loss: 7619.447754, mean_absolute_error: 302.710907, mean_q: -3.271096\n",
      " 3116/5000: episode: 3115, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.831 [0.755, 0.899], loss: 5.420495, mean_absolute_error: 298.718506, mean_q: -3.291566\n",
      " 3117/5000: episode: 3116, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 1390864.625000, mean_absolute_error: 394.803589, mean_q: -3.303479\n",
      " 3118/5000: episode: 3117, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.901, 0.984], loss: 5.545468, mean_absolute_error: 298.976257, mean_q: -3.329306\n",
      " 3119/5000: episode: 3118, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.719 [0.629, 0.803], loss: 5.601373, mean_absolute_error: 299.100342, mean_q: -3.346050\n",
      " 3120/5000: episode: 3119, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 5.634017, mean_absolute_error: 299.200500, mean_q: -3.355789\n",
      " 3121/5000: episode: 3120, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.938, 0.997], loss: 7607.213867, mean_absolute_error: 303.396912, mean_q: -3.359818\n",
      " 3122/5000: episode: 3121, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 1390541.625000, mean_absolute_error: 395.296143, mean_q: -3.357955\n",
      " 3123/5000: episode: 3122, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.531, 0.719], loss: 5488.568848, mean_absolute_error: 299.465393, mean_q: -3.372263\n",
      " 3124/5000: episode: 3123, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [0.077, 0.209], loss: 5.712653, mean_absolute_error: 299.551910, mean_q: -3.379134\n",
      " 3125/5000: episode: 3124, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 5.719131, mean_absolute_error: 299.620331, mean_q: -3.381050\n",
      " 3126/5000: episode: 3125, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.935, 0.996], loss: 5632.194336, mean_absolute_error: 299.751038, mean_q: -3.379148\n",
      " 3127/5000: episode: 3126, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.078, 0.212], loss: 13227.397461, mean_absolute_error: 303.898926, mean_q: -3.372407\n",
      " 3128/5000: episode: 3127, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.326, 0.517], loss: 5.648617, mean_absolute_error: 299.752625, mean_q: -3.360136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3129/5000: episode: 3128, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.184, 0.355], loss: 7596.186035, mean_absolute_error: 303.882446, mean_q: -3.344991\n",
      " 3130/5000: episode: 3129, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.900, 0.984], loss: 5.539815, mean_absolute_error: 299.803467, mean_q: -3.327608\n",
      " 3131/5000: episode: 3130, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.929 [0.875, 0.972], loss: 5.480193, mean_absolute_error: 299.824036, mean_q: -3.309648\n",
      " 3132/5000: episode: 3131, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.892, 0.981], loss: 5.420611, mean_absolute_error: 299.838867, mean_q: -3.291601\n",
      " 3133/5000: episode: 3132, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.441, 0.634], loss: 15177.300781, mean_absolute_error: 308.054169, mean_q: -3.272784\n",
      " 3134/5000: episode: 3133, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.254, 0.438], loss: 7589.469727, mean_absolute_error: 303.976562, mean_q: -3.250974\n",
      " 3135/5000: episode: 3134, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.893 [0.829, 0.947], loss: 15169.925781, mean_absolute_error: 308.091125, mean_q: -3.227605\n",
      " 3136/5000: episode: 3135, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [0.096, 0.238], loss: 5.132708, mean_absolute_error: 299.923950, mean_q: -3.202969\n",
      " 3137/5000: episode: 3136, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.950, 0.999], loss: 5.090892, mean_absolute_error: 299.975616, mean_q: -3.189891\n",
      " 3138/5000: episode: 3137, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.819 [0.740, 0.889], loss: 15157.185547, mean_absolute_error: 308.225342, mean_q: -3.185444\n",
      " 3139/5000: episode: 3138, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.138, 0.296], loss: 5.046072, mean_absolute_error: 300.104401, mean_q: -3.175814\n",
      " 3140/5000: episode: 3139, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.182, 0.352], loss: 5633.615234, mean_absolute_error: 300.247101, mean_q: -3.174438\n",
      " 3141/5000: episode: 3140, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 13268.536133, mean_absolute_error: 304.447357, mean_q: -3.178201\n",
      " 3142/5000: episode: 3141, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.901 [0.839, 0.953], loss: 7573.519043, mean_absolute_error: 304.411224, mean_q: -3.175611\n",
      " 3143/5000: episode: 3142, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.071, 0.201], loss: 1382420.500000, mean_absolute_error: 392.182709, mean_q: -3.169071\n",
      " 3144/5000: episode: 3143, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 5.059487, mean_absolute_error: 300.495300, mean_q: -3.180034\n",
      " 3145/5000: episode: 3144, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.154, 0.317], loss: 7567.839355, mean_absolute_error: 304.655609, mean_q: -3.183850\n",
      " 3146/5000: episode: 3145, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.029], loss: 1389833.000000, mean_absolute_error: 396.507690, mean_q: -3.181912\n",
      " 3147/5000: episode: 3146, duration: 0.089s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 5563.120605, mean_absolute_error: 300.784424, mean_q: -3.196869\n",
      " 3148/5000: episode: 3147, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 13144.365234, mean_absolute_error: 304.959717, mean_q: -3.204398\n",
      " 3149/5000: episode: 3148, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.096 [0.045, 0.157], loss: 5.139426, mean_absolute_error: 300.929108, mean_q: -3.205065\n",
      " 3150/5000: episode: 3149, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 1402830.250000, mean_absolute_error: 400.998047, mean_q: -3.213217\n",
      " 3151/5000: episode: 3150, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [0.087, 0.225], loss: 7556.747559, mean_absolute_error: 305.242004, mean_q: -3.243122\n",
      " 3152/5000: episode: 3151, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.307], loss: 7554.364258, mean_absolute_error: 305.371643, mean_q: -3.261326\n",
      " 3153/5000: episode: 3152, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.894 [0.831, 0.948], loss: 5.386529, mean_absolute_error: 301.445129, mean_q: -3.281234\n",
      " 3154/5000: episode: 3153, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.445, 0.638], loss: 7549.719727, mean_absolute_error: 305.637390, mean_q: -3.301480\n",
      " 3155/5000: episode: 3154, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 5.489673, mean_absolute_error: 301.683960, mean_q: -3.312510\n",
      " 3156/5000: episode: 3155, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.897, 0.983], loss: 5.507652, mean_absolute_error: 301.770050, mean_q: -3.317931\n",
      " 3157/5000: episode: 3156, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 1396615.375000, mean_absolute_error: 401.710754, mean_q: -3.329408\n",
      " 3158/5000: episode: 3157, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.077], loss: 7541.807617, mean_absolute_error: 306.095825, mean_q: -3.363410\n",
      " 3159/5000: episode: 3158, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.623 [0.528, 0.715], loss: 1394642.125000, mean_absolute_error: 398.064697, mean_q: -3.386145\n",
      " 3160/5000: episode: 3159, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 7536.616699, mean_absolute_error: 306.416107, mean_q: -3.419720\n",
      " 3161/5000: episode: 3160, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.587 [0.490, 0.681], loss: 5.929622, mean_absolute_error: 302.513641, mean_q: -3.442725\n",
      " 3162/5000: episode: 3161, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.893, 0.981], loss: 6.019176, mean_absolute_error: 302.659607, mean_q: -3.468633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3163/5000: episode: 3162, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.038, 0.145], loss: 6.112514, mean_absolute_error: 302.804077, mean_q: -3.495431\n",
      " 3164/5000: episode: 3163, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.506 [0.409, 0.602], loss: 6.172216, mean_absolute_error: 302.919800, mean_q: -3.512465\n",
      " 3165/5000: episode: 3164, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.126], loss: 5649.707520, mean_absolute_error: 303.029266, mean_q: -3.521978\n",
      " 3166/5000: episode: 3165, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.858 [0.786, 0.920], loss: 1380786.500000, mean_absolute_error: 394.755219, mean_q: -3.523956\n",
      " 3167/5000: episode: 3166, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.706 [0.615, 0.791], loss: 6.275626, mean_absolute_error: 303.187683, mean_q: -3.541775\n",
      " 3168/5000: episode: 3167, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.206, 0.381], loss: 6.348593, mean_absolute_error: 303.309601, mean_q: -3.562311\n",
      " 3169/5000: episode: 3168, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 6.427557, mean_absolute_error: 303.438049, mean_q: -3.584403\n",
      " 3170/5000: episode: 3169, duration: 0.105s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.039, 0.148], loss: 13340.632812, mean_absolute_error: 307.673645, mean_q: -3.598602\n",
      " 3171/5000: episode: 3170, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.005, 0.070], loss: 6.498779, mean_absolute_error: 303.622375, mean_q: -3.604213\n",
      " 3172/5000: episode: 3171, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.045], loss: 6.501327, mean_absolute_error: 303.687744, mean_q: -3.604919\n",
      " 3173/5000: episode: 3172, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.237, 0.419], loss: 1380354.125000, mean_absolute_error: 395.388611, mean_q: -3.601392\n",
      " 3174/5000: episode: 3173, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.303, 0.492], loss: 6.538507, mean_absolute_error: 303.839783, mean_q: -3.615215\n",
      " 3175/5000: episode: 3174, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.648, 0.819], loss: 7514.714844, mean_absolute_error: 307.945068, mean_q: -3.622613\n",
      " 3176/5000: episode: 3175, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.220 [0.144, 0.304], loss: 5660.987793, mean_absolute_error: 304.016541, mean_q: -3.634991\n",
      " 3177/5000: episode: 3176, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: 2767700.750000, mean_absolute_error: 491.399170, mean_q: -3.649482\n",
      " 3178/5000: episode: 3177, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.899, 0.983], loss: 6.839843, mean_absolute_error: 304.314087, mean_q: -3.697606\n",
      " 3179/5000: episode: 3178, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.705, 0.863], loss: 6.970217, mean_absolute_error: 304.471619, mean_q: -3.732689\n",
      " 3180/5000: episode: 3179, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.090], loss: 13241.931641, mean_absolute_error: 308.648987, mean_q: -3.756853\n",
      " 3181/5000: episode: 3180, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.434, 0.627], loss: 5879.264648, mean_absolute_error: 304.836853, mean_q: -3.781089\n",
      " 3182/5000: episode: 3181, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.857, 0.963], loss: 7504.697754, mean_absolute_error: 308.878906, mean_q: -3.804209\n",
      " 3183/5000: episode: 3182, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.797 [0.715, 0.871], loss: 7.287296, mean_absolute_error: 304.976593, mean_q: -3.816669\n",
      " 3184/5000: episode: 3183, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.552, 0.737], loss: 5739.780273, mean_absolute_error: 305.090149, mean_q: -3.822096\n",
      " 3185/5000: episode: 3184, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.836 [0.760, 0.903], loss: 7499.238281, mean_absolute_error: 309.156128, mean_q: -3.820426\n",
      " 3186/5000: episode: 3185, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 7497.286621, mean_absolute_error: 309.222534, mean_q: -3.813792\n",
      " 3187/5000: episode: 3186, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.430, 0.623], loss: 7.276487, mean_absolute_error: 305.299255, mean_q: -3.813836\n",
      " 3188/5000: episode: 3187, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.068 [0.026, 0.121], loss: 7493.359863, mean_absolute_error: 309.398071, mean_q: -3.818420\n",
      " 3189/5000: episode: 3188, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.933 [0.881, 0.975], loss: 7491.087891, mean_absolute_error: 309.483215, mean_q: -3.816453\n",
      " 3190/5000: episode: 3189, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.115, 0.265], loss: 14970.420898, mean_absolute_error: 313.555389, mean_q: -3.809828\n",
      " 3191/5000: episode: 3190, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.500, 0.690], loss: 1379176.875000, mean_absolute_error: 397.186066, mean_q: -3.799020\n",
      " 3192/5000: episode: 3191, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 7.292347, mean_absolute_error: 305.761292, mean_q: -3.817991\n",
      " 3193/5000: episode: 3192, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.619, 0.795], loss: 7482.308105, mean_absolute_error: 309.890930, mean_q: -3.839134\n",
      " 3194/5000: episode: 3193, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.605, 0.783], loss: 24865.003906, mean_absolute_error: 310.104736, mean_q: -3.851167\n",
      " 3195/5000: episode: 3194, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.106, 0.253], loss: 14949.208008, mean_absolute_error: 314.080322, mean_q: -3.851677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3196/5000: episode: 3195, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.982, 1.000], loss: 5747.080078, mean_absolute_error: 306.183655, mean_q: -3.846347\n",
      " 3197/5000: episode: 3196, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.604 [0.508, 0.697], loss: 7.364034, mean_absolute_error: 306.244202, mean_q: -3.836717\n",
      " 3198/5000: episode: 3197, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.421 [0.326, 0.517], loss: 7.317061, mean_absolute_error: 306.293518, mean_q: -3.824457\n",
      " 3199/5000: episode: 3198, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.192, 0.365], loss: 7.262356, mean_absolute_error: 306.335114, mean_q: -3.810130\n",
      " 3200/5000: episode: 3199, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.902 [0.840, 0.954], loss: 7469.342285, mean_absolute_error: 310.346497, mean_q: -3.794411\n",
      " 3201/5000: episode: 3200, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.524 [0.427, 0.620], loss: 7.136560, mean_absolute_error: 306.394958, mean_q: -3.776978\n",
      " 3202/5000: episode: 3201, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [0.060, 0.183], loss: 7.068200, mean_absolute_error: 306.417358, mean_q: -3.758841\n",
      " 3203/5000: episode: 3202, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 14925.069336, mean_absolute_error: 314.387878, mean_q: -3.740092\n",
      " 3204/5000: episode: 3203, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 13246.257812, mean_absolute_error: 310.446228, mean_q: -3.718902\n",
      " 3205/5000: episode: 3204, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.096 [0.045, 0.158], loss: 5809.742188, mean_absolute_error: 306.493408, mean_q: -3.694766\n",
      " 3206/5000: episode: 3205, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.957, 1.000], loss: 7461.890625, mean_absolute_error: 310.450500, mean_q: -3.669295\n",
      " 3207/5000: episode: 3206, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.792 [0.710, 0.866], loss: 13395.875977, mean_absolute_error: 310.564819, mean_q: -3.643726\n",
      " 3208/5000: episode: 3207, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.430, 0.623], loss: 17763.824219, mean_absolute_error: 306.780518, mean_q: -3.617036\n",
      " 3209/5000: episode: 3208, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.230 [0.152, 0.315], loss: 6.439017, mean_absolute_error: 306.492645, mean_q: -3.587598\n",
      " 3210/5000: episode: 3209, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.937, 0.997], loss: 6.339041, mean_absolute_error: 306.492554, mean_q: -3.559629\n",
      " 3211/5000: episode: 3210, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [0.074, 0.205], loss: 7455.186035, mean_absolute_error: 310.460541, mean_q: -3.532330\n",
      " 3212/5000: episode: 3211, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.265, 0.451], loss: 14901.163086, mean_absolute_error: 314.432098, mean_q: -3.505051\n",
      " 3213/5000: episode: 3212, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 6.049974, mean_absolute_error: 306.515137, mean_q: -3.477498\n",
      " 3214/5000: episode: 3213, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.449, 0.642], loss: 7450.164551, mean_absolute_error: 310.487396, mean_q: -3.450727\n",
      " 3215/5000: episode: 3214, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.928, 0.994], loss: 13375.189453, mean_absolute_error: 310.592346, mean_q: -3.423271\n",
      " 3216/5000: episode: 3215, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.649, 0.819], loss: 5.765360, mean_absolute_error: 306.532623, mean_q: -3.394691\n",
      " 3217/5000: episode: 3216, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.868 [0.798, 0.928], loss: 5.674496, mean_absolute_error: 306.534241, mean_q: -3.367826\n",
      " 3218/5000: episode: 3217, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.496 [0.399, 0.593], loss: 5.589157, mean_absolute_error: 306.532440, mean_q: -3.342399\n",
      " 3219/5000: episode: 3218, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.192, 0.364], loss: 5.507421, mean_absolute_error: 306.533142, mean_q: -3.317862\n",
      " 3220/5000: episode: 3219, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.886, 0.978], loss: 5.428345, mean_absolute_error: 306.534729, mean_q: -3.293949\n",
      " 3221/5000: episode: 3220, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 7442.023438, mean_absolute_error: 310.489990, mean_q: -3.269992\n",
      " 3222/5000: episode: 3221, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.301, 0.490], loss: 5802.451660, mean_absolute_error: 306.567841, mean_q: -3.245365\n",
      " 3223/5000: episode: 3222, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 5.189157, mean_absolute_error: 306.535675, mean_q: -3.220539\n",
      " 3224/5000: episode: 3223, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.619, 0.795], loss: 5.112140, mean_absolute_error: 306.533875, mean_q: -3.196543\n",
      " 3225/5000: episode: 3224, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.934, 0.996], loss: 14871.013672, mean_absolute_error: 314.441315, mean_q: -3.172904\n",
      " 3226/5000: episode: 3225, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.876 [0.808, 0.934], loss: 4.958781, mean_absolute_error: 306.549591, mean_q: -3.148216\n",
      " 3227/5000: episode: 3226, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.880 [0.813, 0.938], loss: 7434.772461, mean_absolute_error: 310.510742, mean_q: -3.123379\n",
      " 3228/5000: episode: 3227, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.911, 0.988], loss: 1399611.500000, mean_absolute_error: 406.111664, mean_q: -3.098149\n",
      " 3229/5000: episode: 3228, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 7430.791016, mean_absolute_error: 310.591064, mean_q: -3.091930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3230/5000: episode: 3229, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 4.747402, mean_absolute_error: 306.707977, mean_q: -3.080364\n",
      " 3231/5000: episode: 3230, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.961, 1.000], loss: 1384620.750000, mean_absolute_error: 398.364044, mean_q: -3.066349\n",
      " 3232/5000: episode: 3231, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [0.080, 0.214], loss: 7423.943359, mean_absolute_error: 310.796936, mean_q: -3.071358\n",
      " 3233/5000: episode: 3232, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.153, 0.316], loss: 7421.529785, mean_absolute_error: 310.877502, mean_q: -3.070997\n",
      " 3234/5000: episode: 3233, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.887 [0.821, 0.943], loss: 4.702104, mean_absolute_error: 307.012787, mean_q: -3.065628\n",
      " 3235/5000: episode: 3234, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.860 [0.789, 0.922], loss: 13327.193359, mean_absolute_error: 311.085144, mean_q: -3.056867\n",
      " 3236/5000: episode: 3235, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.905, 0.986], loss: 7415.414062, mean_absolute_error: 311.048279, mean_q: -3.044077\n",
      " 3237/5000: episode: 3236, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.917 [0.859, 0.964], loss: 4.591563, mean_absolute_error: 307.160767, mean_q: -3.029367\n",
      " 3238/5000: episode: 3237, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.330 [0.241, 0.423], loss: 7411.926758, mean_absolute_error: 311.122192, mean_q: -3.014075\n",
      " 3239/5000: episode: 3238, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 7410.154297, mean_absolute_error: 311.149841, mean_q: -2.996952\n",
      " 3240/5000: episode: 3239, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.911 [0.851, 0.960], loss: 4.438385, mean_absolute_error: 307.247864, mean_q: -2.978391\n",
      " 3241/5000: episode: 3240, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 5946.754883, mean_absolute_error: 307.366211, mean_q: -2.960160\n",
      " 3242/5000: episode: 3241, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.340, 0.532], loss: 4.326666, mean_absolute_error: 307.277252, mean_q: -2.940655\n",
      " 3243/5000: episode: 3242, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 7405.371582, mean_absolute_error: 311.206024, mean_q: -2.921636\n",
      " 3244/5000: episode: 3243, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.039, 0.146], loss: 4.214578, mean_absolute_error: 307.293152, mean_q: -2.902301\n",
      " 3245/5000: episode: 3244, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.959], loss: 4.158784, mean_absolute_error: 307.301514, mean_q: -2.883019\n",
      " 3246/5000: episode: 3245, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.098, 0.241], loss: 28088.750000, mean_absolute_error: 319.165527, mean_q: -2.875240\n",
      " 3247/5000: episode: 3246, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.631, 0.805], loss: 4.127609, mean_absolute_error: 307.408051, mean_q: -2.872190\n",
      " 3248/5000: episode: 3247, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.506, 0.696], loss: 4.109470, mean_absolute_error: 307.466614, mean_q: -2.865870\n",
      " 3249/5000: episode: 3248, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.592, 0.772], loss: 4.081652, mean_absolute_error: 307.515259, mean_q: -2.856150\n",
      " 3250/5000: episode: 3249, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.307], loss: 7395.564453, mean_absolute_error: 311.465607, mean_q: -2.844071\n",
      " 3251/5000: episode: 3250, duration: 0.078s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.450, 0.642], loss: 7393.843750, mean_absolute_error: 311.498657, mean_q: -2.830446\n",
      " 3252/5000: episode: 3251, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 7392.312988, mean_absolute_error: 311.527191, mean_q: -2.815568\n",
      " 3253/5000: episode: 3252, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.158, 0.322], loss: 3.921941, mean_absolute_error: 307.646790, mean_q: -2.799693\n",
      " 3254/5000: episode: 3253, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.928 [0.873, 0.971], loss: 7389.249512, mean_absolute_error: 311.578796, mean_q: -2.782962\n",
      " 3255/5000: episode: 3254, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.942, 0.998], loss: 3.825250, mean_absolute_error: 307.702576, mean_q: -2.764954\n",
      " 3256/5000: episode: 3255, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.950, 0.999], loss: 3.776385, mean_absolute_error: 307.724670, mean_q: -2.747231\n",
      " 3257/5000: episode: 3256, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.309, 0.499], loss: 7384.559570, mean_absolute_error: 311.704254, mean_q: -2.752584\n",
      " 3258/5000: episode: 3257, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.339, 0.531], loss: 7382.611816, mean_absolute_error: 311.827301, mean_q: -2.772806\n",
      " 3259/5000: episode: 3258, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.217 [0.141, 0.300], loss: 11888.422852, mean_absolute_error: 308.208191, mean_q: -2.784828\n",
      " 3260/5000: episode: 3259, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [0.057, 0.178], loss: 3.888474, mean_absolute_error: 308.121826, mean_q: -2.787718\n",
      " 3261/5000: episode: 3260, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.842 [0.768, 0.908], loss: 3.882566, mean_absolute_error: 308.197876, mean_q: -2.785599\n",
      " 3262/5000: episode: 3261, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.035, 0.140], loss: 3.867200, mean_absolute_error: 308.264526, mean_q: -2.780079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3263/5000: episode: 3262, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 5853.535156, mean_absolute_error: 308.378784, mean_q: -2.783563\n",
      " 3264/5000: episode: 3263, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.606, 0.784], loss: 3.900845, mean_absolute_error: 308.439697, mean_q: -2.792150\n",
      " 3265/5000: episode: 3264, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 3.910720, mean_absolute_error: 308.515594, mean_q: -2.795684\n",
      " 3266/5000: episode: 3265, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 20632.724609, mean_absolute_error: 316.401642, mean_q: -2.793959\n",
      " 3267/5000: episode: 3266, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.548, 0.733], loss: 7365.260742, mean_absolute_error: 312.545410, mean_q: -2.797182\n",
      " 3268/5000: episode: 3267, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.070 [0.027, 0.124], loss: 3.970861, mean_absolute_error: 308.793823, mean_q: -2.817106\n",
      " 3269/5000: episode: 3268, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.603, 0.781], loss: 7360.872070, mean_absolute_error: 312.807526, mean_q: -2.840011\n",
      " 3270/5000: episode: 3269, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.307], loss: 4.075613, mean_absolute_error: 309.039124, mean_q: -2.854035\n",
      " 3271/5000: episode: 3270, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.565, 0.748], loss: 7357.294922, mean_absolute_error: 312.999207, mean_q: -2.860826\n",
      " 3272/5000: episode: 3271, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.423, 0.616], loss: 4.097973, mean_absolute_error: 309.193604, mean_q: -2.861856\n",
      " 3273/5000: episode: 3272, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.623, 0.798], loss: 4.091227, mean_absolute_error: 309.249756, mean_q: -2.859499\n",
      " 3274/5000: episode: 3273, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.177 [0.108, 0.255], loss: 14702.688477, mean_absolute_error: 317.035736, mean_q: -2.854407\n",
      " 3275/5000: episode: 3274, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 14699.885742, mean_absolute_error: 317.077271, mean_q: -2.845777\n",
      " 3276/5000: episode: 3275, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.292, 0.480], loss: 4.018979, mean_absolute_error: 309.378662, mean_q: -2.834129\n",
      " 3277/5000: episode: 3276, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.219, 0.398], loss: 13216.909180, mean_absolute_error: 313.296265, mean_q: -2.820871\n",
      " 3278/5000: episode: 3277, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.354 [0.264, 0.449], loss: 3.932570, mean_absolute_error: 309.430969, mean_q: -2.803486\n",
      " 3279/5000: episode: 3278, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.498, 0.688], loss: 7346.417480, mean_absolute_error: 313.309509, mean_q: -2.785643\n",
      " 3280/5000: episode: 3279, duration: 0.089s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.051, 0.167], loss: 3.831900, mean_absolute_error: 309.458984, mean_q: -2.767357\n",
      " 3281/5000: episode: 3280, duration: 0.044s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.248 [0.168, 0.335], loss: 7344.255859, mean_absolute_error: 313.333557, mean_q: -2.749440\n",
      " 3282/5000: episode: 3281, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 7343.184082, mean_absolute_error: 313.344238, mean_q: -2.731154\n",
      " 3283/5000: episode: 3282, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.705, 0.863], loss: 1377369.500000, mean_absolute_error: 400.930817, mean_q: -2.712526\n",
      " 3284/5000: episode: 3283, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.958, 1.000], loss: 1377327.375000, mean_absolute_error: 400.995728, mean_q: -2.716190\n",
      " 3285/5000: episode: 3284, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [0.093, 0.234], loss: 3.750530, mean_absolute_error: 309.674988, mean_q: -2.737806\n",
      " 3286/5000: episode: 3285, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.916 [0.857, 0.963], loss: 3.788338, mean_absolute_error: 309.770081, mean_q: -2.751576\n",
      " 3287/5000: episode: 3286, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [0.078, 0.212], loss: 1377141.250000, mean_absolute_error: 401.270996, mean_q: -2.758577\n",
      " 3288/5000: episode: 3287, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.486 [0.389, 0.583], loss: 3.872997, mean_absolute_error: 309.977692, mean_q: -2.782163\n",
      " 3289/5000: episode: 3288, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 3.913828, mean_absolute_error: 310.084290, mean_q: -2.796795\n",
      " 3290/5000: episode: 3289, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.857 [0.785, 0.920], loss: 3.935564, mean_absolute_error: 310.168579, mean_q: -2.804553\n",
      " 3291/5000: episode: 3290, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.065], loss: 1376902.125000, mean_absolute_error: 401.639709, mean_q: -2.807323\n",
      " 3292/5000: episode: 3291, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.068 [0.026, 0.121], loss: 1376827.375000, mean_absolute_error: 401.749298, mean_q: -2.827780\n",
      " 3293/5000: episode: 3292, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.595 [0.499, 0.689], loss: 4.098680, mean_absolute_error: 310.502197, mean_q: -2.862103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3294/5000: episode: 3293, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 7326.620605, mean_absolute_error: 314.466919, mean_q: -2.885709\n",
      " 3295/5000: episode: 3294, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.533, 0.720], loss: 7325.029297, mean_absolute_error: 314.567810, mean_q: -2.900611\n",
      " 3296/5000: episode: 3295, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 7323.123535, mean_absolute_error: 314.659149, mean_q: -2.907680\n",
      " 3297/5000: episode: 3296, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 4.230527, mean_absolute_error: 310.898895, mean_q: -2.907789\n",
      " 3298/5000: episode: 3297, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.836 [0.760, 0.903], loss: 7319.127441, mean_absolute_error: 314.802124, mean_q: -2.903938\n",
      " 3299/5000: episode: 3298, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 7317.402832, mean_absolute_error: 314.852905, mean_q: -2.896152\n",
      " 3300/5000: episode: 3299, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.423, 0.617], loss: 7315.586426, mean_absolute_error: 314.896606, mean_q: -2.884871\n",
      " 3301/5000: episode: 3300, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.929, 0.994], loss: 4.125406, mean_absolute_error: 311.104553, mean_q: -2.871423\n",
      " 3302/5000: episode: 3301, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.021], loss: 7312.154297, mean_absolute_error: 314.971222, mean_q: -2.857376\n",
      " 3303/5000: episode: 3302, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.042], loss: 4.041481, mean_absolute_error: 311.170532, mean_q: -2.842055\n",
      " 3304/5000: episode: 3303, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 6045.381348, mean_absolute_error: 311.272339, mean_q: -2.826025\n",
      " 3305/5000: episode: 3304, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.061], loss: 12103.031250, mean_absolute_error: 311.374512, mean_q: -2.808123\n",
      " 3306/5000: episode: 3305, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.085 [0.037, 0.143], loss: 3.888197, mean_absolute_error: 311.219879, mean_q: -2.787619\n",
      " 3307/5000: episode: 3306, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.414, 0.608], loss: 3.832272, mean_absolute_error: 311.224915, mean_q: -2.767491\n",
      " 3308/5000: episode: 3307, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.901, 0.984], loss: 3.777741, mean_absolute_error: 311.230164, mean_q: -2.747724\n",
      " 3309/5000: episode: 3308, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.800 [0.719, 0.874], loss: 3.725153, mean_absolute_error: 311.234589, mean_q: -2.728525\n",
      " 3310/5000: episode: 3309, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.820, 0.942], loss: 14603.400391, mean_absolute_error: 318.884979, mean_q: -2.709498\n",
      " 3311/5000: episode: 3310, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [0.106, 0.252], loss: 7302.222168, mean_absolute_error: 315.073547, mean_q: -2.689360\n",
      " 3312/5000: episode: 3311, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.905 [0.844, 0.956], loss: 3.564006, mean_absolute_error: 311.266418, mean_q: -2.668834\n",
      " 3313/5000: episode: 3312, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.971, 1.000], loss: 1398249.125000, mean_absolute_error: 414.103088, mean_q: -2.648911\n",
      " 3314/5000: episode: 3313, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.957, 1.000], loss: 3.512805, mean_absolute_error: 311.363953, mean_q: -2.649587\n",
      " 3315/5000: episode: 3314, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.318, 0.508], loss: 3.503382, mean_absolute_error: 311.430054, mean_q: -2.646029\n",
      " 3316/5000: episode: 3315, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 13256.814453, mean_absolute_error: 315.326355, mean_q: -2.639112\n",
      " 3317/5000: episode: 3316, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.098], loss: 3.454981, mean_absolute_error: 311.522797, mean_q: -2.627681\n",
      " 3318/5000: episode: 3317, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.078], loss: 13340.734375, mean_absolute_error: 315.439270, mean_q: -2.613665\n",
      " 3319/5000: episode: 3318, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.922 [0.866, 0.968], loss: 13339.067383, mean_absolute_error: 315.459564, mean_q: -2.595489\n",
      " 3320/5000: episode: 3319, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.799 [0.718, 0.872], loss: 7287.486328, mean_absolute_error: 315.394287, mean_q: -2.574184\n",
      " 3321/5000: episode: 3320, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.280, 0.467], loss: 3.258534, mean_absolute_error: 311.597412, mean_q: -2.551855\n",
      " 3322/5000: episode: 3321, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.574, 0.756], loss: 13293.125977, mean_absolute_error: 315.463928, mean_q: -2.529816\n",
      " 3323/5000: episode: 3322, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 7282.985840, mean_absolute_error: 315.418091, mean_q: -2.506205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3324/5000: episode: 3323, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.067 [0.025, 0.120], loss: 3.085535, mean_absolute_error: 311.626587, mean_q: -2.483164\n",
      " 3325/5000: episode: 3324, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.923, 0.993], loss: 1383531.375000, mean_absolute_error: 406.796814, mean_q: -2.461272\n",
      " 3326/5000: episode: 3325, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.832 [0.755, 0.900], loss: 3.030066, mean_absolute_error: 311.715698, mean_q: -2.460733\n",
      " 3327/5000: episode: 3326, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.895 [0.831, 0.949], loss: 7275.636230, mean_absolute_error: 315.574371, mean_q: -2.456065\n",
      " 3328/5000: episode: 3327, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.894, 0.981], loss: 2.997147, mean_absolute_error: 311.834290, mean_q: -2.447325\n",
      " 3329/5000: episode: 3328, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.689, 0.851], loss: 2.969549, mean_absolute_error: 311.882172, mean_q: -2.436027\n",
      " 3330/5000: episode: 3329, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.597, 0.776], loss: 7269.729004, mean_absolute_error: 315.710083, mean_q: -2.422996\n",
      " 3331/5000: episode: 3330, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.039], loss: 14532.855469, mean_absolute_error: 319.531982, mean_q: -2.407880\n",
      " 3332/5000: episode: 3331, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.178, 0.347], loss: 2.860996, mean_absolute_error: 311.990601, mean_q: -2.391068\n",
      " 3333/5000: episode: 3332, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.948, 0.999], loss: 1383371.250000, mean_absolute_error: 407.149445, mean_q: -2.373934\n",
      " 3334/5000: episode: 3333, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [0.054, 0.174], loss: 5926.919922, mean_absolute_error: 312.108917, mean_q: -2.377435\n",
      " 3335/5000: episode: 3334, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.241, 0.422], loss: 2.823724, mean_absolute_error: 312.176331, mean_q: -2.375436\n",
      " 3336/5000: episode: 3335, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.522, 0.710], loss: 14512.656250, mean_absolute_error: 319.794250, mean_q: -2.370735\n",
      " 3337/5000: episode: 3336, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.950, 0.999], loss: 14507.833008, mean_absolute_error: 319.851868, mean_q: -2.361653\n",
      " 3338/5000: episode: 3337, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.357, 0.549], loss: 14502.125000, mean_absolute_error: 319.907623, mean_q: -2.348694\n",
      " 3339/5000: episode: 3338, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.371, 0.564], loss: 2.749436, mean_absolute_error: 312.452209, mean_q: -2.343967\n",
      " 3340/5000: episode: 3339, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.470, 0.662], loss: 14488.788086, mean_absolute_error: 320.097260, mean_q: -2.346937\n",
      " 3341/5000: episode: 3340, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.264 [0.182, 0.352], loss: 2.804152, mean_absolute_error: 312.720337, mean_q: -2.367186\n",
      " 3342/5000: episode: 3341, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.630 [0.535, 0.721], loss: 7238.791016, mean_absolute_error: 316.694458, mean_q: -2.411576\n",
      " 3343/5000: episode: 3342, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.817 [0.739, 0.888], loss: 7235.658691, mean_absolute_error: 316.893158, mean_q: -2.453814\n",
      " 3344/5000: episode: 3343, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 3.088971, mean_absolute_error: 313.303589, mean_q: -2.484547\n",
      " 3345/5000: episode: 3344, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 3.173086, mean_absolute_error: 313.469543, mean_q: -2.518161\n",
      " 3346/5000: episode: 3345, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.252, 0.435], loss: 7228.315918, mean_absolute_error: 317.380951, mean_q: -2.552943\n",
      " 3347/5000: episode: 3346, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.644, 0.815], loss: 21671.929688, mean_absolute_error: 325.039856, mean_q: -2.589019\n",
      " 3348/5000: episode: 3347, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.766 [0.681, 0.844], loss: 3.442128, mean_absolute_error: 313.973907, mean_q: -2.622787\n",
      " 3349/5000: episode: 3348, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.107 [0.053, 0.171], loss: 14437.795898, mean_absolute_error: 321.605072, mean_q: -2.645757\n",
      " 3350/5000: episode: 3349, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.686, 0.848], loss: 3.537570, mean_absolute_error: 314.245422, mean_q: -2.658914\n",
      " 3351/5000: episode: 3350, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.298 [0.212, 0.389], loss: 7215.403809, mean_absolute_error: 318.085449, mean_q: -2.666169\n",
      " 3352/5000: episode: 3351, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.902, 0.985], loss: 3.562768, mean_absolute_error: 314.436951, mean_q: -2.668370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3353/5000: episode: 3352, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.103 [0.050, 0.166], loss: 1374743.750000, mean_absolute_error: 405.755310, mean_q: -2.667109\n",
      " 3354/5000: episode: 3353, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.469 [0.373, 0.566], loss: 7209.562988, mean_absolute_error: 318.352539, mean_q: -2.684986\n",
      " 3355/5000: episode: 3354, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.890 [0.825, 0.945], loss: 3.635189, mean_absolute_error: 314.719604, mean_q: -2.695364\n",
      " 3356/5000: episode: 3355, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.635, 0.808], loss: 3.647767, mean_absolute_error: 314.802063, mean_q: -2.700025\n",
      " 3357/5000: episode: 3356, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.127], loss: 1374529.750000, mean_absolute_error: 406.107117, mean_q: -2.699964\n",
      " 3358/5000: episode: 3357, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.079, 0.213], loss: 7202.447266, mean_absolute_error: 318.714966, mean_q: -2.718476\n",
      " 3359/5000: episode: 3358, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.616, 0.792], loss: 7200.575684, mean_absolute_error: 318.814453, mean_q: -2.729357\n",
      " 3360/5000: episode: 3359, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 7198.648926, mean_absolute_error: 318.899323, mean_q: -2.733984\n",
      " 3361/5000: episode: 3360, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.218 [0.142, 0.301], loss: 7196.671387, mean_absolute_error: 318.970886, mean_q: -2.732792\n",
      " 3362/5000: episode: 3361, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.189, 0.361], loss: 6041.602051, mean_absolute_error: 315.317017, mean_q: -2.726973\n",
      " 3363/5000: episode: 3362, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 1374259.250000, mean_absolute_error: 406.578003, mean_q: -2.717732\n",
      " 3364/5000: episode: 3363, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.852 [0.780, 0.916], loss: 3.726078, mean_absolute_error: 315.461914, mean_q: -2.728864\n",
      " 3365/5000: episode: 3364, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.549 [0.452, 0.645], loss: 3.741209, mean_absolute_error: 315.542847, mean_q: -2.734401\n",
      " 3366/5000: episode: 3365, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.417 [0.323, 0.514], loss: 7188.384766, mean_absolute_error: 319.319794, mean_q: -2.735366\n",
      " 3367/5000: episode: 3366, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.841 [0.766, 0.907], loss: 1374081.000000, mean_absolute_error: 406.869812, mean_q: -2.732005\n",
      " 3368/5000: episode: 3367, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 3.777877, mean_absolute_error: 315.778259, mean_q: -2.747773\n",
      " 3369/5000: episode: 3368, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.159, 0.323], loss: 3.799561, mean_absolute_error: 315.870087, mean_q: -2.755651\n",
      " 3370/5000: episode: 3369, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.741 [0.653, 0.822], loss: 3.868415, mean_absolute_error: 316.006653, mean_q: -2.780516\n",
      " 3371/5000: episode: 3370, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.885, 0.977], loss: 1373764.750000, mean_absolute_error: 407.352936, mean_q: -2.818110\n",
      " 3372/5000: episode: 3371, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.071 [0.027, 0.125], loss: 4.111994, mean_absolute_error: 316.369385, mean_q: -2.866750\n",
      " 3373/5000: episode: 3372, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.737 [0.649, 0.819], loss: 1380704.125000, mean_absolute_error: 411.387146, mean_q: -2.901901\n",
      " 3374/5000: episode: 3373, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.877, 0.973], loss: 4.345525, mean_absolute_error: 316.715759, mean_q: -2.947059\n",
      " 3375/5000: episode: 3374, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.780, 0.916], loss: 1387643.750000, mean_absolute_error: 415.405273, mean_q: -2.979968\n",
      " 3376/5000: episode: 3375, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.301, 0.490], loss: 4.574121, mean_absolute_error: 317.061035, mean_q: -3.023606\n",
      " 3377/5000: episode: 3376, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.048], loss: 4.670029, mean_absolute_error: 317.217285, mean_q: -3.055151\n",
      " 3378/5000: episode: 3377, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.732, 0.883], loss: 7166.398926, mean_absolute_error: 321.034729, mean_q: -3.076594\n",
      " 3379/5000: episode: 3378, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.030], loss: 4.775623, mean_absolute_error: 317.457764, mean_q: -3.089509\n",
      " 3380/5000: episode: 3379, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.174, 0.343], loss: 7162.920898, mean_absolute_error: 321.230225, mean_q: -3.096422\n",
      " 3381/5000: episode: 3380, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.167, 0.334], loss: 4.801387, mean_absolute_error: 317.620056, mean_q: -3.097835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3382/5000: episode: 3381, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.374 [0.282, 0.469], loss: 6140.924316, mean_absolute_error: 317.681213, mean_q: -3.095101\n",
      " 3383/5000: episode: 3382, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.250, 0.433], loss: 4.769724, mean_absolute_error: 317.728210, mean_q: -3.087600\n",
      " 3384/5000: episode: 3383, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [0.065, 0.191], loss: 14310.543945, mean_absolute_error: 325.122681, mean_q: -3.078027\n",
      " 3385/5000: episode: 3384, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.135, 0.293], loss: 4.700083, mean_absolute_error: 317.804199, mean_q: -3.064969\n",
      " 3386/5000: episode: 3385, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.526 [0.429, 0.622], loss: 4.652553, mean_absolute_error: 317.836365, mean_q: -3.049427\n",
      " 3387/5000: episode: 3386, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 4.600746, mean_absolute_error: 317.862366, mean_q: -3.032396\n",
      " 3388/5000: episode: 3387, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.670 [0.576, 0.758], loss: 4.548287, mean_absolute_error: 317.881836, mean_q: -3.015053\n",
      " 3389/5000: episode: 3388, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [0.096, 0.238], loss: 14297.841797, mean_absolute_error: 325.240662, mean_q: -2.996805\n",
      " 3390/5000: episode: 3389, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.620 [0.525, 0.713], loss: 1372683.875000, mean_absolute_error: 409.046082, mean_q: -2.986582\n",
      " 3391/5000: episode: 3390, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.341, 0.533], loss: 4.520710, mean_absolute_error: 318.072205, mean_q: -3.005896\n",
      " 3392/5000: episode: 3391, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 1386024.250000, mean_absolute_error: 413.034729, mean_q: -3.018147\n",
      " 3393/5000: episode: 3392, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.927, 0.994], loss: 4.633729, mean_absolute_error: 318.317993, mean_q: -3.043250\n",
      " 3394/5000: episode: 3393, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.665 [0.571, 0.754], loss: 4.682255, mean_absolute_error: 318.431946, mean_q: -3.059149\n",
      " 3395/5000: episode: 3394, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.873 [0.805, 0.932], loss: 4.708701, mean_absolute_error: 318.521332, mean_q: -3.067779\n",
      " 3396/5000: episode: 3395, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.006, 0.074], loss: 7139.690430, mean_absolute_error: 322.253845, mean_q: -3.070211\n",
      " 3397/5000: episode: 3396, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.705 [0.614, 0.791], loss: 1378615.375000, mean_absolute_error: 409.830780, mean_q: -3.067632\n",
      " 3398/5000: episode: 3397, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [0.054, 0.174], loss: 20460.675781, mean_absolute_error: 326.085785, mean_q: -3.082568\n",
      " 3399/5000: episode: 3398, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 4.768446, mean_absolute_error: 318.850342, mean_q: -3.087186\n",
      " 3400/5000: episode: 3399, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.183, 0.354], loss: 7133.097656, mean_absolute_error: 322.578674, mean_q: -3.086095\n",
      " 3401/5000: episode: 3400, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.927, 0.994], loss: 6248.508789, mean_absolute_error: 319.018280, mean_q: -3.080606\n",
      " 3402/5000: episode: 3401, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.621, 0.796], loss: 4.719120, mean_absolute_error: 319.034821, mean_q: -3.071172\n",
      " 3403/5000: episode: 3402, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.173, 0.341], loss: 4.684976, mean_absolute_error: 319.071411, mean_q: -3.060038\n",
      " 3404/5000: episode: 3403, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.692, 0.853], loss: 13515.509766, mean_absolute_error: 322.852051, mean_q: -3.046656\n",
      " 3405/5000: episode: 3404, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.896 [0.833, 0.950], loss: 4.591936, mean_absolute_error: 319.116730, mean_q: -3.029490\n",
      " 3406/5000: episode: 3405, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.346 [0.256, 0.440], loss: 4.539386, mean_absolute_error: 319.130798, mean_q: -3.012100\n",
      " 3407/5000: episode: 3406, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.943, 0.998], loss: 4.485545, mean_absolute_error: 319.142700, mean_q: -2.994178\n",
      " 3408/5000: episode: 3407, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.920, 0.992], loss: 7123.656738, mean_absolute_error: 322.804596, mean_q: -2.975099\n",
      " 3409/5000: episode: 3408, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 4.368455, mean_absolute_error: 319.177094, mean_q: -2.954826\n",
      " 3410/5000: episode: 3409, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.342 [0.252, 0.435], loss: 1371964.000000, mean_absolute_error: 410.244598, mean_q: -2.934978\n",
      " 3411/5000: episode: 3410, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.105], loss: 7119.777832, mean_absolute_error: 322.906494, mean_q: -2.937669\n",
      " 3412/5000: episode: 3411, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.543, 0.729], loss: 1378998.625000, mean_absolute_error: 414.011261, mean_q: -2.934656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3413/5000: episode: 3412, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.304 [0.218, 0.395], loss: 7116.572266, mean_absolute_error: 323.077087, mean_q: -2.949197\n",
      " 3414/5000: episode: 3413, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.874 [0.805, 0.933], loss: 7114.943359, mean_absolute_error: 323.168854, mean_q: -2.956895\n",
      " 3415/5000: episode: 3414, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.937, 0.996], loss: 7113.145508, mean_absolute_error: 323.246857, mean_q: -2.959020\n",
      " 3416/5000: episode: 3415, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.356 [0.266, 0.451], loss: 4.372763, mean_absolute_error: 319.681854, mean_q: -2.956284\n",
      " 3417/5000: episode: 3416, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.158, 0.322], loss: 1378003.500000, mean_absolute_error: 410.843964, mean_q: -2.950229\n",
      " 3418/5000: episode: 3417, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.049 [0.014, 0.096], loss: 4.391789, mean_absolute_error: 319.841248, mean_q: -2.962710\n",
      " 3419/5000: episode: 3418, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.660 [0.567, 0.750], loss: 1385735.000000, mean_absolute_error: 418.202332, mean_q: -2.969432\n",
      " 3420/5000: episode: 3419, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.049 [0.014, 0.095], loss: 14205.765625, mean_absolute_error: 327.311035, mean_q: -2.992045\n",
      " 3421/5000: episode: 3420, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.823 [0.745, 0.892], loss: 1371376.875000, mean_absolute_error: 411.182922, mean_q: -3.005226\n",
      " 3422/5000: episode: 3421, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.475 [0.379, 0.572], loss: 1371279.125000, mean_absolute_error: 411.331604, mean_q: -3.034143\n",
      " 3423/5000: episode: 3422, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.069], loss: 6450.413086, mean_absolute_error: 320.614746, mean_q: -3.074634\n",
      " 3424/5000: episode: 3423, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.069 [0.026, 0.122], loss: 4.811332, mean_absolute_error: 320.648041, mean_q: -3.101042\n",
      " 3425/5000: episode: 3424, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 1370998.875000, mean_absolute_error: 411.740540, mean_q: -3.118896\n",
      " 3426/5000: episode: 3425, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.036, 0.141], loss: 7094.653809, mean_absolute_error: 324.521729, mean_q: -3.151810\n",
      " 3427/5000: episode: 3426, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.923, 0.993], loss: 5.038313, mean_absolute_error: 321.026550, mean_q: -3.173370\n",
      " 3428/5000: episode: 3427, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.679, 0.843], loss: 14178.803711, mean_absolute_error: 328.354401, mean_q: -3.185783\n",
      " 3429/5000: episode: 3428, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.957 [0.913, 0.989], loss: 5.127526, mean_absolute_error: 321.251190, mean_q: -3.201351\n",
      " 3430/5000: episode: 3429, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.895 [0.831, 0.948], loss: 5.186195, mean_absolute_error: 321.380463, mean_q: -3.219620\n",
      " 3431/5000: episode: 3430, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.073], loss: 5.219907, mean_absolute_error: 321.481201, mean_q: -3.230070\n",
      " 3432/5000: episode: 3431, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.100], loss: 5.234007, mean_absolute_error: 321.563904, mean_q: -3.234431\n",
      " 3433/5000: episode: 3432, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.130, 0.286], loss: 5.232415, mean_absolute_error: 321.632690, mean_q: -3.233939\n",
      " 3434/5000: episode: 3433, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.939, 0.997], loss: 5.217531, mean_absolute_error: 321.688477, mean_q: -3.229335\n",
      " 3435/5000: episode: 3434, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.268 [0.185, 0.357], loss: 6308.160156, mean_absolute_error: 321.737152, mean_q: -3.221619\n",
      " 3436/5000: episode: 3435, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.169, 0.337], loss: 5.193320, mean_absolute_error: 321.798218, mean_q: -3.221831\n",
      " 3437/5000: episode: 3436, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.051], loss: 5.215671, mean_absolute_error: 321.877228, mean_q: -3.228759\n",
      " 3438/5000: episode: 3437, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.049], loss: 5.221898, mean_absolute_error: 321.940033, mean_q: -3.230686\n",
      " 3439/5000: episode: 3438, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.148, 0.309], loss: 6462.634277, mean_absolute_error: 322.101288, mean_q: -3.239347\n",
      " 3440/5000: episode: 3439, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.590, 0.770], loss: 5.285399, mean_absolute_error: 322.126404, mean_q: -3.250277\n",
      " 3441/5000: episode: 3440, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.487, 0.678], loss: 5.336581, mean_absolute_error: 322.244995, mean_q: -3.265981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3442/5000: episode: 3441, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.178, 0.347], loss: 6477.970215, mean_absolute_error: 322.444061, mean_q: -3.284386\n",
      " 3443/5000: episode: 3442, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.478 [0.382, 0.575], loss: 5.463148, mean_absolute_error: 322.493011, mean_q: -3.304495\n",
      " 3444/5000: episode: 3443, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.397 [0.304, 0.493], loss: 1369810.625000, mean_absolute_error: 413.522919, mean_q: -3.327156\n",
      " 3445/5000: episode: 3444, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.286, 0.474], loss: 1369699.750000, mean_absolute_error: 413.681152, mean_q: -3.363320\n",
      " 3446/5000: episode: 3445, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.367, 0.560], loss: 7065.282715, mean_absolute_error: 326.571381, mean_q: -3.409607\n",
      " 3447/5000: episode: 3446, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.012, 0.089], loss: 5.926692, mean_absolute_error: 323.142883, mean_q: -3.441875\n",
      " 3448/5000: episode: 3447, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.586 [0.489, 0.680], loss: 6.003131, mean_absolute_error: 323.268555, mean_q: -3.464005\n",
      " 3449/5000: episode: 3448, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [0.010, 0.084], loss: 6.049088, mean_absolute_error: 323.374756, mean_q: -3.477243\n",
      " 3450/5000: episode: 3449, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.028], loss: 7059.299805, mean_absolute_error: 327.075226, mean_q: -3.494899\n",
      " 3451/5000: episode: 3450, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.413, 0.607], loss: 6.179561, mean_absolute_error: 323.624542, mean_q: -3.514554\n",
      " 3452/5000: episode: 3451, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.828 [0.751, 0.897], loss: 7055.938965, mean_absolute_error: 327.306396, mean_q: -3.525414\n",
      " 3453/5000: episode: 3452, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.561 [0.464, 0.656], loss: 6.229495, mean_absolute_error: 323.814819, mean_q: -3.528729\n",
      " 3454/5000: episode: 3453, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.608 [0.513, 0.702], loss: 1383086.125000, mean_absolute_error: 421.870483, mean_q: -3.527366\n",
      " 3455/5000: episode: 3454, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.995], loss: 6.320483, mean_absolute_error: 324.035828, mean_q: -3.554414\n",
      " 3456/5000: episode: 3455, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.856 [0.785, 0.919], loss: 6.421288, mean_absolute_error: 324.186707, mean_q: -3.582654\n",
      " 3457/5000: episode: 3456, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.593, 0.773], loss: 6.488815, mean_absolute_error: 324.307312, mean_q: -3.601448\n",
      " 3458/5000: episode: 3457, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.046 [0.012, 0.091], loss: 7046.918945, mean_absolute_error: 327.970093, mean_q: -3.611762\n",
      " 3459/5000: episode: 3458, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.434 [0.339, 0.531], loss: 6427.420898, mean_absolute_error: 324.478455, mean_q: -3.614572\n",
      " 3460/5000: episode: 3459, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.038], loss: 21120.767578, mean_absolute_error: 335.233582, mean_q: -3.610090\n",
      " 3461/5000: episode: 3460, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.151], loss: 6.522735, mean_absolute_error: 324.625732, mean_q: -3.610851\n",
      " 3462/5000: episode: 3461, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.398, 0.592], loss: 6.546255, mean_absolute_error: 324.727356, mean_q: -3.617357\n",
      " 3463/5000: episode: 3462, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.916 [0.858, 0.964], loss: 1396533.875000, mean_absolute_error: 429.851318, mean_q: -3.617798\n",
      " 3464/5000: episode: 3463, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.789 [0.706, 0.864], loss: 6593.621582, mean_absolute_error: 325.035339, mean_q: -3.632871\n",
      " 3465/5000: episode: 3464, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.694 [0.602, 0.780], loss: 7033.626465, mean_absolute_error: 328.629578, mean_q: -3.639039\n",
      " 3466/5000: episode: 3465, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.172, 0.340], loss: 20507.785156, mean_absolute_error: 332.280060, mean_q: -3.638856\n",
      " 3467/5000: episode: 3466, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.957 [0.914, 0.989], loss: 7028.702637, mean_absolute_error: 328.805237, mean_q: -3.631645\n",
      " 3468/5000: episode: 3467, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.437, 0.630], loss: 1368110.375000, mean_absolute_error: 416.132019, mean_q: -3.631745\n",
      " 3469/5000: episode: 3468, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 6.695299, mean_absolute_error: 325.518982, mean_q: -3.658317\n",
      " 3470/5000: episode: 3469, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.181 [0.111, 0.260], loss: 14036.157227, mean_absolute_error: 332.741333, mean_q: -3.673882\n",
      " 3471/5000: episode: 3470, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 7018.430664, mean_absolute_error: 329.322937, mean_q: -3.679521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3472/5000: episode: 3471, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.236 [0.157, 0.321], loss: 14024.121094, mean_absolute_error: 332.965515, mean_q: -3.678464\n",
      " 3473/5000: episode: 3472, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.675, 0.840], loss: 14017.917969, mean_absolute_error: 333.058228, mean_q: -3.671619\n",
      " 3474/5000: episode: 3473, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.919 [0.862, 0.966], loss: 6.702015, mean_absolute_error: 326.079041, mean_q: -3.660151\n",
      " 3475/5000: episode: 3474, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.974, 1.000], loss: 6652.989746, mean_absolute_error: 326.236145, mean_q: -3.646278\n",
      " 3476/5000: episode: 3475, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.934, 0.996], loss: 6.588511, mean_absolute_error: 326.207489, mean_q: -3.629017\n",
      " 3477/5000: episode: 3476, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.926, 0.994], loss: 13635.007812, mean_absolute_error: 329.850159, mean_q: -3.610784\n",
      " 3478/5000: episode: 3477, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.204, 0.379], loss: 1374637.500000, mean_absolute_error: 420.549500, mean_q: -3.589677\n",
      " 3479/5000: episode: 3478, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.470 [0.374, 0.567], loss: 6998.226562, mean_absolute_error: 329.896973, mean_q: -3.589555\n",
      " 3480/5000: episode: 3479, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.018], loss: 1367550.375000, mean_absolute_error: 417.196655, mean_q: -3.583833\n",
      " 3481/5000: episode: 3480, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.672 [0.579, 0.761], loss: 6.513933, mean_absolute_error: 326.618408, mean_q: -3.608414\n",
      " 3482/5000: episode: 3481, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.390 [0.297, 0.485], loss: 13976.271484, mean_absolute_error: 333.798859, mean_q: -3.634627\n",
      " 3483/5000: episode: 3482, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.833 [0.757, 0.901], loss: 6989.246094, mean_absolute_error: 330.417572, mean_q: -3.650296\n",
      " 3484/5000: episode: 3483, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.334 [0.245, 0.427], loss: 6987.184082, mean_absolute_error: 330.525848, mean_q: -3.658094\n",
      " 3485/5000: episode: 3484, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [0.068, 0.196], loss: 13963.479492, mean_absolute_error: 334.124634, mean_q: -3.659162\n",
      " 3486/5000: episode: 3485, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.113, 0.262], loss: 6.722316, mean_absolute_error: 327.229248, mean_q: -3.665692\n",
      " 3487/5000: episode: 3486, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.869, 0.970], loss: 6.764557, mean_absolute_error: 327.352020, mean_q: -3.677195\n",
      " 3488/5000: episode: 3487, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.062], loss: 6978.701172, mean_absolute_error: 330.944763, mean_q: -3.680727\n",
      " 3489/5000: episode: 3488, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.484, 0.675], loss: 6976.877441, mean_absolute_error: 331.019745, mean_q: -3.676915\n",
      " 3490/5000: episode: 3489, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 6545.935059, mean_absolute_error: 327.586243, mean_q: -3.667781\n",
      " 3491/5000: episode: 3490, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.093 [0.043, 0.154], loss: 13940.276367, mean_absolute_error: 334.619080, mean_q: -3.653955\n",
      " 3492/5000: episode: 3491, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.014, 0.094], loss: 6.615419, mean_absolute_error: 327.672607, mean_q: -3.636422\n",
      " 3493/5000: episode: 3492, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.164, 0.329], loss: 13933.945312, mean_absolute_error: 334.685516, mean_q: -3.617206\n",
      " 3494/5000: episode: 3493, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.214, 0.391], loss: 1366819.125000, mean_absolute_error: 418.421448, mean_q: -3.595798\n",
      " 3495/5000: episode: 3494, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.940 [0.889, 0.979], loss: 6.509972, mean_absolute_error: 327.858521, mean_q: -3.607316\n",
      " 3496/5000: episode: 3495, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.750, 0.896], loss: 6.565406, mean_absolute_error: 327.986694, mean_q: -3.622646\n",
      " 3497/5000: episode: 3496, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.276 [0.192, 0.365], loss: 27830.056641, mean_absolute_error: 342.026306, mean_q: -3.630532\n",
      " 3498/5000: episode: 3497, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.132], loss: 6.590855, mean_absolute_error: 328.193726, mean_q: -3.629663\n",
      " 3499/5000: episode: 3498, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.301, 0.490], loss: 6.568731, mean_absolute_error: 328.274323, mean_q: -3.623564\n",
      " 3500/5000: episode: 3499, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.900, 0.984], loss: 6.531814, mean_absolute_error: 328.340149, mean_q: -3.613364\n",
      " 3501/5000: episode: 3500, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 6.486898, mean_absolute_error: 328.390625, mean_q: -3.600916\n",
      " 3502/5000: episode: 3501, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.954, 1.000], loss: 6.435725, mean_absolute_error: 328.430481, mean_q: -3.586680\n",
      " 3503/5000: episode: 3502, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.067 [0.025, 0.119], loss: 6.376714, mean_absolute_error: 328.466492, mean_q: -3.570194\n",
      " 3504/5000: episode: 3503, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.044], loss: 13726.287109, mean_absolute_error: 332.073486, mean_q: -3.550916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3505/5000: episode: 3504, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.929 [0.875, 0.972], loss: 13556.777344, mean_absolute_error: 332.007751, mean_q: -3.527077\n",
      " 3506/5000: episode: 3505, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 6.128637, mean_absolute_error: 328.531372, mean_q: -3.500039\n",
      " 3507/5000: episode: 3506, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [0.075, 0.207], loss: 13884.282227, mean_absolute_error: 335.470825, mean_q: -3.473248\n",
      " 3508/5000: episode: 3507, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.953 [0.907, 0.987], loss: 6943.565918, mean_absolute_error: 332.016693, mean_q: -3.446157\n",
      " 3509/5000: episode: 3508, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.097 [0.046, 0.159], loss: 5.848460, mean_absolute_error: 328.565155, mean_q: -3.419076\n",
      " 3510/5000: episode: 3509, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.690 [0.598, 0.777], loss: 5.756477, mean_absolute_error: 328.584167, mean_q: -3.392075\n",
      " 3511/5000: episode: 3510, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [0.060, 0.183], loss: 20637.199219, mean_absolute_error: 335.623230, mean_q: -3.365191\n",
      " 3512/5000: episode: 3511, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 13406.179688, mean_absolute_error: 328.757782, mean_q: -3.335923\n",
      " 3513/5000: episode: 3512, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.031], loss: 6933.849609, mean_absolute_error: 332.092834, mean_q: -3.304880\n",
      " 3514/5000: episode: 3513, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.960], loss: 6932.028320, mean_absolute_error: 332.103973, mean_q: -3.275172\n",
      " 3515/5000: episode: 3514, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [0.086, 0.223], loss: 6776.585938, mean_absolute_error: 328.764557, mean_q: -3.246449\n",
      " 3516/5000: episode: 3515, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.899, 0.983], loss: 13852.119141, mean_absolute_error: 335.568329, mean_q: -3.217859\n",
      " 3517/5000: episode: 3516, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.306], loss: 6926.767578, mean_absolute_error: 332.159607, mean_q: -3.201356\n",
      " 3518/5000: episode: 3517, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.704, 0.862], loss: 5.104541, mean_absolute_error: 328.776733, mean_q: -3.194165\n",
      " 3519/5000: episode: 3518, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.062], loss: 5.072738, mean_absolute_error: 328.832642, mean_q: -3.184196\n",
      " 3520/5000: episode: 3519, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.082 [0.035, 0.139], loss: 6921.603516, mean_absolute_error: 332.322296, mean_q: -3.172068\n",
      " 3521/5000: episode: 3520, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.161, 0.325], loss: 4.988875, mean_absolute_error: 328.917786, mean_q: -3.157758\n",
      " 3522/5000: episode: 3521, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.255 [0.174, 0.342], loss: 4.940604, mean_absolute_error: 328.949707, mean_q: -3.142439\n",
      " 3523/5000: episode: 3522, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [0.080, 0.215], loss: 13830.718750, mean_absolute_error: 335.852600, mean_q: -3.125447\n",
      " 3524/5000: episode: 3523, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.204, 0.379], loss: 6916.430664, mean_absolute_error: 332.433472, mean_q: -3.105973\n",
      " 3525/5000: episode: 3524, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.105 [0.051, 0.168], loss: 6914.750488, mean_absolute_error: 332.458618, mean_q: -3.085040\n",
      " 3526/5000: episode: 3525, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [0.055, 0.174], loss: 1366265.500000, mean_absolute_error: 419.699707, mean_q: -3.062269\n",
      " 3527/5000: episode: 3526, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.904, 0.986], loss: 4.691660, mean_absolute_error: 329.145691, mean_q: -3.062221\n",
      " 3528/5000: episode: 3527, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.640 [0.546, 0.731], loss: 6908.415039, mean_absolute_error: 332.650269, mean_q: -3.058450\n",
      " 3529/5000: episode: 3528, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.854 [0.782, 0.918], loss: 1366124.375000, mean_absolute_error: 419.950500, mean_q: -3.062551\n",
      " 3530/5000: episode: 3529, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.598 [0.501, 0.691], loss: 1366017.125000, mean_absolute_error: 420.114380, mean_q: -3.094645\n",
      " 3531/5000: episode: 3530, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.215, 0.393], loss: 6901.751953, mean_absolute_error: 333.111938, mean_q: -3.139043\n",
      " 3532/5000: episode: 3531, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.808 [0.728, 0.880], loss: 6899.638184, mean_absolute_error: 333.273438, mean_q: -3.169844\n",
      " 3533/5000: episode: 3532, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [0.069, 0.198], loss: 1372595.000000, mean_absolute_error: 424.010071, mean_q: -3.189674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3534/5000: episode: 3533, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.942, 0.998], loss: 5.199125, mean_absolute_error: 330.157227, mean_q: -3.223632\n",
      " 3535/5000: episode: 3534, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.148], loss: 13781.979492, mean_absolute_error: 337.130707, mean_q: -3.246984\n",
      " 3536/5000: episode: 3535, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.285, 0.473], loss: 5.315755, mean_absolute_error: 330.418579, mean_q: -3.259600\n",
      " 3537/5000: episode: 3536, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.148], loss: 6889.644531, mean_absolute_error: 333.926361, mean_q: -3.265295\n",
      " 3538/5000: episode: 3537, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.849 [0.776, 0.914], loss: 1372213.625000, mean_absolute_error: 424.583496, mean_q: -3.265352\n",
      " 3539/5000: episode: 3538, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.682, 0.845], loss: 5.393233, mean_absolute_error: 330.723389, mean_q: -3.283276\n",
      " 3540/5000: episode: 3539, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 13690.348633, mean_absolute_error: 334.308777, mean_q: -3.293910\n",
      " 3541/5000: episode: 3540, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.750, 0.896], loss: 5.436512, mean_absolute_error: 330.908600, mean_q: -3.296427\n",
      " 3542/5000: episode: 3541, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.881 [0.814, 0.938], loss: 6688.979492, mean_absolute_error: 331.015869, mean_q: -3.306420\n",
      " 3543/5000: episode: 3542, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.939, 0.997], loss: 6880.062012, mean_absolute_error: 334.512970, mean_q: -3.319329\n",
      " 3544/5000: episode: 3543, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.280, 0.467], loss: 5.524637, mean_absolute_error: 331.205750, mean_q: -3.323045\n",
      " 3545/5000: episode: 3544, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.444, 0.637], loss: 6847.242676, mean_absolute_error: 331.369019, mean_q: -3.319172\n",
      " 3546/5000: episode: 3545, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 5.516635, mean_absolute_error: 331.373901, mean_q: -3.320637\n",
      " 3547/5000: episode: 3546, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.540, 0.727], loss: 6872.386719, mean_absolute_error: 334.865814, mean_q: -3.327765\n",
      " 3548/5000: episode: 3547, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.335 [0.246, 0.429], loss: 6870.729980, mean_absolute_error: 334.945801, mean_q: -3.328981\n",
      " 3549/5000: episode: 3548, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 6869.259766, mean_absolute_error: 335.014130, mean_q: -3.325336\n",
      " 3550/5000: episode: 3549, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.445, 0.638], loss: 5.506854, mean_absolute_error: 331.682129, mean_q: -3.317691\n",
      " 3551/5000: episode: 3550, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.286, 0.473], loss: 1364657.625000, mean_absolute_error: 422.264130, mean_q: -3.306899\n",
      " 3552/5000: episode: 3551, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.085 [0.037, 0.143], loss: 5.500267, mean_absolute_error: 331.841431, mean_q: -3.315705\n",
      " 3553/5000: episode: 3552, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.434, 0.627], loss: 5.507107, mean_absolute_error: 331.927429, mean_q: -3.317767\n",
      " 3554/5000: episode: 3553, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.857, 0.963], loss: 5.497446, mean_absolute_error: 331.993744, mean_q: -3.314855\n",
      " 3555/5000: episode: 3554, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.542 [0.445, 0.637], loss: 6859.694824, mean_absolute_error: 335.425659, mean_q: -3.307673\n",
      " 3556/5000: episode: 3555, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.222 [0.145, 0.305], loss: 5.437017, mean_absolute_error: 332.084991, mean_q: -3.296580\n",
      " 3557/5000: episode: 3556, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 5.389212, mean_absolute_error: 332.112976, mean_q: -3.282051\n",
      " 3558/5000: episode: 3557, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.867 [0.798, 0.928], loss: 5.333083, mean_absolute_error: 332.130432, mean_q: -3.264910\n",
      " 3559/5000: episode: 3558, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.963, 1.000], loss: 5.275042, mean_absolute_error: 332.142761, mean_q: -3.247089\n",
      " 3560/5000: episode: 3559, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 6855.527344, mean_absolute_error: 335.528870, mean_q: -3.228868\n",
      " 3561/5000: episode: 3560, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.959], loss: 6854.656738, mean_absolute_error: 335.541077, mean_q: -3.209208\n",
      " 3562/5000: episode: 3561, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.710 [0.620, 0.795], loss: 5.085824, mean_absolute_error: 332.179169, mean_q: -3.188302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3563/5000: episode: 3562, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.385, 0.579], loss: 6852.348145, mean_absolute_error: 335.600006, mean_q: -3.179220\n",
      " 3564/5000: episode: 3563, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.636 [0.542, 0.728], loss: 5.052049, mean_absolute_error: 332.295227, mean_q: -3.177694\n",
      " 3565/5000: episode: 3564, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.497, 0.687], loss: 5.074257, mean_absolute_error: 332.386108, mean_q: -3.184673\n",
      " 3566/5000: episode: 3565, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.184, 0.354], loss: 5.112981, mean_absolute_error: 332.488159, mean_q: -3.196806\n",
      " 3567/5000: episode: 3566, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [0.098, 0.241], loss: 5.130275, mean_absolute_error: 332.575134, mean_q: -3.202209\n",
      " 3568/5000: episode: 3567, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.031], loss: 6737.443359, mean_absolute_error: 332.645416, mean_q: -3.202410\n",
      " 3569/5000: episode: 3568, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.971], loss: 5.114402, mean_absolute_error: 332.697449, mean_q: -3.197250\n",
      " 3570/5000: episode: 3569, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.600, 0.779], loss: 13746.068359, mean_absolute_error: 336.185486, mean_q: -3.188396\n",
      " 3571/5000: episode: 3570, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.332 [0.243, 0.426], loss: 6842.454102, mean_absolute_error: 336.129761, mean_q: -3.174094\n",
      " 3572/5000: episode: 3571, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.621 [0.525, 0.713], loss: 6841.022461, mean_absolute_error: 336.160309, mean_q: -3.157285\n",
      " 3573/5000: episode: 3572, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.437 [0.342, 0.534], loss: 13673.672852, mean_absolute_error: 339.583618, mean_q: -3.150802\n",
      " 3574/5000: episode: 3573, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.353, 0.546], loss: 6837.265137, mean_absolute_error: 336.308594, mean_q: -3.151215\n",
      " 3575/5000: episode: 3574, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.413, 0.606], loss: 4.954049, mean_absolute_error: 333.026245, mean_q: -3.146713\n",
      " 3576/5000: episode: 3575, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.896 [0.833, 0.949], loss: 20489.857422, mean_absolute_error: 343.159302, mean_q: -3.138613\n",
      " 3577/5000: episode: 3576, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.907 [0.846, 0.957], loss: 4.888738, mean_absolute_error: 333.158813, mean_q: -3.125896\n",
      " 3578/5000: episode: 3577, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 6816.679688, mean_absolute_error: 333.249756, mean_q: -3.110729\n",
      " 3579/5000: episode: 3578, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.071], loss: 4.783746, mean_absolute_error: 333.269226, mean_q: -3.092136\n",
      " 3580/5000: episode: 3579, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.607 [0.511, 0.700], loss: 4.726016, mean_absolute_error: 333.307922, mean_q: -3.073416\n",
      " 3581/5000: episode: 3580, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.972, 1.000], loss: 4.668138, mean_absolute_error: 333.337402, mean_q: -3.054532\n",
      " 3582/5000: episode: 3581, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.354, 0.546], loss: 6820.691406, mean_absolute_error: 336.706940, mean_q: -3.035157\n",
      " 3583/5000: episode: 3582, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [0.007, 0.076], loss: 6818.958008, mean_absolute_error: 336.764008, mean_q: -3.026729\n",
      " 3584/5000: episode: 3583, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.970], loss: 4.582811, mean_absolute_error: 333.502350, mean_q: -3.026478\n",
      " 3585/5000: episode: 3584, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [0.106, 0.253], loss: 6815.243164, mean_absolute_error: 336.911499, mean_q: -3.021543\n",
      " 3586/5000: episode: 3585, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [0.009, 0.082], loss: 2727263.750000, mean_absolute_error: 514.555847, mean_q: -3.012152\n",
      " 3587/5000: episode: 3586, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.640, 0.812], loss: 4.641650, mean_absolute_error: 333.819397, mean_q: -3.045851\n",
      " 3588/5000: episode: 3587, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.329, 0.520], loss: 4.711710, mean_absolute_error: 333.966705, mean_q: -3.068759\n",
      " 3589/5000: episode: 3588, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.400, 0.594], loss: 1363340.250000, mean_absolute_error: 424.554535, mean_q: -3.094448\n",
      " 3590/5000: episode: 3589, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.005, 0.070], loss: 4.945210, mean_absolute_error: 334.327637, mean_q: -3.143904\n",
      " 3591/5000: episode: 3590, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 5.057589, mean_absolute_error: 334.494324, mean_q: -3.179437\n",
      " 3592/5000: episode: 3591, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.897, 0.983], loss: 5.135134, mean_absolute_error: 334.625610, mean_q: -3.203726\n",
      " 3593/5000: episode: 3592, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.331 [0.242, 0.424], loss: 6971.472656, mean_absolute_error: 334.805756, mean_q: -3.219577\n",
      " 3594/5000: episode: 3593, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.039], loss: 5.207724, mean_absolute_error: 334.807800, mean_q: -3.226297\n",
      " 3595/5000: episode: 3594, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.281, 0.468], loss: 6798.103027, mean_absolute_error: 338.218445, mean_q: -3.238894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3596/5000: episode: 3595, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.852 [0.779, 0.916], loss: 5.295363, mean_absolute_error: 335.003448, mean_q: -3.253340\n",
      " 3597/5000: episode: 3596, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.964 [0.924, 0.993], loss: 5.314956, mean_absolute_error: 335.084839, mean_q: -3.259355\n",
      " 3598/5000: episode: 3597, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.078 [0.032, 0.134], loss: 6794.803711, mean_absolute_error: 338.464905, mean_q: -3.260331\n",
      " 3599/5000: episode: 3598, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.015], loss: 5.302100, mean_absolute_error: 335.203857, mean_q: -3.255409\n",
      " 3600/5000: episode: 3599, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.270 [0.187, 0.358], loss: 6792.617676, mean_absolute_error: 338.561462, mean_q: -3.246522\n",
      " 3601/5000: episode: 3600, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.209 [0.134, 0.291], loss: 13577.290039, mean_absolute_error: 341.913086, mean_q: -3.234296\n",
      " 3602/5000: episode: 3601, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.687 [0.595, 0.774], loss: 5.183908, mean_absolute_error: 335.330109, mean_q: -3.218909\n",
      " 3603/5000: episode: 3602, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.650, 0.820], loss: 1369325.375000, mean_absolute_error: 429.052765, mean_q: -3.202605\n",
      " 3604/5000: episode: 3603, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.759 [0.674, 0.839], loss: 7056.664551, mean_absolute_error: 335.558105, mean_q: -3.207520\n",
      " 3605/5000: episode: 3604, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.049], loss: 6785.066406, mean_absolute_error: 338.831207, mean_q: -3.206044\n",
      " 3606/5000: episode: 3605, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.499, 0.689], loss: 5.124210, mean_absolute_error: 335.583130, mean_q: -3.200315\n",
      " 3607/5000: episode: 3606, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.707, 0.865], loss: 6782.257812, mean_absolute_error: 338.937653, mean_q: -3.191160\n",
      " 3608/5000: episode: 3607, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.949, 0.999], loss: 6780.570312, mean_absolute_error: 338.984924, mean_q: -3.178618\n",
      " 3609/5000: episode: 3608, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.918 [0.860, 0.965], loss: 6778.932617, mean_absolute_error: 339.022522, mean_q: -3.163107\n",
      " 3610/5000: episode: 3609, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.648 [0.554, 0.739], loss: 1362332.500000, mean_absolute_error: 426.120178, mean_q: -3.145300\n",
      " 3611/5000: episode: 3610, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.343, 0.535], loss: 13546.542969, mean_absolute_error: 342.439697, mean_q: -3.149811\n",
      " 3612/5000: episode: 3611, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.896, 0.982], loss: 4.959058, mean_absolute_error: 335.923126, mean_q: -3.148304\n",
      " 3613/5000: episode: 3612, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.062], loss: 7011.201172, mean_absolute_error: 336.055908, mean_q: -3.143391\n",
      " 3614/5000: episode: 3613, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 6770.947266, mean_absolute_error: 339.327881, mean_q: -3.133277\n",
      " 3615/5000: episode: 3614, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.904 [0.842, 0.955], loss: 6769.216309, mean_absolute_error: 339.372192, mean_q: -3.119143\n",
      " 3616/5000: episode: 3615, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.578 [0.481, 0.673], loss: 6767.540039, mean_absolute_error: 339.407898, mean_q: -3.102532\n",
      " 3617/5000: episode: 3616, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.901, 0.985], loss: 1369156.625000, mean_absolute_error: 426.662781, mean_q: -3.108362\n",
      " 3618/5000: episode: 3617, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.006, 0.071], loss: 6763.575684, mean_absolute_error: 339.697388, mean_q: -3.151714\n",
      " 3619/5000: episode: 3618, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.433 [0.338, 0.529], loss: 5.067041, mean_absolute_error: 336.572174, mean_q: -3.182407\n",
      " 3620/5000: episode: 3619, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.982, 1.000], loss: 7054.659668, mean_absolute_error: 336.779816, mean_q: -3.203146\n",
      " 3621/5000: episode: 3620, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.363 [0.272, 0.458], loss: 6758.396973, mean_absolute_error: 340.083923, mean_q: -3.213413\n",
      " 3622/5000: episode: 3621, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.405, 0.599], loss: 6756.762695, mean_absolute_error: 340.197083, mean_q: -3.228384\n",
      " 3623/5000: episode: 3622, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [0.061, 0.184], loss: 5.272394, mean_absolute_error: 337.046509, mean_q: -3.246274\n",
      " 3624/5000: episode: 3623, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.222, 0.401], loss: 6753.106934, mean_absolute_error: 340.427643, mean_q: -3.256224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3625/5000: episode: 3624, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.553 [0.456, 0.648], loss: 6751.252441, mean_absolute_error: 340.516510, mean_q: -3.259415\n",
      " 3626/5000: episode: 3625, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.164, 0.329], loss: 5.308817, mean_absolute_error: 337.321930, mean_q: -3.257471\n",
      " 3627/5000: episode: 3626, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.305, 0.495], loss: 1361360.750000, mean_absolute_error: 427.680908, mean_q: -3.251622\n",
      " 3628/5000: episode: 3627, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 6746.125977, mean_absolute_error: 340.768890, mean_q: -3.264870\n",
      " 3629/5000: episode: 3628, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 1361230.750000, mean_absolute_error: 427.892090, mean_q: -3.269670\n",
      " 3630/5000: episode: 3629, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.632, 0.805], loss: 6741.191895, mean_absolute_error: 341.023560, mean_q: -3.291227\n",
      " 3631/5000: episode: 3630, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 5.461814, mean_absolute_error: 337.888123, mean_q: -3.304091\n",
      " 3632/5000: episode: 3631, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.272 [0.189, 0.361], loss: 6736.969238, mean_absolute_error: 341.247498, mean_q: -3.310584\n",
      " 3633/5000: episode: 3632, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.402, 0.596], loss: 2721911.000000, mean_absolute_error: 518.607666, mean_q: -3.311189\n",
      " 3634/5000: episode: 3633, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 5.619617, mean_absolute_error: 338.262878, mean_q: -3.351497\n",
      " 3635/5000: episode: 3634, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.870 [0.801, 0.930], loss: 5.753326, mean_absolute_error: 338.443207, mean_q: -3.391146\n",
      " 3636/5000: episode: 3635, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.467, 0.659], loss: 6729.686523, mean_absolute_error: 341.869812, mean_q: -3.429379\n",
      " 3637/5000: episode: 3636, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 13926.399414, mean_absolute_error: 342.122314, mean_q: -3.455033\n",
      " 3638/5000: episode: 3637, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.073 [0.029, 0.128], loss: 13445.647461, mean_absolute_error: 345.384094, mean_q: -3.469250\n",
      " 3639/5000: episode: 3638, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 1367065.500000, mean_absolute_error: 432.497070, mean_q: -3.485997\n",
      " 3640/5000: episode: 3639, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [0.066, 0.193], loss: 13435.369141, mean_absolute_error: 345.716461, mean_q: -3.526271\n",
      " 3641/5000: episode: 3640, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [0.009, 0.083], loss: 6718.091309, mean_absolute_error: 342.646271, mean_q: -3.553591\n",
      " 3642/5000: episode: 3641, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.329, 0.520], loss: 21021.460938, mean_absolute_error: 342.917542, mean_q: -3.570881\n",
      " 3643/5000: episode: 3642, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.062 [0.022, 0.114], loss: 6713.104492, mean_absolute_error: 342.899902, mean_q: -3.576256\n",
      " 3644/5000: episode: 3643, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 6710.547363, mean_absolute_error: 342.996979, mean_q: -3.575124\n",
      " 3645/5000: episode: 3644, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.831 [0.754, 0.899], loss: 6.370149, mean_absolute_error: 339.857666, mean_q: -3.568356\n",
      " 3646/5000: episode: 3645, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.622, 0.797], loss: 14293.614258, mean_absolute_error: 340.042999, mean_q: -3.557679\n",
      " 3647/5000: episode: 3646, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.057], loss: 6.275411, mean_absolute_error: 339.988037, mean_q: -3.541714\n",
      " 3648/5000: episode: 3647, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.505 [0.408, 0.602], loss: 6701.326172, mean_absolute_error: 343.251373, mean_q: -3.524602\n",
      " 3649/5000: episode: 3648, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.037], loss: 6.188106, mean_absolute_error: 340.104248, mean_q: -3.516984\n",
      " 3650/5000: episode: 3649, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.734, 0.884], loss: 6696.896484, mean_absolute_error: 343.415741, mean_q: -3.515882\n",
      " 3651/5000: episode: 3650, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.892, 0.980], loss: 6694.471191, mean_absolute_error: 343.498657, mean_q: -3.509562\n",
      " 3652/5000: episode: 3651, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.736 [0.648, 0.818], loss: 6.127038, mean_absolute_error: 340.355652, mean_q: -3.499582\n",
      " 3653/5000: episode: 3652, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.074 [0.030, 0.129], loss: 6.083087, mean_absolute_error: 340.414642, mean_q: -3.487004\n",
      " 3654/5000: episode: 3653, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 6.031020, mean_absolute_error: 340.464478, mean_q: -3.472045\n",
      " 3655/5000: episode: 3654, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.821, 0.942], loss: 7272.743164, mean_absolute_error: 340.611450, mean_q: -3.455472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3656/5000: episode: 3655, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.869, 0.969], loss: 6685.380859, mean_absolute_error: 343.736755, mean_q: -3.435515\n",
      " 3657/5000: episode: 3656, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.502 [0.405, 0.598], loss: 5.829417, mean_absolute_error: 340.556274, mean_q: -3.413504\n",
      " 3658/5000: episode: 3657, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.458, 0.651], loss: 5.793925, mean_absolute_error: 340.608582, mean_q: -3.403093\n",
      " 3659/5000: episode: 3658, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.297, 0.485], loss: 5.826522, mean_absolute_error: 340.713562, mean_q: -3.412656\n",
      " 3660/5000: episode: 3659, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 5.870144, mean_absolute_error: 340.827820, mean_q: -3.425410\n",
      " 3661/5000: episode: 3660, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.272, 0.458], loss: 7187.480469, mean_absolute_error: 340.973083, mean_q: -3.430677\n",
      " 3662/5000: episode: 3661, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.640, 0.812], loss: 13814.418945, mean_absolute_error: 344.245697, mean_q: -3.441069\n",
      " 3663/5000: episode: 3662, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.481, 0.672], loss: 13343.533203, mean_absolute_error: 347.516998, mean_q: -3.453753\n",
      " 3664/5000: episode: 3663, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 5.984439, mean_absolute_error: 341.225037, mean_q: -3.458607\n",
      " 3665/5000: episode: 3664, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 5.981084, mean_absolute_error: 341.298828, mean_q: -3.457637\n",
      " 3666/5000: episode: 3665, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.532, 0.719], loss: 7112.881836, mean_absolute_error: 341.365295, mean_q: -3.451674\n",
      " 3667/5000: episode: 3666, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.980, 1.000], loss: 5.920203, mean_absolute_error: 341.398682, mean_q: -3.439989\n",
      " 3668/5000: episode: 3667, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.913 [0.853, 0.961], loss: 13904.695312, mean_absolute_error: 344.686371, mean_q: -3.425659\n",
      " 3669/5000: episode: 3668, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.790, 0.923], loss: 5.808986, mean_absolute_error: 341.453918, mean_q: -3.407515\n",
      " 3670/5000: episode: 3669, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.053 [0.016, 0.100], loss: 13916.367188, mean_absolute_error: 344.737976, mean_q: -3.387973\n",
      " 3671/5000: episode: 3670, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.308 [0.221, 0.400], loss: 5.665567, mean_absolute_error: 341.498810, mean_q: -3.365175\n",
      " 3672/5000: episode: 3671, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 5.587882, mean_absolute_error: 341.518951, mean_q: -3.342017\n",
      " 3673/5000: episode: 3672, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.115, 0.265], loss: 5.511055, mean_absolute_error: 341.536591, mean_q: -3.318956\n",
      " 3674/5000: episode: 3673, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.898, 0.983], loss: 5.433764, mean_absolute_error: 341.553802, mean_q: -3.295593\n",
      " 3675/5000: episode: 3674, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.652 [0.558, 0.742], loss: 1365608.500000, mean_absolute_error: 434.906250, mean_q: -3.283566\n",
      " 3676/5000: episode: 3675, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.910 [0.850, 0.959], loss: 7150.195312, mean_absolute_error: 341.794373, mean_q: -3.313860\n",
      " 3677/5000: episode: 3676, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.085 [0.037, 0.143], loss: 1392660.250000, mean_absolute_error: 444.842438, mean_q: -3.343147\n",
      " 3678/5000: episode: 3677, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 6650.938477, mean_absolute_error: 345.317505, mean_q: -3.380617\n",
      " 3679/5000: episode: 3678, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.349 [0.259, 0.443], loss: 6648.154785, mean_absolute_error: 345.485901, mean_q: -3.406380\n",
      " 3680/5000: episode: 3679, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.385 [0.292, 0.481], loss: 5.860146, mean_absolute_error: 342.460419, mean_q: -3.422491\n",
      " 3681/5000: episode: 3680, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.374 [0.282, 0.469], loss: 5.890836, mean_absolute_error: 342.574097, mean_q: -3.431444\n",
      " 3682/5000: episode: 3681, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 6641.724121, mean_absolute_error: 345.826721, mean_q: -3.434319\n",
      " 3683/5000: episode: 3682, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.982, 1.000], loss: 1364909.000000, mean_absolute_error: 435.977661, mean_q: -3.430687\n",
      " 3684/5000: episode: 3683, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.078 [0.032, 0.135], loss: 6637.819336, mean_absolute_error: 346.024170, mean_q: -3.443881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3685/5000: episode: 3684, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 5.949014, mean_absolute_error: 342.973145, mean_q: -3.448352\n",
      " 3686/5000: episode: 3685, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.061 [0.021, 0.112], loss: 1358071.000000, mean_absolute_error: 433.153259, mean_q: -3.459029\n",
      " 3687/5000: episode: 3686, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.449, 0.642], loss: 6.116169, mean_absolute_error: 343.278992, mean_q: -3.496476\n",
      " 3688/5000: episode: 3687, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.616, 0.792], loss: 6.205127, mean_absolute_error: 343.426605, mean_q: -3.521819\n",
      " 3689/5000: episode: 3688, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.598 [0.502, 0.692], loss: 6.302052, mean_absolute_error: 343.575470, mean_q: -3.549226\n",
      " 3690/5000: episode: 3689, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.555, 0.740], loss: 6.400628, mean_absolute_error: 343.720459, mean_q: -3.576884\n",
      " 3691/5000: episode: 3690, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.870, 0.970], loss: 6.460980, mean_absolute_error: 343.839081, mean_q: -3.593713\n",
      " 3692/5000: episode: 3691, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [0.005, 0.068], loss: 6.492200, mean_absolute_error: 343.937805, mean_q: -3.602387\n",
      " 3693/5000: episode: 3692, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.414], loss: 6622.687500, mean_absolute_error: 347.154968, mean_q: -3.604328\n",
      " 3694/5000: episode: 3693, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.075 [0.030, 0.130], loss: 6.484086, mean_absolute_error: 344.076294, mean_q: -3.600135\n",
      " 3695/5000: episode: 3694, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.065 [0.024, 0.118], loss: 6620.496582, mean_absolute_error: 347.263916, mean_q: -3.592508\n",
      " 3696/5000: episode: 3695, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.623 [0.528, 0.715], loss: 6619.415039, mean_absolute_error: 347.305145, mean_q: -3.581321\n",
      " 3697/5000: episode: 3696, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.878 [0.810, 0.936], loss: 19842.201172, mean_absolute_error: 353.613220, mean_q: -3.567359\n",
      " 3698/5000: episode: 3697, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.133, 0.289], loss: 6.304036, mean_absolute_error: 344.246979, mean_q: -3.549785\n",
      " 3699/5000: episode: 3698, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.074 [0.029, 0.129], loss: 6.238486, mean_absolute_error: 344.280212, mean_q: -3.531276\n",
      " 3700/5000: episode: 3699, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 6.209650, mean_absolute_error: 344.335754, mean_q: -3.523103\n",
      " 3701/5000: episode: 3700, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.322 [0.233, 0.414], loss: 6612.628906, mean_absolute_error: 347.535980, mean_q: -3.522152\n",
      " 3702/5000: episode: 3701, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.260, 0.445], loss: 6611.394531, mean_absolute_error: 347.595032, mean_q: -3.516468\n",
      " 3703/5000: episode: 3702, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.862 [0.792, 0.924], loss: 13214.079102, mean_absolute_error: 350.772644, mean_q: -3.507032\n",
      " 3704/5000: episode: 3703, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 6.107007, mean_absolute_error: 344.566284, mean_q: -3.493855\n",
      " 3705/5000: episode: 3704, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.971, 1.000], loss: 6.053866, mean_absolute_error: 344.606171, mean_q: -3.478617\n",
      " 3706/5000: episode: 3705, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 5.995682, mean_absolute_error: 344.640381, mean_q: -3.461855\n",
      " 3707/5000: episode: 3706, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.730, 0.881], loss: 6604.601074, mean_absolute_error: 347.820984, mean_q: -3.455890\n",
      " 3708/5000: episode: 3707, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.749 [0.663, 0.830], loss: 5.977427, mean_absolute_error: 344.779968, mean_q: -3.456579\n",
      " 3709/5000: episode: 3708, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.606, 0.783], loss: 7263.297363, mean_absolute_error: 344.855072, mean_q: -3.452762\n",
      " 3710/5000: episode: 3709, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.253 [0.172, 0.340], loss: 7406.417969, mean_absolute_error: 344.974609, mean_q: -3.443518\n",
      " 3711/5000: episode: 3710, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.741 [0.654, 0.823], loss: 6599.205078, mean_absolute_error: 348.054382, mean_q: -3.429662\n",
      " 3712/5000: episode: 3711, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.867 [0.798, 0.928], loss: 5.827467, mean_absolute_error: 344.977142, mean_q: -3.412932\n",
      " 3713/5000: episode: 3712, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 5.766199, mean_absolute_error: 345.009857, mean_q: -3.394938\n",
      " 3714/5000: episode: 3713, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.444, 0.637], loss: 5.700075, mean_absolute_error: 345.031769, mean_q: -3.375411\n",
      " 3715/5000: episode: 3714, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.824 [0.746, 0.893], loss: 1356891.875000, mean_absolute_error: 435.024933, mean_q: -3.354385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3716/5000: episode: 3715, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.688, 0.850], loss: 6592.455078, mean_absolute_error: 348.240295, mean_q: -3.354960\n",
      " 3717/5000: episode: 3716, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.884 [0.818, 0.941], loss: 5.612720, mean_absolute_error: 345.206787, mean_q: -3.349439\n",
      " 3718/5000: episode: 3717, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.867 [0.797, 0.928], loss: 5.580908, mean_absolute_error: 345.275177, mean_q: -3.339931\n",
      " 3719/5000: episode: 3718, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.036], loss: 5.539505, mean_absolute_error: 345.334381, mean_q: -3.327515\n",
      " 3720/5000: episode: 3719, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.961, 1.000], loss: 6584.674805, mean_absolute_error: 348.484436, mean_q: -3.313108\n",
      " 3721/5000: episode: 3720, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.148, 0.309], loss: 5.437785, mean_absolute_error: 345.424316, mean_q: -3.296813\n",
      " 3722/5000: episode: 3721, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.402 [0.309, 0.499], loss: 6581.491699, mean_absolute_error: 348.558167, mean_q: -3.279163\n",
      " 3723/5000: episode: 3722, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.342 [0.252, 0.436], loss: 5.316648, mean_absolute_error: 345.491943, mean_q: -3.259873\n",
      " 3724/5000: episode: 3723, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.037, 0.144], loss: 6578.585449, mean_absolute_error: 348.613159, mean_q: -3.240344\n",
      " 3725/5000: episode: 3724, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.511 [0.414, 0.608], loss: 5.186935, mean_absolute_error: 345.541016, mean_q: -3.219850\n",
      " 3726/5000: episode: 3725, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.391 [0.298, 0.487], loss: 6576.060059, mean_absolute_error: 348.652954, mean_q: -3.199349\n",
      " 3727/5000: episode: 3726, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 13846.603516, mean_absolute_error: 348.673279, mean_q: -3.177742\n",
      " 3728/5000: episode: 3727, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.911, 0.988], loss: 6573.012695, mean_absolute_error: 348.721924, mean_q: -3.165523\n",
      " 3729/5000: episode: 3728, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.178 [0.108, 0.256], loss: 1356579.375000, mean_absolute_error: 435.664551, mean_q: -3.161298\n",
      " 3730/5000: episode: 3729, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.328, 0.519], loss: 13132.364258, mean_absolute_error: 352.046021, mean_q: -3.188054\n",
      " 3731/5000: episode: 3730, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 6565.776367, mean_absolute_error: 349.131317, mean_q: -3.214650\n",
      " 3732/5000: episode: 3731, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.530 [0.433, 0.626], loss: 6563.092773, mean_absolute_error: 349.274353, mean_q: -3.231339\n",
      " 3733/5000: episode: 3732, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.870, 0.970], loss: 5.252843, mean_absolute_error: 346.315735, mean_q: -3.240248\n",
      " 3734/5000: episode: 3733, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.689, 0.851], loss: 6558.636230, mean_absolute_error: 349.487854, mean_q: -3.243595\n",
      " 3735/5000: episode: 3734, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [0.090, 0.229], loss: 5.256219, mean_absolute_error: 346.496338, mean_q: -3.241289\n",
      " 3736/5000: episode: 3735, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.064 [0.023, 0.115], loss: 13884.181641, mean_absolute_error: 349.683289, mean_q: -3.246477\n",
      " 3737/5000: episode: 3736, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.090], loss: 13902.618164, mean_absolute_error: 349.807770, mean_q: -3.254645\n",
      " 3738/5000: episode: 3737, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.096 [0.045, 0.157], loss: 7497.329590, mean_absolute_error: 346.902618, mean_q: -3.254636\n",
      " 3739/5000: episode: 3738, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.982, 1.000], loss: 6547.620117, mean_absolute_error: 349.957336, mean_q: -3.248751\n",
      " 3740/5000: episode: 3739, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.140, 0.298], loss: 5.249740, mean_absolute_error: 346.955994, mean_q: -3.239290\n",
      " 3741/5000: episode: 3740, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.864 [0.793, 0.925], loss: 5.207370, mean_absolute_error: 347.008606, mean_q: -3.226188\n",
      " 3742/5000: episode: 3741, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.858 [0.786, 0.920], loss: 6542.133301, mean_absolute_error: 350.110107, mean_q: -3.210199\n",
      " 3743/5000: episode: 3742, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 5.099654, mean_absolute_error: 347.089111, mean_q: -3.192636\n",
      " 3744/5000: episode: 3743, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.908, 0.987], loss: 6539.073242, mean_absolute_error: 350.171722, mean_q: -3.173652\n",
      " 3745/5000: episode: 3744, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.453 [0.357, 0.550], loss: 21625.861328, mean_absolute_error: 350.401123, mean_q: -3.152884\n",
      " 3746/5000: episode: 3745, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.813 [0.734, 0.885], loss: 4.896225, mean_absolute_error: 347.150513, mean_q: -3.128289\n",
      " 3747/5000: episode: 3746, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.051 [0.015, 0.099], loss: 13868.170898, mean_absolute_error: 350.212158, mean_q: -3.103265\n",
      " 3748/5000: episode: 3747, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.138, 0.297], loss: 4.731993, mean_absolute_error: 347.165344, mean_q: -3.075359\n",
      " 3749/5000: episode: 3748, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.065 [0.023, 0.117], loss: 4.687422, mean_absolute_error: 347.206024, mean_q: -3.060837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3750/5000: episode: 3749, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 1362289.875000, mean_absolute_error: 440.209717, mean_q: -3.056222\n",
      " 3751/5000: episode: 3750, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.208, 0.384], loss: 1355691.250000, mean_absolute_error: 437.273132, mean_q: -3.070964\n",
      " 3752/5000: episode: 3751, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.346 [0.256, 0.440], loss: 1355589.000000, mean_absolute_error: 437.423370, mean_q: -3.101667\n",
      " 3753/5000: episode: 3752, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.513 [0.416, 0.609], loss: 4.948111, mean_absolute_error: 347.735107, mean_q: -3.144826\n",
      " 3754/5000: episode: 3753, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.102 [0.049, 0.165], loss: 5.045839, mean_absolute_error: 347.892975, mean_q: -3.175740\n",
      " 3755/5000: episode: 3754, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 13905.413086, mean_absolute_error: 351.065125, mean_q: -3.196856\n",
      " 3756/5000: episode: 3755, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.135], loss: 5.145561, mean_absolute_error: 348.118103, mean_q: -3.206978\n",
      " 3757/5000: episode: 3756, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.933, 0.995], loss: 7443.807129, mean_absolute_error: 348.226868, mean_q: -3.210090\n",
      " 3758/5000: episode: 3757, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.507, 0.696], loss: 5.183567, mean_absolute_error: 348.289673, mean_q: -3.218803\n",
      " 3759/5000: episode: 3758, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.232 [0.154, 0.317], loss: 5.223072, mean_absolute_error: 348.388641, mean_q: -3.231050\n",
      " 3760/5000: episode: 3759, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 14051.163086, mean_absolute_error: 351.594818, mean_q: -3.246658\n",
      " 3761/5000: episode: 3760, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.498, 0.688], loss: 5.326130, mean_absolute_error: 348.607361, mean_q: -3.262780\n",
      " 3762/5000: episode: 3761, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.560 [0.464, 0.656], loss: 21737.257812, mean_absolute_error: 351.963074, mean_q: -3.283519\n",
      " 3763/5000: episode: 3762, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.856 [0.784, 0.919], loss: 5.456449, mean_absolute_error: 348.861755, mean_q: -3.302467\n",
      " 3764/5000: episode: 3763, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.148], loss: 5.531543, mean_absolute_error: 349.004028, mean_q: -3.325122\n",
      " 3765/5000: episode: 3764, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.738 [0.650, 0.820], loss: 6508.013672, mean_absolute_error: 352.169373, mean_q: -3.350246\n",
      " 3766/5000: episode: 3765, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.848 [0.774, 0.912], loss: 7646.333496, mean_absolute_error: 349.367310, mean_q: -3.365929\n",
      " 3767/5000: episode: 3766, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 6504.997559, mean_absolute_error: 352.377045, mean_q: -3.372512\n",
      " 3768/5000: episode: 3767, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.727 [0.638, 0.810], loss: 1354369.750000, mean_absolute_error: 439.232300, mean_q: -3.372760\n",
      " 3769/5000: episode: 3768, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 5.752663, mean_absolute_error: 349.568024, mean_q: -3.390950\n",
      " 3770/5000: episode: 3769, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [0.078, 0.211], loss: 5.786717, mean_absolute_error: 349.676636, mean_q: -3.400975\n",
      " 3771/5000: episode: 3770, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.099], loss: 5.799124, mean_absolute_error: 349.765961, mean_q: -3.404620\n",
      " 3772/5000: episode: 3771, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.980 [0.949, 0.999], loss: 12988.054688, mean_absolute_error: 355.863251, mean_q: -3.402720\n",
      " 3773/5000: episode: 3772, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 1354087.125000, mean_absolute_error: 439.686584, mean_q: -3.395477\n",
      " 3774/5000: episode: 3773, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.283 [0.199, 0.373], loss: 12980.238281, mean_absolute_error: 356.046753, mean_q: -3.407900\n",
      " 3775/5000: episode: 3774, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.418 [0.323, 0.514], loss: 5.825055, mean_absolute_error: 350.138977, mean_q: -3.412226\n",
      " 3776/5000: episode: 3775, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.174 [0.105, 0.251], loss: 2714289.000000, mean_absolute_error: 532.752441, mean_q: -3.411554\n",
      " 3777/5000: episode: 3776, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.544 [0.447, 0.640], loss: 6486.597656, mean_absolute_error: 353.422119, mean_q: -3.451397\n",
      " 3778/5000: episode: 3777, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.696, 0.856], loss: 1353679.625000, mean_absolute_error: 440.331512, mean_q: -3.478455\n",
      " 3779/5000: episode: 3778, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.400 [0.306, 0.496], loss: 6.191995, mean_absolute_error: 350.783875, mean_q: -3.518089\n",
      " 3780/5000: episode: 3779, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.593 [0.497, 0.687], loss: 6.286436, mean_absolute_error: 350.942078, mean_q: -3.544825\n",
      " 3781/5000: episode: 3780, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.621, 0.796], loss: 6478.395508, mean_absolute_error: 354.058899, mean_q: -3.561390\n",
      " 3782/5000: episode: 3781, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.191 [0.119, 0.271], loss: 6.374678, mean_absolute_error: 351.173889, mean_q: -3.569624\n",
      " 3783/5000: episode: 3782, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.461, 0.654], loss: 6475.003906, mean_absolute_error: 354.248749, mean_q: -3.571650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3784/5000: episode: 3783, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.926, 0.994], loss: 6.364537, mean_absolute_error: 351.335571, mean_q: -3.566783\n",
      " 3785/5000: episode: 3784, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 6.372577, mean_absolute_error: 351.429626, mean_q: -3.569036\n",
      " 3786/5000: episode: 3785, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.642 [0.547, 0.733], loss: 6469.728516, mean_absolute_error: 354.517334, mean_q: -3.577272\n",
      " 3787/5000: episode: 3786, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.254 [0.173, 0.341], loss: 6468.126953, mean_absolute_error: 354.603271, mean_q: -3.579049\n",
      " 3788/5000: episode: 3787, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.062], loss: 6.395459, mean_absolute_error: 351.696289, mean_q: -3.575439\n",
      " 3789/5000: episode: 3788, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.850 [0.777, 0.915], loss: 6.369632, mean_absolute_error: 351.754395, mean_q: -3.568211\n",
      " 3790/5000: episode: 3789, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.622 [0.526, 0.714], loss: 7548.223145, mean_absolute_error: 351.802826, mean_q: -3.557690\n",
      " 3791/5000: episode: 3790, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.265 [0.183, 0.353], loss: 6.279425, mean_absolute_error: 351.836914, mean_q: -3.542847\n",
      " 3792/5000: episode: 3791, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.296 [0.211, 0.387], loss: 6462.052246, mean_absolute_error: 354.838379, mean_q: -3.526525\n",
      " 3793/5000: episode: 3792, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.039], loss: 6.156837, mean_absolute_error: 351.890442, mean_q: -3.508084\n",
      " 3794/5000: episode: 3793, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.089], loss: 7616.905762, mean_absolute_error: 351.977295, mean_q: -3.500147\n",
      " 3795/5000: episode: 3794, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.329, 0.520], loss: 1352789.750000, mean_absolute_error: 441.735962, mean_q: -3.510265\n",
      " 3796/5000: episode: 3795, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.955 [0.910, 0.988], loss: 6.337924, mean_absolute_error: 352.262146, mean_q: -3.559316\n",
      " 3797/5000: episode: 3796, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.148], loss: 7797.919434, mean_absolute_error: 352.569336, mean_q: -3.604411\n",
      " 3798/5000: episode: 3797, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.227, 0.407], loss: 6452.368652, mean_absolute_error: 355.597473, mean_q: -3.633671\n",
      " 3799/5000: episode: 3798, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.702 [0.611, 0.788], loss: 6450.304688, mean_absolute_error: 355.735596, mean_q: -3.652141\n",
      " 3800/5000: episode: 3799, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 6448.364258, mean_absolute_error: 355.847748, mean_q: -3.661200\n",
      " 3801/5000: episode: 3800, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.546 [0.449, 0.642], loss: 6.709929, mean_absolute_error: 352.981720, mean_q: -3.662313\n",
      " 3802/5000: episode: 3801, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.666, 0.832], loss: 6.694671, mean_absolute_error: 353.063049, mean_q: -3.658145\n",
      " 3803/5000: episode: 3802, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.371, 0.564], loss: 6442.602539, mean_absolute_error: 356.087189, mean_q: -3.649334\n",
      " 3804/5000: episode: 3803, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.940, 0.997], loss: 6.616006, mean_absolute_error: 353.196350, mean_q: -3.636583\n",
      " 3805/5000: episode: 3804, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [0.084, 0.221], loss: 6.562397, mean_absolute_error: 353.246796, mean_q: -3.621816\n",
      " 3806/5000: episode: 3805, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.639 [0.545, 0.731], loss: 6.543077, mean_absolute_error: 353.316254, mean_q: -3.616479\n",
      " 3807/5000: episode: 3806, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.346 [0.256, 0.440], loss: 6.547876, mean_absolute_error: 353.398438, mean_q: -3.617805\n",
      " 3808/5000: episode: 3807, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.780 [0.697, 0.857], loss: 6.535813, mean_absolute_error: 353.463776, mean_q: -3.614470\n",
      " 3809/5000: episode: 3808, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.665, 0.832], loss: 6.508864, mean_absolute_error: 353.517212, mean_q: -3.607009\n",
      " 3810/5000: episode: 3809, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 6432.444824, mean_absolute_error: 356.539917, mean_q: -3.607911\n",
      " 3811/5000: episode: 3810, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.858 [0.786, 0.920], loss: 7794.404785, mean_absolute_error: 353.771545, mean_q: -3.614084\n",
      " 3812/5000: episode: 3811, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.089], loss: 12852.731445, mean_absolute_error: 359.650452, mean_q: -3.613050\n",
      " 3813/5000: episode: 3812, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.780, 0.916], loss: 7704.106445, mean_absolute_error: 353.868164, mean_q: -3.606215\n",
      " 3814/5000: episode: 3813, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.594, 0.774], loss: 7854.033691, mean_absolute_error: 354.026917, mean_q: -3.605579\n",
      " 3815/5000: episode: 3814, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 7862.480469, mean_absolute_error: 354.129395, mean_q: -3.609249\n",
      " 3816/5000: episode: 3815, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 7715.484375, mean_absolute_error: 354.133331, mean_q: -3.606099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3817/5000: episode: 3816, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.077], loss: 6421.128906, mean_absolute_error: 357.124023, mean_q: -3.609152\n",
      " 3818/5000: episode: 3817, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 6.545233, mean_absolute_error: 354.299774, mean_q: -3.617075\n",
      " 3819/5000: episode: 3818, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 6.595779, mean_absolute_error: 354.417755, mean_q: -3.631019\n",
      " 3820/5000: episode: 3819, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.437, 0.630], loss: 6.656834, mean_absolute_error: 354.539368, mean_q: -3.647790\n",
      " 3821/5000: episode: 3820, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.506 [0.409, 0.603], loss: 7889.684570, mean_absolute_error: 354.765961, mean_q: -3.668125\n",
      " 3822/5000: episode: 3821, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.895, 0.982], loss: 6.808786, mean_absolute_error: 354.793396, mean_q: -3.689199\n",
      " 3823/5000: episode: 3822, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.606 [0.510, 0.699], loss: 14211.411133, mean_absolute_error: 357.874084, mean_q: -3.701519\n",
      " 3824/5000: episode: 3823, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.935 [0.882, 0.976], loss: 6411.636230, mean_absolute_error: 357.902527, mean_q: -3.704347\n",
      " 3825/5000: episode: 3824, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 6410.341797, mean_absolute_error: 357.971069, mean_q: -3.701554\n",
      " 3826/5000: episode: 3825, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 6.822535, mean_absolute_error: 355.110779, mean_q: -3.692924\n",
      " 3827/5000: episode: 3826, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.099 [0.047, 0.161], loss: 6.776265, mean_absolute_error: 355.158051, mean_q: -3.680376\n",
      " 3828/5000: episode: 3827, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.381 [0.289, 0.477], loss: 6.719295, mean_absolute_error: 355.191528, mean_q: -3.664868\n",
      " 3829/5000: episode: 3828, duration: 0.043s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.970], loss: 6405.642578, mean_absolute_error: 358.130646, mean_q: -3.647013\n",
      " 3830/5000: episode: 3829, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.580 [0.484, 0.675], loss: 6404.520508, mean_absolute_error: 358.154388, mean_q: -3.627119\n",
      " 3831/5000: episode: 3830, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.750, 0.896], loss: 6.503694, mean_absolute_error: 355.264099, mean_q: -3.605576\n",
      " 3832/5000: episode: 3831, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.554 [0.457, 0.649], loss: 7868.200195, mean_absolute_error: 355.357849, mean_q: -3.583707\n",
      " 3833/5000: episode: 3832, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.075 [0.030, 0.130], loss: 6401.272461, mean_absolute_error: 358.201324, mean_q: -3.559966\n",
      " 3834/5000: episode: 3833, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.969, 1.000], loss: 6.253984, mean_absolute_error: 355.300415, mean_q: -3.535661\n",
      " 3835/5000: episode: 3834, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.521, 0.709], loss: 14105.721680, mean_absolute_error: 358.220673, mean_q: -3.511322\n",
      " 3836/5000: episode: 3835, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 7709.963867, mean_absolute_error: 355.351105, mean_q: -3.496357\n",
      " 3837/5000: episode: 3836, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.479 [0.382, 0.576], loss: 6.089738, mean_absolute_error: 355.412170, mean_q: -3.488911\n",
      " 3838/5000: episode: 3837, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 33334.210938, mean_absolute_error: 367.116577, mean_q: -3.477856\n",
      " 3839/5000: episode: 3838, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.031], loss: 5.987201, mean_absolute_error: 355.523987, mean_q: -3.459405\n",
      " 3840/5000: episode: 3839, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.003, 0.059], loss: 5.961366, mean_absolute_error: 355.606201, mean_q: -3.451931\n",
      " 3841/5000: episode: 3840, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.552, 0.737], loss: 5.962328, mean_absolute_error: 355.703369, mean_q: -3.452209\n",
      " 3842/5000: episode: 3841, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.478 [0.382, 0.575], loss: 5.944465, mean_absolute_error: 355.781738, mean_q: -3.447033\n",
      " 3843/5000: episode: 3842, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.684, 0.847], loss: 6384.525879, mean_absolute_error: 358.737183, mean_q: -3.437453\n",
      " 3844/5000: episode: 3843, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 14324.960938, mean_absolute_error: 358.892242, mean_q: -3.424502\n",
      " 3845/5000: episode: 3844, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.101 [0.048, 0.164], loss: 5.807591, mean_absolute_error: 355.941193, mean_q: -3.407105\n",
      " 3846/5000: episode: 3845, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.068 [0.025, 0.121], loss: 5.744423, mean_absolute_error: 355.977631, mean_q: -3.388520\n",
      " 3847/5000: episode: 3846, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 6377.921387, mean_absolute_error: 358.896973, mean_q: -3.368912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3848/5000: episode: 3847, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.896, 0.982], loss: 5.607771, mean_absolute_error: 356.038971, mean_q: -3.347961\n",
      " 3849/5000: episode: 3848, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [0.064, 0.190], loss: 5.534836, mean_absolute_error: 356.065887, mean_q: -3.326112\n",
      " 3850/5000: episode: 3849, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.905 [0.844, 0.956], loss: 5.460475, mean_absolute_error: 356.090942, mean_q: -3.303686\n",
      " 3851/5000: episode: 3850, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.931 [0.878, 0.974], loss: 5.427037, mean_absolute_error: 356.143524, mean_q: -3.293552\n",
      " 3852/5000: episode: 3851, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 1350359.625000, mean_absolute_error: 445.719849, mean_q: -3.291353\n",
      " 3853/5000: episode: 3852, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 7763.989746, mean_absolute_error: 356.368256, mean_q: -3.319718\n",
      " 3854/5000: episode: 3853, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.956 [0.913, 0.989], loss: 6367.605957, mean_absolute_error: 359.386780, mean_q: -3.347116\n",
      " 3855/5000: episode: 3854, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.971], loss: 5.661549, mean_absolute_error: 356.637207, mean_q: -3.363981\n",
      " 3856/5000: episode: 3855, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.169, 0.336], loss: 5.688756, mean_absolute_error: 356.748840, mean_q: -3.372057\n",
      " 3857/5000: episode: 3856, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.859 [0.788, 0.922], loss: 5.733654, mean_absolute_error: 356.879272, mean_q: -3.385342\n",
      " 3858/5000: episode: 3857, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.757 [0.670, 0.836], loss: 5.793000, mean_absolute_error: 357.013580, mean_q: -3.402822\n",
      " 3859/5000: episode: 3858, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.538, 0.724], loss: 12709.571289, mean_absolute_error: 362.889526, mean_q: -3.424472\n",
      " 3860/5000: episode: 3859, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 5.944383, mean_absolute_error: 357.311371, mean_q: -3.447009\n",
      " 3861/5000: episode: 3860, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.885, 0.977], loss: 6353.406250, mean_absolute_error: 360.299377, mean_q: -3.460738\n",
      " 3862/5000: episode: 3861, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.534 [0.437, 0.630], loss: 6.011118, mean_absolute_error: 357.547241, mean_q: -3.466310\n",
      " 3863/5000: episode: 3862, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 6.010099, mean_absolute_error: 357.639709, mean_q: -3.466016\n",
      " 3864/5000: episode: 3863, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 19030.527344, mean_absolute_error: 366.280945, mean_q: -3.461143\n",
      " 3865/5000: episode: 3864, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.933 [0.880, 0.975], loss: 1363779.750000, mean_absolute_error: 450.183868, mean_q: -3.450969\n",
      " 3866/5000: episode: 3865, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.215 [0.139, 0.298], loss: 5.986238, mean_absolute_error: 357.907532, mean_q: -3.459126\n",
      " 3867/5000: episode: 3866, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.833 [0.757, 0.901], loss: 7886.965332, mean_absolute_error: 358.032806, mean_q: -3.461465\n",
      " 3868/5000: episode: 3867, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 5.979290, mean_absolute_error: 358.077179, mean_q: -3.457118\n",
      " 3869/5000: episode: 3868, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.368 [0.277, 0.463], loss: 12669.759766, mean_absolute_error: 363.828552, mean_q: -3.449018\n",
      " 3870/5000: episode: 3869, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.249, 0.432], loss: 5.909572, mean_absolute_error: 358.193787, mean_q: -3.436898\n",
      " 3871/5000: episode: 3870, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.100 [0.048, 0.162], loss: 5.858875, mean_absolute_error: 358.242310, mean_q: -3.422120\n",
      " 3872/5000: episode: 3871, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.821 [0.743, 0.891], loss: 6332.794434, mean_absolute_error: 361.124908, mean_q: -3.405414\n",
      " 3873/5000: episode: 3872, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.793 [0.711, 0.868], loss: 8014.096680, mean_absolute_error: 358.405823, mean_q: -3.386865\n",
      " 3874/5000: episode: 3873, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.682, 0.846], loss: 5.668403, mean_absolute_error: 358.345459, mean_q: -3.366017\n",
      " 3875/5000: episode: 3874, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.678 [0.586, 0.766], loss: 6328.847656, mean_absolute_error: 361.199371, mean_q: -3.344962\n",
      " 3876/5000: episode: 3875, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.956, 1.000], loss: 7851.750000, mean_absolute_error: 358.390137, mean_q: -3.322720\n",
      " 3877/5000: episode: 3876, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.367 [0.276, 0.462], loss: 8015.619141, mean_absolute_error: 358.478485, mean_q: -3.298528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3878/5000: episode: 3877, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.061], loss: 5.357121, mean_absolute_error: 358.394836, mean_q: -3.272262\n",
      " 3879/5000: episode: 3878, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [0.084, 0.221], loss: 7899.349609, mean_absolute_error: 358.426239, mean_q: -3.245418\n",
      " 3880/5000: episode: 3879, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.537 [0.440, 0.633], loss: 5.178584, mean_absolute_error: 358.394653, mean_q: -3.217256\n",
      " 3881/5000: episode: 3880, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.667, 0.834], loss: 8056.864746, mean_absolute_error: 358.496735, mean_q: -3.189159\n",
      " 3882/5000: episode: 3881, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 4.994678, mean_absolute_error: 358.383423, mean_q: -3.159595\n",
      " 3883/5000: episode: 3882, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.557, 0.741], loss: 1361779.375000, mean_absolute_error: 453.453888, mean_q: -3.131650\n",
      " 3884/5000: episode: 3883, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.400 [0.307, 0.497], loss: 4.892295, mean_absolute_error: 358.439819, mean_q: -3.127033\n",
      " 3885/5000: episode: 3884, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.419 [0.325, 0.516], loss: 6318.194336, mean_absolute_error: 361.317139, mean_q: -3.119377\n",
      " 3886/5000: episode: 3885, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.179 [0.109, 0.256], loss: 1349054.875000, mean_absolute_error: 447.952759, mean_q: -3.108307\n",
      " 3887/5000: episode: 3886, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 4.865163, mean_absolute_error: 358.641937, mean_q: -3.118347\n",
      " 3888/5000: episode: 3887, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 8062.416016, mean_absolute_error: 358.857178, mean_q: -3.134834\n",
      " 3889/5000: episode: 3888, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.236, 0.417], loss: 4.975494, mean_absolute_error: 358.875702, mean_q: -3.153519\n",
      " 3890/5000: episode: 3889, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.249 [0.169, 0.336], loss: 5.009806, mean_absolute_error: 358.972351, mean_q: -3.164377\n",
      " 3891/5000: episode: 3890, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.902 [0.840, 0.954], loss: 6310.472168, mean_absolute_error: 361.868896, mean_q: -3.168926\n",
      " 3892/5000: episode: 3891, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.539 [0.442, 0.635], loss: 14176.859375, mean_absolute_error: 361.935120, mean_q: -3.167152\n",
      " 3893/5000: episode: 3892, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.588 [0.492, 0.682], loss: 4.991451, mean_absolute_error: 359.163818, mean_q: -3.158573\n",
      " 3894/5000: episode: 3893, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.214, 0.391], loss: 6307.220215, mean_absolute_error: 362.016327, mean_q: -3.147635\n",
      " 3895/5000: episode: 3894, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 6306.172852, mean_absolute_error: 362.049011, mean_q: -3.134196\n",
      " 3896/5000: episode: 3895, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.075 [0.030, 0.130], loss: 6305.053711, mean_absolute_error: 362.077332, mean_q: -3.118678\n",
      " 3897/5000: episode: 3896, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 4.812923, mean_absolute_error: 359.293182, mean_q: -3.101555\n",
      " 3898/5000: episode: 3897, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.894 [0.830, 0.948], loss: 4.756790, mean_absolute_error: 359.316467, mean_q: -3.083409\n",
      " 3899/5000: episode: 3898, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.728, 0.880], loss: 6301.948730, mean_absolute_error: 362.145020, mean_q: -3.064534\n",
      " 3900/5000: episode: 3899, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 6300.348145, mean_absolute_error: 362.166504, mean_q: -3.044050\n",
      " 3901/5000: episode: 3900, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 4.570303, mean_absolute_error: 359.386353, mean_q: -3.022344\n",
      " 3902/5000: episode: 3901, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.248, 0.431], loss: 14383.278320, mean_absolute_error: 362.310547, mean_q: -3.000931\n",
      " 3903/5000: episode: 3902, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.355 [0.264, 0.450], loss: 4.435118, mean_absolute_error: 359.420715, mean_q: -2.977294\n",
      " 3904/5000: episode: 3903, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.211, 0.388], loss: 4.367338, mean_absolute_error: 359.431366, mean_q: -2.954449\n",
      " 3905/5000: episode: 3904, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.920 [0.863, 0.966], loss: 6293.747559, mean_absolute_error: 362.276123, mean_q: -2.944215\n",
      " 3906/5000: episode: 3905, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 4.331684, mean_absolute_error: 359.546600, mean_q: -2.942360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3907/5000: episode: 3906, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.467, 0.659], loss: 1356392.750000, mean_absolute_error: 449.003601, mean_q: -2.937124\n",
      " 3908/5000: episode: 3907, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.009], loss: 4.355872, mean_absolute_error: 359.709503, mean_q: -2.950566\n",
      " 3909/5000: episode: 3908, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.909 [0.849, 0.959], loss: 6288.359375, mean_absolute_error: 362.593079, mean_q: -2.957379\n",
      " 3910/5000: episode: 3909, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.942, 0.998], loss: 7898.412109, mean_absolute_error: 359.873871, mean_q: -2.958177\n",
      " 3911/5000: episode: 3910, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.713, 0.869], loss: 7940.161133, mean_absolute_error: 359.950562, mean_q: -2.953129\n",
      " 3912/5000: episode: 3911, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.821, 0.942], loss: 4.330103, mean_absolute_error: 359.974670, mean_q: -2.941823\n",
      " 3913/5000: episode: 3912, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.833 [0.757, 0.901], loss: 6283.056641, mean_absolute_error: 362.799500, mean_q: -2.927567\n",
      " 3914/5000: episode: 3913, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.732, 0.883], loss: 4.239048, mean_absolute_error: 360.038757, mean_q: -2.910717\n",
      " 3915/5000: episode: 3914, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.734 [0.645, 0.816], loss: 6280.681641, mean_absolute_error: 362.846191, mean_q: -2.892516\n",
      " 3916/5000: episode: 3915, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.951, 0.999], loss: 4.130873, mean_absolute_error: 360.077698, mean_q: -2.873325\n",
      " 3917/5000: episode: 3916, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.170, 0.337], loss: 6278.611816, mean_absolute_error: 362.877686, mean_q: -2.854297\n",
      " 3918/5000: episode: 3917, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.701, 0.860], loss: 4.017427, mean_absolute_error: 360.104370, mean_q: -2.833582\n",
      " 3919/5000: episode: 3918, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.484 [0.388, 0.581], loss: 3.956295, mean_absolute_error: 360.111816, mean_q: -2.811933\n",
      " 3920/5000: episode: 3919, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.176, 0.345], loss: 3.896299, mean_absolute_error: 360.120361, mean_q: -2.790523\n",
      " 3921/5000: episode: 3920, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.413], loss: 3.835586, mean_absolute_error: 360.123077, mean_q: -2.768688\n",
      " 3922/5000: episode: 3921, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.060], loss: 3.809084, mean_absolute_error: 360.155609, mean_q: -2.759103\n",
      " 3923/5000: episode: 3922, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.077], loss: 1362453.750000, mean_absolute_error: 452.417908, mean_q: -2.758722\n",
      " 3924/5000: episode: 3923, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.465, 0.657], loss: 3.855118, mean_absolute_error: 360.328491, mean_q: -2.775731\n",
      " 3925/5000: episode: 3924, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [0.070, 0.200], loss: 1347985.250000, mean_absolute_error: 449.756531, mean_q: -2.784731\n",
      " 3926/5000: episode: 3925, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.884 [0.818, 0.941], loss: 3.952245, mean_absolute_error: 360.558716, mean_q: -2.810493\n",
      " 3927/5000: episode: 3926, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.581 [0.484, 0.675], loss: 3.999454, mean_absolute_error: 360.675018, mean_q: -2.827234\n",
      " 3928/5000: episode: 3927, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.846 [0.772, 0.911], loss: 6266.125488, mean_absolute_error: 363.576477, mean_q: -2.849211\n",
      " 3929/5000: episode: 3928, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.827 [0.751, 0.896], loss: 6264.193359, mean_absolute_error: 363.716431, mean_q: -2.873555\n",
      " 3930/5000: episode: 3929, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.941 [0.890, 0.980], loss: 6262.351562, mean_absolute_error: 363.834167, mean_q: -2.888632\n",
      " 3931/5000: episode: 3930, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.435 [0.339, 0.531], loss: 4.197978, mean_absolute_error: 361.166748, mean_q: -2.896578\n",
      " 3932/5000: episode: 3931, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.507, 0.696], loss: 14432.517578, mean_absolute_error: 364.114868, mean_q: -2.898947\n",
      " 3933/5000: episode: 3932, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.370 [0.278, 0.465], loss: 4.189773, mean_absolute_error: 361.321136, mean_q: -2.893744\n",
      " 3934/5000: episode: 3933, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.287, 0.475], loss: 8178.750000, mean_absolute_error: 361.483337, mean_q: -2.884879\n",
      " 3935/5000: episode: 3934, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.594, 0.773], loss: 4.126735, mean_absolute_error: 361.430725, mean_q: -2.871885\n",
      " 3936/5000: episode: 3935, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.329 [0.240, 0.422], loss: 4.085073, mean_absolute_error: 361.470825, mean_q: -2.857347\n",
      " 3937/5000: episode: 3936, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.051], loss: 4.034950, mean_absolute_error: 361.498901, mean_q: -2.839757\n",
      " 3938/5000: episode: 3937, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.122, 0.275], loss: 3.977623, mean_absolute_error: 361.519226, mean_q: -2.819505\n",
      " 3939/5000: episode: 3938, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.735 [0.647, 0.817], loss: 6247.830566, mean_absolute_error: 364.292786, mean_q: -2.798845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3940/5000: episode: 3939, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 12488.784180, mean_absolute_error: 367.064209, mean_q: -2.777485\n",
      " 3941/5000: episode: 3940, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.465, 0.657], loss: 3.797563, mean_absolute_error: 361.589233, mean_q: -2.754926\n",
      " 3942/5000: episode: 3941, duration: 0.110s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.836 [0.761, 0.903], loss: 6242.639648, mean_absolute_error: 364.362213, mean_q: -2.732391\n",
      " 3943/5000: episode: 3942, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.172 [0.104, 0.249], loss: 14369.615234, mean_absolute_error: 364.471619, mean_q: -2.708335\n",
      " 3944/5000: episode: 3943, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.906 [0.846, 0.957], loss: 3.599040, mean_absolute_error: 361.684570, mean_q: -2.681924\n",
      " 3945/5000: episode: 3944, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.413 [0.319, 0.510], loss: 1347333.000000, mean_absolute_error: 451.002869, mean_q: -2.656685\n",
      " 3946/5000: episode: 3945, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 3.530767, mean_absolute_error: 361.802490, mean_q: -2.656355\n",
      " 3947/5000: episode: 3946, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.043], loss: 8194.283203, mean_absolute_error: 361.978394, mean_q: -2.652397\n",
      " 3948/5000: episode: 3947, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.914, 0.990], loss: 3.494366, mean_absolute_error: 361.930511, mean_q: -2.642621\n",
      " 3949/5000: episode: 3948, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.111 [0.055, 0.176], loss: 3.459540, mean_absolute_error: 361.977051, mean_q: -2.629415\n",
      " 3950/5000: episode: 3949, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.832 [0.756, 0.900], loss: 3.420153, mean_absolute_error: 362.019928, mean_q: -2.614398\n",
      " 3951/5000: episode: 3950, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 3.379673, mean_absolute_error: 362.054016, mean_q: -2.598874\n",
      " 3952/5000: episode: 3951, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [0.067, 0.194], loss: 14367.192383, mean_absolute_error: 364.921021, mean_q: -2.595095\n",
      " 3953/5000: episode: 3952, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.064], loss: 6221.151855, mean_absolute_error: 364.929016, mean_q: -2.596842\n",
      " 3954/5000: episode: 3953, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [0.069, 0.197], loss: 3.366000, mean_absolute_error: 362.280151, mean_q: -2.593610\n",
      " 3955/5000: episode: 3954, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 3.347354, mean_absolute_error: 362.341736, mean_q: -2.586413\n",
      " 3956/5000: episode: 3955, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 6215.744141, mean_absolute_error: 365.109833, mean_q: -2.575992\n",
      " 3957/5000: episode: 3956, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.048], loss: 3.287710, mean_absolute_error: 362.433868, mean_q: -2.563258\n",
      " 3958/5000: episode: 3957, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.481 [0.384, 0.578], loss: 14387.761719, mean_absolute_error: 365.272552, mean_q: -2.549253\n",
      " 3959/5000: episode: 3958, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.794 [0.712, 0.868], loss: 3.207444, mean_absolute_error: 362.502075, mean_q: -2.531763\n",
      " 3960/5000: episode: 3959, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.194, 0.368], loss: 3.161802, mean_absolute_error: 362.531128, mean_q: -2.513678\n",
      " 3961/5000: episode: 3960, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.114, 0.264], loss: 6207.727051, mean_absolute_error: 365.270752, mean_q: -2.494724\n",
      " 3962/5000: episode: 3961, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 3.064682, mean_absolute_error: 362.590454, mean_q: -2.474755\n",
      " 3963/5000: episode: 3962, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 6204.190918, mean_absolute_error: 365.322052, mean_q: -2.454084\n",
      " 3964/5000: episode: 3963, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 2.961246, mean_absolute_error: 362.634674, mean_q: -2.432617\n",
      " 3965/5000: episode: 3964, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.964, 1.000], loss: 2.910384, mean_absolute_error: 362.652008, mean_q: -2.411627\n",
      " 3966/5000: episode: 3965, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.270 [0.187, 0.359], loss: 2.859143, mean_absolute_error: 362.664673, mean_q: -2.390294\n",
      " 3967/5000: episode: 3966, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 8115.072754, mean_absolute_error: 362.725342, mean_q: -2.368938\n",
      " 3968/5000: episode: 3967, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.594 [0.498, 0.688], loss: 2.756089, mean_absolute_error: 362.670105, mean_q: -2.346803\n",
      " 3969/5000: episode: 3968, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.729, 0.881], loss: 1353151.125000, mean_absolute_error: 454.630707, mean_q: -2.324819\n",
      " 3970/5000: episode: 3969, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.961 [0.919, 0.991], loss: 6196.195801, mean_absolute_error: 365.430420, mean_q: -2.326538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3971/5000: episode: 3970, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.074 [0.029, 0.129], loss: 2.703277, mean_absolute_error: 362.785980, mean_q: -2.324200\n",
      " 3972/5000: episode: 3971, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.310 [0.223, 0.402], loss: 2.692028, mean_absolute_error: 362.830109, mean_q: -2.319357\n",
      " 3973/5000: episode: 3972, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.967, 1.000], loss: 8046.789062, mean_absolute_error: 362.883392, mean_q: -2.312312\n",
      " 3974/5000: episode: 3973, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.917, 0.990], loss: 1352992.750000, mean_absolute_error: 454.868805, mean_q: -2.313539\n",
      " 3975/5000: episode: 3974, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.054 [0.017, 0.102], loss: 2.751378, mean_absolute_error: 363.068176, mean_q: -2.344795\n",
      " 3976/5000: episode: 3975, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.931, 0.995], loss: 2.802121, mean_absolute_error: 363.192749, mean_q: -2.366328\n",
      " 3977/5000: episode: 3976, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.399 [0.305, 0.495], loss: 2.835525, mean_absolute_error: 363.294739, mean_q: -2.380397\n",
      " 3978/5000: episode: 3977, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 2.884855, mean_absolute_error: 363.413574, mean_q: -2.401022\n",
      " 3979/5000: episode: 3978, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.532 [0.435, 0.629], loss: 2.944307, mean_absolute_error: 363.541626, mean_q: -2.425647\n",
      " 3980/5000: episode: 3979, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.876, 0.973], loss: 2.983608, mean_absolute_error: 363.650574, mean_q: -2.441789\n",
      " 3981/5000: episode: 3980, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.902, 0.985], loss: 8265.057617, mean_absolute_error: 363.839294, mean_q: -2.451358\n",
      " 3982/5000: episode: 3981, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.013], loss: 8053.505371, mean_absolute_error: 363.808228, mean_q: -2.453684\n",
      " 3983/5000: episode: 3982, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.953 [0.908, 0.987], loss: 3.035581, mean_absolute_error: 363.894287, mean_q: -2.462973\n",
      " 3984/5000: episode: 3983, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [0.078, 0.212], loss: 3.071793, mean_absolute_error: 364.001923, mean_q: -2.477626\n",
      " 3985/5000: episode: 3984, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.269], loss: 8071.290527, mean_absolute_error: 364.097168, mean_q: -2.485322\n",
      " 3986/5000: episode: 3985, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.950 [0.903, 0.985], loss: 3.093849, mean_absolute_error: 364.163727, mean_q: -2.486508\n",
      " 3987/5000: episode: 3986, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.327 [0.238, 0.419], loss: 3.087151, mean_absolute_error: 364.219818, mean_q: -2.483814\n",
      " 3988/5000: episode: 3987, duration: 0.087s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 3.104479, mean_absolute_error: 364.302948, mean_q: -2.490778\n",
      " 3989/5000: episode: 3988, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.955, 1.000], loss: 1345886.625000, mean_absolute_error: 453.589752, mean_q: -2.502915\n",
      " 3990/5000: episode: 3989, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.243 [0.164, 0.330], loss: 3.206328, mean_absolute_error: 364.556396, mean_q: -2.531322\n",
      " 3991/5000: episode: 3990, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.427 [0.332, 0.523], loss: 6167.268066, mean_absolute_error: 367.354797, mean_q: -2.550019\n",
      " 3992/5000: episode: 3991, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.893, 0.981], loss: 6165.116211, mean_absolute_error: 367.462769, mean_q: -2.559587\n",
      " 3993/5000: episode: 3992, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.973 [0.937, 0.997], loss: 3.317155, mean_absolute_error: 364.923340, mean_q: -2.574716\n",
      " 3994/5000: episode: 3993, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.637 [0.543, 0.729], loss: 3.367453, mean_absolute_error: 365.055664, mean_q: -2.594170\n",
      " 3995/5000: episode: 3994, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.230, 0.410], loss: 3.394613, mean_absolute_error: 365.155579, mean_q: -2.604614\n",
      " 3996/5000: episode: 3995, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 3.403211, mean_absolute_error: 365.230286, mean_q: -2.607912\n",
      " 3997/5000: episode: 3996, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.497 [0.400, 0.594], loss: 3.395619, mean_absolute_error: 365.286682, mean_q: -2.605000\n",
      " 3998/5000: episode: 3997, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.987, 1.000], loss: 3.377489, mean_absolute_error: 365.330597, mean_q: -2.598034\n",
      " 3999/5000: episode: 3998, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 6154.822754, mean_absolute_error: 368.020874, mean_q: -2.589059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4000/5000: episode: 3999, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.065], loss: 1345304.375000, mean_absolute_error: 454.542236, mean_q: -2.577692\n",
      " 4001/5000: episode: 4000, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.664, 0.831], loss: 14463.183594, mean_absolute_error: 368.225677, mean_q: -2.589227\n",
      " 4002/5000: episode: 4001, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.787 [0.704, 0.862], loss: 3.363716, mean_absolute_error: 365.560913, mean_q: -2.592730\n",
      " 4003/5000: episode: 4002, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.963 [0.922, 0.992], loss: 3.361179, mean_absolute_error: 365.620544, mean_q: -2.591751\n",
      " 4004/5000: episode: 4003, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.627 [0.532, 0.719], loss: 8256.682617, mean_absolute_error: 365.723206, mean_q: -2.587331\n",
      " 4005/5000: episode: 4004, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.607 [0.511, 0.700], loss: 12293.729492, mean_absolute_error: 371.003174, mean_q: -2.578101\n",
      " 4006/5000: episode: 4005, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.239, 0.421], loss: 3.292130, mean_absolute_error: 365.749298, mean_q: -2.564981\n",
      " 4007/5000: episode: 4006, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.378 [0.286, 0.474], loss: 3.254511, mean_absolute_error: 365.789246, mean_q: -2.550279\n",
      " 4008/5000: episode: 4007, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.708 [0.617, 0.793], loss: 3.215790, mean_absolute_error: 365.819397, mean_q: -2.535056\n",
      " 4009/5000: episode: 4008, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.460 [0.364, 0.557], loss: 3.208234, mean_absolute_error: 365.878693, mean_q: -2.532075\n",
      " 4010/5000: episode: 4009, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.669 [0.576, 0.758], loss: 3.251811, mean_absolute_error: 365.992615, mean_q: -2.549220\n",
      " 4011/5000: episode: 4010, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.043], loss: 6139.701660, mean_absolute_error: 368.754181, mean_q: -2.569978\n",
      " 4012/5000: episode: 4011, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.131], loss: 3.338119, mean_absolute_error: 366.217834, mean_q: -2.582842\n",
      " 4013/5000: episode: 4012, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.031], loss: 8333.183594, mean_absolute_error: 366.369202, mean_q: -2.587925\n",
      " 4014/5000: episode: 4013, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.374 [0.282, 0.469], loss: 3.343903, mean_absolute_error: 366.344971, mean_q: -2.585079\n",
      " 4015/5000: episode: 4014, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 3.329195, mean_absolute_error: 366.384888, mean_q: -2.579386\n",
      " 4016/5000: episode: 4015, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 3.308529, mean_absolute_error: 366.415588, mean_q: -2.571364\n",
      " 4017/5000: episode: 4016, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.532, 0.720], loss: 3.283063, mean_absolute_error: 366.438904, mean_q: -2.561445\n",
      " 4018/5000: episode: 4017, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.050], loss: 3.253117, mean_absolute_error: 366.459869, mean_q: -2.549732\n",
      " 4019/5000: episode: 4018, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [0.091, 0.231], loss: 3.218800, mean_absolute_error: 366.480316, mean_q: -2.536242\n",
      " 4020/5000: episode: 4019, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.864 [0.793, 0.925], loss: 6132.296387, mean_absolute_error: 369.129211, mean_q: -2.521605\n",
      " 4021/5000: episode: 4020, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.176, 0.345], loss: 14371.867188, mean_absolute_error: 369.175903, mean_q: -2.505748\n",
      " 4022/5000: episode: 4021, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 3.094328, mean_absolute_error: 366.530212, mean_q: -2.486701\n",
      " 4023/5000: episode: 4022, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.922 [0.866, 0.968], loss: 3.046303, mean_absolute_error: 366.545502, mean_q: -2.467320\n",
      " 4024/5000: episode: 4023, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [0.054, 0.173], loss: 2.997838, mean_absolute_error: 366.561279, mean_q: -2.447607\n",
      " 4025/5000: episode: 4024, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 2.950126, mean_absolute_error: 366.575378, mean_q: -2.428043\n",
      " 4026/5000: episode: 4025, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.976, 1.000], loss: 2.902272, mean_absolute_error: 366.590393, mean_q: -2.408262\n",
      " 4027/5000: episode: 4026, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.520, 0.709], loss: 6124.322754, mean_absolute_error: 369.262085, mean_q: -2.400292\n",
      " 4028/5000: episode: 4027, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.969 [0.931, 0.995], loss: 2.883242, mean_absolute_error: 366.714355, mean_q: -2.400351\n",
      " 4029/5000: episode: 4028, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.897, 0.983], loss: 2.874946, mean_absolute_error: 366.776062, mean_q: -2.396894\n",
      " 4030/5000: episode: 4029, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.093], loss: 2.860567, mean_absolute_error: 366.824768, mean_q: -2.390889\n",
      " 4031/5000: episode: 4030, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.685, 0.848], loss: 8419.805664, mean_absolute_error: 367.004395, mean_q: -2.395666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4032/5000: episode: 4031, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.836, 0.952], loss: 2.897167, mean_absolute_error: 366.985199, mean_q: -2.406142\n",
      " 4033/5000: episode: 4032, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.042], loss: 12230.787109, mean_absolute_error: 372.286133, mean_q: -2.411430\n",
      " 4034/5000: episode: 4033, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 2.909225, mean_absolute_error: 367.119263, mean_q: -2.411146\n",
      " 4035/5000: episode: 4034, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.527 [0.430, 0.623], loss: 2.900974, mean_absolute_error: 367.169861, mean_q: -2.407723\n",
      " 4036/5000: episode: 4035, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.855 [0.783, 0.919], loss: 2.886541, mean_absolute_error: 367.210022, mean_q: -2.401724\n",
      " 4037/5000: episode: 4036, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 6112.906738, mean_absolute_error: 369.854248, mean_q: -2.393379\n",
      " 4038/5000: episode: 4037, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.939 [0.888, 0.979], loss: 12220.831055, mean_absolute_error: 372.497070, mean_q: -2.382311\n",
      " 4039/5000: episode: 4038, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.849 [0.775, 0.913], loss: 1350321.000000, mean_absolute_error: 458.989410, mean_q: -2.368520\n",
      " 4040/5000: episode: 4039, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.216, 0.394], loss: 14543.812500, mean_absolute_error: 370.155914, mean_q: -2.388610\n",
      " 4041/5000: episode: 4040, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.058 [0.019, 0.107], loss: 6106.123047, mean_absolute_error: 370.187012, mean_q: -2.408883\n",
      " 4042/5000: episode: 4041, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.330, 0.521], loss: 2.933364, mean_absolute_error: 367.698669, mean_q: -2.421133\n",
      " 4043/5000: episode: 4042, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.762 [0.676, 0.841], loss: 2.948743, mean_absolute_error: 367.793121, mean_q: -2.427474\n",
      " 4044/5000: episode: 4043, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.838 [0.763, 0.905], loss: 2.951115, mean_absolute_error: 367.865570, mean_q: -2.428451\n",
      " 4045/5000: episode: 4044, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.205 [0.131, 0.287], loss: 2.943078, mean_absolute_error: 367.920959, mean_q: -2.425140\n",
      " 4046/5000: episode: 4045, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.885 [0.819, 0.941], loss: 2.959773, mean_absolute_error: 368.000305, mean_q: -2.432012\n",
      " 4047/5000: episode: 4046, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.155, 0.319], loss: 2.993226, mean_absolute_error: 368.097900, mean_q: -2.445723\n",
      " 4048/5000: episode: 4047, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.479 [0.383, 0.576], loss: 12188.730469, mean_absolute_error: 373.367615, mean_q: -2.452855\n",
      " 4049/5000: episode: 4048, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.878 [0.810, 0.936], loss: 3.010199, mean_absolute_error: 368.256531, mean_q: -2.452650\n",
      " 4050/5000: episode: 4049, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.540 [0.443, 0.636], loss: 1343629.625000, mean_absolute_error: 457.338562, mean_q: -2.448383\n",
      " 4051/5000: episode: 4050, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [0.082, 0.217], loss: 6091.414062, mean_absolute_error: 371.015137, mean_q: -2.466156\n",
      " 4052/5000: episode: 4051, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.015], loss: 6089.820801, mean_absolute_error: 371.144440, mean_q: -2.489430\n",
      " 4053/5000: episode: 4052, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.555, 0.740], loss: 3.167164, mean_absolute_error: 368.700500, mean_q: -2.515809\n",
      " 4054/5000: episode: 4053, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.950 [0.904, 0.986], loss: 3.208123, mean_absolute_error: 368.808838, mean_q: -2.532031\n",
      " 4055/5000: episode: 4054, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.715 [0.625, 0.799], loss: 6085.649414, mean_absolute_error: 371.468842, mean_q: -2.538988\n",
      " 4056/5000: episode: 4055, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.090], loss: 8354.365234, mean_absolute_error: 368.985138, mean_q: -2.539329\n",
      " 4057/5000: episode: 4056, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.479 [0.382, 0.575], loss: 6083.219727, mean_absolute_error: 371.584473, mean_q: -2.533992\n",
      " 4058/5000: episode: 4057, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 6081.880859, mean_absolute_error: 371.628937, mean_q: -2.525259\n",
      " 4059/5000: episode: 4058, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 6080.119629, mean_absolute_error: 371.675842, mean_q: -2.513174\n",
      " 4060/5000: episode: 4059, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.695, 0.855], loss: 1343141.375000, mean_absolute_error: 458.136444, mean_q: -2.497299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4061/5000: episode: 4060, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 2686165.500000, mean_absolute_error: 547.222412, mean_q: -2.504480\n",
      " 4062/5000: episode: 4061, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.362 [0.271, 0.457], loss: 3.266829, mean_absolute_error: 369.469421, mean_q: -2.555102\n",
      " 4063/5000: episode: 4062, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [0.112, 0.261], loss: 3.363066, mean_absolute_error: 369.647400, mean_q: -2.592479\n",
      " 4064/5000: episode: 4063, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.277 [0.193, 0.366], loss: 1348809.125000, mean_absolute_error: 461.320862, mean_q: -2.618664\n",
      " 4065/5000: episode: 4064, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.961, 1.000], loss: 6067.253906, mean_absolute_error: 372.551819, mean_q: -2.659173\n",
      " 4066/5000: episode: 4065, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 6064.929199, mean_absolute_error: 372.712830, mean_q: -2.687582\n",
      " 4067/5000: episode: 4066, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.048], loss: 3.664088, mean_absolute_error: 370.291321, mean_q: -2.706060\n",
      " 4068/5000: episode: 4067, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.616 [0.520, 0.709], loss: 14566.256836, mean_absolute_error: 373.025940, mean_q: -2.716183\n",
      " 4069/5000: episode: 4068, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.042], loss: 1342335.125000, mean_absolute_error: 459.420685, mean_q: -2.717073\n",
      " 4070/5000: episode: 4069, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.094 [0.043, 0.154], loss: 1342251.625000, mean_absolute_error: 459.554016, mean_q: -2.738012\n",
      " 4071/5000: episode: 4070, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.899, 0.984], loss: 3.851517, mean_absolute_error: 370.803070, mean_q: -2.774434\n",
      " 4072/5000: episode: 4071, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.684 [0.591, 0.771], loss: 3.921222, mean_absolute_error: 370.942657, mean_q: -2.799437\n",
      " 4073/5000: episode: 4072, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.097 [0.045, 0.158], loss: 3.968005, mean_absolute_error: 371.053223, mean_q: -2.816092\n",
      " 4074/5000: episode: 4073, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.106], loss: 26762.167969, mean_absolute_error: 378.877319, mean_q: -2.825902\n",
      " 4075/5000: episode: 4074, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.066], loss: 1341879.500000, mean_absolute_error: 460.117798, mean_q: -2.825228\n",
      " 4076/5000: episode: 4075, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.894 [0.830, 0.948], loss: 4.046735, mean_absolute_error: 371.346222, mean_q: -2.843903\n",
      " 4077/5000: episode: 4076, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.583, 0.764], loss: 8407.066406, mean_absolute_error: 371.453369, mean_q: -2.854662\n",
      " 4078/5000: episode: 4077, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.080 [0.033, 0.137], loss: 6043.959961, mean_absolute_error: 374.075958, mean_q: -2.856736\n",
      " 4079/5000: episode: 4078, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.198 [0.125, 0.279], loss: 4.071196, mean_absolute_error: 371.614807, mean_q: -2.852488\n",
      " 4080/5000: episode: 4079, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.630 [0.535, 0.722], loss: 4.048141, mean_absolute_error: 371.671844, mean_q: -2.844397\n",
      " 4081/5000: episode: 4080, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.735, 0.885], loss: 14682.801758, mean_absolute_error: 374.356140, mean_q: -2.833399\n",
      " 4082/5000: episode: 4081, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.054], loss: 3.971246, mean_absolute_error: 371.766602, mean_q: -2.817243\n",
      " 4083/5000: episode: 4082, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.503 [0.407, 0.600], loss: 3.922312, mean_absolute_error: 371.806396, mean_q: -2.799826\n",
      " 4084/5000: episode: 4083, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.788 [0.705, 0.863], loss: 14517.376953, mean_absolute_error: 374.396210, mean_q: -2.781332\n",
      " 4085/5000: episode: 4084, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 3.810987, mean_absolute_error: 371.869934, mean_q: -2.759792\n",
      " 4086/5000: episode: 4085, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.073 [0.029, 0.128], loss: 1341520.750000, mean_absolute_error: 460.801514, mean_q: -2.751557\n",
      " 4087/5000: episode: 4086, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.092], loss: 3.857014, mean_absolute_error: 372.076202, mean_q: -2.776414\n",
      " 4088/5000: episode: 4087, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.275, 0.461], loss: 3.898562, mean_absolute_error: 372.196350, mean_q: -2.791333\n",
      " 4089/5000: episode: 4088, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 12045.939453, mean_absolute_error: 377.323608, mean_q: -2.798512\n",
      " 4090/5000: episode: 4089, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [0.072, 0.202], loss: 3.921041, mean_absolute_error: 372.377319, mean_q: -2.799372\n",
      " 4091/5000: episode: 4090, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.468 [0.372, 0.565], loss: 3.909955, mean_absolute_error: 372.442535, mean_q: -2.795410\n",
      " 4092/5000: episode: 4091, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.581, 0.763], loss: 3.888229, mean_absolute_error: 372.491089, mean_q: -2.787630\n",
      " 4093/5000: episode: 4092, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.066 [0.024, 0.119], loss: 3.860333, mean_absolute_error: 372.529907, mean_q: -2.777609\n",
      " 4094/5000: episode: 4093, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.178, 0.348], loss: 1341149.000000, mean_absolute_error: 461.406128, mean_q: -2.765807\n",
      " 4095/5000: episode: 4094, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.533, 0.720], loss: 6017.437012, mean_absolute_error: 375.163147, mean_q: -2.776131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4096/5000: episode: 4095, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.773 [0.689, 0.850], loss: 3.864938, mean_absolute_error: 372.743042, mean_q: -2.779265\n",
      " 4097/5000: episode: 4096, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.784 [0.701, 0.860], loss: 3.860959, mean_absolute_error: 372.814484, mean_q: -2.777834\n",
      " 4098/5000: episode: 4097, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [0.095, 0.237], loss: 1340963.750000, mean_absolute_error: 461.707611, mean_q: -2.772667\n",
      " 4099/5000: episode: 4098, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.191, 0.364], loss: 6011.278320, mean_absolute_error: 375.495148, mean_q: -2.788320\n",
      " 4100/5000: episode: 4099, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.707 [0.616, 0.792], loss: 6009.129395, mean_absolute_error: 375.601929, mean_q: -2.795401\n",
      " 4101/5000: episode: 4100, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.690 [0.598, 0.777], loss: 8669.394531, mean_absolute_error: 373.285889, mean_q: -2.795868\n",
      " 4102/5000: episode: 4101, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [0.096, 0.238], loss: 3.896712, mean_absolute_error: 373.274048, mean_q: -2.790670\n",
      " 4103/5000: episode: 4102, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.700, 0.859], loss: 3.872015, mean_absolute_error: 373.329987, mean_q: -2.781810\n",
      " 4104/5000: episode: 4103, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.577 [0.480, 0.671], loss: 3.836389, mean_absolute_error: 373.367737, mean_q: -2.768978\n",
      " 4105/5000: episode: 4104, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.050, 0.167], loss: 3.795277, mean_absolute_error: 373.394562, mean_q: -2.754096\n",
      " 4106/5000: episode: 4105, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.206, 0.382], loss: 17993.189453, mean_absolute_error: 380.883667, mean_q: -2.738348\n",
      " 4107/5000: episode: 4106, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.206, 0.382], loss: 8558.602539, mean_absolute_error: 373.474182, mean_q: -2.720004\n",
      " 4108/5000: episode: 4107, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.306 [0.220, 0.398], loss: 3.646909, mean_absolute_error: 373.461060, mean_q: -2.699707\n",
      " 4109/5000: episode: 4108, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.680 [0.587, 0.768], loss: 8660.999023, mean_absolute_error: 373.552063, mean_q: -2.679170\n",
      " 4110/5000: episode: 4109, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.872, 0.971], loss: 8558.700195, mean_absolute_error: 373.515381, mean_q: -2.656544\n",
      " 4111/5000: episode: 4110, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.179 [0.110, 0.257], loss: 3.466445, mean_absolute_error: 373.491791, mean_q: -2.632038\n",
      " 4112/5000: episode: 4111, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.835 [0.759, 0.902], loss: 3.401825, mean_absolute_error: 373.494507, mean_q: -2.607381\n",
      " 4113/5000: episode: 4112, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.211 [0.136, 0.294], loss: 3.338459, mean_absolute_error: 373.492920, mean_q: -2.582973\n",
      " 4114/5000: episode: 4113, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [0.008, 0.080], loss: 1340677.875000, mean_absolute_error: 462.303101, mean_q: -2.559809\n",
      " 4115/5000: episode: 4114, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.904 [0.842, 0.955], loss: 3.284622, mean_absolute_error: 373.556183, mean_q: -2.562054\n",
      " 4116/5000: episode: 4115, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.897, 0.983], loss: 8562.943359, mean_absolute_error: 373.638153, mean_q: -2.560434\n",
      " 4117/5000: episode: 4116, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.836, 0.952], loss: 5989.316895, mean_absolute_error: 376.121704, mean_q: -2.553843\n",
      " 4118/5000: episode: 4117, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.231 [0.153, 0.316], loss: 3.239667, mean_absolute_error: 373.675781, mean_q: -2.544454\n",
      " 4119/5000: episode: 4118, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.690, 0.851], loss: 5987.946289, mean_absolute_error: 376.177185, mean_q: -2.533362\n",
      " 4120/5000: episode: 4119, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.840 [0.765, 0.907], loss: 3.209539, mean_absolute_error: 373.765198, mean_q: -2.532590\n",
      " 4121/5000: episode: 4120, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.037], loss: 2680895.250000, mean_absolute_error: 551.445435, mean_q: -2.539503\n",
      " 4122/5000: episode: 4121, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.039], loss: 8744.548828, mean_absolute_error: 374.159882, mean_q: -2.589845\n",
      " 4123/5000: episode: 4122, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.250 [0.169, 0.337], loss: 5982.034180, mean_absolute_error: 376.690979, mean_q: -2.624969\n",
      " 4124/5000: episode: 4123, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.664 [0.571, 0.753], loss: 3.511526, mean_absolute_error: 374.357361, mean_q: -2.649104\n",
      " 4125/5000: episode: 4124, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.846 [0.773, 0.911], loss: 3.553165, mean_absolute_error: 374.469269, mean_q: -2.664770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4126/5000: episode: 4125, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 3.576955, mean_absolute_error: 374.561646, mean_q: -2.673679\n",
      " 4127/5000: episode: 4126, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 3.586749, mean_absolute_error: 374.634613, mean_q: -2.677339\n",
      " 4128/5000: episode: 4127, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.347 [0.257, 0.441], loss: 3.619459, mean_absolute_error: 374.728485, mean_q: -2.689524\n",
      " 4129/5000: episode: 4128, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.940, 0.997], loss: 5974.412109, mean_absolute_error: 377.297577, mean_q: -2.707276\n",
      " 4130/5000: episode: 4129, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 3.694057, mean_absolute_error: 374.927795, mean_q: -2.717109\n",
      " 4131/5000: episode: 4130, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.382 [0.290, 0.478], loss: 5972.050781, mean_absolute_error: 377.462952, mean_q: -2.721135\n",
      " 4132/5000: episode: 4131, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.020], loss: 11938.033203, mean_absolute_error: 379.984375, mean_q: -2.720169\n",
      " 4133/5000: episode: 4132, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.898 [0.835, 0.951], loss: 1339638.625000, mean_absolute_error: 463.866669, mean_q: -2.714493\n",
      " 4134/5000: episode: 4133, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.902, 0.985], loss: 5967.721191, mean_absolute_error: 377.701782, mean_q: -2.730159\n",
      " 4135/5000: episode: 4134, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.054 [0.017, 0.102], loss: 3.785656, mean_absolute_error: 375.386414, mean_q: -2.750602\n",
      " 4136/5000: episode: 4135, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.138, 0.296], loss: 8634.542969, mean_absolute_error: 375.550659, mean_q: -2.774651\n",
      " 4137/5000: episode: 4136, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.410 [0.316, 0.506], loss: 11920.283203, mean_absolute_error: 380.542664, mean_q: -2.788290\n",
      " 4138/5000: episode: 4137, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.901 [0.839, 0.953], loss: 3.904290, mean_absolute_error: 375.753387, mean_q: -2.793384\n",
      " 4139/5000: episode: 4138, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.196 [0.123, 0.277], loss: 8623.656250, mean_absolute_error: 375.855469, mean_q: -2.792500\n",
      " 4140/5000: episode: 4139, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 5955.182617, mean_absolute_error: 378.365234, mean_q: -2.785304\n",
      " 4141/5000: episode: 4140, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 14731.421875, mean_absolute_error: 378.506866, mean_q: -2.774792\n",
      " 4142/5000: episode: 4141, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.038, 0.146], loss: 3.808976, mean_absolute_error: 376.042114, mean_q: -2.759064\n",
      " 4143/5000: episode: 4142, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.821 [0.743, 0.891], loss: 3.759384, mean_absolute_error: 376.083130, mean_q: -2.741037\n",
      " 4144/5000: episode: 4143, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.380 [0.287, 0.475], loss: 3.707043, mean_absolute_error: 376.122040, mean_q: -2.721882\n",
      " 4145/5000: episode: 4144, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 3.653224, mean_absolute_error: 376.156952, mean_q: -2.702044\n",
      " 4146/5000: episode: 4145, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.021], loss: 3.633873, mean_absolute_error: 376.221863, mean_q: -2.694876\n",
      " 4147/5000: episode: 4146, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.476 [0.380, 0.573], loss: 3.639629, mean_absolute_error: 376.305939, mean_q: -2.697010\n",
      " 4148/5000: episode: 4147, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.968, 1.000], loss: 3.633767, mean_absolute_error: 376.374939, mean_q: -2.694837\n",
      " 4149/5000: episode: 4148, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.624 [0.529, 0.716], loss: 3.618391, mean_absolute_error: 376.431732, mean_q: -2.689127\n",
      " 4150/5000: episode: 4149, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.106 [0.052, 0.170], loss: 8860.007812, mean_absolute_error: 376.578979, mean_q: -2.680955\n",
      " 4151/5000: episode: 4150, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 3.563993, mean_absolute_error: 376.506378, mean_q: -2.668829\n",
      " 4152/5000: episode: 4151, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 3.528347, mean_absolute_error: 376.529053, mean_q: -2.655444\n",
      " 4153/5000: episode: 4152, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.038], loss: 3.490210, mean_absolute_error: 376.545746, mean_q: -2.641048\n",
      " 4154/5000: episode: 4153, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.340 [0.251, 0.434], loss: 3.449984, mean_absolute_error: 376.558685, mean_q: -2.625779\n",
      " 4155/5000: episode: 4154, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.869 [0.800, 0.929], loss: 3.407756, mean_absolute_error: 376.570221, mean_q: -2.609653\n",
      " 4156/5000: episode: 4155, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.572 [0.475, 0.667], loss: 3.362912, mean_absolute_error: 376.583008, mean_q: -2.592420\n",
      " 4157/5000: episode: 4156, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 3.316873, mean_absolute_error: 376.593872, mean_q: -2.574606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4158/5000: episode: 4157, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.866 [0.796, 0.927], loss: 3.268421, mean_absolute_error: 376.599304, mean_q: -2.555725\n",
      " 4159/5000: episode: 4158, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [0.098, 0.241], loss: 3.218362, mean_absolute_error: 376.599091, mean_q: -2.536070\n",
      " 4160/5000: episode: 4159, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.678 [0.585, 0.766], loss: 3.201235, mean_absolute_error: 376.637024, mean_q: -2.529310\n",
      " 4161/5000: episode: 4160, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 3.206575, mean_absolute_error: 376.702911, mean_q: -2.531420\n",
      " 4162/5000: episode: 4161, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.956 [0.911, 0.988], loss: 1344684.250000, mean_absolute_error: 467.849426, mean_q: -2.529095\n",
      " 4163/5000: episode: 4162, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.747 [0.660, 0.828], loss: 14762.934570, mean_absolute_error: 379.378540, mean_q: -2.547256\n",
      " 4164/5000: episode: 4163, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.878 [0.810, 0.936], loss: 1338624.875000, mean_absolute_error: 465.645050, mean_q: -2.555644\n",
      " 4165/5000: episode: 4164, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.522, 0.710], loss: 3.338878, mean_absolute_error: 377.129822, mean_q: -2.583136\n",
      " 4166/5000: episode: 4165, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.092 [0.042, 0.153], loss: 3.386633, mean_absolute_error: 377.251984, mean_q: -2.601550\n",
      " 4167/5000: episode: 4166, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.495, 0.685], loss: 3.415864, mean_absolute_error: 377.351379, mean_q: -2.612758\n",
      " 4168/5000: episode: 4167, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.507, 0.697], loss: 3.429503, mean_absolute_error: 377.434448, mean_q: -2.617970\n",
      " 4169/5000: episode: 4168, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.806 [0.726, 0.878], loss: 3.464408, mean_absolute_error: 377.537659, mean_q: -2.631265\n",
      " 4170/5000: episode: 4169, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.914, 0.990], loss: 3.514089, mean_absolute_error: 377.653595, mean_q: -2.650071\n",
      " 4171/5000: episode: 4170, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.714, 0.870], loss: 3.544110, mean_absolute_error: 377.747681, mean_q: -2.661371\n",
      " 4172/5000: episode: 4171, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.372 [0.280, 0.467], loss: 3.557376, mean_absolute_error: 377.828094, mean_q: -2.666349\n",
      " 4173/5000: episode: 4172, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.911 [0.851, 0.960], loss: 5914.141602, mean_absolute_error: 380.290588, mean_q: -2.666494\n",
      " 4174/5000: episode: 4173, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.972, 1.000], loss: 3.546944, mean_absolute_error: 377.951172, mean_q: -2.662436\n",
      " 4175/5000: episode: 4174, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.042 [0.010, 0.085], loss: 3.528497, mean_absolute_error: 377.995544, mean_q: -2.655500\n",
      " 4176/5000: episode: 4175, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.021], loss: 5911.218750, mean_absolute_error: 380.421631, mean_q: -2.645322\n",
      " 4177/5000: episode: 4176, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 3.463527, mean_absolute_error: 378.051361, mean_q: -2.630930\n",
      " 4178/5000: episode: 4177, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 5909.496094, mean_absolute_error: 380.461426, mean_q: -2.614441\n",
      " 4179/5000: episode: 4178, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.903, 0.985], loss: 5908.422852, mean_absolute_error: 380.480133, mean_q: -2.596585\n",
      " 4180/5000: episode: 4179, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.632, 0.805], loss: 8944.393555, mean_absolute_error: 378.213074, mean_q: -2.577950\n",
      " 4181/5000: episode: 4180, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 5906.239258, mean_absolute_error: 380.509064, mean_q: -2.557057\n",
      " 4182/5000: episode: 4181, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.593 [0.497, 0.687], loss: 3.217353, mean_absolute_error: 378.135101, mean_q: -2.535672\n",
      " 4183/5000: episode: 4182, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.623 [0.528, 0.715], loss: 11804.848633, mean_absolute_error: 382.915649, mean_q: -2.513917\n",
      " 4184/5000: episode: 4183, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 3.103949, mean_absolute_error: 378.154846, mean_q: -2.490565\n",
      " 4185/5000: episode: 4184, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.204 [0.130, 0.285], loss: 3.080839, mean_absolute_error: 378.199219, mean_q: -2.481273\n",
      " 4186/5000: episode: 4185, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.097 [0.046, 0.159], loss: 1346609.250000, mean_absolute_error: 466.891357, mean_q: -2.481786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4187/5000: episode: 4186, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.695, 0.856], loss: 1337799.375000, mean_absolute_error: 466.997345, mean_q: -2.501261\n",
      " 4188/5000: episode: 4187, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.259 [0.177, 0.347], loss: 5896.927246, mean_absolute_error: 380.940399, mean_q: -2.537821\n",
      " 4189/5000: episode: 4188, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.928, 0.994], loss: 3.287067, mean_absolute_error: 378.704712, mean_q: -2.563007\n",
      " 4190/5000: episode: 4189, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 5893.744629, mean_absolute_error: 381.193726, mean_q: -2.580041\n",
      " 4191/5000: episode: 4190, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [0.104, 0.249], loss: 3.355855, mean_absolute_error: 378.915649, mean_q: -2.589697\n",
      " 4192/5000: episode: 4191, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [0.086, 0.223], loss: 3.366178, mean_absolute_error: 378.994568, mean_q: -2.593678\n",
      " 4193/5000: episode: 4192, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.744 [0.657, 0.826], loss: 3.364852, mean_absolute_error: 379.060181, mean_q: -2.593167\n",
      " 4194/5000: episode: 4193, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.740, 0.888], loss: 5888.706055, mean_absolute_error: 381.483521, mean_q: -2.588866\n",
      " 4195/5000: episode: 4194, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.054 [0.017, 0.102], loss: 3.333015, mean_absolute_error: 379.163513, mean_q: -2.580866\n",
      " 4196/5000: episode: 4195, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.006, 0.073], loss: 5886.354492, mean_absolute_error: 381.569458, mean_q: -2.570911\n",
      " 4197/5000: episode: 4196, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.547 [0.450, 0.643], loss: 5885.105469, mean_absolute_error: 381.602417, mean_q: -2.557344\n",
      " 4198/5000: episode: 4197, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.406 [0.312, 0.502], loss: 3.230793, mean_absolute_error: 379.267334, mean_q: -2.540965\n",
      " 4199/5000: episode: 4198, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.179, 0.348], loss: 3.220440, mean_absolute_error: 379.330444, mean_q: -2.536889\n",
      " 4200/5000: episode: 4199, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.950 [0.904, 0.986], loss: 3.232369, mean_absolute_error: 379.415222, mean_q: -2.541585\n",
      " 4201/5000: episode: 4200, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 5879.490234, mean_absolute_error: 381.844910, mean_q: -2.541664\n",
      " 4202/5000: episode: 4201, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.400 [0.307, 0.496], loss: 5877.904297, mean_absolute_error: 381.905457, mean_q: -2.535860\n",
      " 4203/5000: episode: 4202, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.039], loss: 11749.020508, mean_absolute_error: 384.314850, mean_q: -2.525433\n",
      " 4204/5000: episode: 4203, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.512 [0.415, 0.608], loss: 14821.265625, mean_absolute_error: 382.088043, mean_q: -2.511966\n",
      " 4205/5000: episode: 4204, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.536, 0.723], loss: 3.114848, mean_absolute_error: 379.701324, mean_q: -2.494936\n",
      " 4206/5000: episode: 4205, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.258 [0.177, 0.346], loss: 5870.298340, mean_absolute_error: 382.120850, mean_q: -2.490790\n",
      " 4207/5000: episode: 4206, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.262, 0.446], loss: 11733.778320, mean_absolute_error: 384.560089, mean_q: -2.495189\n",
      " 4208/5000: episode: 4207, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.100 [0.047, 0.162], loss: 1336932.750000, mean_absolute_error: 468.491638, mean_q: -2.494039\n",
      " 4209/5000: episode: 4208, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.120, 0.271], loss: 3.162809, mean_absolute_error: 380.087677, mean_q: -2.514078\n",
      " 4210/5000: episode: 4209, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.945, 0.998], loss: 3.190927, mean_absolute_error: 380.200012, mean_q: -2.525233\n",
      " 4211/5000: episode: 4210, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.018], loss: 1336735.625000, mean_absolute_error: 468.819214, mean_q: -2.529566\n",
      " 4212/5000: episode: 4211, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.877 [0.809, 0.935], loss: 5858.917969, mean_absolute_error: 382.768677, mean_q: -2.554374\n",
      " 4213/5000: episode: 4212, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 3.304973, mean_absolute_error: 380.552002, mean_q: -2.569982\n",
      " 4214/5000: episode: 4213, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.129, 0.284], loss: 11707.734375, mean_absolute_error: 385.316589, mean_q: -2.578900\n",
      " 4215/5000: episode: 4214, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.941, 0.997], loss: 3.334052, mean_absolute_error: 380.738342, mean_q: -2.581267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4216/5000: episode: 4215, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 5852.235840, mean_absolute_error: 383.138153, mean_q: -2.579739\n",
      " 4217/5000: episode: 4216, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.477 [0.380, 0.573], loss: 3.316183, mean_absolute_error: 380.868652, mean_q: -2.574338\n",
      " 4218/5000: episode: 4217, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.106], loss: 11695.831055, mean_absolute_error: 385.570099, mean_q: -2.566410\n",
      " 4219/5000: episode: 4218, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.699, 0.858], loss: 3.265223, mean_absolute_error: 380.967834, mean_q: -2.554474\n",
      " 4220/5000: episode: 4219, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.065 [0.024, 0.118], loss: 3.230161, mean_absolute_error: 381.012146, mean_q: -2.540716\n",
      " 4221/5000: episode: 4220, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.921 [0.865, 0.967], loss: 3.192924, mean_absolute_error: 381.047821, mean_q: -2.526024\n",
      " 4222/5000: episode: 4221, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.274, 0.461], loss: 3.154499, mean_absolute_error: 381.075409, mean_q: -2.510772\n",
      " 4223/5000: episode: 4222, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 3.114831, mean_absolute_error: 381.096100, mean_q: -2.494929\n",
      " 4224/5000: episode: 4223, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.405 [0.311, 0.501], loss: 3.072646, mean_absolute_error: 381.117035, mean_q: -2.477970\n",
      " 4225/5000: episode: 4224, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.036], loss: 9001.322266, mean_absolute_error: 381.206299, mean_q: -2.460246\n",
      " 4226/5000: episode: 4225, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 14680.338867, mean_absolute_error: 383.462585, mean_q: -2.440514\n",
      " 4227/5000: episode: 4226, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.401 [0.307, 0.497], loss: 1342119.625000, mean_absolute_error: 471.962830, mean_q: -2.418472\n",
      " 4228/5000: episode: 4227, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.769 [0.684, 0.847], loss: 11671.412109, mean_absolute_error: 385.894043, mean_q: -2.434700\n",
      " 4229/5000: episode: 4228, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.365 [0.273, 0.460], loss: 5835.262695, mean_absolute_error: 383.712769, mean_q: -2.454962\n",
      " 4230/5000: episode: 4229, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.062 [0.022, 0.114], loss: 3.046519, mean_absolute_error: 381.514069, mean_q: -2.467408\n",
      " 4231/5000: episode: 4230, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.210 [0.135, 0.292], loss: 5831.938965, mean_absolute_error: 383.912140, mean_q: -2.474113\n",
      " 4232/5000: episode: 4231, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.942, 0.998], loss: 3.066599, mean_absolute_error: 381.680786, mean_q: -2.475530\n",
      " 4233/5000: episode: 4232, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.436 [0.340, 0.532], loss: 3.058428, mean_absolute_error: 381.738495, mean_q: -2.472228\n",
      " 4234/5000: episode: 4233, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.065 [0.024, 0.117], loss: 3.041049, mean_absolute_error: 381.781494, mean_q: -2.465191\n",
      " 4235/5000: episode: 4234, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.293 [0.208, 0.384], loss: 3.017919, mean_absolute_error: 381.817444, mean_q: -2.455794\n",
      " 4236/5000: episode: 4235, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.552, 0.737], loss: 5826.412598, mean_absolute_error: 384.151825, mean_q: -2.444093\n",
      " 4237/5000: episode: 4236, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [0.070, 0.199], loss: 2.955676, mean_absolute_error: 381.882416, mean_q: -2.430327\n",
      " 4238/5000: episode: 4237, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 1341671.750000, mean_absolute_error: 472.670837, mean_q: -2.414331\n",
      " 4239/5000: episode: 4238, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.338 [0.248, 0.431], loss: 5822.388184, mean_absolute_error: 384.297668, mean_q: -2.421226\n",
      " 4240/5000: episode: 4239, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 2.934944, mean_absolute_error: 382.076538, mean_q: -2.421785\n",
      " 4241/5000: episode: 4240, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 2.925702, mean_absolute_error: 382.134155, mean_q: -2.417968\n",
      " 4242/5000: episode: 4241, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.557 [0.460, 0.652], loss: 2.907619, mean_absolute_error: 382.176636, mean_q: -2.410480\n",
      " 4243/5000: episode: 4242, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.721 [0.631, 0.805], loss: 2.914414, mean_absolute_error: 382.244080, mean_q: -2.413297\n",
      " 4244/5000: episode: 4243, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.018], loss: 5816.416992, mean_absolute_error: 384.625732, mean_q: -2.423812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4245/5000: episode: 4244, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [0.083, 0.219], loss: 2.950072, mean_absolute_error: 382.413055, mean_q: -2.428021\n",
      " 4246/5000: episode: 4245, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.975 [0.940, 0.997], loss: 2.950151, mean_absolute_error: 382.474976, mean_q: -2.428054\n",
      " 4247/5000: episode: 4246, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.667 [0.574, 0.756], loss: 2.936999, mean_absolute_error: 382.517029, mean_q: -2.422633\n",
      " 4248/5000: episode: 4247, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 2.913792, mean_absolute_error: 382.546936, mean_q: -2.413039\n",
      " 4249/5000: episode: 4248, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.965, 1.000], loss: 5812.003418, mean_absolute_error: 384.854736, mean_q: -2.401859\n",
      " 4250/5000: episode: 4249, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.482 [0.385, 0.578], loss: 2.887414, mean_absolute_error: 382.627014, mean_q: -2.402087\n",
      " 4251/5000: episode: 4250, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.562 [0.465, 0.657], loss: 11617.524414, mean_absolute_error: 387.276001, mean_q: -2.410816\n",
      " 4252/5000: episode: 4251, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.692 [0.601, 0.779], loss: 2.914419, mean_absolute_error: 382.779663, mean_q: -2.413299\n",
      " 4253/5000: episode: 4252, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.844 [0.769, 0.909], loss: 5807.958008, mean_absolute_error: 385.116821, mean_q: -2.410913\n",
      " 4254/5000: episode: 4253, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.846 [0.772, 0.911], loss: 9049.061523, mean_absolute_error: 382.934052, mean_q: -2.403934\n",
      " 4255/5000: episode: 4254, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.095 [0.044, 0.157], loss: 5805.858887, mean_absolute_error: 385.226135, mean_q: -2.406198\n",
      " 4256/5000: episode: 4255, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.099 [0.047, 0.162], loss: 20577.267578, mean_absolute_error: 387.614563, mean_q: -2.415675\n",
      " 4257/5000: episode: 4256, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.602 [0.506, 0.695], loss: 2.921653, mean_absolute_error: 383.121704, mean_q: -2.416293\n",
      " 4258/5000: episode: 4257, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.915 [0.857, 0.963], loss: 2.914053, mean_absolute_error: 383.188171, mean_q: -2.413147\n",
      " 4259/5000: episode: 4258, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.088 [0.039, 0.148], loss: 1335057.375000, mean_absolute_error: 471.653534, mean_q: -2.406618\n",
      " 4260/5000: episode: 4259, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.654 [0.560, 0.744], loss: 1344147.000000, mean_absolute_error: 471.908813, mean_q: -2.435473\n",
      " 4261/5000: episode: 4260, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.631 [0.536, 0.723], loss: 5795.370117, mean_absolute_error: 385.898071, mean_q: -2.489831\n",
      " 4262/5000: episode: 4261, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [0.012, 0.090], loss: 14779.621094, mean_absolute_error: 386.095032, mean_q: -2.529102\n",
      " 4263/5000: episode: 4262, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.181], loss: 5790.397949, mean_absolute_error: 386.247009, mean_q: -2.553448\n",
      " 4264/5000: episode: 4263, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.070 [0.027, 0.124], loss: 11572.200195, mean_absolute_error: 388.643250, mean_q: -2.568480\n",
      " 4265/5000: episode: 4264, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.740 [0.653, 0.822], loss: 3.319971, mean_absolute_error: 384.245605, mean_q: -2.575809\n",
      " 4266/5000: episode: 4265, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.735, 0.885], loss: 1340171.125000, mean_absolute_error: 474.992737, mean_q: -2.591312\n",
      " 4267/5000: episode: 4266, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.519 [0.422, 0.616], loss: 3.477351, mean_absolute_error: 384.594543, mean_q: -2.636177\n",
      " 4268/5000: episode: 4267, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 9210.556641, mean_absolute_error: 384.851685, mean_q: -2.668944\n",
      " 4269/5000: episode: 4268, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.646 [0.551, 0.736], loss: 14797.632812, mean_absolute_error: 387.147125, mean_q: -2.689862\n",
      " 4270/5000: episode: 4269, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.020], loss: 3.648148, mean_absolute_error: 385.008911, mean_q: -2.700166\n",
      " 4271/5000: episode: 4270, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.539 [0.442, 0.635], loss: 14977.669922, mean_absolute_error: 387.415924, mean_q: -2.704578\n",
      " 4272/5000: episode: 4271, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.843 [0.769, 0.909], loss: 3.649542, mean_absolute_error: 385.163452, mean_q: -2.700682\n",
      " 4273/5000: episode: 4272, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.230, 0.410], loss: 3.627448, mean_absolute_error: 385.215240, mean_q: -2.692492\n",
      " 4274/5000: episode: 4273, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.044], loss: 3.633243, mean_absolute_error: 385.297791, mean_q: -2.694642\n",
      " 4275/5000: episode: 4274, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.701 [0.610, 0.787], loss: 1333788.125000, mean_absolute_error: 473.711700, mean_q: -2.703110\n",
      " 4276/5000: episode: 4275, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.544 [0.447, 0.640], loss: 9060.915039, mean_absolute_error: 385.542664, mean_q: -2.730177\n",
      " 4277/5000: episode: 4276, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.590 [0.494, 0.684], loss: 3.774037, mean_absolute_error: 385.651672, mean_q: -2.746376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4278/5000: episode: 4277, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.181], loss: 5763.182129, mean_absolute_error: 387.971191, mean_q: -2.755756\n",
      " 4279/5000: episode: 4278, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 3.808631, mean_absolute_error: 385.814880, mean_q: -2.758939\n",
      " 4280/5000: episode: 4279, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.105], loss: 3.805536, mean_absolute_error: 385.874573, mean_q: -2.757817\n",
      " 4281/5000: episode: 4280, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [0.085, 0.222], loss: 3.792474, mean_absolute_error: 385.920868, mean_q: -2.753078\n",
      " 4282/5000: episode: 4281, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.352 [0.261, 0.446], loss: 3.771441, mean_absolute_error: 385.958038, mean_q: -2.745431\n",
      " 4283/5000: episode: 4282, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 5759.309082, mean_absolute_error: 388.213806, mean_q: -2.735430\n",
      " 4284/5000: episode: 4283, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.488 [0.391, 0.585], loss: 5758.623535, mean_absolute_error: 388.239716, mean_q: -2.722968\n",
      " 4285/5000: episode: 4284, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.114 [0.058, 0.180], loss: 3.670491, mean_absolute_error: 386.037445, mean_q: -2.708425\n",
      " 4286/5000: episode: 4285, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.357, 0.549], loss: 3.627779, mean_absolute_error: 386.058472, mean_q: -2.692615\n",
      " 4287/5000: episode: 4286, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [0.088, 0.227], loss: 2672456.500000, mean_absolute_error: 564.870483, mean_q: -2.675732\n",
      " 4288/5000: episode: 4287, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.021], loss: 3.665593, mean_absolute_error: 386.238770, mean_q: -2.706616\n",
      " 4289/5000: episode: 4288, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [0.074, 0.206], loss: 1342495.875000, mean_absolute_error: 474.745392, mean_q: -2.727381\n",
      " 4290/5000: episode: 4289, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [0.091, 0.231], loss: 9302.270508, mean_absolute_error: 386.633728, mean_q: -2.762396\n",
      " 4291/5000: episode: 4290, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [0.009, 0.082], loss: 5748.962891, mean_absolute_error: 388.905701, mean_q: -2.784462\n",
      " 4292/5000: episode: 4291, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.186 [0.115, 0.265], loss: 18419.097656, mean_absolute_error: 386.918884, mean_q: -2.810770\n",
      " 4293/5000: episode: 4292, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.728 [0.639, 0.811], loss: 11486.699219, mean_absolute_error: 391.411316, mean_q: -2.836034\n",
      " 4294/5000: episode: 4293, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.013, 0.093], loss: 4.104810, mean_absolute_error: 387.147827, mean_q: -2.864244\n",
      " 4295/5000: episode: 4294, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.386 [0.293, 0.481], loss: 4.230056, mean_absolute_error: 387.345276, mean_q: -2.907627\n",
      " 4296/5000: episode: 4295, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.767 [0.682, 0.845], loss: 4.355703, mean_absolute_error: 387.537811, mean_q: -2.950509\n",
      " 4297/5000: episode: 4296, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.791 [0.709, 0.866], loss: 4.446986, mean_absolute_error: 387.691956, mean_q: -2.981277\n",
      " 4298/5000: episode: 4297, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.349 [0.259, 0.444], loss: 9405.934570, mean_absolute_error: 387.917114, mean_q: -3.001905\n",
      " 4299/5000: episode: 4298, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 9342.508789, mean_absolute_error: 387.984009, mean_q: -3.012221\n",
      " 4300/5000: episode: 4299, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.220, 0.398], loss: 4.546789, mean_absolute_error: 387.986145, mean_q: -3.014556\n",
      " 4301/5000: episode: 4300, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.925, 0.993], loss: 4.539099, mean_absolute_error: 388.045013, mean_q: -3.012005\n",
      " 4302/5000: episode: 4301, duration: 0.072s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.093 [0.042, 0.153], loss: 9191.620117, mean_absolute_error: 388.095642, mean_q: -3.005267\n",
      " 4303/5000: episode: 4302, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.033], loss: 4.521072, mean_absolute_error: 388.170868, mean_q: -3.006016\n",
      " 4304/5000: episode: 4303, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [0.011, 0.087], loss: 1337689.625000, mean_absolute_error: 478.644562, mean_q: -3.013909\n",
      " 4305/5000: episode: 4304, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.976, 1.000], loss: 4.622952, mean_absolute_error: 388.415955, mean_q: -3.039708\n",
      " 4306/5000: episode: 4305, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.375, 0.568], loss: 4.673446, mean_absolute_error: 388.539032, mean_q: -3.056269\n",
      " 4307/5000: episode: 4306, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.403 [0.310, 0.500], loss: 1331727.500000, mean_absolute_error: 476.807526, mean_q: -3.065208\n",
      " 4308/5000: episode: 4307, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [0.099, 0.243], loss: 4.783788, mean_absolute_error: 388.793579, mean_q: -3.092150\n",
      " 4309/5000: episode: 4308, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.175, 0.344], loss: 1331551.750000, mean_absolute_error: 477.070190, mean_q: -3.108508\n",
      " 4310/5000: episode: 4309, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.919 [0.862, 0.966], loss: 4.936159, mean_absolute_error: 389.079559, mean_q: -3.141025\n",
      " 4311/5000: episode: 4310, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.918 [0.860, 0.965], loss: 20805.277344, mean_absolute_error: 393.632477, mean_q: -3.162846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4312/5000: episode: 4311, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.781 [0.698, 0.858], loss: 5.032854, mean_absolute_error: 389.322449, mean_q: -3.171650\n",
      " 4313/5000: episode: 4312, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.483 [0.387, 0.580], loss: 5716.629395, mean_absolute_error: 391.582886, mean_q: -3.172593\n",
      " 4314/5000: episode: 4313, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.190 [0.118, 0.269], loss: 9286.169922, mean_absolute_error: 389.479309, mean_q: -3.167421\n",
      " 4315/5000: episode: 4314, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.423 [0.329, 0.520], loss: 4.986613, mean_absolute_error: 389.516144, mean_q: -3.157042\n",
      " 4316/5000: episode: 4315, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.807 [0.727, 0.879], loss: 9337.634766, mean_absolute_error: 389.582397, mean_q: -3.144287\n",
      " 4317/5000: episode: 4316, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.301, 0.490], loss: 11419.794922, mean_absolute_error: 393.959961, mean_q: -3.140625\n",
      " 4318/5000: episode: 4317, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 4.985538, mean_absolute_error: 389.734192, mean_q: -3.156701\n",
      " 4319/5000: episode: 4318, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.013, 0.093], loss: 5.090865, mean_absolute_error: 389.901855, mean_q: -3.189882\n",
      " 4320/5000: episode: 4319, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.917, 0.991], loss: 9296.517578, mean_absolute_error: 390.068695, mean_q: -3.223984\n",
      " 4321/5000: episode: 4320, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 15181.013672, mean_absolute_error: 392.442688, mean_q: -3.245005\n",
      " 4322/5000: episode: 4321, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.066], loss: 5.297856, mean_absolute_error: 390.308716, mean_q: -3.254106\n",
      " 4323/5000: episode: 4322, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [0.095, 0.237], loss: 5.305797, mean_absolute_error: 390.400269, mean_q: -3.256544\n",
      " 4324/5000: episode: 4323, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.956, 1.000], loss: 15056.861328, mean_absolute_error: 392.654236, mean_q: -3.254103\n",
      " 4325/5000: episode: 4324, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.905, 0.986], loss: 5.268746, mean_absolute_error: 390.529083, mean_q: -3.245151\n",
      " 4326/5000: episode: 4325, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.046 [0.012, 0.091], loss: 5.229941, mean_absolute_error: 390.575989, mean_q: -3.233174\n",
      " 4327/5000: episode: 4326, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.982, 1.000], loss: 5.183650, mean_absolute_error: 390.614319, mean_q: -3.218829\n",
      " 4328/5000: episode: 4327, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.082 [0.035, 0.139], loss: 1330472.500000, mean_absolute_error: 478.722656, mean_q: -3.202482\n",
      " 4329/5000: episode: 4328, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.238 [0.159, 0.324], loss: 15026.046875, mean_absolute_error: 392.900208, mean_q: -3.209134\n",
      " 4330/5000: episode: 4329, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.020], loss: 9565.440430, mean_absolute_error: 390.915527, mean_q: -3.207595\n",
      " 4331/5000: episode: 4330, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [0.069, 0.198], loss: 5.122187, mean_absolute_error: 390.871857, mean_q: -3.199683\n",
      " 4332/5000: episode: 4331, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.651 [0.557, 0.742], loss: 17067.429688, mean_absolute_error: 397.375671, mean_q: -3.188689\n",
      " 4333/5000: episode: 4332, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.851 [0.778, 0.915], loss: 5690.772949, mean_absolute_error: 393.120056, mean_q: -3.173201\n",
      " 4334/5000: episode: 4333, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.796 [0.715, 0.870], loss: 5688.993164, mean_absolute_error: 393.159302, mean_q: -3.153286\n",
      " 4335/5000: episode: 4334, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.031], loss: 4.903011, mean_absolute_error: 391.040771, mean_q: -3.130457\n",
      " 4336/5000: episode: 4335, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.020], loss: 4.831697, mean_absolute_error: 391.064453, mean_q: -3.107600\n",
      " 4337/5000: episode: 4336, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.171, 0.338], loss: 5684.793457, mean_absolute_error: 393.226562, mean_q: -3.084869\n",
      " 4338/5000: episode: 4337, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.765 [0.680, 0.844], loss: 5683.610840, mean_absolute_error: 393.241913, mean_q: -3.061690\n",
      " 4339/5000: episode: 4338, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 4.616572, mean_absolute_error: 391.118805, mean_q: -3.037609\n",
      " 4340/5000: episode: 4339, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.106 [0.052, 0.170], loss: 4.544339, mean_absolute_error: 391.135162, mean_q: -3.013743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4341/5000: episode: 4340, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 4.472905, mean_absolute_error: 391.149658, mean_q: -2.989955\n",
      " 4342/5000: episode: 4341, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.968 [0.930, 0.995], loss: 4.402253, mean_absolute_error: 391.161652, mean_q: -2.966239\n",
      " 4343/5000: episode: 4342, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.916, 0.990], loss: 5677.808105, mean_absolute_error: 393.309723, mean_q: -2.942752\n",
      " 4344/5000: episode: 4343, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 9524.029297, mean_absolute_error: 391.300110, mean_q: -2.931710\n",
      " 4345/5000: episode: 4344, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.959 [0.917, 0.990], loss: 5675.203613, mean_absolute_error: 393.425385, mean_q: -2.928504\n",
      " 4346/5000: episode: 4345, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.211, 0.388], loss: 4.269820, mean_absolute_error: 391.349182, mean_q: -2.921267\n",
      " 4347/5000: episode: 4346, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.930 [0.876, 0.973], loss: 5672.769043, mean_absolute_error: 393.525146, mean_q: -2.910023\n",
      " 4348/5000: episode: 4347, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [0.076, 0.209], loss: 4.193373, mean_absolute_error: 391.432495, mean_q: -2.894988\n",
      " 4349/5000: episode: 4348, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.985 [0.957, 1.000], loss: 5670.248535, mean_absolute_error: 393.594147, mean_q: -2.878510\n",
      " 4350/5000: episode: 4349, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.133, 0.290], loss: 5668.759766, mean_absolute_error: 393.626648, mean_q: -2.859911\n",
      " 4351/5000: episode: 4350, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.957 [0.913, 0.989], loss: 5667.036133, mean_absolute_error: 393.660339, mean_q: -2.839667\n",
      " 4352/5000: episode: 4351, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.836, 0.952], loss: 5665.259277, mean_absolute_error: 393.692566, mean_q: -2.818441\n",
      " 4353/5000: episode: 4352, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [0.001, 0.048], loss: 3.949395, mean_absolute_error: 391.639404, mean_q: -2.809479\n",
      " 4354/5000: episode: 4353, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.894 [0.831, 0.948], loss: 3.950226, mean_absolute_error: 391.730469, mean_q: -2.809774\n",
      " 4355/5000: episode: 4354, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.514 [0.417, 0.611], loss: 3.940320, mean_absolute_error: 391.802856, mean_q: -2.806248\n",
      " 4356/5000: episode: 4355, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.095 [0.044, 0.156], loss: 5658.388184, mean_absolute_error: 393.978973, mean_q: -2.799157\n",
      " 4357/5000: episode: 4356, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.628 [0.533, 0.720], loss: 3.891078, mean_absolute_error: 391.916260, mean_q: -2.788651\n",
      " 4358/5000: episode: 4357, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 3.892174, mean_absolute_error: 391.998474, mean_q: -2.789044\n",
      " 4359/5000: episode: 4358, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.467 [0.371, 0.564], loss: 3.914500, mean_absolute_error: 392.098816, mean_q: -2.797035\n",
      " 4360/5000: episode: 4359, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [0.054, 0.174], loss: 3.921772, mean_absolute_error: 392.178772, mean_q: -2.799633\n",
      " 4361/5000: episode: 4360, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.533 [0.436, 0.629], loss: 5651.490723, mean_absolute_error: 394.350586, mean_q: -2.797941\n",
      " 4362/5000: episode: 4361, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.440 [0.344, 0.536], loss: 3.900714, mean_absolute_error: 392.296967, mean_q: -2.792104\n",
      " 4363/5000: episode: 4362, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [0.061, 0.185], loss: 3.875710, mean_absolute_error: 392.341888, mean_q: -2.783137\n",
      " 4364/5000: episode: 4363, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 11292.902344, mean_absolute_error: 396.590057, mean_q: -2.771811\n",
      " 4365/5000: episode: 4364, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.617 [0.521, 0.710], loss: 3.802598, mean_absolute_error: 392.423279, mean_q: -2.756752\n",
      " 4366/5000: episode: 4365, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [0.093, 0.233], loss: 5645.288086, mean_absolute_error: 394.562866, mean_q: -2.740177\n",
      " 4367/5000: episode: 4366, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 5643.793457, mean_absolute_error: 394.595886, mean_q: -2.722244\n",
      " 4368/5000: episode: 4367, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 1329524.125000, mean_absolute_error: 480.531677, mean_q: -2.702224\n",
      " 4369/5000: episode: 4368, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.984 [0.956, 1.000], loss: 3.664850, mean_absolute_error: 392.612549, mean_q: -2.706342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4370/5000: episode: 4369, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.001, 0.047], loss: 3.663783, mean_absolute_error: 392.683594, mean_q: -2.705948\n",
      " 4371/5000: episode: 4370, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.094 [0.043, 0.155], loss: 3.652822, mean_absolute_error: 392.738831, mean_q: -2.701896\n",
      " 4372/5000: episode: 4371, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.479, 0.670], loss: 5637.614746, mean_absolute_error: 394.879364, mean_q: -2.694564\n",
      " 4373/5000: episode: 4372, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.981 [0.952, 0.999], loss: 9596.011719, mean_absolute_error: 392.908203, mean_q: -2.683560\n",
      " 4374/5000: episode: 4373, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.905 [0.844, 0.956], loss: 1334960.250000, mean_absolute_error: 482.948914, mean_q: -2.668536\n",
      " 4375/5000: episode: 4374, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.898 [0.835, 0.951], loss: 9494.800781, mean_absolute_error: 392.996704, mean_q: -2.676788\n",
      " 4376/5000: episode: 4375, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.959, 1.000], loss: 1334851.875000, mean_absolute_error: 483.117889, mean_q: -2.677415\n",
      " 4377/5000: episode: 4376, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.013, 0.093], loss: 9598.395508, mean_absolute_error: 393.247925, mean_q: -2.697433\n",
      " 4378/5000: episode: 4377, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 3.666453, mean_absolute_error: 393.276031, mean_q: -2.706934\n",
      " 4379/5000: episode: 4378, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.790 [0.707, 0.865], loss: 3.677212, mean_absolute_error: 393.354675, mean_q: -2.710904\n",
      " 4380/5000: episode: 4379, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [0.103, 0.248], loss: 3.746422, mean_absolute_error: 393.489838, mean_q: -2.736306\n",
      " 4381/5000: episode: 4380, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.893 [0.829, 0.947], loss: 3.857575, mean_absolute_error: 393.663788, mean_q: -2.776616\n",
      " 4382/5000: episode: 4381, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.908 [0.848, 0.958], loss: 9650.132812, mean_absolute_error: 393.915375, mean_q: -2.818300\n",
      " 4383/5000: episode: 4382, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.541, 0.727], loss: 4.085735, mean_absolute_error: 394.010223, mean_q: -2.857578\n",
      " 4384/5000: episode: 4383, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.307], loss: 4.161978, mean_absolute_error: 394.145935, mean_q: -2.884127\n",
      " 4385/5000: episode: 4384, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.952, 0.999], loss: 9674.477539, mean_absolute_error: 394.330750, mean_q: -2.900496\n",
      " 4386/5000: episode: 4385, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.193 [0.121, 0.273], loss: 4.226511, mean_absolute_error: 394.344666, mean_q: -2.906408\n",
      " 4387/5000: episode: 4386, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.076 [0.031, 0.132], loss: 5617.517578, mean_absolute_error: 396.495514, mean_q: -2.905999\n",
      " 4388/5000: episode: 4387, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.822 [0.744, 0.892], loss: 15297.371094, mean_absolute_error: 396.639130, mean_q: -2.900741\n",
      " 4389/5000: episode: 4388, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.174, 0.343], loss: 4.175333, mean_absolute_error: 394.546692, mean_q: -2.888752\n",
      " 4390/5000: episode: 4389, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.596 [0.500, 0.690], loss: 4.126085, mean_absolute_error: 394.579285, mean_q: -2.871659\n",
      " 4391/5000: episode: 4390, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.582, 0.763], loss: 5611.685547, mean_absolute_error: 396.666504, mean_q: -2.851636\n",
      " 4392/5000: episode: 4391, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 4.009716, mean_absolute_error: 394.623169, mean_q: -2.830860\n",
      " 4393/5000: episode: 4392, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.051, 0.168], loss: 3.951135, mean_absolute_error: 394.639404, mean_q: -2.810098\n",
      " 4394/5000: episode: 4393, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.302 [0.216, 0.393], loss: 3.892583, mean_absolute_error: 394.653076, mean_q: -2.789191\n",
      " 4395/5000: episode: 4394, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.455, 0.648], loss: 5607.344238, mean_absolute_error: 396.761475, mean_q: -2.781054\n",
      " 4396/5000: episode: 4395, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.217, 0.394], loss: 3.907560, mean_absolute_error: 394.817505, mean_q: -2.794554\n",
      " 4397/5000: episode: 4396, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.929, 0.994], loss: 3.959160, mean_absolute_error: 394.944550, mean_q: -2.812951\n",
      " 4398/5000: episode: 4397, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.047 [0.013, 0.092], loss: 3.989597, mean_absolute_error: 395.048431, mean_q: -2.823747\n",
      " 4399/5000: episode: 4398, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 5601.241699, mean_absolute_error: 397.191223, mean_q: -2.827628\n",
      " 4400/5000: episode: 4399, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.872 [0.803, 0.932], loss: 3.994310, mean_absolute_error: 395.221832, mean_q: -2.825415\n",
      " 4401/5000: episode: 4400, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.441, 0.634], loss: 3.977147, mean_absolute_error: 395.289673, mean_q: -2.819336\n",
      " 4402/5000: episode: 4401, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.657 [0.563, 0.747], loss: 5596.262695, mean_absolute_error: 397.391968, mean_q: -2.810300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4403/5000: episode: 4402, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.618 [0.523, 0.711], loss: 5594.558105, mean_absolute_error: 397.444519, mean_q: -2.797750\n",
      " 4404/5000: episode: 4403, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.856 [0.784, 0.919], loss: 3.874897, mean_absolute_error: 395.447876, mean_q: -2.782845\n",
      " 4405/5000: episode: 4404, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.927 [0.872, 0.971], loss: 3.830561, mean_absolute_error: 395.485657, mean_q: -2.766873\n",
      " 4406/5000: episode: 4405, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.002, 0.059], loss: 3.784533, mean_absolute_error: 395.515656, mean_q: -2.750194\n",
      " 4407/5000: episode: 4406, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 9594.382812, mean_absolute_error: 395.555420, mean_q: -2.732828\n",
      " 4408/5000: episode: 4407, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.653 [0.559, 0.743], loss: 9798.373047, mean_absolute_error: 395.649536, mean_q: -2.711864\n",
      " 4409/5000: episode: 4408, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.260 [0.178, 0.348], loss: 3.614549, mean_absolute_error: 395.544647, mean_q: -2.687698\n",
      " 4410/5000: episode: 4409, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.837, 0.952], loss: 11170.247070, mean_absolute_error: 399.620972, mean_q: -2.663835\n",
      " 4411/5000: episode: 4410, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.901, 0.985], loss: 3.485089, mean_absolute_error: 395.558594, mean_q: -2.639110\n",
      " 4412/5000: episode: 4411, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.819 [0.741, 0.889], loss: 1327753.750000, mean_absolute_error: 483.449249, mean_q: -2.615064\n",
      " 4413/5000: episode: 4412, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.912 [0.853, 0.961], loss: 3.426137, mean_absolute_error: 395.649536, mean_q: -2.616685\n",
      " 4414/5000: episode: 4413, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.105 [0.051, 0.169], loss: 5581.796387, mean_absolute_error: 397.744843, mean_q: -2.614492\n",
      " 4415/5000: episode: 4414, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.208 [0.133, 0.290], loss: 5580.588867, mean_absolute_error: 397.798706, mean_q: -2.608352\n",
      " 4416/5000: episode: 4415, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.846 [0.773, 0.911], loss: 5579.348633, mean_absolute_error: 397.845398, mean_q: -2.599174\n",
      " 4417/5000: episode: 4416, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.864 [0.793, 0.925], loss: 3.347252, mean_absolute_error: 395.855591, mean_q: -2.586374\n",
      " 4418/5000: episode: 4417, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.634 [0.539, 0.726], loss: 5577.088379, mean_absolute_error: 397.909485, mean_q: -2.571448\n",
      " 4419/5000: episode: 4418, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.801 [0.721, 0.875], loss: 1333143.000000, mean_absolute_error: 485.800873, mean_q: -2.555115\n",
      " 4420/5000: episode: 4419, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.342 [0.252, 0.435], loss: 3.286340, mean_absolute_error: 396.003448, mean_q: -2.562724\n",
      " 4421/5000: episode: 4420, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.396, 0.589], loss: 3.293146, mean_absolute_error: 396.079468, mean_q: -2.565377\n",
      " 4422/5000: episode: 4421, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.495 [0.398, 0.592], loss: 5572.240234, mean_absolute_error: 398.163208, mean_q: -2.563622\n",
      " 4423/5000: episode: 4422, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.920 [0.863, 0.967], loss: 5570.831055, mean_absolute_error: 398.220306, mean_q: -2.557336\n",
      " 4424/5000: episode: 4423, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.099 [0.047, 0.161], loss: 3.248047, mean_absolute_error: 396.253845, mean_q: -2.547744\n",
      " 4425/5000: episode: 4424, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 3.219269, mean_absolute_error: 396.295898, mean_q: -2.536427\n",
      " 4426/5000: episode: 4425, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.987, 1.000], loss: 3.187037, mean_absolute_error: 396.328278, mean_q: -2.523693\n",
      " 4427/5000: episode: 4426, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.927, 0.994], loss: 3.151512, mean_absolute_error: 396.356079, mean_q: -2.509583\n",
      " 4428/5000: episode: 4427, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.107 [0.053, 0.171], loss: 3.113180, mean_absolute_error: 396.379700, mean_q: -2.494267\n",
      " 4429/5000: episode: 4428, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.589 [0.492, 0.683], loss: 3.073649, mean_absolute_error: 396.397156, mean_q: -2.478374\n",
      " 4430/5000: episode: 4429, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.814 [0.735, 0.885], loss: 3.033484, mean_absolute_error: 396.410706, mean_q: -2.462122\n",
      " 4431/5000: episode: 4430, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.444, 0.636], loss: 2.990872, mean_absolute_error: 396.425018, mean_q: -2.444760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4432/5000: episode: 4431, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.101 [0.048, 0.163], loss: 5562.127441, mean_absolute_error: 398.449768, mean_q: -2.426580\n",
      " 4433/5000: episode: 4432, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.360 [0.269, 0.454], loss: 5561.155762, mean_absolute_error: 398.463867, mean_q: -2.407877\n",
      " 4434/5000: episode: 4433, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.149], loss: 9663.242188, mean_absolute_error: 396.501404, mean_q: -2.388587\n",
      " 4435/5000: episode: 4434, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.954 [0.909, 0.988], loss: 2.835791, mean_absolute_error: 396.517090, mean_q: -2.380508\n",
      " 4436/5000: episode: 4435, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.126], loss: 2.840122, mean_absolute_error: 396.589172, mean_q: -2.382326\n",
      " 4437/5000: episode: 4436, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.281, 0.469], loss: 5556.770020, mean_absolute_error: 398.649902, mean_q: -2.380495\n",
      " 4438/5000: episode: 4437, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 5555.574219, mean_absolute_error: 398.701721, mean_q: -2.374657\n",
      " 4439/5000: episode: 4438, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 2.800264, mean_absolute_error: 396.746765, mean_q: -2.365543\n",
      " 4440/5000: episode: 4439, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.325 [0.237, 0.418], loss: 2.775068, mean_absolute_error: 396.784119, mean_q: -2.354873\n",
      " 4441/5000: episode: 4440, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.641, 0.812], loss: 1327086.000000, mean_absolute_error: 484.645966, mean_q: -2.342914\n",
      " 4442/5000: episode: 4441, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.160, 0.325], loss: 15340.995117, mean_absolute_error: 398.984924, mean_q: -2.354959\n",
      " 4443/5000: episode: 4442, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.439, 0.632], loss: 2.783275, mean_absolute_error: 396.988831, mean_q: -2.358354\n",
      " 4444/5000: episode: 4443, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.504 [0.408, 0.601], loss: 2.781868, mean_absolute_error: 397.051270, mean_q: -2.357758\n",
      " 4445/5000: episode: 4444, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.843 [0.769, 0.909], loss: 2.772948, mean_absolute_error: 397.100433, mean_q: -2.353972\n",
      " 4446/5000: episode: 4445, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.038, 0.144], loss: 2.758015, mean_absolute_error: 397.139496, mean_q: -2.347623\n",
      " 4447/5000: episode: 4446, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.825 [0.747, 0.894], loss: 2.737311, mean_absolute_error: 397.171875, mean_q: -2.338791\n",
      " 4448/5000: episode: 4447, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.128, 0.282], loss: 5545.497559, mean_absolute_error: 399.191528, mean_q: -2.328093\n",
      " 4449/5000: episode: 4448, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.189 [0.117, 0.268], loss: 5544.648438, mean_absolute_error: 399.216003, mean_q: -2.315384\n",
      " 4450/5000: episode: 4449, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.122, 0.275], loss: 5543.697754, mean_absolute_error: 399.239685, mean_q: -2.301147\n",
      " 4451/5000: episode: 4450, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.752 [0.666, 0.833], loss: 5542.666016, mean_absolute_error: 399.261688, mean_q: -2.285668\n",
      " 4452/5000: episode: 4451, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.281 [0.197, 0.371], loss: 2.577076, mean_absolute_error: 397.295319, mean_q: -2.269276\n",
      " 4453/5000: episode: 4452, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.285, 0.472], loss: 2.539745, mean_absolute_error: 397.312988, mean_q: -2.252772\n",
      " 4454/5000: episode: 4453, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 2.532824, mean_absolute_error: 397.364014, mean_q: -2.249700\n",
      " 4455/5000: episode: 4454, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.453, 0.646], loss: 5538.833496, mean_absolute_error: 399.422607, mean_q: -2.256107\n",
      " 4456/5000: episode: 4455, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.088 [0.039, 0.147], loss: 1326686.500000, mean_absolute_error: 485.305237, mean_q: -2.257423\n",
      " 4457/5000: episode: 4456, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.197 [0.124, 0.277], loss: 11070.739258, mean_absolute_error: 401.588531, mean_q: -2.280779\n",
      " 4458/5000: episode: 4457, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.663, 0.831], loss: 5535.218262, mean_absolute_error: 399.712769, mean_q: -2.295145\n",
      " 4459/5000: episode: 4458, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.907, 0.987], loss: 11064.911133, mean_absolute_error: 401.780579, mean_q: -2.302897\n",
      " 4460/5000: episode: 4459, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.824 [0.747, 0.894], loss: 2.657904, mean_absolute_error: 397.905151, mean_q: -2.304604\n",
      " 4461/5000: episode: 4460, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.107 [0.053, 0.171], loss: 5530.739258, mean_absolute_error: 399.945496, mean_q: -2.302702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4462/5000: episode: 4461, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.016], loss: 2.637687, mean_absolute_error: 398.025879, mean_q: -2.295818\n",
      " 4463/5000: episode: 4462, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.025], loss: 2.611705, mean_absolute_error: 398.065308, mean_q: -2.284478\n",
      " 4464/5000: episode: 4463, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [0.083, 0.219], loss: 2.580920, mean_absolute_error: 398.093750, mean_q: -2.270968\n",
      " 4465/5000: episode: 4464, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.712 [0.622, 0.797], loss: 1341766.500000, mean_absolute_error: 487.967896, mean_q: -2.256174\n",
      " 4466/5000: episode: 4465, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.056 [0.018, 0.104], loss: 5524.041016, mean_absolute_error: 400.184509, mean_q: -2.263067\n",
      " 4467/5000: episode: 4466, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.643, 0.815], loss: 2.566955, mean_absolute_error: 398.297852, mean_q: -2.264813\n",
      " 4468/5000: episode: 4467, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.358, 0.551], loss: 2.562853, mean_absolute_error: 398.362793, mean_q: -2.263002\n",
      " 4469/5000: episode: 4468, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.583 [0.487, 0.678], loss: 2.552304, mean_absolute_error: 398.414001, mean_q: -2.258338\n",
      " 4470/5000: episode: 4469, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [0.002, 0.058], loss: 2.536513, mean_absolute_error: 398.454895, mean_q: -2.251338\n",
      " 4471/5000: episode: 4470, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.315, 0.505], loss: 5518.270020, mean_absolute_error: 400.448090, mean_q: -2.242488\n",
      " 4472/5000: episode: 4471, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.045], loss: 1331633.375000, mean_absolute_error: 488.240417, mean_q: -2.231078\n",
      " 4473/5000: episode: 4472, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.988, 1.000], loss: 2.547904, mean_absolute_error: 398.662659, mean_q: -2.256390\n",
      " 4474/5000: episode: 4473, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.377 [0.285, 0.473], loss: 2.613113, mean_absolute_error: 398.817230, mean_q: -2.285094\n",
      " 4475/5000: episode: 4474, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.223 [0.146, 0.307], loss: 2.658535, mean_absolute_error: 398.944305, mean_q: -2.304877\n",
      " 4476/5000: episode: 4475, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.991 [0.970, 1.000], loss: 2.688011, mean_absolute_error: 399.045502, mean_q: -2.317625\n",
      " 4477/5000: episode: 4476, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.070 [0.027, 0.123], loss: 1331252.500000, mean_absolute_error: 488.812958, mean_q: -2.324650\n",
      " 4478/5000: episode: 4477, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.508 [0.411, 0.604], loss: 2.768170, mean_absolute_error: 399.267883, mean_q: -2.351943\n",
      " 4479/5000: episode: 4478, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.225 [0.148, 0.309], loss: 2.812507, mean_absolute_error: 399.381348, mean_q: -2.370711\n",
      " 4480/5000: episode: 4479, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 5505.668457, mean_absolute_error: 401.419128, mean_q: -2.382628\n",
      " 4481/5000: episode: 4480, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.192, 0.365], loss: 2.853258, mean_absolute_error: 399.551453, mean_q: -2.387832\n",
      " 4482/5000: episode: 4481, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 5503.356934, mean_absolute_error: 401.561432, mean_q: -2.388367\n",
      " 4483/5000: episode: 4482, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.902 [0.840, 0.954], loss: 2.843072, mean_absolute_error: 399.670441, mean_q: -2.383564\n",
      " 4484/5000: episode: 4483, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.643, 0.815], loss: 2.823397, mean_absolute_error: 399.709595, mean_q: -2.375299\n",
      " 4485/5000: episode: 4484, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.670 [0.577, 0.759], loss: 9760.340820, mean_absolute_error: 399.742188, mean_q: -2.365244\n",
      " 4486/5000: episode: 4485, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.062], loss: 2.799144, mean_absolute_error: 399.795502, mean_q: -2.365070\n",
      " 4487/5000: episode: 4486, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.045], loss: 2.818959, mean_absolute_error: 399.875977, mean_q: -2.373430\n",
      " 4488/5000: episode: 4487, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.234 [0.156, 0.319], loss: 2.826194, mean_absolute_error: 399.942657, mean_q: -2.376475\n",
      " 4489/5000: episode: 4488, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.850 [0.777, 0.915], loss: 5497.083496, mean_absolute_error: 401.934875, mean_q: -2.375210\n",
      " 4490/5000: episode: 4489, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.904 [0.843, 0.955], loss: 2.809541, mean_absolute_error: 400.050385, mean_q: -2.369460\n",
      " 4491/5000: episode: 4490, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.201 [0.128, 0.282], loss: 1325149.125000, mean_absolute_error: 487.786316, mean_q: -2.361116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4492/5000: episode: 4491, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.686 [0.594, 0.774], loss: 5493.391602, mean_absolute_error: 402.136841, mean_q: -2.376316\n",
      " 4493/5000: episode: 4492, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [0.062, 0.187], loss: 2.843955, mean_absolute_error: 400.300842, mean_q: -2.383934\n",
      " 4494/5000: episode: 4493, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.982, 1.000], loss: 2.849635, mean_absolute_error: 400.380432, mean_q: -2.386314\n",
      " 4495/5000: episode: 4494, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.943, 0.998], loss: 15301.046875, mean_absolute_error: 402.382141, mean_q: -2.384922\n",
      " 4496/5000: episode: 4495, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.947 [0.899, 0.984], loss: 2.824229, mean_absolute_error: 400.494263, mean_q: -2.375648\n",
      " 4497/5000: episode: 4496, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.081 [0.034, 0.139], loss: 9867.131836, mean_absolute_error: 400.565613, mean_q: -2.363152\n",
      " 4498/5000: episode: 4497, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [0.007, 0.076], loss: 2.754837, mean_absolute_error: 400.557556, mean_q: -2.346270\n",
      " 4499/5000: episode: 4498, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.751 [0.665, 0.832], loss: 2.712654, mean_absolute_error: 400.571838, mean_q: -2.328229\n",
      " 4500/5000: episode: 4499, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.328 [0.239, 0.421], loss: 2.671120, mean_absolute_error: 400.581970, mean_q: -2.310328\n",
      " 4501/5000: episode: 4500, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.873 [0.804, 0.932], loss: 5482.952637, mean_absolute_error: 402.510315, mean_q: -2.292414\n",
      " 4502/5000: episode: 4501, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.061], loss: 2.587761, mean_absolute_error: 400.598175, mean_q: -2.273977\n",
      " 4503/5000: episode: 4502, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.555, 0.739], loss: 2.546500, mean_absolute_error: 400.604797, mean_q: -2.255768\n",
      " 4504/5000: episode: 4503, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.901 [0.839, 0.953], loss: 5480.798828, mean_absolute_error: 402.533356, mean_q: -2.236998\n",
      " 4505/5000: episode: 4504, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.943 [0.893, 0.981], loss: 2.460471, mean_absolute_error: 400.629456, mean_q: -2.217319\n",
      " 4506/5000: episode: 4505, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.555 [0.458, 0.651], loss: 2.418046, mean_absolute_error: 400.640533, mean_q: -2.198112\n",
      " 4507/5000: episode: 4506, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.703, 0.862], loss: 2.377025, mean_absolute_error: 400.648926, mean_q: -2.179379\n",
      " 4508/5000: episode: 4507, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.856 [0.784, 0.919], loss: 9820.334961, mean_absolute_error: 400.663635, mean_q: -2.161057\n",
      " 4509/5000: episode: 4508, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.387 [0.294, 0.483], loss: 5476.927246, mean_absolute_error: 402.565369, mean_q: -2.139389\n",
      " 4510/5000: episode: 4509, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.203 [0.130, 0.285], loss: 9796.558594, mean_absolute_error: 400.650116, mean_q: -2.116673\n",
      " 4511/5000: episode: 4510, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.982 [0.953, 0.999], loss: 2.192469, mean_absolute_error: 400.642700, mean_q: -2.093024\n",
      " 4512/5000: episode: 4511, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.758 [0.672, 0.838], loss: 5474.672852, mean_absolute_error: 402.544861, mean_q: -2.069652\n",
      " 4513/5000: episode: 4512, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.245 [0.165, 0.331], loss: 2.094186, mean_absolute_error: 400.626404, mean_q: -2.045551\n",
      " 4514/5000: episode: 4513, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.278 [0.194, 0.368], loss: 15508.530273, mean_absolute_error: 402.636719, mean_q: -2.022751\n",
      " 4515/5000: episode: 4514, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [0.061, 0.184], loss: 1.998781, mean_absolute_error: 400.618195, mean_q: -1.998390\n",
      " 4516/5000: episode: 4515, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.922 [0.865, 0.968], loss: 5471.336914, mean_absolute_error: 402.524658, mean_q: -1.975343\n",
      " 4517/5000: episode: 4516, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.430 [0.335, 0.527], loss: 1.908727, mean_absolute_error: 400.619202, mean_q: -1.952830\n",
      " 4518/5000: episode: 4517, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.450 [0.354, 0.547], loss: 1.867725, mean_absolute_error: 400.620850, mean_q: -1.931731\n",
      " 4519/5000: episode: 4518, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.574 [0.478, 0.669], loss: 1.829299, mean_absolute_error: 400.621948, mean_q: -1.911746\n",
      " 4520/5000: episode: 4519, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.944 [0.895, 0.982], loss: 15497.079102, mean_absolute_error: 402.628418, mean_q: -1.892815\n",
      " 4521/5000: episode: 4520, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.870, 0.970], loss: 1.753064, mean_absolute_error: 400.620850, mean_q: -1.871466\n",
      " 4522/5000: episode: 4521, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.406 [0.312, 0.502], loss: 1.714867, mean_absolute_error: 400.621918, mean_q: -1.850954\n",
      " 4523/5000: episode: 4522, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.108 [0.053, 0.172], loss: 1.678626, mean_absolute_error: 400.624146, mean_q: -1.831281\n",
      " 4524/5000: episode: 4523, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.967 [0.928, 0.994], loss: 1.644042, mean_absolute_error: 400.627380, mean_q: -1.812307\n",
      " 4525/5000: episode: 4524, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.188 [0.116, 0.267], loss: 5464.202148, mean_absolute_error: 402.530762, mean_q: -1.794291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4526/5000: episode: 4525, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.007, 0.074], loss: 1.575310, mean_absolute_error: 400.627563, mean_q: -1.773998\n",
      " 4527/5000: episode: 4526, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.003, 0.059], loss: 10924.207031, mean_absolute_error: 404.419617, mean_q: -1.752794\n",
      " 4528/5000: episode: 4527, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.255, 0.439], loss: 5461.880859, mean_absolute_error: 402.520416, mean_q: -1.731672\n",
      " 4529/5000: episode: 4528, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 1.461932, mean_absolute_error: 400.620148, mean_q: -1.708931\n",
      " 4530/5000: episode: 4529, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.613 [0.517, 0.706], loss: 1.421440, mean_absolute_error: 400.616791, mean_q: -1.685084\n",
      " 4531/5000: episode: 4530, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.052], loss: 1.381121, mean_absolute_error: 400.617493, mean_q: -1.660999\n",
      " 4532/5000: episode: 4531, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.850 [0.777, 0.915], loss: 10913.438477, mean_absolute_error: 404.404846, mean_q: -1.637677\n",
      " 4533/5000: episode: 4532, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.966 [0.927, 0.994], loss: 5455.958008, mean_absolute_error: 402.517029, mean_q: -1.614902\n",
      " 4534/5000: episode: 4533, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.790, 0.923], loss: 9982.516602, mean_absolute_error: 400.758545, mean_q: -1.607332\n",
      " 4535/5000: episode: 4534, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.938, 0.997], loss: 1.296370, mean_absolute_error: 400.743286, mean_q: -1.609199\n",
      " 4536/5000: episode: 4535, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.754 [0.667, 0.834], loss: 5451.716309, mean_absolute_error: 402.682861, mean_q: -1.607122\n",
      " 4537/5000: episode: 4536, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.982, 1.000], loss: 1.283648, mean_absolute_error: 400.837585, mean_q: -1.601278\n",
      " 4538/5000: episode: 4537, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 5449.544434, mean_absolute_error: 402.756958, mean_q: -1.594214\n",
      " 4539/5000: episode: 4538, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.934 [0.881, 0.976], loss: 1.280041, mean_absolute_error: 400.945068, mean_q: -1.599026\n",
      " 4540/5000: episode: 4539, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.935 [0.883, 0.976], loss: 10043.800781, mean_absolute_error: 401.144104, mean_q: -1.612712\n",
      " 4541/5000: episode: 4540, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.902 [0.841, 0.954], loss: 5445.484863, mean_absolute_error: 403.032593, mean_q: -1.633123\n",
      " 4542/5000: episode: 4541, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 5443.927246, mean_absolute_error: 403.160278, mean_q: -1.659078\n",
      " 4543/5000: episode: 4542, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.979, 1.000], loss: 1.407797, mean_absolute_error: 401.389984, mean_q: -1.676973\n",
      " 4544/5000: episode: 4543, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.072], loss: 5441.260742, mean_absolute_error: 403.349670, mean_q: -1.687830\n",
      " 4545/5000: episode: 4544, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.823 [0.745, 0.892], loss: 1.433526, mean_absolute_error: 401.542267, mean_q: -1.692237\n",
      " 4546/5000: episode: 4545, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.881 [0.814, 0.938], loss: 1.435606, mean_absolute_error: 401.597015, mean_q: -1.693465\n",
      " 4547/5000: episode: 4546, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.247 [0.167, 0.333], loss: 1.432520, mean_absolute_error: 401.643585, mean_q: -1.691643\n",
      " 4548/5000: episode: 4547, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [0.067, 0.195], loss: 1.425022, mean_absolute_error: 401.682251, mean_q: -1.687207\n",
      " 4549/5000: episode: 4548, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.360 [0.269, 0.455], loss: 1.438337, mean_absolute_error: 401.752533, mean_q: -1.695076\n",
      " 4550/5000: episode: 4549, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.953 [0.908, 0.987], loss: 1.466564, mean_absolute_error: 401.844116, mean_q: -1.711637\n",
      " 4551/5000: episode: 4550, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.077], loss: 1.484524, mean_absolute_error: 401.920166, mean_q: -1.722093\n",
      " 4552/5000: episode: 4551, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.713, 0.869], loss: 5433.635742, mean_absolute_error: 403.850342, mean_q: -1.727701\n",
      " 4553/5000: episode: 4552, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.673 [0.580, 0.761], loss: 1.518505, mean_absolute_error: 402.074219, mean_q: -1.741702\n",
      " 4554/5000: episode: 4553, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.194 [0.122, 0.274], loss: 5431.427246, mean_absolute_error: 404.044617, mean_q: -1.762043\n",
      " 4555/5000: episode: 4554, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.520 [0.423, 0.616], loss: 2648174.000000, mean_absolute_error: 577.499146, mean_q: -1.775248\n",
      " 4556/5000: episode: 4555, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.988 [0.965, 1.000], loss: 1323943.375000, mean_absolute_error: 490.092834, mean_q: -1.835167\n",
      " 4557/5000: episode: 4556, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.013], loss: 1.819340, mean_absolute_error: 402.740326, mean_q: -1.906532\n",
      " 4558/5000: episode: 4557, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [0.011, 0.087], loss: 1.925312, mean_absolute_error: 402.945557, mean_q: -1.961301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4559/5000: episode: 4558, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.401 [0.308, 0.498], loss: 2.007760, mean_absolute_error: 403.111664, mean_q: -2.002876\n",
      " 4560/5000: episode: 4559, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.357 [0.266, 0.452], loss: 9943.379883, mean_absolute_error: 403.256958, mean_q: -2.033319\n",
      " 4561/5000: episode: 4560, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.066], loss: 5421.686035, mean_absolute_error: 405.205353, mean_q: -2.052592\n",
      " 4562/5000: episode: 4561, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [0.003, 0.059], loss: 2.132323, mean_absolute_error: 403.443665, mean_q: -2.064102\n",
      " 4563/5000: episode: 4562, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [0.069, 0.198], loss: 5419.285645, mean_absolute_error: 405.369202, mean_q: -2.070101\n",
      " 4564/5000: episode: 4563, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.106 [0.052, 0.170], loss: 15426.142578, mean_absolute_error: 405.462646, mean_q: -2.071342\n",
      " 4565/5000: episode: 4564, duration: 0.044s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.326, 0.517], loss: 2.133866, mean_absolute_error: 403.632446, mean_q: -2.064849\n",
      " 4566/5000: episode: 4565, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.079 [0.033, 0.136], loss: 2.113988, mean_absolute_error: 403.670013, mean_q: -2.055204\n",
      " 4567/5000: episode: 4566, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 15537.395508, mean_absolute_error: 405.622742, mean_q: -2.044022\n",
      " 4568/5000: episode: 4567, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.072 [0.028, 0.126], loss: 2.059835, mean_absolute_error: 403.727448, mean_q: -2.028697\n",
      " 4569/5000: episode: 4568, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [0.002, 0.055], loss: 5412.138184, mean_absolute_error: 405.591736, mean_q: -2.012966\n",
      " 4570/5000: episode: 4569, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 5410.828613, mean_absolute_error: 405.614563, mean_q: -1.996187\n",
      " 4571/5000: episode: 4570, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.439, 0.632], loss: 1.960253, mean_absolute_error: 403.796875, mean_q: -1.979027\n",
      " 4572/5000: episode: 4571, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.474 [0.378, 0.571], loss: 1323155.500000, mean_absolute_error: 491.364594, mean_q: -1.961830\n",
      " 4573/5000: episode: 4572, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.413], loss: 1.944371, mean_absolute_error: 403.909515, mean_q: -1.970989\n",
      " 4574/5000: episode: 4573, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.886 [0.821, 0.942], loss: 1.953342, mean_absolute_error: 403.983124, mean_q: -1.975533\n",
      " 4575/5000: episode: 4574, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.038, 0.146], loss: 1.954922, mean_absolute_error: 404.042114, mean_q: -1.976333\n",
      " 4576/5000: episode: 4575, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.945 [0.896, 0.982], loss: 1.950508, mean_absolute_error: 404.088989, mean_q: -1.974099\n",
      " 4577/5000: episode: 4576, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.518 [0.421, 0.615], loss: 1.940493, mean_absolute_error: 404.128021, mean_q: -1.969022\n",
      " 4578/5000: episode: 4577, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 1.926040, mean_absolute_error: 404.161469, mean_q: -1.961672\n",
      " 4579/5000: episode: 4578, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.048 [0.013, 0.094], loss: 1.908721, mean_absolute_error: 404.187347, mean_q: -1.952827\n",
      " 4580/5000: episode: 4579, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.932 [0.878, 0.974], loss: 10037.474609, mean_absolute_error: 404.237671, mean_q: -1.942863\n",
      " 4581/5000: episode: 4580, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.315 [0.227, 0.407], loss: 1.862894, mean_absolute_error: 404.218170, mean_q: -1.929230\n",
      " 4582/5000: episode: 4581, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.333, 0.524], loss: 1.833383, mean_absolute_error: 404.225403, mean_q: -1.913880\n",
      " 4583/5000: episode: 4582, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.093 [0.043, 0.154], loss: 5399.500977, mean_absolute_error: 406.054077, mean_q: -1.895950\n",
      " 4584/5000: episode: 4583, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.454 [0.358, 0.551], loss: 5398.687500, mean_absolute_error: 406.056519, mean_q: -1.876507\n",
      " 4585/5000: episode: 4584, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.905, 0.986], loss: 9964.725586, mean_absolute_error: 404.232361, mean_q: -1.856071\n",
      " 4586/5000: episode: 4585, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.095 [0.044, 0.156], loss: 1.682058, mean_absolute_error: 404.226929, mean_q: -1.833153\n",
      " 4587/5000: episode: 4586, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [0.005, 0.070], loss: 5396.176270, mean_absolute_error: 406.045166, mean_q: -1.810242\n",
      " 4588/5000: episode: 4587, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 1.598961, mean_absolute_error: 404.216064, mean_q: -1.787273\n",
      " 4589/5000: episode: 4588, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.925 [0.870, 0.970], loss: 1.561080, mean_absolute_error: 404.211426, mean_q: -1.765964\n",
      " 4590/5000: episode: 4589, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.821 [0.743, 0.891], loss: 1.526115, mean_absolute_error: 404.206604, mean_q: -1.746063\n",
      " 4591/5000: episode: 4590, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.507 [0.410, 0.604], loss: 1.493672, mean_absolute_error: 404.201721, mean_q: -1.727393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4592/5000: episode: 4591, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 1.463056, mean_absolute_error: 404.197235, mean_q: -1.709588\n",
      " 4593/5000: episode: 4592, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.950 [0.903, 0.985], loss: 5392.768555, mean_absolute_error: 406.013458, mean_q: -1.692701\n",
      " 4594/5000: episode: 4593, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.761 [0.675, 0.840], loss: 1.405812, mean_absolute_error: 404.192566, mean_q: -1.675790\n",
      " 4595/5000: episode: 4594, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.615 [0.520, 0.708], loss: 5391.799316, mean_absolute_error: 406.011719, mean_q: -1.659551\n",
      " 4596/5000: episode: 4595, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.024], loss: 1.374894, mean_absolute_error: 404.235229, mean_q: -1.657248\n",
      " 4597/5000: episode: 4596, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.394 [0.301, 0.490], loss: 1.388855, mean_absolute_error: 404.305695, mean_q: -1.665647\n",
      " 4598/5000: episode: 4597, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.974 [0.940, 0.997], loss: 1322888.000000, mean_absolute_error: 491.892548, mean_q: -1.669563\n",
      " 4599/5000: episode: 4598, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.305, 0.494], loss: 1.438745, mean_absolute_error: 404.485443, mean_q: -1.695316\n",
      " 4600/5000: episode: 4599, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.845 [0.771, 0.911], loss: 5387.241211, mean_absolute_error: 406.394836, mean_q: -1.712610\n",
      " 4601/5000: episode: 4600, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 5386.323730, mean_absolute_error: 406.473450, mean_q: -1.723416\n",
      " 4602/5000: episode: 4601, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [0.086, 0.223], loss: 1.496176, mean_absolute_error: 404.729736, mean_q: -1.728842\n",
      " 4603/5000: episode: 4602, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.924 [0.868, 0.969], loss: 5384.203125, mean_absolute_error: 406.598328, mean_q: -1.730375\n",
      " 4604/5000: episode: 4603, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.341 [0.251, 0.435], loss: 5382.918457, mean_absolute_error: 406.650452, mean_q: -1.727959\n",
      " 4605/5000: episode: 4604, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.061 [0.021, 0.111], loss: 1.485507, mean_absolute_error: 404.887634, mean_q: -1.722663\n",
      " 4606/5000: episode: 4605, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.185 [0.114, 0.264], loss: 15420.280273, mean_absolute_error: 406.752747, mean_q: -1.715484\n",
      " 4607/5000: episode: 4606, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.805 [0.725, 0.878], loss: 1.477154, mean_absolute_error: 404.999268, mean_q: -1.717810\n",
      " 4608/5000: episode: 4607, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.868 [0.798, 0.928], loss: 1.495800, mean_absolute_error: 405.089172, mean_q: -1.728624\n",
      " 4609/5000: episode: 4608, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.523 [0.426, 0.619], loss: 1333146.000000, mean_absolute_error: 496.295166, mean_q: -1.747709\n",
      " 4610/5000: episode: 4609, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.003, 0.063], loss: 1327638.000000, mean_absolute_error: 494.690857, mean_q: -1.797808\n",
      " 4611/5000: episode: 4610, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [0.069, 0.197], loss: 1.732601, mean_absolute_error: 405.661255, mean_q: -1.860505\n",
      " 4612/5000: episode: 4611, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.466 [0.370, 0.563], loss: 1327348.500000, mean_absolute_error: 495.127014, mean_q: -1.907874\n",
      " 4613/5000: episode: 4612, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 1.939727, mean_absolute_error: 406.120941, mean_q: -1.968633\n",
      " 4614/5000: episode: 4613, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.899 [0.837, 0.952], loss: 10726.006836, mean_absolute_error: 409.936646, mean_q: -2.028716\n",
      " 4615/5000: episode: 4614, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.912 [0.852, 0.961], loss: 5361.049316, mean_absolute_error: 408.391632, mean_q: -2.084192\n",
      " 4616/5000: episode: 4615, duration: 0.057s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.847 [0.773, 0.912], loss: 2.258010, mean_absolute_error: 406.819855, mean_q: -2.124093\n",
      " 4617/5000: episode: 4616, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [0.011, 0.086], loss: 5355.193848, mean_absolute_error: 408.770477, mean_q: -2.152980\n",
      " 4618/5000: episode: 4617, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [0.000, 0.029], loss: 10157.887695, mean_absolute_error: 407.156128, mean_q: -2.172578\n",
      " 4619/5000: episode: 4618, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.901, 0.985], loss: 5350.661621, mean_absolute_error: 409.020844, mean_q: -2.182631\n",
      " 4620/5000: episode: 4619, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 2.391876, mean_absolute_error: 407.348114, mean_q: -2.186179\n",
      " 4621/5000: episode: 4620, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.239 [0.160, 0.325], loss: 5346.532715, mean_absolute_error: 409.197418, mean_q: -2.185546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4622/5000: episode: 4621, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.971, 1.000], loss: 2.380847, mean_absolute_error: 407.499817, mean_q: -2.181130\n",
      " 4623/5000: episode: 4622, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.085 [0.037, 0.143], loss: 2.365928, mean_absolute_error: 407.554688, mean_q: -2.174283\n",
      " 4624/5000: episode: 4623, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [0.004, 0.067], loss: 2.346945, mean_absolute_error: 407.598633, mean_q: -2.165539\n",
      " 4625/5000: episode: 4624, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.036, 0.141], loss: 5341.027832, mean_absolute_error: 409.394318, mean_q: -2.155261\n",
      " 4626/5000: episode: 4625, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.835 [0.760, 0.903], loss: 5339.775391, mean_absolute_error: 409.464172, mean_q: -2.157013\n",
      " 4627/5000: episode: 4626, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.900 [0.837, 0.952], loss: 2.350141, mean_absolute_error: 407.801025, mean_q: -2.167013\n",
      " 4628/5000: episode: 4627, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.414], loss: 2.358304, mean_absolute_error: 407.875244, mean_q: -2.170775\n",
      " 4629/5000: episode: 4628, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.240 [0.160, 0.325], loss: 2.355693, mean_absolute_error: 407.930267, mean_q: -2.169573\n",
      " 4630/5000: episode: 4629, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.256 [0.175, 0.343], loss: 5335.466309, mean_absolute_error: 409.729462, mean_q: -2.165295\n",
      " 4631/5000: episode: 4630, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.398 [0.304, 0.494], loss: 5334.449219, mean_absolute_error: 409.770294, mean_q: -2.157659\n",
      " 4632/5000: episode: 4631, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.200, 0.374], loss: 10402.294922, mean_absolute_error: 408.157959, mean_q: -2.147217\n",
      " 4633/5000: episode: 4632, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.918 [0.860, 0.965], loss: 2.274092, mean_absolute_error: 408.080444, mean_q: -2.131647\n",
      " 4634/5000: episode: 4633, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.266 [0.184, 0.355], loss: 10660.133789, mean_absolute_error: 411.597656, mean_q: -2.114542\n",
      " 4635/5000: episode: 4634, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.992 [0.973, 1.000], loss: 2.227485, mean_absolute_error: 408.162170, mean_q: -2.109680\n",
      " 4636/5000: episode: 4635, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.743 [0.656, 0.825], loss: 2.267974, mean_absolute_error: 408.291046, mean_q: -2.128777\n",
      " 4637/5000: episode: 4636, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [0.076, 0.209], loss: 2.321205, mean_absolute_error: 408.429626, mean_q: -2.153625\n",
      " 4638/5000: episode: 4637, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.273 [0.190, 0.362], loss: 10227.924805, mean_absolute_error: 408.598938, mean_q: -2.184438\n",
      " 4639/5000: episode: 4638, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.439 [0.344, 0.536], loss: 1325656.875000, mean_absolute_error: 497.815094, mean_q: -2.215935\n",
      " 4640/5000: episode: 4639, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.697 [0.605, 0.783], loss: 10639.842773, mean_absolute_error: 412.411926, mean_q: -2.263348\n",
      " 4641/5000: episode: 4640, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.920 [0.863, 0.966], loss: 10230.379883, mean_absolute_error: 409.116211, mean_q: -2.297050\n",
      " 4642/5000: episode: 4641, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.956 [0.912, 0.989], loss: 2.690782, mean_absolute_error: 409.241730, mean_q: -2.318820\n",
      " 4643/5000: episode: 4642, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.550 [0.453, 0.646], loss: 1325267.500000, mean_absolute_error: 498.395721, mean_q: -2.333052\n",
      " 4644/5000: episode: 4643, duration: 0.058s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [0.005, 0.068], loss: 2.801916, mean_absolute_error: 409.514771, mean_q: -2.366241\n",
      " 4645/5000: episode: 4644, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [0.093, 0.234], loss: 1319766.000000, mean_absolute_error: 496.952240, mean_q: -2.389420\n",
      " 4646/5000: episode: 4645, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.713 [0.623, 0.798], loss: 5310.999512, mean_absolute_error: 411.559814, mean_q: -2.430729\n",
      " 4647/5000: episode: 4646, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.051, 0.167], loss: 10508.496094, mean_absolute_error: 410.086548, mean_q: -2.460033\n",
      " 4648/5000: episode: 4647, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.978, 1.000], loss: 3.072497, mean_absolute_error: 410.100037, mean_q: -2.477910\n",
      " 4649/5000: episode: 4648, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.944, 0.998], loss: 3.096675, mean_absolute_error: 410.189392, mean_q: -2.487644\n",
      " 4650/5000: episode: 4649, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.366 [0.274, 0.461], loss: 5306.131348, mean_absolute_error: 411.979797, mean_q: -2.490356\n",
      " 4651/5000: episode: 4650, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.011], loss: 3.095538, mean_absolute_error: 410.326477, mean_q: -2.487187\n",
      " 4652/5000: episode: 4651, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.909 [0.849, 0.959], loss: 3.077767, mean_absolute_error: 410.387024, mean_q: -2.480035\n",
      " 4653/5000: episode: 4652, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.842 [0.768, 0.908], loss: 3.053251, mean_absolute_error: 410.437073, mean_q: -2.470134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4654/5000: episode: 4653, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.080 [0.034, 0.138], loss: 5300.679199, mean_absolute_error: 412.190369, mean_q: -2.458546\n",
      " 4655/5000: episode: 4654, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.028], loss: 2.990237, mean_absolute_error: 410.513977, mean_q: -2.444501\n",
      " 4656/5000: episode: 4655, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.053 [0.016, 0.101], loss: 2.952804, mean_absolute_error: 410.545685, mean_q: -2.429146\n",
      " 4657/5000: episode: 4656, duration: 0.041s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.348 [0.258, 0.442], loss: 10591.509766, mean_absolute_error: 413.990356, mean_q: -2.413167\n",
      " 4658/5000: episode: 4657, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [0.001, 0.052], loss: 2.870233, mean_absolute_error: 410.600647, mean_q: -2.394927\n",
      " 4659/5000: episode: 4658, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.345 [0.255, 0.439], loss: 5294.486816, mean_absolute_error: 412.333435, mean_q: -2.376184\n",
      " 4660/5000: episode: 4659, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [0.008, 0.078], loss: 2.778731, mean_absolute_error: 410.653351, mean_q: -2.356427\n",
      " 4661/5000: episode: 4660, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 2.727936, mean_absolute_error: 410.677795, mean_q: -2.334781\n",
      " 4662/5000: episode: 4661, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.323 [0.235, 0.416], loss: 1319195.625000, mean_absolute_error: 497.960480, mean_q: -2.311592\n",
      " 4663/5000: episode: 4662, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.344 [0.254, 0.438], loss: 10302.173828, mean_absolute_error: 410.799805, mean_q: -2.315179\n",
      " 4664/5000: episode: 4663, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.249 [0.169, 0.336], loss: 2.675155, mean_absolute_error: 410.870361, mean_q: -2.312074\n",
      " 4665/5000: episode: 4664, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [0.006, 0.073], loss: 2.661496, mean_absolute_error: 410.928497, mean_q: -2.306161\n",
      " 4666/5000: episode: 4665, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.774 [0.690, 0.851], loss: 15586.420898, mean_absolute_error: 412.672241, mean_q: -2.297539\n",
      " 4667/5000: episode: 4666, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.874 [0.806, 0.933], loss: 2.609493, mean_absolute_error: 411.017670, mean_q: -2.283510\n",
      " 4668/5000: episode: 4667, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [0.004, 0.066], loss: 2.575238, mean_absolute_error: 411.052368, mean_q: -2.268466\n",
      " 4669/5000: episode: 4668, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.613, 0.790], loss: 15603.697266, mean_absolute_error: 412.782166, mean_q: -2.252133\n",
      " 4670/5000: episode: 4669, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.859 [0.788, 0.921], loss: 2.492860, mean_absolute_error: 411.111206, mean_q: -2.231873\n",
      " 4671/5000: episode: 4670, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.935, 0.996], loss: 2.448296, mean_absolute_error: 411.134399, mean_q: -2.211824\n",
      " 4672/5000: episode: 4671, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.105 [0.051, 0.168], loss: 2.404470, mean_absolute_error: 411.154602, mean_q: -2.191930\n",
      " 4673/5000: episode: 4672, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.635, 0.808], loss: 2.361223, mean_absolute_error: 411.172333, mean_q: -2.172119\n",
      " 4674/5000: episode: 4673, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [0.000, 0.039], loss: 2.318163, mean_absolute_error: 411.190338, mean_q: -2.152213\n",
      " 4675/5000: episode: 4674, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.987 [0.962, 1.000], loss: 1334638.875000, mean_absolute_error: 500.163635, mean_q: -2.132442\n",
      " 4676/5000: episode: 4675, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.903 [0.841, 0.955], loss: 2.312760, mean_absolute_error: 411.327576, mean_q: -2.149702\n",
      " 4677/5000: episode: 4676, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [0.107, 0.254], loss: 2.360084, mean_absolute_error: 411.460480, mean_q: -2.171595\n",
      " 4678/5000: episode: 4677, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.094 [0.043, 0.154], loss: 2.388613, mean_absolute_error: 411.576569, mean_q: -2.184687\n",
      " 4679/5000: episode: 4678, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.889 [0.824, 0.945], loss: 10352.877930, mean_absolute_error: 411.686615, mean_q: -2.190688\n",
      " 4680/5000: episode: 4679, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [0.082, 0.217], loss: 2.399485, mean_absolute_error: 411.756500, mean_q: -2.189655\n",
      " 4681/5000: episode: 4680, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.226 [0.149, 0.310], loss: 2.390573, mean_absolute_error: 411.818451, mean_q: -2.185583\n",
      " 4682/5000: episode: 4681, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.825 [0.748, 0.894], loss: 1318612.500000, mean_absolute_error: 499.087646, mean_q: -2.178679\n",
      " 4683/5000: episode: 4682, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.055 [0.018, 0.104], loss: 10423.708984, mean_absolute_error: 412.023499, mean_q: -2.195329\n",
      " 4684/5000: episode: 4683, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.209 [0.134, 0.291], loss: 2.427936, mean_absolute_error: 412.090088, mean_q: -2.202604\n",
      " 4685/5000: episode: 4684, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.931 [0.878, 0.974], loss: 2.433767, mean_absolute_error: 412.167114, mean_q: -2.205249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4686/5000: episode: 4685, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.950 [0.903, 0.985], loss: 15758.210938, mean_absolute_error: 417.216644, mean_q: -2.203694\n",
      " 4687/5000: episode: 4686, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.244 [0.164, 0.330], loss: 5252.213379, mean_absolute_error: 413.963257, mean_q: -2.196014\n",
      " 4688/5000: episode: 4687, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.325 [0.237, 0.418], loss: 2.416110, mean_absolute_error: 412.412048, mean_q: -2.197231\n",
      " 4689/5000: episode: 4688, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.965 [0.924, 0.993], loss: 5246.875977, mean_absolute_error: 414.191467, mean_q: -2.205275\n",
      " 4690/5000: episode: 4689, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.655, 0.824], loss: 10403.307617, mean_absolute_error: 412.655975, mean_q: -2.207591\n",
      " 4691/5000: episode: 4690, duration: 0.074s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.567 [0.470, 0.662], loss: 2.430660, mean_absolute_error: 412.726105, mean_q: -2.203840\n",
      " 4692/5000: episode: 4691, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.644 [0.549, 0.735], loss: 2.415871, mean_absolute_error: 412.792297, mean_q: -2.197123\n",
      " 4693/5000: episode: 4692, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.147, 0.308], loss: 1318114.500000, mean_absolute_error: 500.029114, mean_q: -2.188109\n",
      " 4694/5000: episode: 4693, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.649 [0.555, 0.740], loss: 5237.250000, mean_absolute_error: 414.608307, mean_q: -2.204133\n",
      " 4695/5000: episode: 4694, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.882 [0.815, 0.939], loss: 2.448583, mean_absolute_error: 413.062561, mean_q: -2.211954\n",
      " 4696/5000: episode: 4695, duration: 0.109s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.936 [0.884, 0.977], loss: 2.452039, mean_absolute_error: 413.138550, mean_q: -2.213515\n",
      " 4697/5000: episode: 4696, duration: 0.057s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.303 [0.216, 0.394], loss: 2.446028, mean_absolute_error: 413.204285, mean_q: -2.210799\n",
      " 4698/5000: episode: 4697, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.465 [0.369, 0.562], loss: 10460.540039, mean_absolute_error: 416.532043, mean_q: -2.204560\n",
      " 4699/5000: episode: 4698, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 2.410152, mean_absolute_error: 413.317932, mean_q: -2.194519\n",
      " 4700/5000: episode: 4699, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 1328251.875000, mean_absolute_error: 500.526489, mean_q: -2.183079\n",
      " 4701/5000: episode: 4700, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.700, 0.859], loss: 1317774.250000, mean_absolute_error: 500.624268, mean_q: -2.194954\n",
      " 4702/5000: episode: 4701, duration: 0.054s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 5225.247070, mean_absolute_error: 415.253845, mean_q: -2.227724\n",
      " 4703/5000: episode: 4702, duration: 0.113s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.958 [0.915, 0.990], loss: 2.533391, mean_absolute_error: 413.760864, mean_q: -2.249951\n",
      " 4704/5000: episode: 4703, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 2.565853, mean_absolute_error: 413.871277, mean_q: -2.264327\n",
      " 4705/5000: episode: 4704, duration: 0.145s, episode steps: 1, steps per second: 7, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.876 [0.809, 0.935], loss: 5220.936523, mean_absolute_error: 415.578918, mean_q: -2.271327\n",
      " 4706/5000: episode: 4705, duration: 0.158s, episode steps: 1, steps per second: 6, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.422 [0.327, 0.519], loss: 2.582985, mean_absolute_error: 414.025177, mean_q: -2.271877\n",
      " 4707/5000: episode: 4706, duration: 0.104s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.564 [0.467, 0.659], loss: 5218.480469, mean_absolute_error: 415.705444, mean_q: -2.268170\n",
      " 4708/5000: episode: 4707, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 15925.696289, mean_absolute_error: 415.864594, mean_q: -2.260494\n",
      " 4709/5000: episode: 4708, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 10662.849609, mean_absolute_error: 414.271240, mean_q: -2.247457\n",
      " 4710/5000: episode: 4709, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 2.486933, mean_absolute_error: 414.217041, mean_q: -2.229217\n",
      " 4711/5000: episode: 4710, duration: 0.079s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.900, 0.984], loss: 5212.313965, mean_absolute_error: 415.857727, mean_q: -2.209023\n",
      " 4712/5000: episode: 4711, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 5210.532227, mean_absolute_error: 415.886047, mean_q: -2.187927\n",
      " 4713/5000: episode: 4712, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.449 [0.353, 0.546], loss: 2.349872, mean_absolute_error: 414.302856, mean_q: -2.166889\n",
      " 4714/5000: episode: 4713, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [0.100, 0.244], loss: 2.306082, mean_absolute_error: 414.324219, mean_q: -2.146595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4715/5000: episode: 4714, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.041], loss: 2.264009, mean_absolute_error: 414.340881, mean_q: -2.126914\n",
      " 4716/5000: episode: 4715, duration: 0.095s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 2.220004, mean_absolute_error: 414.352295, mean_q: -2.106133\n",
      " 4717/5000: episode: 4716, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 2.174978, mean_absolute_error: 414.359100, mean_q: -2.084655\n",
      " 4718/5000: episode: 4717, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.471 [0.375, 0.568], loss: 5203.379395, mean_absolute_error: 415.969299, mean_q: -2.063691\n",
      " 4719/5000: episode: 4718, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.591 [0.495, 0.685], loss: 1317333.750000, mean_absolute_error: 501.499634, mean_q: -2.041413\n",
      " 4720/5000: episode: 4719, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.685 [0.593, 0.773], loss: 2.094231, mean_absolute_error: 414.455750, mean_q: -2.045573\n",
      " 4721/5000: episode: 4720, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [0.092, 0.232], loss: 2.091776, mean_absolute_error: 414.519501, mean_q: -2.044374\n",
      " 4722/5000: episode: 4721, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.903 [0.841, 0.954], loss: 15920.683594, mean_absolute_error: 416.268555, mean_q: -2.039186\n",
      " 4723/5000: episode: 4722, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.474 [0.378, 0.571], loss: 2.058563, mean_absolute_error: 414.606873, mean_q: -2.028070\n",
      " 4724/5000: episode: 4723, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.510 [0.413, 0.607], loss: 2.032869, mean_absolute_error: 414.643005, mean_q: -2.015368\n",
      " 4725/5000: episode: 4724, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [0.000, 0.035], loss: 2.006092, mean_absolute_error: 414.671265, mean_q: -2.002044\n",
      " 4726/5000: episode: 4725, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 1.978519, mean_absolute_error: 414.693115, mean_q: -1.988231\n",
      " 4727/5000: episode: 4726, duration: 0.064s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.046 [0.012, 0.090], loss: 1.949653, mean_absolute_error: 414.712250, mean_q: -1.973666\n",
      " 4728/5000: episode: 4727, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.897 [0.834, 0.950], loss: 5192.523926, mean_absolute_error: 416.320068, mean_q: -1.958408\n",
      " 4729/5000: episode: 4728, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.961 [0.920, 0.992], loss: 1.887913, mean_absolute_error: 414.749023, mean_q: -1.942148\n",
      " 4730/5000: episode: 4729, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 1.856006, mean_absolute_error: 414.766418, mean_q: -1.925658\n",
      " 4731/5000: episode: 4730, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [0.007, 0.074], loss: 1.824650, mean_absolute_error: 414.779541, mean_q: -1.909314\n",
      " 4732/5000: episode: 4731, duration: 0.094s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.929 [0.874, 0.972], loss: 1.793247, mean_absolute_error: 414.791443, mean_q: -1.892804\n",
      " 4733/5000: episode: 4732, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.267 [0.184, 0.355], loss: 1.762038, mean_absolute_error: 414.803040, mean_q: -1.876252\n",
      " 4734/5000: episode: 4733, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.214 [0.139, 0.297], loss: 5187.512695, mean_absolute_error: 416.396362, mean_q: -1.859913\n",
      " 4735/5000: episode: 4734, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [0.000, 0.042], loss: 1.698126, mean_absolute_error: 414.817688, mean_q: -1.841892\n",
      " 4736/5000: episode: 4735, duration: 0.096s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.732 [0.644, 0.815], loss: 10661.272461, mean_absolute_error: 414.894897, mean_q: -1.823351\n",
      " 4737/5000: episode: 4736, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.703, 0.861], loss: 1.625130, mean_absolute_error: 414.808105, mean_q: -1.801847\n",
      " 4738/5000: episode: 4737, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.396 [0.303, 0.492], loss: 5185.452637, mean_absolute_error: 416.378265, mean_q: -1.780550\n",
      " 4739/5000: episode: 4738, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.770 [0.686, 0.848], loss: 1.550188, mean_absolute_error: 414.788177, mean_q: -1.759789\n",
      " 4740/5000: episode: 4739, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.474, 0.666], loss: 1.514730, mean_absolute_error: 414.787476, mean_q: -1.739534\n",
      " 4741/5000: episode: 4740, duration: 0.084s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.064 [0.023, 0.115], loss: 1.479280, mean_absolute_error: 414.795410, mean_q: -1.719047\n",
      " 4742/5000: episode: 4741, duration: 0.052s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 1.445220, mean_absolute_error: 414.805511, mean_q: -1.699129\n",
      " 4743/5000: episode: 4742, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.979 [0.947, 0.999], loss: 1.413486, mean_absolute_error: 414.812653, mean_q: -1.680360\n",
      " 4744/5000: episode: 4743, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.704 [0.613, 0.790], loss: 5180.215332, mean_absolute_error: 416.391449, mean_q: -1.661249\n",
      " 4745/5000: episode: 4744, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [0.080, 0.214], loss: 1.348741, mean_absolute_error: 414.815765, mean_q: -1.641401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4746/5000: episode: 4745, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.383 [0.290, 0.478], loss: 1.318208, mean_absolute_error: 414.816650, mean_q: -1.622705\n",
      " 4747/5000: episode: 4746, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.558 [0.461, 0.653], loss: 10354.947266, mean_absolute_error: 417.966248, mean_q: -1.605092\n",
      " 4748/5000: episode: 4747, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [0.006, 0.071], loss: 5177.166016, mean_absolute_error: 416.398193, mean_q: -1.586872\n",
      " 4749/5000: episode: 4748, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [0.003, 0.061], loss: 5175.895996, mean_absolute_error: 416.449432, mean_q: -1.583390\n",
      " 4750/5000: episode: 4749, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.531 [0.434, 0.627], loss: 1.266346, mean_absolute_error: 414.960815, mean_q: -1.590444\n",
      " 4751/5000: episode: 4750, duration: 0.100s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [0.001, 0.051], loss: 1.294541, mean_absolute_error: 415.069092, mean_q: -1.608063\n",
      " 4752/5000: episode: 4751, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.979, 1.000], loss: 1322156.500000, mean_absolute_error: 503.860291, mean_q: -1.632792\n",
      " 4753/5000: episode: 4752, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.507 [0.410, 0.603], loss: 1.406703, mean_absolute_error: 415.376617, mean_q: -1.676321\n",
      " 4754/5000: episode: 4753, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.077 [0.032, 0.134], loss: 1.461653, mean_absolute_error: 415.527710, mean_q: -1.708768\n",
      " 4755/5000: episode: 4754, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [0.062, 0.186], loss: 1.502432, mean_absolute_error: 415.648926, mean_q: -1.732454\n",
      " 4756/5000: episode: 4755, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 5165.981445, mean_absolute_error: 417.307404, mean_q: -1.749005\n",
      " 4757/5000: episode: 4756, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.978 [0.946, 0.998], loss: 1.548812, mean_absolute_error: 415.829559, mean_q: -1.759007\n",
      " 4758/5000: episode: 4757, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.811 [0.732, 0.883], loss: 15934.640625, mean_absolute_error: 417.553101, mean_q: -1.764398\n",
      " 4759/5000: episode: 4758, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.603 [0.507, 0.697], loss: 1.555735, mean_absolute_error: 415.946533, mean_q: -1.762936\n",
      " 4760/5000: episode: 4759, duration: 0.092s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.379 [0.287, 0.475], loss: 1.548301, mean_absolute_error: 415.989288, mean_q: -1.758717\n",
      " 4761/5000: episode: 4760, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.776 [0.692, 0.853], loss: 15697.733398, mean_absolute_error: 417.582367, mean_q: -1.752172\n",
      " 4762/5000: episode: 4761, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.388 [0.295, 0.484], loss: 1.517477, mean_absolute_error: 416.051544, mean_q: -1.741112\n",
      " 4763/5000: episode: 4762, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.548, 0.734], loss: 5159.467773, mean_absolute_error: 417.624634, mean_q: -1.729406\n",
      " 4764/5000: episode: 4763, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.750 [0.663, 0.831], loss: 1.474968, mean_absolute_error: 416.092163, mean_q: -1.716537\n",
      " 4765/5000: episode: 4764, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.071 [0.027, 0.125], loss: 1.477324, mean_absolute_error: 416.148773, mean_q: -1.717909\n",
      " 4766/5000: episode: 4765, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.552 [0.455, 0.648], loss: 5156.730469, mean_absolute_error: 417.781433, mean_q: -1.729412\n",
      " 4767/5000: episode: 4766, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.089 [0.040, 0.149], loss: 1332043.125000, mean_absolute_error: 504.912323, mean_q: -1.735263\n",
      " 4768/5000: episode: 4767, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.389 [0.296, 0.485], loss: 10631.334961, mean_absolute_error: 416.505493, mean_q: -1.775875\n",
      " 4769/5000: episode: 4768, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.866 [0.796, 0.927], loss: 1.650942, mean_absolute_error: 416.646973, mean_q: -1.816109\n",
      " 4770/5000: episode: 4769, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.046], loss: 1.729866, mean_absolute_error: 416.832886, mean_q: -1.859035\n",
      " 4771/5000: episode: 4770, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.949 [0.902, 0.985], loss: 5148.735840, mean_absolute_error: 418.564392, mean_q: -1.903179\n",
      " 4772/5000: episode: 4771, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.463 [0.367, 0.560], loss: 1.874607, mean_absolute_error: 417.182465, mean_q: -1.935288\n",
      " 4773/5000: episode: 4772, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.283 [0.198, 0.372], loss: 10622.098633, mean_absolute_error: 417.317444, mean_q: -1.958348\n",
      " 4774/5000: episode: 4773, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.032], loss: 1.945861, mean_absolute_error: 417.405029, mean_q: -1.971745\n",
      " 4775/5000: episode: 4774, duration: 0.109s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.935 [0.882, 0.976], loss: 10284.398438, mean_absolute_error: 420.551910, mean_q: -1.979191\n",
      " 4776/5000: episode: 4775, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.041], loss: 5141.530273, mean_absolute_error: 419.093750, mean_q: -1.980049\n",
      " 4777/5000: episode: 4776, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.420 [0.325, 0.516], loss: 1.955443, mean_absolute_error: 417.630737, mean_q: -1.976596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4778/5000: episode: 4777, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.818 [0.739, 0.888], loss: 1.943895, mean_absolute_error: 417.685608, mean_q: -1.970748\n",
      " 4779/5000: episode: 4778, duration: 0.108s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.867 [0.798, 0.928], loss: 1.928763, mean_absolute_error: 417.728882, mean_q: -1.963058\n",
      " 4780/5000: episode: 4779, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.729 [0.640, 0.812], loss: 1.910635, mean_absolute_error: 417.763000, mean_q: -1.953807\n",
      " 4781/5000: episode: 4780, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.284 [0.199, 0.373], loss: 1.887483, mean_absolute_error: 417.785583, mean_q: -1.941927\n",
      " 4782/5000: episode: 4781, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.070 [0.027, 0.124], loss: 5134.984375, mean_absolute_error: 419.324524, mean_q: -1.928310\n",
      " 4783/5000: episode: 4782, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.635 [0.540, 0.727], loss: 1.859591, mean_absolute_error: 417.859070, mean_q: -1.927518\n",
      " 4784/5000: episode: 4783, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.060 [0.020, 0.110], loss: 1.876964, mean_absolute_error: 417.944977, mean_q: -1.936505\n",
      " 4785/5000: episode: 4784, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.906 [0.845, 0.957], loss: 1.913225, mean_absolute_error: 418.054688, mean_q: -1.955132\n",
      " 4786/5000: episode: 4785, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.064 [0.023, 0.116], loss: 1.961926, mean_absolute_error: 418.181824, mean_q: -1.979872\n",
      " 4787/5000: episode: 4786, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.487 [0.390, 0.584], loss: 1315119.125000, mean_absolute_error: 505.253174, mean_q: -1.996130\n",
      " 4788/5000: episode: 4787, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.065 [0.024, 0.117], loss: 2.068594, mean_absolute_error: 418.453857, mean_q: -2.033008\n",
      " 4789/5000: episode: 4788, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.229 [0.151, 0.313], loss: 1314927.375000, mean_absolute_error: 505.537048, mean_q: -2.059862\n",
      " 4790/5000: episode: 4789, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.333, 0.525], loss: 2.218541, mean_absolute_error: 418.770477, mean_q: -2.105438\n",
      " 4791/5000: episode: 4790, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.804 [0.724, 0.877], loss: 2.290226, mean_absolute_error: 418.918182, mean_q: -2.139199\n",
      " 4792/5000: episode: 4791, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.905 [0.844, 0.956], loss: 2.338547, mean_absolute_error: 419.035553, mean_q: -2.161659\n",
      " 4793/5000: episode: 4792, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [0.116, 0.266], loss: 2.367743, mean_absolute_error: 419.128662, mean_q: -2.175117\n",
      " 4794/5000: episode: 4793, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.666 [0.573, 0.755], loss: 10784.567383, mean_absolute_error: 419.233032, mean_q: -2.182692\n",
      " 4795/5000: episode: 4794, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.078 [0.032, 0.134], loss: 26793.695312, mean_absolute_error: 420.863037, mean_q: -2.183073\n",
      " 4796/5000: episode: 4795, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [0.079, 0.212], loss: 2.366426, mean_absolute_error: 419.286377, mean_q: -2.174512\n",
      " 4797/5000: episode: 4796, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.995 [0.981, 1.000], loss: 2.344179, mean_absolute_error: 419.310974, mean_q: -2.164262\n",
      " 4798/5000: episode: 4797, duration: 0.087s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.965, 1.000], loss: 2.318286, mean_absolute_error: 419.331970, mean_q: -2.152270\n",
      " 4799/5000: episode: 4798, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.782 [0.699, 0.858], loss: 2.289542, mean_absolute_error: 419.350708, mean_q: -2.138880\n",
      " 4800/5000: episode: 4799, duration: 0.065s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.336 [0.246, 0.429], loss: 5117.577637, mean_absolute_error: 420.868164, mean_q: -2.124790\n",
      " 4801/5000: episode: 4800, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.151], loss: 2.225899, mean_absolute_error: 419.381775, mean_q: -2.108929\n",
      " 4802/5000: episode: 4801, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.538 [0.441, 0.634], loss: 5115.987305, mean_absolute_error: 420.899170, mean_q: -2.092554\n",
      " 4803/5000: episode: 4802, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.693 [0.601, 0.780], loss: 2.155498, mean_absolute_error: 419.412903, mean_q: -2.075294\n",
      " 4804/5000: episode: 4803, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.321 [0.233, 0.413], loss: 2.119975, mean_absolute_error: 419.424988, mean_q: -2.058114\n",
      " 4805/5000: episode: 4804, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.941 [0.891, 0.980], loss: 5113.752441, mean_absolute_error: 420.933228, mean_q: -2.041004\n",
      " 4806/5000: episode: 4805, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.048], loss: 10223.950195, mean_absolute_error: 422.441895, mean_q: -2.023273\n",
      " 4807/5000: episode: 4806, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.305 [0.219, 0.397], loss: 2.009123, mean_absolute_error: 419.467438, mean_q: -2.003556\n",
      " 4808/5000: episode: 4807, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.014], loss: 1.969799, mean_absolute_error: 419.489227, mean_q: -1.983842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4809/5000: episode: 4808, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.275 [0.191, 0.364], loss: 1.932016, mean_absolute_error: 419.506195, mean_q: -1.964714\n",
      " 4810/5000: episode: 4809, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.404 [0.310, 0.500], loss: 1.895830, mean_absolute_error: 419.518555, mean_q: -1.946218\n",
      " 4811/5000: episode: 4810, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.548, 0.734], loss: 1.860801, mean_absolute_error: 419.527161, mean_q: -1.928145\n",
      " 4812/5000: episode: 4811, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [0.004, 0.063], loss: 1.827043, mean_absolute_error: 419.533600, mean_q: -1.910566\n",
      " 4813/5000: episode: 4812, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.202, 0.377], loss: 1.791813, mean_absolute_error: 419.534088, mean_q: -1.892047\n",
      " 4814/5000: episode: 4813, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.989 [0.966, 1.000], loss: 5105.810059, mean_absolute_error: 421.023010, mean_q: -1.872458\n",
      " 4815/5000: episode: 4814, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.768 [0.683, 0.846], loss: 10208.288086, mean_absolute_error: 422.516663, mean_q: -1.852672\n",
      " 4816/5000: episode: 4815, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.017], loss: 1.679766, mean_absolute_error: 419.553040, mean_q: -1.831903\n",
      " 4817/5000: episode: 4816, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.493 [0.396, 0.590], loss: 1.640796, mean_absolute_error: 419.563110, mean_q: -1.810516\n",
      " 4818/5000: episode: 4817, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 2628789.500000, mean_absolute_error: 593.390503, mean_q: -1.789209\n",
      " 4819/5000: episode: 4818, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.601 [0.505, 0.695], loss: 1314294.500000, mean_absolute_error: 506.629578, mean_q: -1.824026\n",
      " 4820/5000: episode: 4819, duration: 0.102s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.307 [0.221, 0.399], loss: 1.762199, mean_absolute_error: 419.929504, mean_q: -1.876338\n",
      " 4821/5000: episode: 4820, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.250, 0.433], loss: 1.834151, mean_absolute_error: 420.093292, mean_q: -1.914281\n",
      " 4822/5000: episode: 4821, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.970 [0.932, 0.995], loss: 5095.799805, mean_absolute_error: 421.704376, mean_q: -1.940250\n",
      " 4823/5000: episode: 4822, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [0.000, 0.019], loss: 1.916645, mean_absolute_error: 420.343018, mean_q: -1.956879\n",
      " 4824/5000: episode: 4823, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.101 [0.048, 0.163], loss: 1.936762, mean_absolute_error: 420.438904, mean_q: -1.967127\n",
      " 4825/5000: episode: 4824, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.181 [0.111, 0.259], loss: 1.946946, mean_absolute_error: 420.515442, mean_q: -1.972295\n",
      " 4826/5000: episode: 4825, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.717 [0.627, 0.802], loss: 1.949422, mean_absolute_error: 420.576385, mean_q: -1.973549\n",
      " 4827/5000: episode: 4826, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.052 [0.016, 0.100], loss: 10780.474609, mean_absolute_error: 420.625000, mean_q: -1.971486\n",
      " 4828/5000: episode: 4827, duration: 0.068s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.242 [0.163, 0.328], loss: 10782.191406, mean_absolute_error: 420.656067, mean_q: -1.964408\n",
      " 4829/5000: episode: 4828, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.688 [0.596, 0.775], loss: 1.908632, mean_absolute_error: 420.675598, mean_q: -1.952782\n",
      " 4830/5000: episode: 4829, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [0.055, 0.174], loss: 1.883903, mean_absolute_error: 420.692719, mean_q: -1.940084\n",
      " 4831/5000: episode: 4830, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.654, 0.823], loss: 10172.957031, mean_absolute_error: 423.643311, mean_q: -1.926786\n",
      " 4832/5000: episode: 4831, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.468 [0.372, 0.565], loss: 10784.876953, mean_absolute_error: 420.724976, mean_q: -1.911554\n",
      " 4833/5000: episode: 4832, duration: 0.090s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.431 [0.336, 0.528], loss: 1.794591, mean_absolute_error: 420.734802, mean_q: -1.893514\n",
      " 4834/5000: episode: 4833, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.051, 0.168], loss: 1.760066, mean_absolute_error: 420.746704, mean_q: -1.875201\n",
      " 4835/5000: episode: 4834, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.343 [0.253, 0.437], loss: 1.753176, mean_absolute_error: 420.799011, mean_q: -1.871526\n",
      " 4836/5000: episode: 4835, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.871 [0.802, 0.931], loss: 1.766612, mean_absolute_error: 420.877686, mean_q: -1.878687\n",
      " 4837/5000: episode: 4836, duration: 0.054s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.032], loss: 1.771992, mean_absolute_error: 420.940125, mean_q: -1.881547\n",
      " 4838/5000: episode: 4837, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.409 [0.315, 0.505], loss: 1.770956, mean_absolute_error: 420.990051, mean_q: -1.880997\n",
      " 4839/5000: episode: 4838, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [0.011, 0.088], loss: 10158.921875, mean_absolute_error: 423.950897, mean_q: -1.877647\n",
      " 4840/5000: episode: 4839, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.230, 0.410], loss: 1.750741, mean_absolute_error: 421.072449, mean_q: -1.870225\n",
      " 4841/5000: episode: 4840, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.571 [0.474, 0.666], loss: 1.731035, mean_absolute_error: 421.105164, mean_q: -1.859664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4842/5000: episode: 4841, duration: 0.055s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.519 [0.422, 0.616], loss: 1.707662, mean_absolute_error: 421.127350, mean_q: -1.847059\n",
      " 4843/5000: episode: 4842, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.908 [0.848, 0.958], loss: 15880.677734, mean_absolute_error: 422.601074, mean_q: -1.833992\n",
      " 4844/5000: episode: 4843, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.088 [0.039, 0.147], loss: 1.653237, mean_absolute_error: 421.154510, mean_q: -1.817371\n",
      " 4845/5000: episode: 4844, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.087 [0.039, 0.147], loss: 5075.328125, mean_absolute_error: 422.617004, mean_q: -1.801072\n",
      " 4846/5000: episode: 4845, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.920, 0.992], loss: 1.593626, mean_absolute_error: 421.172089, mean_q: -1.784287\n",
      " 4847/5000: episode: 4846, duration: 0.081s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.911 [0.851, 0.960], loss: 1.561555, mean_absolute_error: 421.177917, mean_q: -1.766232\n",
      " 4848/5000: episode: 4847, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.917 [0.859, 0.964], loss: 11015.767578, mean_absolute_error: 421.267487, mean_q: -1.747012\n",
      " 4849/5000: episode: 4848, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.337 [0.248, 0.430], loss: 1.488637, mean_absolute_error: 421.177673, mean_q: -1.724478\n",
      " 4850/5000: episode: 4849, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.287 [0.202, 0.377], loss: 1.450057, mean_absolute_error: 421.171600, mean_q: -1.701972\n",
      " 4851/5000: episode: 4850, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.986, 1.000], loss: 1.414134, mean_absolute_error: 421.167847, mean_q: -1.680745\n",
      " 4852/5000: episode: 4851, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.066 [0.024, 0.118], loss: 5070.149414, mean_absolute_error: 422.613464, mean_q: -1.660863\n",
      " 4853/5000: episode: 4852, duration: 0.085s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.942 [0.893, 0.981], loss: 1.348838, mean_absolute_error: 421.165161, mean_q: -1.641460\n",
      " 4854/5000: episode: 4853, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.425 [0.330, 0.522], loss: 11057.161133, mean_absolute_error: 421.307861, mean_q: -1.637714\n",
      " 4855/5000: episode: 4854, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.948 [0.901, 0.984], loss: 10133.963867, mean_absolute_error: 424.165619, mean_q: -1.642462\n",
      " 4856/5000: episode: 4855, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 1313394.750000, mean_absolute_error: 508.176575, mean_q: -1.641884\n",
      " 4857/5000: episode: 4856, duration: 0.062s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.010], loss: 10861.151367, mean_absolute_error: 421.488068, mean_q: -1.666112\n",
      " 4858/5000: episode: 4857, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.610 [0.514, 0.703], loss: 5063.077637, mean_absolute_error: 423.013184, mean_q: -1.678657\n",
      " 4859/5000: episode: 4858, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.317, 0.508], loss: 10902.535156, mean_absolute_error: 421.689697, mean_q: -1.683582\n",
      " 4860/5000: episode: 4859, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.972 [0.936, 0.996], loss: 21991.255859, mean_absolute_error: 421.857544, mean_q: -1.682047\n",
      " 4861/5000: episode: 4860, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.351 [0.260, 0.445], loss: 1.401603, mean_absolute_error: 421.762054, mean_q: -1.673278\n",
      " 4862/5000: episode: 4861, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.350 [0.260, 0.445], loss: 1.385423, mean_absolute_error: 421.791321, mean_q: -1.663586\n",
      " 4863/5000: episode: 4862, duration: 0.112s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [0.059, 0.182], loss: 1.367407, mean_absolute_error: 421.816650, mean_q: -1.652727\n",
      " 4864/5000: episode: 4863, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.633 [0.538, 0.724], loss: 5055.980469, mean_absolute_error: 423.271729, mean_q: -1.640961\n",
      " 4865/5000: episode: 4864, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.833 [0.757, 0.901], loss: 1.325855, mean_absolute_error: 421.865967, mean_q: -1.627407\n",
      " 4866/5000: episode: 4865, duration: 0.082s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.004 [0.000, 0.015], loss: 5053.326172, mean_absolute_error: 423.363159, mean_q: -1.627658\n",
      " 4867/5000: episode: 4866, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.742 [0.655, 0.824], loss: 1.342952, mean_absolute_error: 422.030334, mean_q: -1.637873\n",
      " 4868/5000: episode: 4867, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [0.001, 0.047], loss: 10099.216797, mean_absolute_error: 424.962555, mean_q: -1.643119\n",
      " 4869/5000: episode: 4868, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.516 [0.419, 0.613], loss: 1.351357, mean_absolute_error: 422.186707, mean_q: -1.642993\n",
      " 4870/5000: episode: 4869, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.063 [0.022, 0.114], loss: 1.347044, mean_absolute_error: 422.247681, mean_q: -1.640368\n",
      " 4871/5000: episode: 4870, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.957 [0.913, 0.989], loss: 1.339425, mean_absolute_error: 422.295959, mean_q: -1.635719\n",
      " 4872/5000: episode: 4871, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.104 [0.051, 0.167], loss: 1.328981, mean_absolute_error: 422.334717, mean_q: -1.629326\n",
      " 4873/5000: episode: 4872, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 5044.069336, mean_absolute_error: 423.783691, mean_q: -1.621592\n",
      " 4874/5000: episode: 4873, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [0.000, 0.033], loss: 1.300853, mean_absolute_error: 422.393494, mean_q: -1.611980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4875/5000: episode: 4874, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.024], loss: 11110.120117, mean_absolute_error: 422.515015, mean_q: -1.601725\n",
      " 4876/5000: episode: 4875, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [0.077, 0.211], loss: 1.260728, mean_absolute_error: 422.421509, mean_q: -1.586909\n",
      " 4877/5000: episode: 4876, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.853 [0.780, 0.917], loss: 1.236096, mean_absolute_error: 422.421906, mean_q: -1.571321\n",
      " 4878/5000: episode: 4877, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.783 [0.700, 0.859], loss: 1.209189, mean_absolute_error: 422.422852, mean_q: -1.554113\n",
      " 4879/5000: episode: 4878, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.908 [0.848, 0.958], loss: 1.178178, mean_absolute_error: 422.422577, mean_q: -1.534042\n",
      " 4880/5000: episode: 4879, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.094, 0.235], loss: 1.147319, mean_absolute_error: 422.420319, mean_q: -1.513806\n",
      " 4881/5000: episode: 4880, duration: 0.104s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.257 [0.176, 0.345], loss: 1.119054, mean_absolute_error: 422.417664, mean_q: -1.495030\n",
      " 4882/5000: episode: 4881, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.478 [0.381, 0.575], loss: 1.092867, mean_absolute_error: 422.414551, mean_q: -1.477423\n",
      " 4883/5000: episode: 4882, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.339 [0.250, 0.433], loss: 1.068403, mean_absolute_error: 422.411316, mean_q: -1.460782\n",
      " 4884/5000: episode: 4883, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.786 [0.703, 0.862], loss: 1.045437, mean_absolute_error: 422.408020, mean_q: -1.444985\n",
      " 4885/5000: episode: 4884, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.914 [0.856, 0.963], loss: 1.023788, mean_absolute_error: 422.404510, mean_q: -1.429935\n",
      " 4886/5000: episode: 4885, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.452 [0.356, 0.549], loss: 15897.034180, mean_absolute_error: 423.813446, mean_q: -1.415374\n",
      " 4887/5000: episode: 4886, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.324 [0.235, 0.416], loss: 5036.094238, mean_absolute_error: 423.846924, mean_q: -1.412872\n",
      " 4888/5000: episode: 4887, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.779 [0.695, 0.856], loss: 1.011288, mean_absolute_error: 422.509247, mean_q: -1.421173\n",
      " 4889/5000: episode: 4888, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.107 [0.053, 0.171], loss: 1.017863, mean_absolute_error: 422.566895, mean_q: -1.425789\n",
      " 4890/5000: episode: 4889, duration: 0.077s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.753 [0.666, 0.833], loss: 1317789.500000, mean_absolute_error: 510.812531, mean_q: -1.427147\n",
      " 4891/5000: episode: 4890, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.289 [0.204, 0.380], loss: 5032.410156, mean_absolute_error: 424.142395, mean_q: -1.452823\n",
      " 4892/5000: episode: 4891, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.316 [0.229, 0.409], loss: 5031.001465, mean_absolute_error: 424.248352, mean_q: -1.470345\n",
      " 4893/5000: episode: 4892, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.144, 0.304], loss: 1317591.500000, mean_absolute_error: 511.116699, mean_q: -1.481441\n",
      " 4894/5000: episode: 4893, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.192 [0.120, 0.272], loss: 10054.358398, mean_absolute_error: 425.893890, mean_q: -1.515257\n",
      " 4895/5000: episode: 4894, duration: 0.067s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.984, 1.000], loss: 5025.586914, mean_absolute_error: 424.670685, mean_q: -1.553361\n",
      " 4896/5000: episode: 4895, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.851 [0.778, 0.915], loss: 11167.317383, mean_absolute_error: 423.601562, mean_q: -1.608599\n",
      " 4897/5000: episode: 4896, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.990, 1.000], loss: 1.381890, mean_absolute_error: 423.716736, mean_q: -1.661462\n",
      " 4898/5000: episode: 4897, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.951 [0.905, 0.986], loss: 1.449547, mean_absolute_error: 423.889557, mean_q: -1.701673\n",
      " 4899/5000: episode: 4898, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.412 [0.318, 0.508], loss: 1.498739, mean_absolute_error: 424.024872, mean_q: -1.730323\n",
      " 4900/5000: episode: 4899, duration: 0.106s, episode steps: 1, steps per second: 9, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.816 [0.737, 0.887], loss: 1.529346, mean_absolute_error: 424.129150, mean_q: -1.747911\n",
      " 4901/5000: episode: 4900, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.918, 0.991], loss: 1.546946, mean_absolute_error: 424.211426, mean_q: -1.757946\n",
      " 4902/5000: episode: 4901, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.946 [0.898, 0.983], loss: 1.555030, mean_absolute_error: 424.282227, mean_q: -1.762536\n",
      " 4903/5000: episode: 4902, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.064 [0.023, 0.116], loss: 21976.753906, mean_absolute_error: 424.363220, mean_q: -1.762777\n",
      " 4904/5000: episode: 4903, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [0.000, 0.012], loss: 1.541417, mean_absolute_error: 424.380493, mean_q: -1.754800\n",
      " 4905/5000: episode: 4904, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.291, 0.479], loss: 10021.362305, mean_absolute_error: 427.171631, mean_q: -1.745168\n",
      " 4906/5000: episode: 4905, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.844 [0.770, 0.910], loss: 22406.552734, mean_absolute_error: 424.632202, mean_q: -1.733220\n",
      " 4907/5000: episode: 4906, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.228 [0.150, 0.312], loss: 1.473528, mean_absolute_error: 424.457581, mean_q: -1.715699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4908/5000: episode: 4907, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.971 [0.935, 0.996], loss: 1.444649, mean_absolute_error: 424.466278, mean_q: -1.698793\n",
      " 4909/5000: episode: 4908, duration: 0.074s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.106 [0.052, 0.171], loss: 1.416325, mean_absolute_error: 424.474792, mean_q: -1.682048\n",
      " 4910/5000: episode: 4909, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.297 [0.211, 0.387], loss: 1.388407, mean_absolute_error: 424.483276, mean_q: -1.665378\n",
      " 4911/5000: episode: 4910, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.224 [0.147, 0.308], loss: 1.359449, mean_absolute_error: 424.485046, mean_q: -1.647908\n",
      " 4912/5000: episode: 4911, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.676 [0.584, 0.765], loss: 1.329395, mean_absolute_error: 424.484619, mean_q: -1.629580\n",
      " 4913/5000: episode: 4912, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [0.055, 0.175], loss: 1.299887, mean_absolute_error: 424.489197, mean_q: -1.611382\n",
      " 4914/5000: episode: 4913, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.941 [0.891, 0.980], loss: 1.271551, mean_absolute_error: 424.493835, mean_q: -1.593711\n",
      " 4915/5000: episode: 4914, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [0.000, 0.036], loss: 1311673.625000, mean_absolute_error: 511.212830, mean_q: -1.576433\n",
      " 4916/5000: episode: 4915, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [0.000, 0.027], loss: 5002.253906, mean_absolute_error: 425.951630, mean_q: -1.586330\n",
      " 4917/5000: episode: 4916, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.993 [0.975, 1.000], loss: 1.288278, mean_absolute_error: 424.694580, mean_q: -1.604165\n",
      " 4918/5000: episode: 4917, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.241 [0.162, 0.327], loss: 1.328467, mean_absolute_error: 424.825195, mean_q: -1.629010\n",
      " 4919/5000: episode: 4918, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [0.064, 0.190], loss: 1311410.375000, mean_absolute_error: 511.629944, mean_q: -1.645777\n",
      " 4920/5000: episode: 4919, duration: 0.070s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.954, 1.000], loss: 1.419296, mean_absolute_error: 425.108093, mean_q: -1.683812\n",
      " 4921/5000: episode: 4920, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.108 [0.054, 0.173], loss: 1.466523, mean_absolute_error: 425.250122, mean_q: -1.711614\n",
      " 4922/5000: episode: 4921, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [0.000, 0.040], loss: 1316140.375000, mean_absolute_error: 513.396667, mean_q: -1.729836\n",
      " 4923/5000: episode: 4922, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [0.002, 0.056], loss: 1.560925, mean_absolute_error: 425.532257, mean_q: -1.765876\n",
      " 4924/5000: episode: 4923, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.091 [0.041, 0.151], loss: 4989.358398, mean_absolute_error: 427.016785, mean_q: -1.787789\n",
      " 4925/5000: episode: 4924, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.896 [0.833, 0.950], loss: 11055.846680, mean_absolute_error: 425.775391, mean_q: -1.799306\n",
      " 4926/5000: episode: 4925, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.489 [0.392, 0.586], loss: 1.627608, mean_absolute_error: 425.844849, mean_q: -1.803222\n",
      " 4927/5000: episode: 4926, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.608 [0.512, 0.701], loss: 1.628358, mean_absolute_error: 425.905914, mean_q: -1.803638\n",
      " 4928/5000: episode: 4927, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.885, 0.977], loss: 11238.832031, mean_absolute_error: 426.030548, mean_q: -1.801052\n",
      " 4929/5000: episode: 4928, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.868 [0.798, 0.928], loss: 4983.484863, mean_absolute_error: 427.328979, mean_q: -1.792295\n",
      " 4930/5000: episode: 4929, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.926 [0.871, 0.971], loss: 1.586712, mean_absolute_error: 426.003967, mean_q: -1.780411\n",
      " 4931/5000: episode: 4930, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.312 [0.225, 0.404], loss: 1.561635, mean_absolute_error: 426.020325, mean_q: -1.766277\n",
      " 4932/5000: episode: 4931, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.907 [0.847, 0.958], loss: 1.558877, mean_absolute_error: 426.075562, mean_q: -1.764716\n",
      " 4933/5000: episode: 4932, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.682 [0.589, 0.770], loss: 1.574594, mean_absolute_error: 426.160156, mean_q: -1.773595\n",
      " 4934/5000: episode: 4933, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.724 [0.635, 0.808], loss: 1.605988, mean_absolute_error: 426.265625, mean_q: -1.791199\n",
      " 4935/5000: episode: 4934, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.674 [0.582, 0.763], loss: 1315525.750000, mean_absolute_error: 514.356323, mean_q: -1.814706\n",
      " 4936/5000: episode: 4935, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.829 [0.752, 0.897], loss: 1.727470, mean_absolute_error: 426.563538, mean_q: -1.857747\n",
      " 4937/5000: episode: 4936, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.962 [0.921, 0.992], loss: 1.787471, mean_absolute_error: 426.708069, mean_q: -1.889751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4938/5000: episode: 4937, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.086 [0.038, 0.145], loss: 4973.709473, mean_absolute_error: 428.155365, mean_q: -1.911415\n",
      " 4939/5000: episode: 4938, duration: 0.049s, episode steps: 1, steps per second: 21, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.997 [0.989, 1.000], loss: 1.852887, mean_absolute_error: 426.911560, mean_q: -1.924039\n",
      " 4940/5000: episode: 4939, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.952 [0.906, 0.986], loss: 11366.636719, mean_absolute_error: 427.087067, mean_q: -1.931274\n",
      " 4941/5000: episode: 4940, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.809 [0.730, 0.881], loss: 4971.171875, mean_absolute_error: 428.363586, mean_q: -1.930216\n",
      " 4942/5000: episode: 4941, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.083 [0.036, 0.141], loss: 9938.962891, mean_absolute_error: 429.732208, mean_q: -1.924328\n",
      " 4943/5000: episode: 4942, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.630 [0.535, 0.722], loss: 1.835936, mean_absolute_error: 427.109436, mean_q: -1.915213\n",
      " 4944/5000: episode: 4943, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.990 [0.967, 1.000], loss: 1.815916, mean_absolute_error: 427.140717, mean_q: -1.904737\n",
      " 4945/5000: episode: 4944, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.605 [0.509, 0.698], loss: 1.793229, mean_absolute_error: 427.167297, mean_q: -1.892795\n",
      " 4946/5000: episode: 4945, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.444, 0.637], loss: 1.766239, mean_absolute_error: 427.186707, mean_q: -1.878489\n",
      " 4947/5000: episode: 4946, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.795 [0.713, 0.869], loss: 1310052.250000, mean_absolute_error: 513.801208, mean_q: -1.861576\n",
      " 4948/5000: episode: 4947, duration: 0.043s, episode steps: 1, steps per second: 23, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.976 [0.943, 0.998], loss: 1.753571, mean_absolute_error: 427.306885, mean_q: -1.871737\n",
      " 4949/5000: episode: 4948, duration: 0.122s, episode steps: 1, steps per second: 8, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.689 [0.597, 0.776], loss: 1.762835, mean_absolute_error: 427.390564, mean_q: -1.876677\n",
      " 4950/5000: episode: 4949, duration: 0.050s, episode steps: 1, steps per second: 20, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.985, 1.000], loss: 4961.137207, mean_absolute_error: 428.783020, mean_q: -1.877010\n",
      " 4951/5000: episode: 4950, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.536 [0.439, 0.632], loss: 1.754820, mean_absolute_error: 427.531982, mean_q: -1.872403\n",
      " 4952/5000: episode: 4951, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.541 [0.444, 0.637], loss: 4957.653320, mean_absolute_error: 428.908783, mean_q: -1.865136\n",
      " 4953/5000: episode: 4952, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.986 [0.960, 1.000], loss: 1.722495, mean_absolute_error: 427.647583, mean_q: -1.855068\n",
      " 4954/5000: episode: 4953, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.891 [0.827, 0.946], loss: 1.700749, mean_absolute_error: 427.696411, mean_q: -1.843315\n",
      " 4955/5000: episode: 4954, duration: 0.046s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.428 [0.333, 0.524], loss: 1.677145, mean_absolute_error: 427.737000, mean_q: -1.830472\n",
      " 4956/5000: episode: 4955, duration: 0.076s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.659 [0.566, 0.749], loss: 1.652901, mean_absolute_error: 427.769196, mean_q: -1.817186\n",
      " 4957/5000: episode: 4956, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.907 [0.847, 0.958], loss: 1.628100, mean_absolute_error: 427.794312, mean_q: -1.803494\n",
      " 4958/5000: episode: 4957, duration: 0.036s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.977 [0.945, 0.998], loss: 9897.327148, mean_absolute_error: 430.469421, mean_q: -1.804298\n",
      " 4959/5000: episode: 4958, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.983 [0.954, 1.000], loss: 1.646246, mean_absolute_error: 427.955292, mean_q: -1.813522\n",
      " 4960/5000: episode: 4959, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.075 [0.030, 0.130], loss: 4945.933594, mean_absolute_error: 429.387207, mean_q: -1.831615\n",
      " 4961/5000: episode: 4960, duration: 0.098s, episode steps: 1, steps per second: 10, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.861 [0.790, 0.923], loss: 1314466.125000, mean_absolute_error: 516.088867, mean_q: -1.853792\n",
      " 4962/5000: episode: 4961, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.994 [0.977, 1.000], loss: 1.796510, mean_absolute_error: 428.430389, mean_q: -1.894526\n",
      " 4963/5000: episode: 4962, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.065], loss: 11364.991211, mean_absolute_error: 428.701050, mean_q: -1.939041\n",
      " 4964/5000: episode: 4963, duration: 0.045s, episode steps: 1, steps per second: 22, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.063 [0.022, 0.114], loss: 1.967443, mean_absolute_error: 428.824310, mean_q: -1.982655\n",
      " 4965/5000: episode: 4964, duration: 0.088s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.022], loss: 2.031934, mean_absolute_error: 428.978943, mean_q: -2.014904\n",
      " 4966/5000: episode: 4965, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.375 [0.283, 0.471], loss: 2.075909, mean_absolute_error: 429.099457, mean_q: -2.036601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4967/5000: episode: 4966, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.897 [0.833, 0.950], loss: 9865.245117, mean_absolute_error: 431.767639, mean_q: -2.049719\n",
      " 4968/5000: episode: 4967, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.402, 0.596], loss: 2.115008, mean_absolute_error: 429.276611, mean_q: -2.055700\n",
      " 4969/5000: episode: 4968, duration: 0.063s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [0.113, 0.263], loss: 2.118340, mean_absolute_error: 429.343719, mean_q: -2.057320\n",
      " 4970/5000: episode: 4969, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.677 [0.585, 0.765], loss: 2.114367, mean_absolute_error: 429.396729, mean_q: -2.055388\n",
      " 4971/5000: episode: 4970, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.643 [0.548, 0.734], loss: 11448.886719, mean_absolute_error: 429.523010, mean_q: -2.050635\n",
      " 4972/5000: episode: 4971, duration: 0.073s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.698 [0.607, 0.784], loss: 2.084887, mean_absolute_error: 429.464264, mean_q: -2.041002\n",
      " 4973/5000: episode: 4972, duration: 0.041s, episode steps: 1, steps per second: 25, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [0.009, 0.081], loss: 2.061537, mean_absolute_error: 429.486328, mean_q: -2.029535\n",
      " 4974/5000: episode: 4973, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.499 [0.403, 0.596], loss: 4927.969727, mean_absolute_error: 430.786072, mean_q: -2.016665\n",
      " 4975/5000: episode: 4974, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.477 [0.380, 0.574], loss: 2.006701, mean_absolute_error: 429.524780, mean_q: -2.002348\n",
      " 4976/5000: episode: 4975, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.755 [0.669, 0.835], loss: 1308723.875000, mean_absolute_error: 516.038330, mean_q: -1.987597\n",
      " 4977/5000: episode: 4976, duration: 0.089s, episode steps: 1, steps per second: 11, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [0.000, 0.044], loss: 11438.382812, mean_absolute_error: 429.705719, mean_q: -2.000351\n",
      " 4978/5000: episode: 4977, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.748 [0.661, 0.829], loss: 2.011366, mean_absolute_error: 429.697113, mean_q: -2.004675\n",
      " 4979/5000: episode: 4978, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.057 [0.019, 0.107], loss: 9846.245117, mean_absolute_error: 432.306091, mean_q: -2.004604\n",
      " 4980/5000: episode: 4979, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.937 [0.885, 0.977], loss: 16369.333008, mean_absolute_error: 431.160858, mean_q: -1.999518\n",
      " 4981/5000: episode: 4980, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.722 [0.633, 0.806], loss: 4921.480957, mean_absolute_error: 431.128052, mean_q: -1.988637\n",
      " 4982/5000: episode: 4981, duration: 0.060s, episode steps: 1, steps per second: 17, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.246 [0.166, 0.333], loss: 1.954098, mean_absolute_error: 429.894836, mean_q: -1.975916\n",
      " 4983/5000: episode: 4982, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [0.011, 0.087], loss: 1.927577, mean_absolute_error: 429.926300, mean_q: -1.962455\n",
      " 4984/5000: episode: 4983, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [0.102, 0.246], loss: 1.898559, mean_absolute_error: 429.955750, mean_q: -1.947619\n",
      " 4985/5000: episode: 4984, duration: 0.033s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.576 [0.479, 0.671], loss: 1.867871, mean_absolute_error: 429.983032, mean_q: -1.931807\n",
      " 4986/5000: episode: 4985, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [0.005, 0.067], loss: 1.837332, mean_absolute_error: 430.004425, mean_q: -1.915941\n",
      " 4987/5000: episode: 4986, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.326 [0.237, 0.419], loss: 1.807103, mean_absolute_error: 430.020752, mean_q: -1.900106\n",
      " 4988/5000: episode: 4987, duration: 0.069s, episode steps: 1, steps per second: 14, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [0.000, 0.045], loss: 1308441.625000, mean_absolute_error: 516.558655, mean_q: -1.898059\n",
      " 4989/5000: episode: 4988, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.960 [0.917, 0.991], loss: 1.871087, mean_absolute_error: 430.252197, mean_q: -1.933470\n",
      " 4990/5000: episode: 4989, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [0.000, 0.026], loss: 9818.697266, mean_absolute_error: 432.913147, mean_q: -1.958735\n",
      " 4991/5000: episode: 4990, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.364 [0.272, 0.458], loss: 4908.513672, mean_absolute_error: 431.773743, mean_q: -1.974748\n",
      " 4992/5000: episode: 4991, duration: 0.056s, episode steps: 1, steps per second: 18, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.711 [0.621, 0.796], loss: 1.969644, mean_absolute_error: 430.619385, mean_q: -1.983764\n",
      " 4993/5000: episode: 4992, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.282 [0.198, 0.372], loss: 1.977949, mean_absolute_error: 430.702148, mean_q: -1.987944\n",
      " 4994/5000: episode: 4993, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.996 [0.983, 1.000], loss: 1.978483, mean_absolute_error: 430.768219, mean_q: -1.988213\n",
      " 4995/5000: episode: 4994, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.318 [0.231, 0.411], loss: 1.972639, mean_absolute_error: 430.820526, mean_q: -1.985272\n",
      " 4996/5000: episode: 4995, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [0.000, 0.023], loss: 9803.164062, mean_absolute_error: 433.363495, mean_q: -1.979734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4997/5000: episode: 4996, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [0.004, 0.064], loss: 1307958.250000, mean_absolute_error: 517.347534, mean_q: -1.970616\n",
      " 4998/5000: episode: 4997, duration: 0.075s, episode steps: 1, steps per second: 13, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.812 [0.733, 0.884], loss: 1.977463, mean_absolute_error: 431.020264, mean_q: -1.987700\n",
      " 4999/5000: episode: 4998, duration: 0.061s, episode steps: 1, steps per second: 16, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [0.059, 0.181], loss: 1.996585, mean_absolute_error: 431.120697, mean_q: -1.997292\n",
      " 5000/5000: episode: 4999, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 0.001, mean reward: 0.001 [0.001, 0.001], mean action: 0.000 [0.000, 0.000], mean observation: 0.709 [0.618, 0.794], loss: 2.001564, mean_absolute_error: 431.201660, mean_q: -1.999782\n",
      "done, took 232.384 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('duel_dqn_{}_weights.h5f'.format('stock_bot'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 0.001, steps: 1\n",
      "Episode 2: reward: 0.001, steps: 1\n",
      "Episode 3: reward: 0.001, steps: 1\n",
      "Episode 4: reward: 0.001, steps: 1\n",
      "Episode 5: reward: 0.001, steps: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b2cbdbe10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
